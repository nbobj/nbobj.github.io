{"meta":{"title":"XYZhi's学习笔记","subtitle":"","description":"","author":"xyz","url":"http://sv.pointcut.cc","root":"/"},"pages":[{"title":"categories","date":"2022-02-16T16:33:38.000Z","updated":"2022-03-14T09:09:11.123Z","comments":true,"path":"categories/index.html","permalink":"http://sv.pointcut.cc/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2022-02-16T16:50:45.000Z","updated":"2022-03-22T19:57:58.612Z","comments":true,"path":"tags/index.html","permalink":"http://sv.pointcut.cc/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"05-Nacos中的AbstractNacosTaskExecuteEngine","slug":"springcloud/nacos中的一些细节/05-Nacos中的AbstractNacosTaskExecuteEngine","date":"2021-11-27T12:00:40.000Z","updated":"2022-03-23T09:03:57.393Z","comments":true,"path":"blog/springcloud/nacos中的一些细节/05-Nacos中的AbstractNacosTaskExecuteEngine/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/nacos%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82/05-Nacos%E4%B8%AD%E7%9A%84AbstractNacosTaskExecuteEngine/","excerpt":"","text":"这是nacos源码中的提供的一个任务执行引擎，在源码中提供了两个实现 NacosDelayTaskExecuteEngine DistroExecuteTaskExecuteEngine 其中NacosDelayTaskExecuteEngine是单线程的，隔一段时间后任务取出来然后全部执行，而且任务是交给NacosTaskProcessor来处理的，如果某个任务不存在NacosTaskProcessor，那就不会执行该任务。 DistroExecuteTaskExecuteEngine更像一个线程池优化版本，jdk的线程池只有一个队列，所以线程都必须竞争的获取任务执行，而DistroExecuteTaskExecuteEngine就优化了这个任务，其内部提供了多条队列，每一条队列只会交给固定的一个线程负责","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"04-Server和Client动态获取nacos地址","slug":"springcloud/nacos中的一些细节/04-Server和Client动态获取nacos地址","date":"2021-11-27T12:00:39.000Z","updated":"2022-03-23T09:03:57.392Z","comments":true,"path":"blog/springcloud/nacos中的一些细节/04-Server和Client动态获取nacos地址/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/nacos%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82/04-Server%E5%92%8CClient%E5%8A%A8%E6%80%81%E8%8E%B7%E5%8F%96nacos%E5%9C%B0%E5%9D%80/","excerpt":"","text":"客户端动态获取Nacos地址1234567891011spring: application: name: mall-user #微服务名称 #配置nacos注册中心地址 cloud: nacos: discovery: # server-addr: 127.0.0.1:8848 # 通过这个地址来获取nacos地址 # 如果不指定端口的话，或默认使用8080 endpoint: http://127.0.0.1:8888 这样配置后客户端就能动态的获取Nacos的地址了。 server-addr不能配置，不然会使用server-addr的值 需要提供一个HTTP服务 12345678API /nacos/serverlistMETHOD GET返回值：地址1地址2注意：返回值必须是一个字符串，而且每个地址后面都需要上一个换行符（\\n\\r） 源码： 首先会通过NacosDiscoveryProperties的getNacosProperties方法 处理endpoint，然后会在NacosNamingService初始化的时候创建了一个NamingProxy对象。在NamingProxy对象初始化时会调用其initRefreshTask方法，在这个方法中会添加两个定时任务： 一个是30s运行一次的获取nacos地址任务 12345678910111213141516171819202122232425262728293031// NamingProxyNamingProxy.refreshSrvIfNeed --&gt; NamingProxy.getServerListFromEndpoint public List&lt;String&gt; getServerListFromEndpoint() &#123; try &#123; String urlString = &quot;http://&quot; + endpoint + &quot;/nacos/serverlist&quot;; Header header = builderHeader(); HttpRestResult&lt;String&gt; restResult = nacosRestTemplate.get(urlString, header, Query.EMPTY, String.class); if (!restResult.ok()) &#123; throw new IOException( &quot;Error while requesting: &quot; + urlString + &quot;&#x27;. Server returned: &quot; + restResult.getCode()); &#125; String content = restResult.getData(); List&lt;String&gt; list = new ArrayList&lt;String&gt;(); for (String line : IoUtils.readLines(new StringReader(content))) &#123; if (!line.trim().isEmpty()) &#123; list.add(line.trim()); &#125; &#125; return list; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null;&#125; 一个是5s运行一次的登录任 这个任务就是nacos需要登录的话，就会定时执行 nacos地址可能会变 服务端动态的获取集群中nacos地址nacos集训默认是通过每个节点下的$&#123;nacos.home&#125;/conf/cluster.conf文件来获取集群中的节点的。 也可以通过在配置文件上加上： 1234567# 默认值file，就是通过cluster.conf文件来指定nacos.core.member.lookup.type=address-serveraddress.server.domain=jmenv.tbsite.net# 默认值就是这个，根据需要配置address.server.port=8080# 默认值就是这个，根据需要配置address.server.url=/nacos/serverlist 或者通过系统的环境变量： 项目配置： 1nacos.core.member.lookup.type=address-server 系统的环境变量： 1234567address_server_domain=address-server# 默认值就是这个，根据需要配置address_server_port=8080# 默认值就是这个，根据需要配置address_server_url=/nacos/serverlist 来启用动态的获取Nacos集群中节点服务，这个服务是通过HTTP获取的： 1234567API: 更具配置method: GET返回值：地址1地址2注意：返回值必须是一个字符串，而且每个地址后面都需要上一个换行符（\\n\\r） 源码: 在启动时会创建一个ServerMemberManager对象，这个对象就是用来创建管理nacos的节点信息的（无论是集群还是单机）。在初始化是会执行一个init方法： 12345678910111213141516171819202122232425// ServerMemberManagerprotected void init() throws NacosException &#123; Loggers.CORE.info(&quot;Nacos-related cluster resource initialization&quot;); this.port = EnvUtil.getProperty(&quot;server.port&quot;, Integer.class, 8848); this.localAddress = InetUtils.getSelfIP() + &quot;:&quot; + port; this.self = MemberUtil.singleParse(this.localAddress); this.self.setExtendVal(MemberMetaDataConstants.VERSION, VersionUtils.version); // serverList是ServerMemberManager的成员变量，里面包含了nacos的节点 // 这里添加自己的信息 serverList.put(self.getAddress(), self); // register NodeChangeEvent publisher to NotifyManager registerClusterEvent(); // 这里就是用来初始化和启动nacos节点获取服务的 // Initializes the lookup mode initAndStartLookup(); if (serverList.isEmpty()) &#123; throw new NacosException(NacosException.SERVER_ERROR, &quot;cannot get serverlist, so exit.&quot;); &#125; Loggers.CORE.info(&quot;The cluster resource is initialized&quot;);&#125; 这里看initAndStartLookup： 12345678910111213141516171819202122232425262728293031// ServerMemberManagerprivate void initAndStartLookup() throws NacosException &#123; this.lookup = LookupFactory.createLookUp(this); this.lookup.start();&#125;// LookupFactorypublic static MemberLookup createLookUp(ServerMemberManager memberManager) throws NacosException &#123; if (!EnvUtil.getStandaloneMode()) &#123; // 集群模式 // 这里会nacos.core.member.lookup.type的值 String lookupType = EnvUtil.getProperty(LOOKUP_MODE_TYPE); // 这里会选择用哪种模式 LookupType type = chooseLookup(lookupType); // 这里会更具LookupType来选择MemberLookup对象 // type = file : FileConfigMemberLookup // type = address-server : AddressServerMemberLookup // address-server模式会启动一个定时任务，没5s运行过一次 LOOK_UP = find(type); currentLookupType = type; &#125; else &#123; // 单机模式 LOOK_UP = new StandaloneMemberLookup(); &#125; // 这里很重要把前面创建的ServerMemberManager对象传入到MemberLookup中 // 在MemberLookup获取完节点信息后会调用ServerMemberManager#memberChange方法 LOOK_UP.injectMemberManager(memberManager); Loggers.CLUSTER.info(&quot;Current addressing mode selection : &#123;&#125;&quot;, LOOK_UP.getClass().getSimpleName()); return LOOK_UP;&#125; 这里看AddressServerMemberLookup的start方法 1234567public void start() throws NacosException &#123; if (start.compareAndSet(false, true)) &#123; this.maxFailCount = Integer.parseInt(EnvUtil.getProperty(&quot;maxHealthCheckFailCount&quot;, &quot;12&quot;)); initAddressSys(); run(); &#125;&#125; run方法会调用syncFromAddressUrl和启动一个定时任务是调用该方法： 1234567891011121314151617181920212223private void syncFromAddressUrl() throws Exception &#123; RestResult&lt;String&gt; result = restTemplate .get(addressServerUrl, Header.EMPTY, Query.EMPTY, genericType.getType()); if (result.ok()) &#123; isAddressServerHealth = true; Reader reader = new StringReader(result.getData()); try &#123; // ServerMemberManager.memberChange方法调用 afterLookup(MemberUtil .readServerConf(EnvUtil.analyzeClusterConf(reader))); &#125; catch (Throwable e) &#123; Loggers.CLUSTER.error(&quot;[serverlist] exception for analyzeClusterConf, error : &#123;&#125;&quot;, ExceptionUtil.getAllExceptionMsg(e)); &#125; addressServerFailCount = 0; &#125; else &#123; addressServerFailCount++; if (addressServerFailCount &gt;= maxFailCount) &#123; isAddressServerHealth = false; &#125; Loggers.CLUSTER.error(&quot;[serverlist] failed to get serverlist, error code &#123;&#125;&quot;, result.getCode()); &#125;&#125; 在1.4版本，如果一个节点想加入到集群中，而该节点采用的是文件指定节点信息，如果文件的信息不全会导致新加入的节点与部分机器不能通信，就比如下图的情况： 就是ServerMemberManager中的serverList在file模式下不会新增数据，除非手动的修改cluster.conf 这种情况下会出现问题的，Nacos1修改的数据无法同步到Nacos3。所以建议还是使用address-server这种模式，这样就能动态的感知节点的变化了 在集群下其实在整个nacos集群下，会有一个Nginx作为网关的存在，所以客户端其实不用管nacos的地址，只需要知道nginx的就好了。","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"03-Nacos的发布与订阅","slug":"springcloud/nacos中的一些细节/03-Nacos的发布与订阅","date":"2021-11-27T12:00:38.000Z","updated":"2022-03-23T09:03:57.389Z","comments":true,"path":"blog/springcloud/nacos中的一些细节/03-Nacos的发布与订阅/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/nacos%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82/03-Nacos%E7%9A%84%E5%8F%91%E5%B8%83%E4%B8%8E%E8%AE%A2%E9%98%85/","excerpt":"","text":"Nacos定义了 EventPublisher、Subscriber。用来完成事件的发布和订阅。对于消息提供了 Event、SlowEvent用来定义消息 Event类型的消息是独自占用一条队列和一个线程 SlowEvent类型的消息是共享一条队列和一个线程 只有是使用默认提供的EventPublisher才符合这个定义，如果是自定义的那不一定要遵循这个规则 Subscriber是更具需求自己实现的。而 EventPublisher在Nacos中提供了两个实现DefaultPublisher和DefaultSharePublisher。其中DefaultPublisher定义了消息的处理逻辑，大致的逻辑如下： 12345678910111213141516171819202122232425262728293031323334353637383940public class DefaultPublisher extends Thread implements EventPublisher &#123; ConcurrentHashSet&lt;Subscriber&gt; subscribers = new ConcurrentHashSet&lt;Subscriber&gt;(); BlockingQueue&lt;Event&gt; queue = new ArrayBlockingQueue&lt;Event&gt;(bufferSize); public void run() &#123; for (; ; ) &#123; if (shutdown) &#123; break; &#125; final Event event = queue.take(); for (Subscriber subscriber : subscribers) &#123; // 进行过期消息过滤 // Event的sequence默认是单调递增的 if (subscriber.ignoreExpireEvent() &amp;&amp; lastEventSequence &gt; event.sequence()) &#123; continue; &#125; Runnable job = new Runnable() &#123; @Override public void run() &#123; subscriber.onEvent(event); &#125; &#125;; Executor executor = subscriber.executor(); if (executor != null) &#123; executor.execute(job); &#125; else &#123; try &#123; job.run(); &#125; catch (Throwable e) &#123; LOGGER.error(&quot;Event callback exception : &#123;&#125;&quot;, e); &#125; &#125; &#125; // 更新最后处理的消息的序号 UPDATER.compareAndSet(this, lastEventSequence, Math.max(lastEventSequence, event.sequence())); &#125; &#125;&#125; 逻辑很简单，但由于subscribers的设计，DefaultPublisher只能处理一种类型的消息。 DefaultSharePublisher是DefaultPublisher的子类，重写了一些方法，使其能处理多种类型的消息。其核心结构如下： 1Map&lt;Class&lt;? extends SlowEvent&gt;, Set&lt;Subscriber&gt;&gt; subMappings 真是因为定义了这一结构，使得其支持处理多种类型的消息，但这些消息都共享一个线程。 除了上边的的类，Nacos还提供了NotifyCenter，该类提供了发布事件和订阅者注册的API。该类的大致结构如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class NotifyCenter &#123; static final NotifyCenter INSTANCE = new NotifyCenter(); DefaultSharePublisher sharePublisher; final Map&lt;String, EventPublisher&gt; publisherMap = new ConcurrentHashMap&lt;String, EventPublisher&gt;(16); static Class&lt;? extends EventPublisher&gt; clazz = null; // 这个是 publisherMap 中， EventPublisher的工厂 static BiFunction&lt;Class&lt;? extends Event&gt;, Integer, EventPublisher&gt; publisherFactory static &#123; // 这里使用了JDK的 SPI 来定义publisherMap中的EventPublisher的类型 // 默认是DefaultPublisher final ServiceLoader&lt;EventPublisher&gt; loader = ServiceLoader.load(EventPublisher.class); Iterator&lt;EventPublisher&gt; iterator = loader.iterator(); if (iterator.hasNext()) &#123; clazz = iterator.next().getClass(); &#125; else &#123; clazz = DefaultPublisher.class; &#125; publisherFactory = new BiFunction&lt;Class&lt;? extends Event&gt;, Integer, EventPublisher&gt;() &#123; @Override public EventPublisher apply(Class&lt;? extends Event&gt; cls, Integer buffer) &#123; try &#123; EventPublisher publisher = clazz.newInstance(); publisher.init(cls, buffer); return publisher; &#125; catch (Throwable ex) &#123; LOGGER.error(&quot;Service class newInstance has error : &#123;&#125;&quot;, ex); throw new NacosRuntimeException(SERVER_ERROR, ex); &#125; &#125; &#125;; INSTANCE.sharePublisher = new DefaultSharePublisher(); INSTANCE.sharePublisher.init(SlowEvent.class, shareBufferSize); // 添加一个钩子，在JDK关闭前会调用 // Runtime.getRuntime().addShutdownHook(new Thread(runnable)); ThreadUtils.addShutdownHook(new Runnable() &#123; @Override public void run() &#123; INSTANCE.sharePublisher.shutdown(); for (Map.Entry&lt;String, EventPublisher&gt; entry : INSTANCE.publisherMap.entrySet()) &#123; EventPublisher eventPublisher = entry.getValue(); eventPublisher.shutdown(); &#125; &#125; &#125;); &#125; // 后面的就是发布事件和订阅者注册的方法了 .... &#125; 这个类就是EventPublisher的管理类。不仅提供了EventPublisher初始化了关闭逻辑，还提供了一些对应的API。 这些API在Server和Client端都能使用","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"02-Nacos Server中的Secured注解","slug":"springcloud/nacos中的一些细节/02-Nacos Server中的Secured注解","date":"2021-11-27T12:00:37.000Z","updated":"2022-03-23T09:03:57.388Z","comments":true,"path":"blog/springcloud/nacos中的一些细节/02-Nacos Server中的Secured注解/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/nacos%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82/02-Nacos%20Server%E4%B8%AD%E7%9A%84Secured%E6%B3%A8%E8%A7%A3/","excerpt":"","text":"在Nacos中，该注解放在提供http的方法上，表示该接口需要做认证。 定义如下： 1234567891011121314151617181920212223242526272829@Retention(RetentionPolicy.RUNTIME)public @interface Secured &#123; /** * 表示读或这写 * * @return action type, default READ */ ActionTypes action() default ActionTypes.READ; /** * 请求的资源是什么，格式的花可以看 * 对于服务注册，看NamingResourceParser * 对于配置，看ConfigResourceParser * * @return resource name */ String resource() default StringUtils.EMPTY; /** * 返回资源的名称， * 对于服务注册，看NamingResourceParser * 对于配置，看ConfigResourceParser * 优先级低于resource() * * @return class type of resource parser */ Class&lt;? extends ResourceParser&gt; parser() default DefaultResourceParser.class;&#125; 处理该注解的类为AuthFilter，一个Servlet的Filter。需要做认证的有两个前提： 123if (method.isAnnotationPresent(Secured.class) &amp;&amp; authConfigs.isAuthEnabled()) &#123; ....&#125; 方法上有Secured注解和开启了认证（默认不开启），Nacos server开启认证是通过提供（环境变类，配置，启动参数） 1nacos.core.auth.enabled=true 该值默认为false。 而负责认证的类为NacosAuthManager，使用的是jwt + Spring security","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"01-Nacos Server服务注册表的写时复制","slug":"springcloud/nacos中的一些细节/01-Nacos Server服务注册表的写时复制","date":"2021-11-27T12:00:36.000Z","updated":"2022-03-23T09:03:57.388Z","comments":true,"path":"blog/springcloud/nacos中的一些细节/01-Nacos Server服务注册表的写时复制/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/nacos%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82/01-Nacos%20Server%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E8%A1%A8%E7%9A%84%E5%86%99%E6%97%B6%E5%A4%8D%E5%88%B6/","excerpt":"","text":"跟踪Service#onChange代码到Service#updateIPs的: 1234567// Service#updateIPsfor (Map.Entry&lt;String, List&lt;Instance&gt;&gt; entry : ipMap.entrySet()) &#123; //make every ip mine List&lt;Instance&gt; entryIPs = entry.getValue(); // Map&lt;String, Cluster&gt; clusterMap = new HashMap&lt;&gt;(); clusterMap.get(entry.getKey()).updateIps(entryIPs, ephemeral);&#125; 这里就是更新注册表的地方。这里查看下Cluster#updateIps 1234567891011121314public void updateIps(List&lt;Instance&gt; ips, boolean ephemeral) &#123; Set&lt;Instance&gt; toUpdateInstances = ephemeral ? ephemeralInstances : persistentInstances; // 进行一些对ips和toUpdateInstances的一些操作 .... toUpdateInstances = new HashSet&lt;&gt;(ips); if (ephemeral) &#123; ephemeralInstances = toUpdateInstances; &#125; else &#123; persistentInstances = toUpdateInstances; &#125;&#125; 其中ephemeralInstances和persistentInstances是Service对象的成员变量，分别用来保存临时实例和持久实例，同时他们的定义如下： 123private Set&lt;Instance&gt; persistentInstances = new HashSet&lt;&gt;();private Set&lt;Instance&gt; ephemeralInstances = new HashSet&lt;&gt;(); 如果跟完Service#onChange，在更新注册表的时候是完全没有添加任何锁的，设置是volatile。 服务的实例是共享的资源，读和写操作肯定会存在并发的，而为了保证并发执行，最简单的就是加锁，这样会导致读和写会互斥，会牺牲了读的性能，但能保证读的实时性。而在spring cloud的项目中，因为有重试和负载均衡机制，服务的实时性其实不是很重要。所以nacos就采用了这种写时复制的思想，并且写操作只有一个线程执行，这样能提高读操作的性能。","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"27-Seata客户端源码","slug":"springcloud/27-Seata客户端源码","date":"2021-11-27T12:00:35.000Z","updated":"2022-03-23T09:03:57.379Z","comments":true,"path":"blog/springcloud/27-Seata客户端源码/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/27-Seata%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%BA%90%E7%A0%81/","excerpt":"","text":"源码思维导图https://www.processon.com/view/link/622f7e351e085307a24202e7 Seata工作原理 Seata源码分析","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"26-Seata入门","slug":"springcloud/26-Seata入门","date":"2021-11-27T12:00:34.000Z","updated":"2022-03-23T09:03:57.325Z","comments":true,"path":"blog/springcloud/26-Seata入门/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/26-Seata%E5%85%A5%E9%97%A8/","excerpt":"","text":"seata 1.4.2 Seata 官方文档 Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。整体机制就是一个二阶段提交。 而且 Seata AT 模式是一种非侵入式的分布式事务解决方案，Seata 在内部做了对数据库操作的代理层，我们使用 Seata AT 模式时，实际上用的是 Seata 自带的数据源代理 DataSourceProxy，Seata 在这层代理中加入了很多逻辑，比如插入回滚 undo_log 日志，检查全局锁等。 &#x3D;&#x3D;Seata 定义了全局事务的框架。全局事务 定义为若干 分支事务 的整体协调。&#x3D;&#x3D; 在 Seata 的架构中，一共有三个角色： TC (Transaction Coordinator) - 事务协调者 维护全局和分支事务的状态，驱动全局事务提交或回滚。 TM (Transaction Manager) - 事务管理器 定义全局事务的范围：开始全局事务、提交或回滚全局事务。 RM (Resource Manager) - 资源管理器 管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。 其中，TC 为单独部署的 Server 服务端，TM 和 RM 为嵌入到应用中的 Client 客户端。 在 Seata 中，一个分布式事务的生命周期如下： TM ，作为全局事务的发起者，请求 TC 开启一个全局事务。TC 会生成一个 XID 作为该全局事务的编号。XID，会在微服务的调用链路中传播，保证将多个微服务的子事务关联在一起。 RM 请求 TC 将本地事务注册为全局事务的分支事务，通过全局事务的 XID 进行关联。 TM 请求 TC 告诉 XID 对应的全局事务是进行提交还是回滚。 TC 驱动 RM 们将 XID 对应的自己的本地事务进行提交还是回滚。 Seata 的 全局事务 处理过程，分为两个阶段： 执行阶段 ：执行 分支事务，并 保证 执行结果满足是 可回滚的（Rollbackable） 和 持久化的（Durable）。 完成阶段： 根据 执行阶段 结果形成的决议，应用通过 TM 发出的全局提交或回滚的请求给 TC，TC 命令 RM 驱动 分支事务 进行 Commit 或 Rollback。 Seata 中所谓 事务模式（AT、TCC、XA、Saga） 是指：运行在 Seata 全局事务框架下的 分支事务 的行为模式。准确地讲，应该叫作 分支事务模式。 不同的 事务模式 区别在于 分支事务 使用不同的方式达到全局事务两个阶段的目标。即，回答以下两个问题： 执行阶段 ：如何执行并 保证 执行结果满足是 可回滚的（Rollbackable） 和 持久化的（Durable）。 完成阶段： 收到 TC 的命令后，如何做到分支的提交或回滚？ AT 模式AT模式的核心是对业务无侵入，是一种改进后的两阶段提交， 业务表：product Field Type Key id bigint(20) PRI name varchar(100) since varchar(100) AT 分支事务的业务逻辑： 1update product set name = &#x27;GTS&#x27; where name = &#x27;TXC&#x27;; 第一阶段 解析 SQL：得到 SQL 的类型（UPDATE），表（product），条件（where name &#x3D; ‘TXC’）等相关的信息。 查询前镜像：根据解析得到的条件信息，生成查询语句，定位数据。 1select id, name, since from product where name = &#x27;TXC&#x27;; 得到前镜像： id name since 1 TXC 2014 执行业务 SQL：更新这条记录的 name 为 ‘GTS’。 查询后镜像：根据前镜像的结果，通过 主键 定位数据。 1select id, name, since from product where id = 1; 得到后镜像： id name since 1 GTS 2014 插入回滚日志：把前后镜像数据以及业务 SQL 相关的信息组成一条回滚日志记录，插入到 UNDO_LOG 表中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&#123; &quot;branchId&quot;: 641789253, &quot;undoItems&quot;: [ &#123; &quot;afterImage&quot;: &#123; &quot;rows&quot;: [ &#123; &quot;fields&quot;: [ &#123; &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: 4, &quot;value&quot;: 1 &#125;, &#123; &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: 12, &quot;value&quot;: &quot;GTS&quot; &#125;, &#123; &quot;name&quot;: &quot;since&quot;, &quot;type&quot;: 12, &quot;value&quot;: &quot;2014&quot; &#125; ] &#125; ], &quot;tableName&quot;: &quot;product&quot; &#125;, &quot;beforeImage&quot;: &#123; &quot;rows&quot;: [ &#123; &quot;fields&quot;: [ &#123; &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: 4, &quot;value&quot;: 1 &#125;, &#123; &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: 12, &quot;value&quot;: &quot;TXC&quot; &#125;, &#123; &quot;name&quot;: &quot;since&quot;, &quot;type&quot;: 12, &quot;value&quot;: &quot;2014&quot; &#125; ] &#125; ], &quot;tableName&quot;: &quot;product&quot; &#125;, &quot;sqlType&quot;: &quot;UPDATE&quot; &#125; ], &quot;xid&quot;: &quot;xid:xxx&quot;&#125; 提交前，向 TC 注册分支：申请 product 表中，主键值等于 1 的记录的 全局锁 。 如果当前存在全局锁，则会回滚本地事务，通过 while 循环不断地重新竞争获取本地锁和全局锁。 本地事务提交：业务数据的更新和前面步骤中生成的 UNDO LOG 一并提交。 将本地事务提交的结果上报给 TC。 第二阶段——提交 收到 TC 的分支提交请求，把请求放入一个异步任务的队列中，马上返回提交成功的结果给 TC。 异步任务阶段的分支提交请求将异步和批量地删除相应 UNDO LOG 记录。 第二阶段——回滚 收到 TC 的分支回滚请求，开启一个本地事务，执行如下操作。 通过 XID 和 Branch ID 查找到相应的 UNDO LOG 记录。 数据校验：拿 UNDO LOG 中的后镜与当前数据进行比较，如果有不同，说明数据被当前全局事务之外的动作做了修改。这种情况，需要根据配置策略来做处理。 根据 UNDO LOG 中的前镜像和业务 SQL 的相关信息生成并执行回滚的语句： 1update product set name = &#x27;TXC&#x27; where id = 1; 删除本地对应的UNDO LOG 提交本地事务。并把本地事务的执行结果（即分支事务回滚的结果）上报给 TC。 整体执行流程: Seata 事务隔离级别在讲 Seata 事务隔离级之前，我们先来回顾一下数据库事务的隔离级别，目前数据库事务的隔离级别一共有 4 种，由低到高分别为： Read uncommitted：读未提交 Read committed：读已提交 Repeatable read：可重复读 Serializable：序列化 而 Seata AT 模式的事务隔离是建立在支事务的本地隔离级别基础之上的，在数据库本地隔离级别读已提交或以上的前提下，Seata 设计了由事务协调器维护的全局写排他锁，来保证事务间的写隔离，同时，将全局事务默认定义在读未提交的隔离级别上。 我们知道 Seata 的事务是一个全局事务，它包含了若干个分支本地事务，在全局事务执行过程中（全局事务还没执行完），某个本地事务提交了，如果 Seata 没有采取任务措施，则会导致已提交的本地事务被读取，造成脏读，如果数据在全局事务提交前已提交的本地事务被修改，则会造成脏写。由此可以看出，传统意义的脏读是读到了未提交的数据，Seata 脏读是读到了全局事务下未提交的数据，全局事务可能包含多个本地事务，某个本地事务提交了不代表全局事务提交了。 应用如果需要达到全局的读已提交，Seata 也提供了全局锁机制实现全局事务读已提交。但是默认情况下，Seata 的全局事务是工作在读未提交隔离级别的，保证绝大多数场景的高效性。 写隔离 一阶段本地事务提交前，需要确保先拿到 全局锁（GlobalLock） 。 拿不到 全局锁 ，不能提交本地事务。 拿 全局锁 的尝试被限制在一定范围内，超出范围将放弃，并回滚本地事务，释放本地锁。 以一个示例来说明： 两个全局事务 tx1 和 tx2，分别对 a 表的 m 字段进行更新操作，m 的初始值 1000。 tx1 先开始，开启本地事务，拿到本地锁，更新操作 m &#x3D; 1000 - 100 &#x3D; 900。本地事务提交前，先拿到该记录的 全局锁 ，本地提交释放本地锁。 tx2 后开始，开启本地事务，拿到本地锁，更新操作 m &#x3D; 900 - 100 &#x3D; 800。本地事务提交前，尝试拿该记录的 全局锁 ，tx1 全局提交前，该记录的全局锁被 tx1 持有，tx2 需要重试等待 全局锁 。 tx1 二阶段全局提交，释放 全局锁 。tx2 拿到 全局锁 提交本地事务。 如果 tx1 的二阶段全局回滚，则 tx1 需要重新获取该数据的本地锁，进行反向补偿的更新操作，实现分支的回滚。 此时，如果 tx2 仍在等待该数据的 全局锁，同时持有本地锁，则 tx1 的分支回滚会失败。分支的回滚会一直重试，直到 tx2 的 全局锁 等锁超时，放弃 全局锁 并回滚本地事务释放本地锁，tx1 的分支回滚最终成功。 因为整个过程 全局锁 在 tx1 结束前一直是被 tx1 持有的，所以不会发生 脏写 的问题。 读隔离在数据库本地事务隔离级别 读已提交（Read Committed） 或以上的基础上，Seata（AT 模式）的默认全局隔离级别是 读未提交（Read Uncommitted） 。 如果应用在特定场景下，必需要求全局的 读已提交 ，目前 Seata 的方式是通过@GlobalTransactional（@GlobalLock） + SELECT FOR UPDATE（在修改数据前需要执行这sql获取本地锁） 语句的代理。 SELECT FOR UPDATE 语句的执行会申请 全局锁 ，如果 全局锁 被其他事务持有，则释放本地锁（回滚 SELECT FOR UPDATE 语句的本地执行）并重试。这个过程中，查询是被 block 住的，直到 全局锁 拿到，即读取的相关数据是 已提交 的，才返回。 出于总体性能上的考虑，Seata 目前的方案并没有对所有 SELECT 语句都进行代理，仅针对 FOR UPDATE 的 SELECT 语句。 设计亮点和存在的问题设计亮点相比与其它分布式事务框架，Seata架构的亮点主要有几个: 应用层基于SQL解析实现了自动补偿，从而最大程度的降低业务侵入性； 将分布式事务中TC（事务协调者）独立部署，负责事务的注册、回滚； 通过全局锁实现了写隔离与读隔离。 存在的问题 性能损耗 一条Update的SQL，则需要全局事务xid获取（与TC通讯）、before image（解析SQL，查询一次数据库）、after image（查询一次数据库）、insert undo log（写一次数据库）、before commit（与TC通讯，判断锁冲突），这些操作都需要一次远程通讯RPC，而且是同步的。另外undo log写入时blob字段的插入性能也是不高的。每条写SQL都会增加这么多开销,粗略估计会增加5倍响应时间。 性价比 为了进行自动补偿，需要对所有交易生成前后镜像并持久化，可是在实际业务场景下，这个是成功率有多高，或者说分布式事务失败需要回滚的有多少比率？按照二八原则预估，为了20%的交易回滚，需要将80%的成功交易的响应时间增加5倍，这样的代价相比于让应用开发一个补偿交易是否是值得？ 全局锁 热点数据 相比XA，Seata 虽然在一阶段成功后会释放数据库锁，但一阶段在commit前全局锁的判定也拉长了对数据锁的占有时间，这个开销比XA的prepare低多少需要根据实际业务场景进行测试。全局锁的引入实现了隔离性，但带来的问题就是阻塞，降低并发性，尤其是热点数据，这个问题会更加严重。 回滚锁释放时间 Seata在回滚时，需要先删除各节点的undo log，然后才能释放TC内存中的锁，所以如果第二阶段是回滚，释放锁的时间会更长。 死锁问题 Seata的引入全局锁会额外增加死锁的风险，但如果出现死锁，会不断进行重试，最后靠等待全局锁超时，这种方式并不优雅，也延长了对数据库锁的占有时间。 SQL限制 不支持 SQL 嵌套 不支持多表复杂 SQL 不支持存储过程、触发器 不支持批量更新 SQL 详情看SQL限制 (seata.io) seata-tcc模式一个分布式的全局事务，整体是 两阶段提交 的模型。全局事务是由若干分支事务组成的，分支事务要满足 两阶段提交 的模型要求，即需要每个分支事务都具备自己的： 一阶段 prepare 行为 二阶段 commit 或 rollback 行为 AT 模式基于 支持本地 ACID 事务 的 关系型数据库： 一阶段 prepare 行为：在本地事务中，一并提交业务数据更新和相应回滚日志记录。 二阶段 commit 行为：马上成功结束，自动 异步批量清理回滚日志。 二阶段 rollback 行为：通过回滚日志，自动 生成补偿操作，完成数据回滚。 相应的，TCC 模式，不依赖于底层数据资源的事务支持： 一阶段 prepare 行为：调用 自定义 的 prepare 逻辑，完成资源的预留。 二阶段 commit 行为：调用 自定义 的 commit 逻辑，完成业务数据的真正修改。 二阶段 rollback 行为：调用 自定义 的 rollback 逻辑，释放预留的资源。 TCC模式与AT模式的主要区别如下： 在使用上，TCC模式依赖用户自行实现的3个方法try、confirm、cannel成本较大；并且，需要开发者自行保证幂等、事务防悬挂和空回滚；AT模式依赖全局事务注解和代理数据源，代码基本不需要改动，对业务无侵入性。 TCC模式作用范围在应用成，本质上是实现针对某个业务逻辑的正向和反响操作；AT模式的作用范围在底层数据源，通过保存操作行记录的前、后镜像和生成反向SQL语句进行补偿操作，对上层应用透明。 TCC模式事务并发控制由业务自行加锁，AT模式由Seata框架自动加锁。 seata-server部署下载seata&#x2F;seata 最简单的部署 修改registry.conf 12345678910111213141516registry &#123; # file 、nacos 、eureka、redis、zk、consul、etcd3、sofa type = &quot;file&quot; file &#123; name = &quot;file.conf&quot; &#125;&#125;config &#123; # file、nacos 、apollo、zk、consul、etcd3 type = &quot;file&quot; file &#123; name = &quot;file.conf&quot; &#125;&#125; 修改file.conf store mode支持file、db、redis，这里选择数据库 12345678910111213141516171819202122232425262728## transaction log store, only used in seata-serverstore &#123; ## store mode: file、db、redis mode = &quot;db&quot; ## rsa decryption public key publicKey = &quot;&quot; ## database store property db &#123; ## the implement of javax.sql.DataSource, such as DruidDataSource(druid)/BasicDataSource(dbcp)/HikariDataSource(hikari) etc. datasource = &quot;druid&quot; ## mysql/oracle/postgresql/h2/oceanbase etc. dbType = &quot;mysql&quot; driverClassName = &quot;com.mysql.cj.jdbc.Driver&quot; ## if using mysql to store the data, recommend add rewriteBatchedStatements=true in jdbc connection param url = &quot;jdbc:mysql://192.168.2.148:3306/seata?rewriteBatchedStatements=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=Asia/Shanghai&quot; user = &quot;root&quot; password = &quot;458233365&quot; minConn = 5 maxConn = 100 globalTable = &quot;global_table&quot; branchTable = &quot;branch_table&quot; lockTable = &quot;lock_table&quot; queryLimit = 100 maxWait = 5000 &#125;&#125; 执行启动就好了 1seata-server.sh -h 192.168.2.163 -p 8091 -n 1 这种比较适合使用单机。下面使用Nacos 使用nacos作为配置中心和注册中心 配置registry.conf 1234567891011121314151617181920212223242526272829registry &#123; # file 、nacos 、eureka、redis、zk、consul、etcd3、sofa type = &quot;nacos&quot; nacos &#123; application = &quot;seata-server&quot; serverAddr = &quot;127.0.0.1:8848&quot; group = &quot;SEATA_GROUP&quot; namespace = &quot;173aefb4-26c7-4c3b-a71e-176ce21ba798&quot; cluster = &quot;default&quot; username = &quot;&quot; password = &quot;&quot; &#125;&#125;config &#123; # file、nacos 、apollo、zk、consul、etcd3 type = &quot;nacos&quot; nacos &#123; serverAddr = &quot;127.0.0.1:8848&quot; namespace = &quot;173aefb4-26c7-4c3b-a71e-176ce21ba798&quot; group = &quot;SEATA_GROUP&quot; username = &quot;&quot; password = &quot;&quot; dataId = &quot;seataServer.properties&quot; &#125;&#125; 要加上group dataId，要配合下面的配置 在nacos中添加如下配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154transport.type=TCPtransport.server=NIOtransport.heartbeat=truetransport.threadFactory.bossThreadPrefix=NettyBosstransport.threadFactory.workerThreadPrefix=NettyServerNIOWorkertransport.threadFactory.serverExecutorThreadPrefix=NettyServerBizHandlertransport.threadFactory.shareBossWorker=false# TC的用来接收请求的线程数transport.threadFactory.bossThreadSize=1# TC用来处理IO的线程数，可以指定具体的数字或者这些值# default(NettyRuntime.availableProcessors() * 2)# BusyPin(NettyRuntime.availableProcessors() + 1)# Pin(NettyRuntime.availableProcessors())# Auto(NettyRuntime.availableProcessors() * 2 + 1)transport.threadFactory.workerThreadSize=defaulttransport.shutdown.wait=3# 这个必须的，对全局事务分组。后面的default在nacos中就是clusterservice.vgroupMapping.my_test_tx_group=default################### 客户端相关 endclient.rm.asyncCommitBufferLimit=10000# 重试获取全局锁间隔client.rm.lock.retryInterval=10# 获取全局重试次数client.rm.lock.retryTimes=30# 冲突时重试策略分支回滚client.rm.lock.retryPolicyBranchRollbackOnConflict=trueclient.rm.reportRetryCount=5client.rm.tableMetaCheckEnable=falseclient.rm.tableMetaCheckerInterval=60000# sql解析，默认使用druid-sqlclient.rm.sqlParserType=druid# 当分支事务提交成功后，通过这个配置来判断是否报告给TC（TC默认是分支事务提交成功的，只有失败的时候一定要报告）client.rm.reportSuccessEnable=falseclient.rm.sagaBranchRegisterEnable=false# 提交重试最大次数client.tm.commitRetryCount=5# 回滚重试最大次数client.tm.rollbackRetryCount=5# 全局事务超时时间，默认60秒client.tm.defaultGlobalTransactionTimeout=60000# TC服务不可用时是否降级，如果真的降级了会导致全局事务和全局锁失效并且继续执行代码，所以不建议开启client.tm.degradeCheck=false# 触发降级的最大请求失败次数client.tm.degradeCheckAllowTimes=10# 降级检查执行周期client.tm.degradeCheckPeriod=2000# 客户端发送消息时的使用的负载均衡策略# 默认时RandomLoadBalance（随机），还有下面的值# RoundRobinLoadBalance（轮询）# ConsistentHashLoadBalance（一致性哈希）# LeastActiveLoadBalance（最不活跃）client.loadBalance.type=RandomLoadBalance# 虚拟节点数，一致性哈希策略才有用client.loadBalance.visualNodes=10# TC地址列表&quot;,&quot;隔开，使用服务注册的话，这个没有必要配置service.default.grouplist=127.0.0.1:8091# 是用来禁止全局事务和全局锁service.disableGlobalTransaction=false# 消息的body序列化，默认使用SeataSerializer，还有protobuf等transport.serialization=seata# 是否对body进行压缩，默认不使用transport.compressor=nonetransport.threadFactory.clientSelectorThreadPrefix=NettyClientSelectortransport.threadFactory.clientWorkerThreadPrefix=NettyClientWorkerThread# 客户端的Selector的数量，或者说EventLoop的数量，也可以说是客户端处理IO的线程数transport.threadFactory.clientSelectorThreadSize=1# 是否开启批量发送，只有这个方法有用 Object sendSyncRequest(Object msg)transport.enableClientBatchSendRequest=true# 当分支事务回滚时，是否需要用前后镜像与当前数据做比较client.undo.dataValidation=true# undo日志的格式（以二进制的形式保存到数据库）client.undo.logSerialization=jackson# 只关心更新列client.undo.onlyCareUpdateColumns=true# 设置undo日志的表名client.undo.logTable=undo_log# undo日志是否能压缩client.undo.compress.enable=trueclient.undo.compress.type=zip# undo日志开启压缩的阈值client.undo.compress.threshold=64klog.exceptionRate=100################### 客户端相关 beginstore.mode=dbstore.publicKey=# store.mode=file的配置store.file.dir=file_store/datastore.file.maxBranchSessionSize=16384store.file.maxGlobalSessionSize=512store.file.fileWriteBufferCacheSize=16384store.file.flushDiskMode=asyncstore.file.sessionReloadReadSize=100# store.mode=db的配置store.db.datasource=druidstore.db.dbType=mysqlstore.db.driverClassName=com.mysql.jdbc.Driverstore.db.url=jdbc:mysql://192.168.2.148:3306/seata?rewriteBatchedStatements=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=Asia/Shanghaistore.db.user=rootstore.db.password=458233365store.db.minConn=5store.db.maxConn=30store.db.globalTable=global_tablestore.db.branchTable=branch_tablestore.db.queryLimit=100store.db.lockTable=lock_tablestore.db.maxWait=5000# store.mode=redis的配置store.redis.mode=singlestore.redis.single.host=192.168.2.148store.redis.single.port=6379store.redis.sentinel.masterName=store.redis.sentinel.sentinelHosts=store.redis.maxConn=10store.redis.minConn=1store.redis.maxTotal=100store.redis.database=0store.redis.password=store.redis.queryLimit=100# TCserver.recovery.committingRetryPeriod=1000server.recovery.asynCommittingRetryPeriod=1000server.recovery.rollbackingRetryPeriod=1000server.recovery.timeoutRetryPeriod=1000server.maxCommitRetryTimeout=-1server.maxRollbackRetryTimeout=-1server.rollbackRetryTimeoutUnlockEnable=false# TC关于undo日志配置# TC会想RM发删除日志的请求# undo日志的有效时间server.undo.logSaveDays=7# TC执行删除undo日志任务的执行间隔server.undo.logDeletePeriod=86400000# 监控metrics.enabled=falsemetrics.registryType=compactmetrics.exporterList=prometheusmetrics.exporterPrometheusPort=9898 根据情况精简配置 这里要使用properties格式，yaml格式不生效的 启动 1seata-server.sh -h 192.168.2.163 -p 8091 -n 1 支持的启动参数 参数 全写 作用 备注 -h –host 指定在注册中心注册的 IP 不指定时获取当前的 IP，外部访问部署在云环境和容器中的 server 建议指定 -p –port 指定 server 启动的端口 默认为 8091 -m –storeMode 事务日志存储方式 支持file,db,redis，默认为 file 注:redis需seata-server 1.3版本及以上 -n –serverNode 用于指定seata-server节点ID 如 1,2,3…, 默认为 1 -e –seataEnv 指定 seata-server 运行环境 如 dev, test 等, 服务启动时会使用 registry-dev.conf 这样的配置 在seata链接的数据库执行的脚本mysql.sql 集群启动 1238091/seata-server.sh -h 192.168.2.163 -p 8091 -n 18092/seata-server.sh -h 192.168.2.163 -p 8092 -n 28093/seata-server.sh -h 192.168.2.163 -p 8093 -n 3 Seata使用12345678910111213141516&lt;!-- seata--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-seata&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--nacos 注册中心--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 如果使用的spring-cloud-alibaba.version过低或过高，想换个seata版本，可以这样 123456789101112131415161718192021222324252627&lt;!-- seata--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-seata&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;io.seata&lt;/groupId&gt; &lt;artifactId&gt;seata-all&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.seata&lt;/groupId&gt; &lt;artifactId&gt;seata-all&lt;/artifactId&gt; &lt;version&gt;$&#123;seata.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!--nacos 注册中心--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 在配置文件添加seata配置 1234567891011121314151617181920seata: # seata 服务分组，要与服务端nacos-config.txt中service.vgroup_mapping的后缀对应 tx-service-group: my_test_tx_group registry: # 指定nacos作为注册中心 type: nacos nacos: server-addr: 127.0.0.1:8848 namespace: &quot;173aefb4-26c7-4c3b-a71e-176ce21ba798&quot; group: SEATA_GROUP # 想起配置看ConfigurationKeys类 config: # 指定nacos作为配置中心 type: nacos nacos: server-addr: 127.0.0.1:8848 namespace: &quot;173aefb4-26c7-4c3b-a71e-176ce21ba798&quot; group: SEATA_GROUP dataId: &quot;seataServer.properties&quot; 其中tx-service-group的值是这个 在业务数据库添加一张undo_log表（这个表明可以配置） 12345678910111213CREATE TABLE IF NOT EXISTS `undo_log`( `branch_id` BIGINT NOT NULL COMMENT &#x27;branch transaction id&#x27;, `xid` VARCHAR(128) NOT NULL COMMENT &#x27;global transaction id&#x27;, `context` VARCHAR(128) NOT NULL COMMENT &#x27;undo_log context,such as serialization&#x27;, `rollback_info` LONGBLOB NOT NULL COMMENT &#x27;rollback info&#x27;, `log_status` INT(11) NOT NULL COMMENT &#x27;0:normal status,1:defense status&#x27;, `log_created` DATETIME(6) NOT NULL COMMENT &#x27;create datetime&#x27;, `log_modified` DATETIME(6) NOT NULL COMMENT &#x27;modify datetime&#x27;, UNIQUE KEY `ux_undo_log` (`xid`, `branch_id`)) ENGINE = InnoDB AUTO_INCREMENT = 1 DEFAULT CHARSET = utf8 COMMENT =&#x27;AT transaction mode undo table&#x27;; 每一服务都配置完后就能使用@GlobalTransactional注解，注册全局事务了 详细的可以看seata&#x2F;seata-samples官方提供的例子 AT模式使用配置代理数据源 1234seata: # 不需要创建代理的数据源BeanName excludesForAutoProxying: dataSourceProxyMode: AT 上边是默认配置 自动配置类SeataDataSourceBeanPostProcessor，一个BeanPostProcessor，根据上边的配置，为DataSource创建一个DataSourceProxy，保存到一个Map中，上边的就是配置代理模式和排除的数据源 使用@GlobalTransactional开启全局事务 XA模式上层编程模型与 AT 模式完全相同。只需要修改数据源代理，即可实现 XA 模式与 AT 模式之间的切换。 1234seata: # 不需要创建代理的DataSource权限定名，数组 excludesForAutoProxying: dataSourceProxyMode: XA 使用@GlobalTransactional开启全局事务 在默认情况下，项目中不能同时使用AT模式和XA模式，就算通过 1234@Bean(name = &quot;account&quot;)public DataSourceProxy payDataSourceProxy(@Qualifier(&quot;originAccount&quot;) DataSource dataSource)&#123; return new DataSourceProxy(dataSource);&#125; 这样提供了一个DataSourceProxy，都会因为上边的配置，最终为originAccount数据源生成一个DataSourceProxyXA代理，这是SeataDataSourceBeanPostProcessor这个类所决定的，为了支持两种模式，就需要对这个BeanPostProcessor进行改造。 XA模式基本是很少使用了 TCC模式12345seata: # 不需要创建代理的DataSource权限定名，数组 excludesForAutoProxying: # AT和TCC使用同一个代理数据源 dataSourceProxyMode: AT 定义TCC接口 123456789101112131415161718192021222324252627282930313233343536373839404142/** * 这里定义tcc的接口 * 一定要定义在接口上 * 我们使用springCloud的远程调用 * 那么这里使用LocalTCC便可 * */@LocalTCCpublic interface TccService &#123; /** * 定义两阶段提交 * name = 该tcc的bean名称,全局唯一 * commitMethod = commit 为二阶段确认方法 * rollbackMethod = rollback 为二阶段取消方法 * BusinessActionContextParameter注解 传递参数到二阶段中 * * @param params -入参 * @return String */ @TwoPhaseBusinessAction(name = &quot;insert&quot;, commitMethod = &quot;commitTcc&quot;, rollbackMethod = &quot;cancel&quot;) String prepare( @BusinessActionContextParameter(paramName = &quot;params&quot;) Map&lt;String, String&gt; params ); /** * 确认方法、可以另命名，但要保证与commitMethod一致 * context可以传递try方法的参数 * * @param context 上下文 * @return boolean */ boolean commit(BusinessActionContext context); /** * 二阶段取消方法 * * @param context 上下文 * @return boolean */ boolean cancel(BusinessActionContext context);&#125; @LocalTCC一定需要注解在接口上，此接口可以是寻常的业务接口，只要实现了TCC的两阶段提交对应方法便可，TCC相关注解如下： @TwoPhaseBusinessAction 注解try方法，其中name为当前tcc方法的bean名称，写方法名便可（全局唯一），commitMethod指向提交方法，rollbackMethod指向事务回滚方法。指定好三个方法之后，seata会根据全局事务的成功或失败，去帮我们自动调用提交方法或者回滚方法。 @BusinessActionContextParameter 注解可以将参数传递到二阶段（commitMethod&#x2F;rollbackMethod）的方法。 BusinessActionContext 便是指TCC事务上下文 使用 在业务所在系统中开启全局事务并执行服务 A 和服务 B 的 TCC 预留资源方法： 1234567@GlobalTransactionalpublic String doTransactionCommit() &#123; //服务A事务参与者 tccActionOne.prepare(null,&quot;one&quot;); //服务B事务参与者 tccActionTwo.prepare(null,&quot;two&quot;);&#125; 在实现类中，使用context.getActionContext(“params”)便可以得到一阶段try中定义的参数。 注意1：此处亦不可以捕获异常（同理切面处理异常），否则TCC将识别该操作为成功，二阶段直接执行commitMethod。 **注意2：**TCC模式要**开发者自行**保证幂等、事务防悬挂和空回滚 @GlobalTransactional开启全局事务 @GlobalTransactional注解可以在本地事务提交前，查询全局锁是否存在，如果当前存在全局锁，则会回滚本地事务，通过 while 循环不断地重新竞争获取本地锁和全局锁。 所以在seata中是通过全局锁来防止脏写和脏读 @GlobalTransactional注解能和Spring的事务一起使用 @GlobalLock注解（全局锁）+ FOR UPDATE123456789101112131415161718192021222324// 声明事务仅在单个本地RM中执行// 但事务需要确保要更新（或选择更新）的记录不在全局事务中// 在上述情况下，使用此注解而不是@GlobalTransaction将有助于提高性能。// @see io.seata.spring.annotation.GlobalTransactionScanner#wrapIfNecessary(Object, String, Object)用于TM、GlobalLock和TCC模式的扫描器// @see io.seata.spring.annotation.GlobalTransactionalInterceptor#handleGlobalLock(MethodInvocation)@GlobalLock的拦截器// @see io.seata.spring.annotation.datasource.SeataAutoDataSourceProxyAdvice#invoke(MethodInvocation) GlobalLockLogic和AT/XA模式的拦截器@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.METHOD,ElementType.TYPE&#125;)@Inheritedpublic @interface GlobalLock &#123; /** * 自定义全局锁重试间隔（单位：毫秒） * 您可以使用它覆盖“client.rm.lock.retryInterval”的全局配置，默认10 * 注意：0或负数将不起作用（这意味着返回到全局配置） */ int lockRetryInternal() default 0; /** * 自定义全局锁重试次数 * 您可以使用它覆盖“client.rm.lock.retryTimes”的全局配置，默认30 * 注：负数无效（这意味着返回全局配置 */ int lockRetryTimes() default -1;&#125; 该注解的作用就是：对于某条数据进行更新操作，如果全局事务正在进行，当某个本地事务需要更新该数据时，需要使用@GlobalLock确保其不会对全局事务正在操作的数据进行修改。 如果不对这些数据进行修改限制，在全局事务执行的过程中，一阶段会直接提交本地事务，其他本地事务可直接修改该数据，会导致全局事务二阶段回滚时，发现数据被修改过，认为数据已经脏了，回滚失败。 使用@GlobalLock，在执行本地事务时，去获取该数据的全局锁，如果获取不到，说明该数据正在被全局事务执行，可以进行重试获取。 注意事项 更新之前，在查询方法中添加排它锁，比如根据ID 查询时，需要如下SQL 书写： 123&lt;select id=&quot;selectById&quot; parameterType=&quot;integer&quot; resultType=&quot;com.hnmqet.demo01.entity.AccountTbl&quot;&gt; SELECT id,user_id,money FROM account_tbl WHERE id=#&#123;id&#125; FOR UPDATE&lt;/select&gt; 这是因为，只有添加了 FOR UPDATE，Seata 才会进行创建重试的执行器，这样事务失败时，会释放本地锁，等待一定时间再重试。 如果不添加，需要分两种情况讨论 执行sql前，没有开启事务 比如这样的伪代码： 12345@GlobalLockfun() &#123; sql1 sql2&#125; 这种情况可以不加FOR UPDATE。比如在sql1，如果检查到对应的全局锁已经被其他全局事务占用，会回滚并过一段时间再重试，直到超过了重试次数为止。 执行sql前，已经开启了开启事务 比如这样的伪代码： 12345678@GlobalLockfun() &#123; 代码1 @Transactional &#123; sql1 sql2 &#125;&#125; 这种情况是和上边的执行会有所不同。因为在执行sql1、sql2的时候不会进行全局锁的检查，但是，会继续添加前镜像和后镜像，直到最后发生了commit才会进行全局锁的检查，而且commit的执行流程和上边的又不同，如果对应的全局锁已经被占用了，会回滚本地事务释放锁，但不会重试，而是直接抛出错误！ 我是不明白为什么要设计成这样，这是看源码时发现的，源码版本是1.4.2。这种行为可以通过配置改变 1client.rm.lock.retryPolicyBranchRollbackOnConflict=true 默认就是true，也就是上边的行为，如果该为fale，只是行为相反了而已，改了还是没什么用。所以建议第一句sql语句是select for update。这样就能统一行为释放本地锁+重试了。而且这样也能避免了无谓的undo日志生成。 select for update在失败的时候，对于执行sql前，已经开启了开启事务这种情况，并不是直接Connection.rollback，而是执行了Connection.rollback(savepoint)。在这种情况在执行前会先创建一个savepoint，这样就能避免整个本地事务完全回滚了。 注意@GlobalLock是不负责错误回滚的，也就是说如果@GlobalLock的方法内只有一个事务，这种情况没什么问题。但是如果有多个事务的话，那就需要根据业务来判断是否需要把多个事务进行拆分了。 从执行过程和提交过程可以看出，既然开启全局事务 @GlobalTransactional注解可以在事务提交前，查询全局锁是否存在，那为什么 Seata 还要设计多处一个 @GlobalLock注解呢？ 因为并不是所有的数据库操作都需要开启全局事务，而开启全局事务是一个比较重的操作，需要向 TC 发起开启全局事务等 RPC 过程，而@GlobalLock注解只会在执行过程中查询全局锁是否存在，不会去开启全局事务，因此在不需要全局事务，而又需要检查全局锁避免脏读脏写时，使用@GlobalLock注解是一个更加轻量的操作。 Seata APIGlobalTransaction全局事务：包括开启事务、提交、回滚、获取当前状态等方法。 默认实现为DefaultGlobalTransaction 1234567891011121314151617181920212223242526272829303132333435363738public interface GlobalTransaction &#123; /** * 开启一个全局事务（使用默认的事务名和超时时间） */ void begin() throws TransactionException; /** * 开启一个全局事务，并指定超时时间（使用默认的事务名） */ void begin(int timeout) throws TransactionException; /** * 开启一个全局事务，并指定事务名和超时时间 */ void begin(int timeout, String name) throws TransactionException; /** * 全局提交 */ void commit() throws TransactionException; /** * 全局回滚 */ void rollback() throws TransactionException; /** * 获取事务的当前状态 */ GlobalStatus getStatus() throws TransactionException; /** * 获取事务的 XID */ String getXid();&#125; GlobalTransactionContextGlobalTransaction 实例的获取需要通过 GlobalTransactionContext： 123456789101112131415161718192021222324252627/** * 获取当前的全局事务实例，如果没有则创建一个新的实例。 */public static GlobalTransaction getCurrentOrCreate() &#123; GlobalTransaction tx = getCurrent(); if (tx == null) &#123; return createNew(); &#125; return tx;&#125;/** * 重新载入给定 XID 的全局事务实例，这个实例不允许执行开启事务的操作。 * 这个 API 通常用于失败的事务的后续集中处理。 * 比如：全局提交超时，后续集中处理通过重新载入该实例，通过实例方法获取事务当前状态，并根据状态判断是否需要重试全局提交操作。 */public static GlobalTransaction reload(String xid) throws TransactionException &#123; GlobalTransaction tx = new DefaultGlobalTransaction(xid, GlobalStatus.UnKnown, GlobalTransactionRole.Launcher) &#123; @Override public void begin(int timeout, String name) throws TransactionException &#123; throw new IllegalStateException(&quot;Never BEGIN on a RELOADED GlobalTransaction. &quot;); &#125; &#125;; return tx;&#125; TransactionalTemplate 对应的是@GlobalTransaction注解 事务化模板：通过上述 GlobalTransaction 和 GlobalTransactionContext API 把一个业务服务的调用包装成带有分布式事务支持的服务。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class TransactionalTemplate &#123; public Object execute(TransactionalExecutor business) throws TransactionalExecutor.ExecutionException &#123; // 1. 获取当前全局事务实例或创建新的实例 GlobalTransaction tx = GlobalTransactionContext.getCurrentOrCreate(); // 2. 开启全局事务 try &#123; tx.begin(business.timeout(), business.name()); &#125; catch (TransactionException txe) &#123; // 2.1 开启失败 throw new TransactionalExecutor.ExecutionException(tx, txe, TransactionalExecutor.Code.BeginFailure); &#125; Object rs = null; try &#123; // 3. 调用业务服务 rs = business.execute(); &#125; catch (Throwable ex) &#123; // 业务调用本身的异常 try &#123; // 全局回滚 tx.rollback(); // 3.1 全局回滚成功：抛出原始业务异常 throw new TransactionalExecutor.ExecutionException(tx, TransactionalExecutor.Code.RollbackDone, ex); &#125; catch (TransactionException txe) &#123; // 3.2 全局回滚失败： throw new TransactionalExecutor.ExecutionException(tx, txe, TransactionalExecutor.Code.RollbackFailure, ex); &#125; &#125; // 4. 全局提交 try &#123; tx.commit(); &#125; catch (TransactionException txe) &#123; // 4.1 全局提交失败： throw new TransactionalExecutor.ExecutionException(tx, txe, TransactionalExecutor.Code.CommitFailure); &#125; return rs; &#125;&#125; 上边的只是简化的代码和Spring提供的TransactionTemplate的逻辑基本一样。 它还支持事务传播。在事务传播上，这里管理资源是XID；而对于Spring的事物，管理的资源则是数据库链接对象 模板方法执行的异常：ExecutionException 123456789101112131415class ExecutionException extends Exception &#123; // 发生异常的事务实例 private GlobalTransaction transaction; // 异常编码： // BeginFailure（开启事务失败） // CommitFailure（全局提交失败） // RollbackFailure（全局回滚失败） // RollbackDone（全局回滚成功） private Code code; // 触发回滚的业务原始异常 private Throwable originalException;&#125; 外层调用逻辑 try-catch 这个异常，根据异常编码进行处理： BeginFailure （开启事务失败）：getCause() 得到开启事务失败的框架异常，getOriginalException() 为空。 CommitFailure （全局提交失败）：getCause() 得到全局提交失败的框架异常，getOriginalException() 为空。 RollbackFailure （全局回滚失败）：getCause() 得到全局回滚失败的框架异常，getOriginalException() 业务应用的原始异常。 RollbackDone （全局回滚成功）：getCause() 为空，getOriginalException() 业务应用的原始异常。 GlobalLockTemplate 对应@GlobalLock注解 RootContext事务的根上下文：负责在应用的运行时，维护 XID 。 12345678910111213141516171819202122232425262728293031323334/** * 得到当前应用运行时的全局事务 XID */public static String getXID() &#123; return CONTEXT_HOLDER.get(KEY_XID);&#125;/** * 将全局事务 XID 绑定到当前应用的运行时中 */public static void bind(String xid) &#123; if (LOGGER.isDebugEnabled()) &#123; LOGGER.debug(&quot;bind &quot; + xid); &#125; CONTEXT_HOLDER.put(KEY_XID, xid);&#125;/** * 将全局事务 XID 从当前应用的运行时中解除绑定，同时将 XID 返回 */public static String unbind() &#123; String xid = CONTEXT_HOLDER.remove(KEY_XID); if (LOGGER.isDebugEnabled()) &#123; LOGGER.debug(&quot;unbind &quot; + xid); &#125; return xid;&#125;/** * 判断当前应用的运行时是否处于全局事务的上下文中 */public static boolean inGlobalTransaction() &#123; return CONTEXT_HOLDER.get(KEY_XID) != null;&#125; 上边的实现都是基于 RootContext 中维护的 XID 来做的。 应用的当前运行的操作是否在一个全局事务的上下文中，就是看 RootContext 中是否有 XID。 RootContext 的默认实现是基于 ThreadLocal 的，即 XID 保存在当前线程上下文中。 该类有两个典型的应用场景： 远程调用事务上下文的传播 远程调用前获取当前 XID： 1String xid = RootContext.getXID(); 远程调用过程把 XID 也传递到服务提供方，在执行服务提供方的业务逻辑前，把 XID 绑定到当前应用的运行时： 1RootContext.bind(rpcXid); 事务的暂停和恢复 在一个全局事务中，如果需要某些业务逻辑不在全局事务的管辖范围内，则在调用前，把 XID 解绑： 1String unbindXid = RootContext.unbind(); 待相关业务逻辑执行完成，再把 XID 绑定回去，即可实现全局事务的恢复： 1RootContext.bind(unbindXid); TransactionHookManager管理TransactionHook实现的类，在分支事务开始前通过TransactionHookManager#registerHook添加TransactionHook实现 而TransactionHook会在分支事务的各个阶段执行 12345678910111213141516171819202122232425262728293031323334353637public interface TransactionHook &#123; /** * */ void beforeBegin(); /** * after tx begin */ void afterBegin(); /** * before tx commit */ void beforeCommit(); /** * after tx commit */ void afterCommit(); /** * before tx rollback */ void beforeRollback(); /** * after tx rollback */ void afterRollback(); /** * after tx all Completed */ void afterCompletion();&#125; 与Spring的事物API比较 DefaultGlobalTransaction作用和DataSourceTransactionManager一样 Seata的TransactionalTemplate和Spring的TransactionTemplate一样 更准确是和Spring的TransactionInterceptor逻辑一样，都包含了事务的基本处理逻辑和事物传播能力 TransactionHookManager+RootContext和Spring的TransactionSynchronizationManager功能一样 附录事务状态 (seata.io) Seata事务隔离","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"25-事务简介","slug":"springcloud/25-事务简介","date":"2021-11-27T12:00:33.000Z","updated":"2022-03-23T09:03:57.316Z","comments":true,"path":"blog/springcloud/25-事务简介/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/25-%E4%BA%8B%E5%8A%A1%E7%AE%80%E4%BB%8B/","excerpt":"","text":"事务(Transaction)是访问并可能更新数据库中各种数据项的一个程序执行单元(unit)。在关系数据库中，一个事务由一组SQL语句组成。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。 原子性（Atomicity） 要不全部成功，要不全部失败 隔离性（Isolation） 保证其它的状态转换不会影响到本次状态转换。 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 一致性（Consistency） 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则。 比如：银行存的金额大于0、转账时a转给了b100圆，那a减少100，b增加了100等 持久性（Durability） 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 &#x3D;&#x3D;数据库某些操作的原子性和隔离性都是保证一致性的一种手段，在操作执行完成后保证符合所有既定的约束则是一种结果&#x3D;&#x3D; 大多数场景下，我们的应用都只需要操作单一的数据库，这种情况下的事务称之为本地事务(Local Transaction)。本地事务的ACID特性是数据库直接提供支持。本地事务应用架构如下所示： 在JDBC编程中，开启一个事务有两个关键 同一个数据库链接 关闭自动提交 java.sql.Connection对象来开启、关闭或者提交事务。代码如下所示： 12345678910Connection conn = ... //获取数据库连接conn.setAutoCommit(false); //开启事务try&#123; //...执行增删改查sql conn.commit(); //提交事务&#125;catch (Exception e) &#123; conn.rollback();//事务回滚&#125;finally&#123; conn.close();//关闭链接&#125; 当下互联网发展如火如荼，绝大部分公司都进行了数据库拆分和服务化(SOA)。在这种情况下，完成某一个业务功能可能需要横跨多个服务，操作多个数据库。这就涉及到到了分布式事务，用需要操作的资源位于多个资源服务器上，而应用需要保证对于多个资源服务器的数据的操作，要么全部成功，要么全部失败。&#x3D;&#x3D;本质上来说，分布式事务就是为了保证不同资源服务器的数据一致性。&#x3D;&#x3D; DTP模型 构成DTP模型的5个基本元素： 应用程序(Application Program ，简称AP)：用于定义事务边界(即定义事务的开始和结束)，并且在事务边界内对资源进行操作。 资源管理器(Resource Manager，简称RM)：如数据库、文件系统等，并提供访问资源的方式。 事务管理器(Transaction Manager ，简称TM)：负责分配事务唯一标识，监控事务的执行进度，并负责事务的提交、回滚等。 通信资源管理器(Communication Resource Manager，简称CRM)：控制一个TM域(TM domain)内或者跨TM域的分布式应用之间的通信。 通信协议(Communication Protocol，简称CP)：提供CRM提供的分布式应用节点之间的底层通信服务。 XA规范 在DTP本地模型实例中，由AP、RMs和TM组成，不需要其他元素。AP、RM和TM之间，彼此都需要进行交互，如下图所示： 这张图中(1)表示AP-RM的交互接口，(2)表示AP-TM的交互接口，(3)表示RM-TM的交互接口。 XA规范的最主要的作用是，就是定义了RM-TM的交互接口，XA规范除了定义的RM-TM交互的接口(XA Interface)之外，还对两阶段提交协议进行了优化。 两阶段协议(two-phase commit)是在OSI TP标准中提出的；在DTP参考模型(&lt;&gt;)中，指定了全局事务的提交要使用two-phase commit协议；而XA规范(&lt;&lt; Distributed Transaction Processing: The XA Specification&gt;&gt;)只是定义了两阶段提交协议中需要使用到的接口，也就是上述提到的RM-TM交互的接口，因为两阶段提交过程中的参与方，只有TM和RMs。 阶段1： TM通知各个RM准备提交它们的事务分支。如果RM判断自己进行的工作可以被提交，那就对工作内容进行持久化，再给TM肯定答复；要是发生了其他情况，那给TM的都是否定答复。在发送了否定答复并回滚了已经的工作后，RM就可以丢弃这个事务分支信息。 以mysql数据库为例，在第一阶段，事务管理器向所有涉及到的数据库服务器发出prepare”准备提交”请求，数据库收到请求后执行数据修改和日志记录等处理，处理完成后只是把事务的状态改成”可以提交”,然后把结果返回给事务管理器。 阶段2 TM根据阶段1各个RM prepare的结果，决定是提交还是回滚事务。如果所有的RM都prepare成功，那么TM通知所有的RM进行提交；如果有RM prepare失败的话，则TM通知所有RM回滚自己的事务分支。 以mysql数据库为例，如果第一阶段中所有数据库都prepare成功，那么事务管理器向数据库服务器发出”确认提交”请求，数据库服务器把事务的”可以提交”状态改为”提交完成”状态，然后返回应答。如果在第一阶段内有任何一个数据库的操作发生了错误，或者事务管理器收不到某个数据库的回应，则认为事务失败，回撤所有数据库的事务。数据库服务器收不到第二阶段的确认提交请求，也会把”可以提交”的事务回撤。 XA协议有2PC的缺点外，还有一个影响性能的问题，那就是在第一阶段中，资源是锁定的，也就是说本地事务是一直开启着。 TCCTCC 是 “Try”、“Confirm”、“Cancel” 的缩写。 Try：尝试执行业务，这个阶段会完成所有业务的检查，预留必须的业务资源。比如： 123假如：原本库存是100，每次减少10线程Atry 库存还是100 剩下资源90 锁定的资源10，库存不变，更新后面两个信息，更新完后事务结束线程Btry 库存还是100 剩下资源80 锁定的资源20，库存不变，更新后面两个信息，更新完后事务结束 也就是说，经过try的时候不会真正的执行业务逻辑，只是预留这一次操作的资源。 Confirm：确认执行业务。这个阶段不需要做任何检查，真正执行业务，并且使用 Try 阶段预留的资源。Confirm 阶段的操作需要满足幂等性。 就是真正的释放资源了 Cancel：取消执行业务。如果 Try 阶段有任何一个业务执行失败，这会进入 Cancel 流程。本阶段主要回滚 Try 阶段锁定的资源。 在 TCC 模型执行的过程中，还可能会出现各种异常，其中最为常见的有空回滚、幂等、悬挂 空回滚 空回滚指的是在一个分布式事务中，在没有调用参与方的 Try 方法的情况下，TM 驱动二阶段回滚调用了参与方的 Cancel 方法。 如上图所示，全局事务开启后，参与者 A 分支注册完成之后会执行参与者一阶段 RPC 方法，如果此时参与者 A 所在的机器发生宕机，网络异常，都会造成 RPC 调用失败，即参与者 A 一阶段方法未成功执行，但是此时全局事务已经开启，Seata 必须要推进到终态，在全局事务回滚时会调用参与者 A 的 Cancel 方法，从而造成空回滚。 幂等 如上图所示，参与者 A 执行完二阶段之后，由于网络抖动或者宕机问题，会造成 TC 收不到参与者 A 执行二阶段的返回结果，TC 会重复发起调用，直到二阶段执行结果成功。 悬挂 悬挂指的是二阶段 Cancel 方法比 一阶段 Try 方法优先执行，由于允许空回滚的原因，在执行完二阶段 Cancel 方法之后直接空回滚返回成功，此时全局事务已结束，但是由于 Try 方法随后执行，这就会造成一阶段 Try 方法预留的资源永远无法提交和释放了。 如上图所示，在执行参与者 A 的一阶段 Try 方法时，出现网路拥堵，由于 Seata 全局事务有超时限制，执行 Try 方法超时后，TM 决议全局回滚，回滚完成后如果此时 RPC 请求才到达参与者 A，执行 Try 方法进行资源预留，从而造成悬挂。 TCC 的所有操作都在业务层面完成，不依赖于底层的 RPC 框架，这是优点。但是，也因此会对业务造成很大的入侵性。对现有业务逻辑的改造成本会比较大，全部需要改造为 TCC 对应的阶段模式、幂等。特别是 Cancel 阶段，需要业务编写额外的回滚代码。 TCC和XA XA是资源层面的分布式事务，强一致性，在两阶段提交的整个过程中，一直会持有资源的锁。 TCC是业务层面的分布式事务，最终一致性，不会一直持有资源的锁。而且还有非常重要的一点，就是TCC不局限于数据库，因为TCC的所有操作都是在业务层面完成，通过业务代码来保证一致 Saga 方案Saga 也是最终一致性的柔性事务。Saga 的实现方式是，把一个长事务，拆分成多个子事务执行，每个子事务都有相应的执行模块和回滚模块。 当事务正常执行时，会 T1、T2、T3… 顺序执行下去。当出现异常的时候，则每个子事务分别调用自己的回滚模块，直到第一个子事务也回滚完成，从而达到最终一致性。由于是最终一致，因此也会要求每个服务的幂等。 Saga 方案和 TCC 比较相似，对代码也有很强的入侵性。由于是柔性事务，也会存在和可靠消息方案中的问题，如：同时操作一个资源会有数据一致性问题；同一个事务中如果数据被其他事务修改，会有回滚失败的风险。 基于可靠消息的方案基于可靠消息的分布式事务方案是一种异步柔性事务，使用的是最终一致性，因此不能保证数据的强一致性。 该方案主要是通过一个中间消息管理服务，来保证消息往下一个服务传递，从而让事务继续传递。可靠消息只能保证消息不丢失，但是无法保证消息只传递一次，因此下游服务必须要实现幂等。 流程如下： 可靠消息的的吞吐量明显会优于同步事务。但是缺点也十分明显，首先是没有资源的锁定，当消息到达某个服务可能业务无法执行下去。其次，当下游事务失败，需要整体事务回滚时，有可能前面的数据已经被改变，无法回滚。 还是以一开始的业务场景为例，服务 A 先增加了积分，然后服务 B 进行扣款，发现余额不足，但是此时积分已经被用户使用，导致无法回滚。 该事务模型适用于流水线式的业务模型，并且各数据之间不会有关联，避免出现脏数据。 RocketMQ事务消息流程概要下面看RocketMq的事务消息： RocketMQ采用了2PC的思想来实现了提交事务消息，同时增加一个补偿逻辑来处理二阶段超时或者失败的消息，如下图所示。 上图说明了事务消息的大致方案，其中分为两个流程：正常事务消息的发送及提交、事务消息的补偿流程。 事务消息发送及提交： (1) 发送消息（half消息）。 (2) 服务端响应消息写入结果。 (3) 根据发送结果执行本地事务（如果写入失败，此时half消息对业务不可见，本地逻辑不执行）。 (4) 根据本地事务状态执行Commit或者Rollback（Commit操作生成消息索引，消息对消费者可见） 补偿流程： (1) 对没有Commit&#x2F;Rollback的事务消息（pending状态的消息），从服务端发起一次“回查” (2) Producer收到回查消息，检查回查消息对应的本地事务的状态 (3) 根据本地事务状态，重新Commit或者Rollback 其中，补偿阶段用于解决消息Commit或者Rollback发生超时或者失败的情况。 整个流程如下： 发送的消息对消费者不可见，直到本地事物执行成功了，消费者才能消费到该消息。","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"24-Spring Cloud中如何实现SSO","slug":"springcloud/24-Spring Cloud中如何实现SSO","date":"2021-11-27T12:00:32.000Z","updated":"2022-03-23T09:03:57.305Z","comments":true,"path":"blog/springcloud/24-Spring Cloud中如何实现SSO/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/24-Spring%20Cloud%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0SSO/","excerpt":"","text":"架构设计分析 微服务接入网关实现单点登录设计思路： 网关整合 OAuth2.0 有两种思路，一种是授权服务器生成令牌, 所有请求统一在网关层验证，判断权限等操作；另一种是由各资源服务处理，网关只做请求转发。 比较常用的是第一种，把API网关作为OAuth2.0的资源服务器角色，实现接入客户端权限拦截、令牌解析并转发当前登录用户信息给微服务，这样下游微服务就不需要关心令牌格式解析以及OAuth2.0相关机制了。 网关在认证授权体系里主要负责两件事： 作为OAuth2.0的资源服务器角色，实现接入方访问权限拦截。 令牌解析并转发当前登录用户信息（明文token）给微服务 微服务拿到明文token(明文token中包含登录用户的身份和权限信息)后也需要做两件事： 用户授权拦截（看当前用户是否有权访问该资源） 将用户信息存储进当前线程上下文（有利于后续业务逻辑随时获取当前用户信息） 授权服务使用前边的就好了 授权服务需要开启密码模式 核心代码，网关自定义全局过滤器进行身份认证 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586@Component@Order(0)public class AuthenticationFilter implements GlobalFilter, InitializingBean &#123; @Autowired private RestTemplate restTemplate; private static Set&lt;String&gt; shouldSkipUrl = new LinkedHashSet&lt;&gt;(); @Override public void afterPropertiesSet() throws Exception &#123; // 不拦截认证的请求 shouldSkipUrl.add(&quot;/oauth/token&quot;); shouldSkipUrl.add(&quot;/oauth/check_token&quot;); shouldSkipUrl.add(&quot;/user/getCurrentUser&quot;); &#125; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; String requestPath = exchange.getRequest().getURI().getPath(); //不需要认证的url if(shouldSkip(requestPath)) &#123; return chain.filter(exchange); &#125; //获取请求头 String authHeader = exchange.getRequest().getHeaders().getFirst(&quot;Authorization&quot;); //请求头为空 if(StringUtils.isEmpty(authHeader)) &#123; // 返回正确信息给前端 throw new RuntimeException(&quot;请求头为空&quot;); &#125; TokenInfo tokenInfo=null; try &#123; //获取token信息 tokenInfo = getTokenInfo(authHeader); &#125;catch (Exception e) &#123; throw new RuntimeException(&quot;校验令牌异常&quot;); &#125; // tokenInfo exchange.getAttributes().put(&quot;tokenInfo&quot;,tokenInfo); return chain.filter(exchange); &#125; private boolean shouldSkip(String reqPath) &#123; for(String skipPath:shouldSkipUrl) &#123; if(reqPath.contains(skipPath)) &#123; return true; &#125; &#125; return false; &#125; private TokenInfo getTokenInfo(String authHeader) &#123; // 往授权服务发请求 /oauth/check_token // 获取token的值 String token = StringUtils.substringAfter(authHeader, &quot;bearer &quot;); HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_FORM_URLENCODED); //必须 basicAuth clienId clientSecret headers.setBasicAuth(MDA.clientId, MDA.clientSecret); MultiValueMap&lt;String, String&gt; params = new LinkedMultiValueMap&lt;&gt;(); params.add(&quot;token&quot;, token); HttpEntity&lt;MultiValueMap&lt;String, String&gt;&gt; entity = new HttpEntity&lt;&gt;(params, headers); ResponseEntity&lt;TokenInfo&gt; response = restTemplate.exchange(MDA.checkTokenUrl, HttpMethod.POST, entity, TokenInfo.class); return response.getBody(); &#125;&#125;public class MDA &#123; public static final String clientId = &quot;gateway-server&quot;; public static final String clientSecret = &quot;123123&quot;; public static final String checkTokenUrl = &quot;http://auth-server/oauth/check_token&quot;;&#125; 网关自定义全局过滤器进行权限校验： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@Component@Order(1)public class AuthorizationFilter implements GlobalFilter, InitializingBean &#123; private static Set&lt;String&gt; shouldSkipUrl = new LinkedHashSet&lt;&gt;(); @Override public void afterPropertiesSet() throws Exception &#123; // 不拦截认证的请求 shouldSkipUrl.add(&quot;/oauth/token&quot;); shouldSkipUrl.add(&quot;/oauth/check_token&quot;); shouldSkipUrl.add(&quot;/user/getCurrentUser&quot;); &#125; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; String requestPath = exchange.getRequest().getURI().getPath(); //不需要认证的url if(shouldSkip(requestPath)) &#123; return chain.filter(exchange); &#125; TokenInfo tokenInfo = exchange.getAttribute(&quot;tokenInfo&quot;); if(!tokenInfo.isActive()) &#123; throw new RuntimeException(&quot;token过期&quot;); &#125; hasPremisson(tokenInfo,requestPath); return chain.filter(exchange); &#125; private boolean shouldSkip(String reqPath) &#123; for(String skipPath:shouldSkipUrl) &#123; if(reqPath.contains(skipPath)) &#123; return true; &#125; &#125; return false; &#125; private boolean hasPremisson(TokenInfo tokenInfo,String currentUrl) &#123; boolean hasPremisson = false; //登录用户的权限集合判断 List&lt;String&gt; premessionList = Arrays.asList(tokenInfo.getAuthorities()); for (String url: premessionList) &#123; if(currentUrl.contains(url)) &#123; hasPremisson = true; break; &#125; &#125; if(!hasPremisson)&#123; throw new RuntimeException(&quot;没有权限&quot;); &#125; return hasPremisson; &#125;&#125;","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"23-Spring Security Oauth2整合JWT实现授权服务","slug":"springcloud/23-Spring Security Oauth2整合JWT实现授权服务","date":"2021-11-27T12:00:31.000Z","updated":"2022-03-23T09:03:57.304Z","comments":true,"path":"blog/springcloud/23-Spring Security Oauth2整合JWT实现授权服务/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/23-Spring%20Security%20Oauth2%E6%95%B4%E5%90%88JWT%E5%AE%9E%E7%8E%B0%E6%8E%88%E6%9D%83%E6%9C%8D%E5%8A%A1/","excerpt":"","text":"JSON Web Token（JWT）是一个开放的行业标准（RFC 7519），它定义了一种简介的、自包含的协议格式，用于在通信双方传递json对象，传递的信息经过数字签名可以被验证和信任。JWT可以使用HMAC算法或使用RSA的公钥&#x2F;私钥对来签名，防止被篡改。 能携带一些json格式信息 可以使用HMAC算法或使用RSA的公钥&#x2F;私钥对来签名 官网： https://jwt.io/ 标准： https://tools.ietf.org/html/rfc7519 JWT令牌的优点： jwt基于json，非常方便解析。 可以在令牌中自定义丰富的内容，易扩展。 通过非对称加密算法及数字签名技术，JWT防止篡改，安全性高。 资源服务使用JWT可不依赖授权服务即可完成授权。 缺点： ​ JWT令牌较长，占存储空间比较大。 JWT组成一个JWT实际上就是一个字符串，它由三部分组成，头部（header）、载荷（payload）与签名（signature）。 头部（header）头部用于描述关于该JWT的最基本的信息：类型（即JWT）以及签名所用的算法（如HMACSHA256或RSA）等。 这也可以被表示成一个JSON对象： 1234&#123; &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;&#125; 然后将头部进行base64编码，构成了第一部分: 1eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9 载荷（payload）第二部分是载荷，就是存放有效信息的地方。这些有效信息包含三个部分： 标准中注册的声明（建议但不强制使用） iss: jwt签发者 sub: jwt所面向的用户 aud: 接收jwt的一方 exp: jwt的过期时间，这个过期时间必须要大于签发时间 nbf: 定义在什么时间之前，该jwt都是不可用的. iat: jwt的签发时间 jti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。 公共的声明 公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.但不建议添加敏感信息，因为该部分在客户端可解密. 私有的声明 私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64是对称解密的，意味着该部分信息可以归类为明文信息。 12345&#123; &quot;sub&quot;: &quot;1234567890&quot;, &quot;name&quot;: &quot;John Doe&quot;, &quot;iat&quot;: 1516239022&#125; 然后将其进行base64编码，得到Jwt的第二部分: 1eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ 签名（signature）jwt的第三部分是一个签证信息，这个签证信息由三部分组成： header (base64后的) payload (base64后的) secret(盐，一定要保密） 这个部分需要base64加密后的header和base64加密后的payload使用”.”连接组成的字符串，然后通过header中声明的加密方式进行加盐secret组合加密，然后就构成了jwt的第三部分: 12var encodedString = base64UrlEncode(header) + &#x27;.&#x27; + base64UrlEncode(payload);var signature = HMACSHA256(encodedString, &#x27;xyz&#x27;); // xKfoLM2mu__s7OvZKiyA63EkmUFLgybVmcDTV7kAS0Y 的到第三部分 1xKfoLM2mu__s7OvZKiyA63EkmUFLgybVmcDTV7kAS0Y 将这三部分用.连接成一个完整的字符串,构成了最终的jwt: 1eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.xKfoLM2mu__s7OvZKiyA63EkmUFLgybVmcDTV7kAS0Y 注意： 3部分中前两部分只是进行了base64编码，所以不能放敏感信息。第三部分是经过加密的。 secret是保存在服务器端的，jwt的签发生成也是在服务器端的，secret就是用来进行jwt的签发和jwt的验证，所以，它就是你服务端的私钥，在任何场景都不应该流露出去。一旦客户端得知这个secret, 那就意味着客户端是可以自我签发jwt了。 如何应用一般是在请求头里加入Authorization，并加上Bearer标注： 12345fetch(&#x27;api/user/1&#x27;, &#123; headers: &#123; &#x27;Authorization&#x27;: &#x27;Bearer &#x27; + token &#125;&#125;) 服务端会验证token，如果验证通过就会返回相应的资源。整个流程就是这样的: 使用JJWTJJWT是一个提供端到端的JWT创建和验证的Java库，永远免费和开源(Apache License，版本2.0)。JJW很容易使用和理解。它被设计成一个以建筑为中心的流畅界面，隐藏了它的大部分复杂性。 引入依赖 123456 &lt;!--JWT依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.9.1&lt;/version&gt;&lt;/dependency&gt; 创建token 12345678910111213141516171819202122232425262728293031@Testpublic void test() &#123; //创建一个JwtBuilder对象 JwtBuilder jwtBuilder = Jwts.builder() //声明的标识&#123;&quot;jti&quot;:&quot;666&quot;&#125; .setId(&quot;666&quot;) //主体，用户&#123;&quot;sub&quot;:&quot;Fox&quot;&#125; .setSubject(&quot;xyz&quot;) //创建日期&#123;&quot;ita&quot;:&quot;xxxxxx&quot;&#125; .setIssuedAt(new Date()) //设置过期时间 1分钟 .setExpiration(new Date(System.currentTimeMillis() + 60*1000)) //自定义claims，传自己的参数 //直接传入map // .addClaims(map) .claim(&quot;roles&quot;,&quot;admin&quot;) .claim(&quot;logo&quot;,&quot;xxx.jpg&quot;) //签名手段，参数1：算法，参数2：盐 .signWith(SignatureAlgorithm.HS256, SECRETKEY); //获取token jwt String token = jwtBuilder.compact(); System.out.println(token); //三部分的base64解密 System.out.println(&quot;=========&quot;); String[] split = token.split(&quot;\\\\.&quot;); System.out.println(Base64Codec.BASE64.decodeToString(split[0])); System.out.println(Base64Codec.BASE64.decodeToString(split[1])); //base64无法解密 System.out.println(Base64Codec.BASE64.decodeToString(split[2]));&#125; 验证token 1234567891011121314151617181920212223@Testpublic void testParseToken()&#123; //token String token =&quot;eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI2NjYiLCJzdWIiOiJ4eXoiLCJpYXQiOjE2NDY5MTg1MDMsImV4cCI6MTY0NjkxODU2Mywicm9sZXMiOiJhZG1pbiIsImxvZ28iOiJ4eHguanBnIn0.V1kiZFUo8wJ-b3A2v1g_3a8vBaDoOhvoIVa8vVSYRkg&quot;; //解析token获取载荷中的声明对象 Claims claims = Jwts.parser() .setSigningKey(SECRETKEY) .parseClaimsJws(token) .getBody(); System.out.println(&quot;id:&quot;+claims.getId()); System.out.println(&quot;subject:&quot;+claims.getSubject()); System.out.println(&quot;issuedAt:&quot;+claims.getIssuedAt()); DateFormat sf =new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); System.out.println(&quot;签发时间:&quot;+sf.format(claims.getIssuedAt())); System.out.println(&quot;过期时间:&quot;+sf.format(claims.getExpiration())); System.out.println(&quot;当前时间:&quot;+sf.format(new Date())); System.out.println(&quot;roles:&quot;+claims.get(&quot;roles&quot;)); System.out.println(&quot;logo:&quot;+claims.get(&quot;logo&quot;));&#125; 验证成功： 失败： 过期： 串改token： 对于这种跑错的，只需要捕捉JwtException 所以解析token也就是验证token Spring Security Oauth2整合JWT引入依赖 1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-jwt&lt;/artifactId&gt; &lt;version&gt;1.1.1.RELEASE&lt;/version&gt;&lt;/dependency&gt; &lt;!--JWT依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.9.1&lt;/version&gt;&lt;/dependency&gt; 完整的配置类 JwtTokenEnhancer： jwt内容增强 123456789101112131415161718192021/** * JWT内容增强器 * 根据自己业务添加字段到Jwt中。 */public class JwtTokenEnhancer implements TokenEnhancer &#123; @Override public OAuth2AccessToken enhance(OAuth2AccessToken accessToken, OAuth2Authentication authentication) &#123; User memberDetails = (User) authentication.getPrincipal(); Map&lt;String, Object&gt; additionalInfo = new HashMap&lt;&gt;(); additionalInfo.put(&quot;userName&quot;, memberDetails.getUsername()); additionalInfo.put(&quot;enhance&quot;, &quot;enhance info&quot;); Map&lt;String, Object&gt; retMap = new HashMap&lt;&gt;(); retMap.put(&quot;additionalInfo&quot;, additionalInfo); ((DefaultOAuth2AccessToken) accessToken).setAdditionalInformation(retMap); return accessToken; &#125;&#125; UserService：用来获取用户信息 123456789101112131415/*** 获取用户信息的，可以从member微服务或数据库获取*/@Servicepublic class UserService implements UserDetailsService &#123; @Autowired private PasswordEncoder passwordEncoder; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; String password = passwordEncoder.encode(&quot;123456&quot;); return new User(&quot;fox&quot;,password, AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;admin&quot;)); &#125;&#125; JwtTokenStoreConfig：jwt配置 1234567891011121314151617181920212223@Configurationpublic class JwtTokenStoreConfig &#123; @Bean public TokenStore jwtTokenStore()&#123; return new JwtTokenStore(jwtAccessTokenConverter()); &#125; @Bean public JwtAccessTokenConverter jwtAccessTokenConverter()&#123; JwtAccessTokenConverter accessTokenConverter = new JwtAccessTokenConverter(); // 配置JWT使用的秘钥 // 建议使用非对称加密 accessTokenConverter.setSigningKey(&quot;123123&quot;); return accessTokenConverter; &#125; @Bean public JwtTokenEnhancer jwtTokenEnhancer() &#123; return new JwtTokenEnhancer(); &#125;&#125; AuthorizationServerConfig：在授权服务器配置中指定令牌的存储策略为JWT 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113@Configuration@EnableAuthorizationServerpublic class AuthorizationServerConfig extends AuthorizationServerConfigurerAdapter &#123; @Autowired private PasswordEncoder passwordEncoder; @Autowired private AuthenticationManager authenticationManagerBean; // 获取用户信息 @Autowired private UserService userService; @Autowired private JwtAccessTokenConverter jwtAccessTokenConverter; @Autowired private JwtTokenEnhancer jwtTokenEnhancer; @Resource(name = &quot;jwtTokenStore&quot;) private TokenStore tokenStore; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; // 配置JWT的内容增强器 TokenEnhancerChain enhancerChain = new TokenEnhancerChain(); List&lt;TokenEnhancer&gt; delegates = new ArrayList&lt;&gt;(); delegates.add(jwtTokenEnhancer); delegates.add(jwtAccessTokenConverter); enhancerChain.setTokenEnhancers(delegates); endpoints.authenticationManager(authenticationManagerBean) //使用密码模式需要配置 .tokenStore(tokenStore) //配置存储令牌策略 .accessTokenConverter(jwtAccessTokenConverter) .tokenEnhancer(enhancerChain) //配置tokenEnhancer .reuseRefreshTokens(false) //refresh_token是否重复使用 .userDetailsService(userService) //刷新令牌授权包含对用户信息的检查 .allowedTokenEndpointRequestMethods(HttpMethod.GET,HttpMethod.POST); //支持GET,POST请求 &#125; /** * 授权服务器安全配置 * @param security * @throws Exception */ @Override public void configure(AuthorizationServerSecurityConfigurer security) throws Exception &#123; // 允许表单认证 security.allowFormAuthenticationForClients() // 第三方客户端校验token需要带入 clientId 和clientSecret来校验 .checkTokenAccess(&quot;isAuthenticated()&quot;) // 来获取我们的tokenKey需要带入clientId,clientSecret，单点登录需要 .tokenKeyAccess(&quot;isAuthenticated()&quot;); &#125; // 配置信任的客户段，建议使用jdbc @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; /** *授权码模式 *http://localhost:8080/oauth/authorize?response_type=code&amp;client_id=client&amp;redirect_uri=http://localhost:8081/login&amp;scope=all *http://localhost:8080/oauth/authorize?response_type=code&amp;client_id=client * * password模式 * http://localhost:8080/oauth/token?username=fox&amp;password=123456&amp;grant_type=password&amp;client_id=client&amp;client_secret=123123&amp;scope=all * * 客户端模式 * http://localhost:8080/oauth/token?grant_type=client_credentials&amp;scope=all&amp;client_id=client&amp;client_secret=123123 * * 刷新令牌 * http://localhost:8080/oauth/token?grant_type=refresh_token&amp;client_id=client&amp;client_secret=123123&amp;refresh_token=[refresh_token值] */ clients.inMemory() //配置client_id .withClient(&quot;client&quot;) //配置client-secret .secret(passwordEncoder.encode(&quot;123123&quot;)) //配置访问token的有效期 .accessTokenValiditySeconds(3600) //配置刷新token的有效期 .refreshTokenValiditySeconds(864000) //配置redirect_uri，用于授权成功后跳转 .redirectUris(&quot;http://localhost:8081/login&quot;, &quot;http://localhost:8082/login&quot;) //自动授权配置 .autoApprove(true) //配置申请的权限范围 .scopes(&quot;all&quot;) /** * 配置grant_type，表示授权类型 * authorization_code: 授权码 * password： 密码 * client_credentials: 客户端 * refresh_token: 更新令牌 */ .authorizedGrantTypes(&quot;authorization_code&quot;,&quot;password&quot;,&quot;refresh_token&quot;); &#125; // @Override// public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123;// clients.withClientDetails(clientDetailsService())// &#125;//// @Autowired// private DataSource dataSource;//// // 使用jdbc存储// @Bean// public ClientDetailsService clientDetailsService()&#123;// return new JdbcClientDetailsService(dataSource);// &#125;&#125; WebSecurityConfig： 12345678910111213141516171819202122232425262728293031323334353637383940@Configurationpublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; // 获取用户信息 @Autowired private UserService userService; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth.userDetailsService(userService); &#125; @Bean public PasswordEncoder passwordEncoder()&#123; return new BCryptPasswordEncoder(); &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.formLogin().permitAll() .and().authorizeRequests() .antMatchers(&quot;/oauth/**&quot;).permitAll() .anyRequest().authenticated() .and().logout().permitAll() .and().csrf().disable(); &#125; @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception &#123; return super.authenticationManagerBean(); &#125; @Override public void configure(WebSecurity web) throws Exception &#123; web.ignoring().antMatchers(&quot;/assets/**&quot;, &quot;/css/**&quot;, &quot;/images/**&quot;); &#125;&#125; 测试： 12345678910111213141516171819202122232425/*** 配置资源，这里的意思是全部请求都需要授权。用于测试的*/@Configuration@EnableResourceServerpublic class TulingResourceServerConfig extends ResourceServerConfigurerAdapter &#123; @Override public void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests() .anyRequest().authenticated(); &#125;&#125;@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @RequestMapping(&quot;/getCurrentUser&quot;) public Object getCurrentUser(Authentication authentication) &#123; return authentication.getPrincipal(); &#125;&#125; 优化：实现JWT非对称加密上边的整合时使用了对称加密，而在生产中建议使用非对称加密啊 生成jks 证书文件使用keytool生成 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647➜ service keytool -genkeypair -helpkeytool -genkeypair [OPTION]...生成密钥对选项: -alias &lt;alias&gt; 要处理的条目的别名 -keyalg &lt;keyalg&gt; 密钥算法名称 -keysize &lt;keysize&gt; 密钥位大小，太少的话不安全，太大的话解密太慢 -sigalg &lt;sigalg&gt; 签名算法名称 -destalias &lt;destalias&gt; 目标别名 -dname &lt;dname&gt; 唯一判别名 -startdate &lt;startdate&gt; 证书有效期开始日期/时间 -ext &lt;value&gt; X.509 扩展 -validity &lt;valDays&gt; 有效天数 -keypass &lt;arg&gt; 密钥口令 -keystore &lt;keystore&gt; 密钥库名称 -storepass &lt;arg&gt; 密钥库口令 -storetype &lt;storetype&gt; 密钥库类型 -providername &lt;providername&gt; 提供方名称 -providerclass &lt;providerclass&gt; 提供方类名 -providerarg &lt;arg&gt; 提供方参数 -providerpath &lt;pathlist&gt; 提供方类路径 -v 详细输出 -protected 通过受保护的机制的口令➜ service keytool -genkeypair -alias jwt -keyalg RSA -keysize 2048 -keystore ./jwt.jks输入密钥库口令:再次输入新口令:您的名字与姓氏是什么? [Unknown]: xyz您的组织单位名称是什么? [Unknown]: xyz您的组织名称是什么? [Unknown]: xyz您所在的城市或区域名称是什么? [Unknown]: gd您所在的省/市/自治区名称是什么? [Unknown]: gd该单位的双字母国家/地区代码是什么? [Unknown]: cnCN=xyz, OU=xyz, O=xyz, L=gd, ST=gd, C=cn是否正确? [否]: y输入 &lt;jwt&gt; 的密钥口令 (如果和密钥库口令相同, 按回车): 将生成的jwt.jks文件cope到授权服务器的resource目录下 使用keytool查看公钥信息： 1keytool -list -rfc --keystore jwt.jks | openssl x509 -inform pem -pubkey 授权服务中增加jwt的属性配置类12345678910111213141516171819202122232425@Getter@Setter@ConfigurationProperties(prefix = &quot;auth2.jwt&quot;)public class JwtCAProperties &#123; /** * 证书名称 */ private String keyPairName; /** * 证书别名 */ private String keyPairAlias; /** * 证书私钥 */ private String keyPairSecret; /** * 证书存储密钥 */ private String keyPairStoreSecret;&#125; yml中添加jwt配置 123456auth2: jwt: keyPairName: jwt.jks keyPairAlias: jwt keyPairSecret: 123456 keyPairStoreSecret: 123456 修改JwtTokenStoreConfig的配置，支持非对称加密12345678910111213141516171819202122232425262728293031323334353637@Configuration@EnableConfigurationProperties(value = JwtCAProperties.class)public class JwtTokenStoreConfig &#123; @Autowired private JwtCAProperties jwtCAProperties; @Bean public KeyPair keyPair() &#123; KeyStoreKeyFactory keyStoreKeyFactory = new KeyStoreKeyFactory(new ClassPathResource(jwtCAProperties.getKeyPairName()), jwtCAProperties.getKeyPairSecret().toCharArray()); return keyStoreKeyFactory.getKeyPair(jwtCAProperties.getKeyPairAlias(), jwtCAProperties.getKeyPairStoreSecret().toCharArray()); &#125; @Bean public TokenStore jwtTokenStore()&#123; return new JwtTokenStore(jwtAccessTokenConverter()); &#125; @Bean public JwtAccessTokenConverter jwtAccessTokenConverter()&#123; JwtAccessTokenConverter accessTokenConverter = new JwtAccessTokenConverter(); // 配置JWT使用的秘钥 // accessTokenConverter.setSigningKey(&quot;123123&quot;); // 配置JWT使用的秘钥 非对称加密 accessTokenConverter.setKeyPair(keyPair()); return accessTokenConverter; &#125; @Bean public JwtTokenEnhancer jwtTokenEnhancer() &#123; return new JwtTokenEnhancer(); &#125;&#125;","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"22-微服务安全Spring Security OAuth2","slug":"springcloud/22-微服务安全Spring Security OAuth2","date":"2021-11-27T12:00:30.000Z","updated":"2022-03-23T09:03:57.294Z","comments":true,"path":"blog/springcloud/22-微服务安全Spring Security OAuth2/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/22-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AE%89%E5%85%A8Spring%20Security%20OAuth2/","excerpt":"","text":"OAuth2.0介绍OAuth协议：https://tools.ietf.org/html/rfc6749 OAuth（Open Authorization）是一个关于&#x3D;&#x3D;授权（authorization）的开放网络标准&#x3D;&#x3D;，允许用户授权第三方应用访问他们存储在另外的服务提供者上的信息，而不需要将用户名和密码提供给第三方移动应用或分享他们数据的所有内容。OAuth在全世界得到广泛应用，目前的版本是2.0版。 协议特点： 简单：不管是OAuth服务提供者还是应用开发者，都很易于理解与使用； 安全：没有涉及到用户密钥等信息，更安全更灵活； 开放：任何服务提供商都可以实现OAuth，任何软件开发商都可以使用OAuth； 应用场景 原生app授权：app登录请求后台接口，为了安全认证，所有请求都带token信息 前后端分离单页面应用：前后端分离框架，前端请求后台数据，需要进行oauth2安全认证 第三方应用授权登录，比如QQ，微博，微信的授权登录。 有一个”云冲印”的网站，可以将用户储存在Google的照片，冲印出来。用户为了使用该服务，必须让”云冲印”读取自己储存在Google上的照片。只有得到用户的授权，Google才会同意”云冲印”读取这些照片。那么，”云冲印”怎样获得用户的授权呢？ 传统方法是，用户将自己的Google用户名和密码，告诉”云冲印”，后者就可以读取用户的照片了。这样的做法有以下几个严重的缺点： “云冲印”为了后续的服务，会保存用户的密码，这样很不安全。 Google不得不部署密码登录，而我们知道，单纯的密码登录并不安全。 “云冲印”拥有了获取用户储存在Google所有资料的权力，用户没法限制”云冲印”获得授权的范围和有效期。 用户只有修改密码，才能收回赋予”云冲印”的权力。但是这样做，会使得其他所有获得用户授权的第三方应用程序全部失效。 只要有一个第三方应用程序被破解，就会导致用户密码泄漏，以及所有被密码保护的数据泄漏。 OAuth2.0认证例子以京东通过微信登录为例： 下面是微信OAuth2.0授权登录流程： 基本概念 Third-party application：第三方应用程序，又称”客户端”（client），即例子中京东。 HTTP service：HTTP服务提供商，简称”服务提供商”，即例子中的微信开放平台。 Resource Owner：资源所有者，又称”用户”（user）。 User Agent：用户代理，比如浏览器。 Authorization server：授权服务器，即服务提供商专门用来处理认证授权的服务器。 Resource server：资源服务器，即服务提供商存放用户生成的资源的服务器。它与授权服务器，可以是同一台服务器，也可以是不同的服务器。OAuth的作用就是让”客户端”安全可控地获取”用户”的授权，与”服务提供商”进行交互。 优缺点优点： 更安全，客户端不接触用户密码，服务器端更易集中保护 广泛传播并被持续采用 短寿命和封装的token 资源服务器和授权服务器解耦 集中式授权，简化客户端 HTTP&#x2F;JSON友好，易于请求和传递token 考虑多种客户端架构场景 客户可以具有不同的信任级别 缺点： 协议框架太宽泛，造成各种实现的兼容性和互操作性差 是一个认证协议，本身并不能告诉你任何用户信息 OAuth2的设计思路OAuth在”客户端”与”服务提供商”之间，设置了一个授权层（authorization layer）。”客户端”不能直接登录”服务提供商”，只能登录授权层，以此将用户与客户端区分开来。”客户端”登录授权层所用的令牌（token），与用户的密码不同。用户可以在登录的时候，指定授权层令牌的权限范围和有效期，”客户端”登录授权层以后，”服务提供商”根据令牌的权限范围和有效期，向”客户端”开放用户储存的资料。 OAuth 2.0的运行流程如下图，摘自RFC 6749： （A）用户打开客户端以后，客户端要求用户给予授权。 （B）用户同意给予客户端授权。 （C）客户端使用上一步获得的授权，向授权服务器申请令牌。 （D）授权服务器对客户端进行认证以后，确认无误，同意发放令牌。 （E）客户端使用令牌，向资源服务器申请获取资源。 （F）资源服务器确认令牌无误，同意向客户端开放资源。 令牌（token）与密码（password）的作用是一样的，都可以进入系统，但是有三点差异。 令牌是短期的，到期会自动失效，用户自己无法修改。密码一般长期有效，用户不修改，就不会发生变化。 令牌可以被数据所有者撤销，会立即失效。密码一般不允许被他人撤销。 令牌有权限范围（scope）。对于网络服务来说，只读令牌就比读写令牌更安全。密码一般是完整权限。 上面这些设计，保证了令牌既可以让第三方应用获得权限，同时又随时可控，不会危及系统安全。这就是 OAuth 2.0 的优点。 客户端授权模式客户端必须得到用户的授权（authorization grant），才能获得令牌（access token）。OAuth 2.0 对于如何颁发令牌的细节，规定得非常详细。具体来说，一共分成四种授权类型（authorization grant），即四种颁发令牌的方式，适用于不同的互联网场景。 授权码模式（authorization code） 密码模式（resource owner password credentials） 简化(隐式)模式（implicit） 客户端模式（client credentials） 不管哪一种授权方式，第三方应用申请令牌之前，都必须先到系统备案，说明自己的身份，然后会拿到两个身份识别码：客户端 ID（client ID）和客户端密钥（client secret）。这是为了防止令牌被滥用，没有备案过的第三方应用，是不会拿到令牌的。 授权码模式授权码（authorization code）方式，指的是第三方应用先申请一个授权码，然后再用该码获取令牌。 这种方式是最常用的流程，安全性也最高，它适用于那些有后端的 Web 应用。授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏。 适用场景：目前市面上主流的第三方验证都是采用这种模式 它的步骤如下： （A）用户访问客户端，后者将前者导向授权服务器。 （B）用户选择是否给予客户端授权。 （C）假设用户给予授权，授权服务器将用户导向客户端事先指定的”重定向URI”（redirection URI），同时附上一个授权码。 （D）客户端收到授权码，附上早先的”重定向URI”，向授权服务器申请令牌。这一步是在客户端的后台的服务器上完成的，对用户不可见。 （E）授权服务器核对了授权码和重定向URI，确认无误后，向客户端发送访问令牌（access token）和更新令牌（refresh token）。 可以看开头的京东微信登录的时序图。 简化(隐式)模式有些 Web 应用是纯前端应用，没有后端。这时就不能用上面的方式了，必须将令牌储存在前端。RFC 6749 就规定了第二种方式，允许直接向前端颁发令牌，这种方式没有授权码这个中间步骤，所以称为（授权码）”隐藏式”（implicit） 简化模式不通过第三方应用程序的服务器，直接在浏览器中向授权服务器申请令牌，跳过了”授权码”这个步骤，所有步骤在浏览器中完成，令牌对访问者是可见的，且客户端不需要认证。 这种方式把令牌直接传给前端，是很不安全的。因此，只能用于一些安全要求不高的场景，并且令牌的有效期必须非常短，通常就是会话期间（session）有效，浏览器关掉，令牌就失效了。 它的步骤如下： （A）客户端将用户导向授权服务器。 （B）用户决定是否给于客户端授权。 （C）假设用户给予授权，授权服务器将用户导向客户端指定的”重定向URI”，并在URI的Hash部分包含了访问令牌。 （D）浏览器向资源服务器发出请求，其中不包括上一步收到的Hash值。 （E）资源服务器返回一个网页，其中包含的代码可以获取Hash值中的令牌。 （F）浏览器执行上一步获得的脚本，提取出令牌。 （G）浏览器将令牌发给客户端。 A 网站提供一个链接，要求用户跳转到 B 网站，授权用户数据给 A 网站使用。 123456https://b.com/oauth/authorize? response_type=token&amp; # response_type参数为token，表示要求直接返回令牌 client_id=CLIENT_ID&amp; redirect_uri=CALLBACK_URL&amp; scope=read 用户跳转到 B 网站，登录后同意给予 A 网站授权。这时，B 网站就会跳回redirect_uri参数指定的跳转网址，并且把令牌作为 URL 参数，传给 A 网站。 1https://a.com/callback#token=ACCESS_TOKEN #token参数就是令牌，A 网站直接在前端拿到令牌。 密码模式如果你高度信任某个应用，RFC 6749 也允许用户把用户名和密码，直接告诉该应用。该应用就使用你的密码，申请令牌，这种方式称为”密码式”（password）。 在这种模式中，用户必须把自己的密码给客户端，但是客户端不得储存密码。这通常用在用户对客户端高度信任的情况下，比如客户端是操作系统的一部分，或者由一个著名公司出品。而授权服务器只有在其他授权模式无法执行的情况下，才能考虑使用这种模式。 适用场景：自家公司搭建的授权服务器 它的步骤如下： （A）用户向客户端提供用户名和密码。 （B）客户端将用户名和密码发给授权服务器，向后者请求令牌。 （C）授权服务器确认无误后，向客户端提供访问令牌。 A 网站要求用户提供 B 网站的用户名和密码，拿到以后，A 就直接向 B 请求令牌。整个过程中，客户端不得保存用户的密码。 12345https://oauth.b.com/token? grant_type=password&amp; # 授权方式是&quot;密码式&quot; username=USERNAME&amp; password=PASSWORD&amp; client_id=CLIENT_ID 密码需要使用非对称加密保护 B 网站验证身份通过后，直接给出令牌。注意，这时不需要跳转，而是把令牌放在 JSON 数据里面，作为 HTTP 回应，A 因此拿到令牌。 客户端模式客户端模式（Client Credentials Grant）指客户端以自己的名义，而不是以用户的名义，向”服务提供商”进行授权。 适用于没有前端的命令行应用，即在命令行下请求令牌。一般用来提供给我们完全信任的服务器端服务。 它的步骤如下： （A）客户端向授权服务器进行身份认证，并要求一个访问令牌。 （B）授权服务器确认无误后，向客户端提供访问令牌。 A 应用在命令行向 B 发出请求。 1234https://oauth.b.com/token? grant_type=client_credentials&amp; client_id=CLIENT_ID&amp; client_secret=CLIENT_SECRET B 网站验证通过以后，直接返回令牌。 Spring Security OAuth2快速开始Spring Security是一个能够为基于Spring的企业应用系统提供声明式的安全访问控制解决方案的安全框架。Spring Security 主要实现了Authentication（认证，解决who are you? ） 和 Access Control（访问控制，也就是what are you allowed to do？，也称为Authorization）。Spring Security在架构上将认证与授权分离，并提供了扩展点。 认证（Authentication） ：用户认证就是判断一个用户的身份是否合法的过程，用户去访问系统资源时系统要求验证用户的身份信息，身份合法方可继续访问，不合法则拒绝访问。常见的用户身份认证方式有：用户名密码登录，二维码登录，手机短信登录，指纹认证等方式。 授权（Authorization）： 授权是用户认证通过根据用户的权限来控制用户访问资源的过程，拥有资源的访问权限则正常访问，没有权限则拒绝访问。 将OAuth2和Spring Security集成，就可以得到一套完整的安全解决方案。我们可以通过Spring Security OAuth2构建一个授权服务器来验证用户身份以提供access_token，并使用这个access_token来从资源服务器请求数据。 授权服务器 Authorize Endpoint ：授权端点，进行授权 Token Endpoint ：令牌端点，经过授权拿到对应的Token Introspection Endpoint ：校验端点，校验Token的合法性 Revocation Endpoint ：撤销端点，撤销授权 4个点在Spring Security OAuth2框架中提供了对应的4个接口 整体架构 流程： 用户访问,此时没有Token。Oauth2RestTemplate会报错，这个报错信息会被Oauth2ClientContextFilter捕获并重定向到授权服务器。 授权服务器通过Authorization Endpoint进行授权，并通过AuthorizationServerTokenServices生成授权码并返回给客户端。 客户端拿到授权码去授权服务器通过Token Endpoint调用AuthorizationServerTokenServices生成Token并返回给客户端 客户端拿到Token去资源服务器访问资源，一般会通过Oauth2AuthenticationManager调用ResourceServerTokenServices进行校验。校验通过可以获取资源。 使用引入依赖spring boot: 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security.oauth&lt;/groupId&gt; &lt;artifactId&gt;spring-security-oauth2&lt;/artifactId&gt; &lt;version&gt;2.3.4.RELEASE&lt;/version&gt;&lt;/dependency&gt; spring cloud: 1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- spring cloud --&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Hoxton.SR8&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 配置 spring security123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@Configurationpublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Autowired private UserService userService; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; // 获取用户信息 auth.userDetailsService(userService); &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.formLogin().permitAll() // 所有请求都需要授权 .and().authorizeRequests() // 下面格式的请求例外 .antMatchers(&quot;/oauth/**&quot;).permitAll() .anyRequest().authenticated() .and().logout().permitAll() .and().csrf().disable(); &#125; @Bean public PasswordEncoder passwordEncoder()&#123; return new BCryptPasswordEncoder(); &#125; @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception &#123; // oauth2 密码模式需要拿到这个bean return super.authenticationManagerBean(); &#125;&#125;@Servicepublic class UserService implements UserDetailsService &#123; @Autowired private PasswordEncoder passwordEncoder; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; String password = passwordEncoder.encode(&quot;123456&quot;); return new User(&quot;fox&quot;,password, AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;admin&quot;)); &#125;&#125;@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @RequestMapping(&quot;/getCurrentUser&quot;) public Object getCurrentUser(Authentication authentication) &#123; return authentication.getPrincipal(); &#125;&#125; 配置授权服务器12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@Configuration@EnableAuthorizationServerpublic class AuthorizationServerConfig extends AuthorizationServerConfigurerAdapter &#123; @Autowired private PasswordEncoder passwordEncoder; @Autowired private AuthenticationManager authenticationManagerBean; @Autowired private UserService userService; @Autowired private TokenStore tokenStore; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; endpoints.authenticationManager(authenticationManagerBean) //使用密码模式需要配置 .tokenStore(tokenStore) //指定token存储到redis .reuseRefreshTokens(false) //refresh_token是否重复使用 .userDetailsService(userService) //刷新令牌授权包含对用户信息的检查 .allowedTokenEndpointRequestMethods(HttpMethod.GET, HttpMethod.POST); //支持GET,POST请求 &#125; @Override public void configure(AuthorizationServerSecurityConfigurer security) throws Exception &#123; // 允许表单认证 security.allowFormAuthenticationForClients(); &#125; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; // 这里处理inMemory还有jdbc() clients.inMemory() //配置client_id .withClient(&quot;client&quot;) //配置client-secret .secret(passwordEncoder.encode(&quot;123123&quot;)) //配置访问token的有效期 .accessTokenValiditySeconds(3600) //配置刷新token的有效期 .refreshTokenValiditySeconds(864000) //配置redirect_uri，用于授权成功后跳转 .redirectUris(&quot;http://www.baidu.com&quot;) //配置申请的权限范围 .scopes(&quot;all&quot;) /* * 配置grant_type，表示授权类型 * authorization_code: 授权码模式 * implicit: 简化模式 * password： 密码模式 * client_credentials: 客户端模式 * refresh_token: 更新令牌 */ .authorizedGrantTypes(&quot;authorization_code&quot;, &quot;implicit&quot;, &quot;password&quot;, &quot;client_credentials&quot;, &quot;refresh_token&quot;); &#125;&#125;@Configurationpublic class RedisConfig &#123; @Autowired private RedisConnectionFactory redisConnectionFactory; @Bean public TokenStore tokenStore()&#123; return new RedisTokenStore(redisConnectionFactory); &#125;&#125; redis配置 12345spring:redis: host: 192.168.2.148 port: 6379 database: 0 授权码模式： 123http://localhost:8080/oauth/authorize?response_type=code&amp;client_id=client&amp;redirect_uri=http://www.baidu.com&amp;scope=allhttp://localhost:8080/oauth/authorize?response_type=code&amp;client_id=client implicit: 简化模式： 1http://localhost:8080/oauth/authorize?client_id=client&amp;response_type=token&amp;scope=all&amp;redirect_uri=http://www.baidu.com password模式： 1http://localhost:8080/oauth/token?username=fox&amp;password=123456&amp;grant_type=password&amp;client_id=client&amp;client_secret=123123&amp;scope=all 客户端模式： 1http://localhost:8080/oauth/token?grant_type=client_credentials&amp;scope=all&amp;client_id=client&amp;client_secret=123123 刷新令牌： 1http://localhost:8080/oauth/token?grant_type=refresh_token&amp;client_id=client&amp;client_secret=123123&amp;refresh_token=[refresh_token值] 资源服务器 资源服务器不是必须的 1234567891011121314@Configuration@EnableResourceServerpublic class ResourceServiceConfig extends ResourceServerConfigurerAdapter &#123; @Override public void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests() .anyRequest().authenticated() // 受保护的资源 .and().requestMatchers().antMatchers(&quot;/user/**&quot;); &#125;&#125; 测试获取授权码http://localhost:8080/oauth/authorize?response_type=code&amp;client_id=client 或者 http://localhost:8080/oauth/authorize?response_type=code&amp;client_id=client&amp;redirect_uri=http://www.baidu.com&amp;scope=all 登录之后进入 允许后 根据授权码通过post请求获取： 调用接口： 或者 简化模式authorizedGrantType添加implicit 测试 http://localhost:8080/oauth/authorize?client_id=client&amp;response_type=token&amp;scope=all&amp;redirect_uri=http://www.baidu.com 登录之后进入授权页面，确定授权后浏览器会重定向到指定路径，并以Hash的形式存放在重定向uri的fargment中： 密码模式通过浏览器测试，需要配置支持get请求和表单验证 http://localhost:8080/oauth/token?username=fox&amp;password=123456&amp;grant_type=password&amp;client_id=client&amp;client_secret=123123&amp;scope=all 密钥要使用非对称加密 通过Postman测试 访问资源 客户端模式获取令牌 更新令牌http://localhost:8080/oauth/token?grant_type=refresh_token&amp;client_id=client&amp;client_secret=123123&amp;refresh_token=dc03bdc2-ca3b-4690-9265-d31a21896d02 扩展 自定义授权页面通过对/oauth/authorize的全局搜索，找到了AuthorizationEndpoint 该方法最后 也就是会转发到/oauth/confirm_access这个地址。再通过搜索 发现这个类的开头没有使用@Controller或者@RequestMapping注解，而是有FrameworkEndpoint注解，所以/oauth/confirm_access请求不会经过RequestMappingHandlerMapping，通过看到代码Spring.security.oauth2包提供了一个FrameworkEndpointHandlerMapping，它是RequestMappingHandlerMapping的子类，并且在DispatcherServlet中的handlerMapping的排序如下 发现，最先经过的是RequestMappingHandlerMapping。 所以，如果可以提供这样一个Controller 12345678910@Controller@SessionAttributes(&quot;authorizationRequest&quot;)public class DemoController &#123; @RequestMapping(&quot;/oauth/confirm_access&quot;) public String getAccessConfirmation(Map&lt;String, Object&gt; model, HttpServletRequest request) &#123; return &quot;自定义登录页面&quot;; &#125; &#125; 就能实现自定义授权页面。还有发现了一个AuthorizationServerEndpointsConfiguration类，看名字就知道，它会完成一些授权的配置。看这段代码 12345678910111213141516171819202122232425262728293031323334353637383940414243@Configuration@Import(TokenKeyEndpointRegistrar.class)public class AuthorizationServerEndpointsConfiguration &#123; private AuthorizationServerEndpointsConfigurer endpoints = new AuthorizationServerEndpointsConfigurer(); @Autowired private ClientDetailsService clientDetailsService; // 这个是我们提供的配置类，在这里把AuthorizationServerConfigurer收集起来 @Autowired private List&lt;AuthorizationServerConfigurer&gt; configurers = Collections.emptyList(); @PostConstruct public void init() &#123; for (AuthorizationServerConfigurer configurer : configurers) &#123; try &#123; configurer.configure(endpoints); &#125; catch (Exception e) &#123; throw new IllegalStateException(&quot;Cannot configure enpdoints&quot;, e); &#125; &#125; endpoints.setClientDetailsService(clientDetailsService); &#125; @Bean public AuthorizationEndpoint authorizationEndpoint() throws Exception &#123; AuthorizationEndpoint authorizationEndpoint = new AuthorizationEndpoint(); FrameworkEndpointHandlerMapping mapping = getEndpointsConfigurer().getFrameworkEndpointHandlerMapping(); // 这里是设置授权页面的地址 authorizationEndpoint.setUserApprovalPage(extractPath(mapping, &quot;/oauth/confirm_access&quot;)); authorizationEndpoint.setProviderExceptionHandler(exceptionTranslator()); authorizationEndpoint.setErrorPage(extractPath(mapping, &quot;/oauth/error&quot;)); authorizationEndpoint.setTokenGranter(tokenGranter()); authorizationEndpoint.setClientDetailsService(clientDetailsService); authorizationEndpoint.setAuthorizationCodeServices(authorizationCodeServices()); authorizationEndpoint.setOAuth2RequestFactory(oauth2RequestFactory()); authorizationEndpoint.setOAuth2RequestValidator(oauth2RequestValidator()); authorizationEndpoint.setUserApprovalHandler(userApprovalHandler()); authorizationEndpoint.setRedirectResolver(redirectResolver()); return authorizationEndpoint; &#125;&#125; 经过分析，只要这样配置后，就能自定义授权页面 1234567@Configurationpublic class AuthorizationServerConfig2 extends AuthorizationServerConfigurerAdapter &#123; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; endpoints.pathMapping(&quot;/oauth/confirm_access&quot;, &quot;自定义的页面&quot;); &#125;&#125; 受信任的客户端详细信息在前面 12345678910111213141516171819202122232425262728293031323334@Configuration@EnableAuthorizationServerpublic class AuthorizationServerConfig extends AuthorizationServerConfigurerAdapter &#123; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; // 这里处理inMemory还有jdbc() clients.inMemory() //配置client_id .withClient(&quot;client&quot;) //配置client-secret .secret(passwordEncoder.encode(&quot;123123&quot;)) //配置访问token的有效期 .accessTokenValiditySeconds(3600) //配置刷新token的有效期 .refreshTokenValiditySeconds(864000) //配置redirect_uri，用于授权成功后跳转 .redirectUris(&quot;http://www.baidu.com&quot;) //配置申请的权限范围 .scopes(&quot;all&quot;) /* * 配置grant_type，表示授权类型 * authorization_code: 授权码模式 * implicit: 简化模式 * password： 密码模式 * client_credentials: 客户端模式 * refresh_token: 更新令牌 */ .authorizedGrantTypes(&quot;authorization_code&quot;, &quot;implicit&quot;, &quot;password&quot;, &quot;client_credentials&quot;, &quot;refresh_token&quot;); &#125;&#125; 这里配置了客户端详细信息，不过clients.inMemory()，也就说，这些数据是存放在内存中的。 这里还可以使用 1clients.jdbc(DataSource) 也就是保存到数据库。问题是使用了jdbc后，不知道数据是怎么存的。进入这个方法看到JdbcClientDetailsServiceBuilder。 这是一个ClientDetailsService实现，而在前面的AuthorizationEndpoint中 所以，可以确定，JdbcClientDetailsService就是需要找的操作数据库的对象。进去看JdbcClientDetailsService的源码发现都是对一张数据库表的操作，表名为oauth_client_details，字段如下： 1234567891011121314CREATE TABLE `oauth_client_details` ( `client_id` varchar(128) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `resource_ids` varchar(256) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `client_secret` varchar(256) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `scope` varchar(256) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `authorized_grant_types` varchar(256) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `web_server_redirect_uri` varchar(256) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `authorities` varchar(256) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `access_token_validity` int(11) NULL DEFAULT NULL, `refresh_token_validity` int(11) NULL DEFAULT NULL, `additional_information` varchar(4096) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `autoapprove` varchar(256) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, PRIMARY KEY (`client_id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic; 所以通过下面的配置 12345678910111213141516@Configurationpublic class AuthorizationServerConfig2 extends AuthorizationServerConfigurerAdapter &#123; @Autowired private DataSource dataSource; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; clients.withClientDetails(clientDetailsService()) &#125; @Bean public ClientDetailsService clientDetailsService()&#123; return new JdbcClientDetailsService(dataSource); &#125;&#125; 这里还可以通过提供自己的 1ClientDetailsService来实现存储在redis中。","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"21-Gateway源码","slug":"springcloud/21-Gateway源码","date":"2021-11-27T12:00:29.000Z","updated":"2022-03-23T09:03:57.272Z","comments":true,"path":"blog/springcloud/21-Gateway源码/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/21-Gateway%E6%BA%90%E7%A0%81/","excerpt":"","text":"前提Gateway没有使用Servlet容器，也就是不是使用SpringMVC处理请求的，而是使用SpringWebFlux处理请求的，由于响应式编程没有了解。但可以通过SpringMVC的处理流程，来了解SpringWebFlux的处理流程。 在SpringMVC请求请求进入了DispatcherServlet后： DispatcherServlet#doDispatch HandlerMapping#getHander 获取对应的handler 通过handler获取HandlerAdapter HandlerAdapter是负责处理特定的handler HandlerAdapter#handle 对handler的调用 SpringWebFlux基于netty实现的，接受到请求后，请求会进入到DispatcherHandler DispatcherHandler#handle HandlerMapping#getHander 获取对应的handler 通过handle选择HandlerAdapter HandlerAdapter是负责处理特定的handler HandlerAdapter#handle SpringWebFlux的调用流程和SpringMVC的基本一摸一样，扩展点都是一样的 HandlerMapping handle类型 HandlerAdapter 对handler的调用 接口名字虽然一样，但包不同 通过看HandlerMapping实发现了一个Gateway提供的实现类RoutePredicateHandlerMapping。 而且在自动配置类GatewayAutoConfiguration中发现 所以RoutePredicateHandlerMapping就能作为Gateway的入口 Gateway源码分析 原理图中的GatewayHandlerMapping就是RoutePredicateHandlerMapping。 RoutePredicateHandlerMappingHandlerMapping的作用就是获取Handler的。看RoutePredicateHandlerMapping的核心代码 通过lookupRoute获取到路由信息和Predicate（断言）的执行（路由匹配） 这里是通过RouteDefinitionRouteLocator来获取所有的路由信息。 123456// RouteDefinitionRouteLocator@Overridepublic Flux&lt;Route&gt; getRoutes() &#123; Flux&lt;Route&gt; routes = this.routeDefinitionLocator.getRouteDefinitions() .map(this::convertToRoute);&#125; 而该类又是通过RouteDefinitionLocator来获取路由信息的。 RouteDefinitionRouteLocator中的RouteDefinitionLocator对象为CompositeRouteDefinitionLocator。这个对象里面包含多个不同的RouteDefinitionLocator对象（这种套路在Spring中很常见）。这里就看两个 PropertiesRouteDefinitionLocator 这个就是获取配置文件中的 1234567891011spring: cloud: gateway: #设置路由：路由id、路由到微服务的uri、断言 routes: - id: order_route #路由ID，全局唯一 # uri: http://localhost:8020 #目标微服务的请求地址和端口 uri: lb://mall-order #lb 整合负载均衡器ribbon,loadbalancer predicates: # Cookie匹配 - Cookie=username, xyz 这种路由信息 DiscoveryClientRouteDefinitionLocator 这个类是通过服务发现+Spel来完成路由的动态配置，这里会基于 123456789101112131415161718spring: application: name: mall-gateway #配置nacos注册中心地址 cloud: nacos: discovery: server-addr: 127.0.0.1:8848 gateway: discovery: locator: enabled: true # 默认就是这个 # 基于Spel表达式，会从ServiceInstance对象中获取值 url-expression: &#x27;lb://&#x27;+serviceId # 使用Spel表达式，从ServiceInstance的metadata属性中拿值 predicates: - Path=&#x27;/&#x27;+metadata[contextPath]+&#x27;/**&#x27; 这个基础配置，来生成一系列的路由信息。 这个类的源码也很好理解，只要看懂下面的代码，那这个类的源码就好好懂了 123456789101112131415161718192021222324252627// Spel的使用public class SpelDemo &#123; public static void main(String[] args) &#123; SpelExpressionParser parser = new SpelExpressionParser(); SimpleEvaluationContext evalCtxt = SimpleEvaluationContext .forReadOnlyDataBinding() .withInstanceMethods() .build(); Expression urlExpr = parser.parseExpression(&quot;&#x27;lb://&#x27;+serverId+&#x27;:&#x27;+metadata[contextPath]&quot;); Map&lt;String, String&gt; data = new HashMap&lt;&gt;(); data.put(&quot;contextPath&quot;, &quot;order&quot;); ServiceInstance instance = new ServiceInstance(); instance.setServerId(&quot;mall-order&quot;); instance.setMetadata(data); System.out.println(urlExpr.getValue(evalCtxt, instance, String.class)); &#125; @Getter @Setter private static class ServiceInstance &#123; private String serverId; private Map&lt;String, String&gt; metadata; &#125;&#125; 这个需要配合服务发现才能使用，不然会报错。 Nacos-discovery提供了NacosReactiveDiscoveryClient这个类来完成服务的获取。 获取完路由信息后，就是路由的匹配了，通过配置的predicates（断言）来进行匹配。 这一步会在把匹配到的路由信息添加到ServerWebExchange中，返回返回WebHandler类型的对象，这个对象的类型是FilteringWebHandler。这个对象是通过@Bean引入到Spring中的。 FilteringWebHandler获取完Handler后，下一步就是获取HandlerAdapter了，Gateway没有提供自己的HandlerAdapter。所以还是会使用SpringWebFlux提供的。 这里看SimpleHandlerAdapter，这个是用来处理WebHandler的 12345678910111213public class SimpleHandlerAdapter implements HandlerAdapter &#123; @Override public boolean supports(Object handler) &#123; return WebHandler.class.isAssignableFrom(handler.getClass()); &#125; @Override public Mono&lt;HandlerResult&gt; handle(ServerWebExchange exchange, Object handler) &#123; WebHandler webHandler = (WebHandler) handler; Mono&lt;Void&gt; mono = webHandler.handle(exchange); return mono.then(Mono.empty()); &#125;&#125; 而FilteringWebHandler就是一个WebHandler。看其handle方法 12345678910111213141516171819202122// FilteringWebHandler@Overridepublic Mono&lt;Void&gt; handle(ServerWebExchange exchange) &#123; // 获取匹配到的Route Route route = exchange.getRequiredAttribute(GATEWAY_ROUTE_ATTR); // 获取配置中的GatewayFilter List&lt;GatewayFilter&gt; gatewayFilters = route.getFilters(); // globalFilters是在构造方法中初始化的，传入GlobalFilter列表，最终通过GatewayFilterAdapter转成GatewayFilter类型的 List&lt;GatewayFilter&gt; combined = new ArrayList&lt;&gt;(this.globalFilters); // 所以这里就有GlobalFilter + GatewayFilter的对象了。 combined.addAll(gatewayFilters); // TODO: needed or cached? AnnotationAwareOrderComparator.sort(combined); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Sorted gatewayFilterFactories: &quot; + combined); &#125; // 这里就执行Filter链了。类比HandlerExecutionChain return new DefaultGatewayFilterChain(combined).filter(exchange);&#125; 这里默认情况下会有这些GlobalFilter 看几个重要的 LoadBalancerClientFilter 完成均衡负载 LoadBalancerClientFilter是通过自动配置类引入的 1234567891011121314public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; ...... // 均衡负载 final ServiceInstance instance = choose(exchange); ...... // 使用新的执行 exchange.getAttributes().put(GATEWAY_REQUEST_URL_ATTR, requestUrl); ......&#125;protected ServiceInstance choose(ServerWebExchange exchange) &#123; // LoadBalancerClient loadBalancer; return loadBalancer.choose( ((URI) exchange.getAttribute(GATEWAY_REQUEST_URL_ATTR)).getHost()); 这里的loadBalancer就是RibbonLoadBalancerClient了。 ribbon是Spring cloud默认的均衡负载器，也可以使用官方的LoadBalancer。如果使用官方的，这里的Filter就变成了ReactiveLoadBalancerClientFilter NettyRoutingFilter 完成对具体服务的调用。 在前面就已经把地址解析完成了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; URI requestUrl = exchange.getRequiredAttribute(GATEWAY_REQUEST_URL_ATTR); String scheme = requestUrl.getScheme(); if (isAlreadyRouted(exchange) || (!&quot;http&quot;.equals(scheme) &amp;&amp; !&quot;https&quot;.equals(scheme))) &#123; return chain.filter(exchange); &#125; setAlreadyRouted(exchange); ServerHttpRequest request = exchange.getRequest(); final HttpMethod method = HttpMethod.valueOf(request.getMethodValue()); final String url = requestUrl.toASCIIString(); HttpHeaders filtered = filterRequest(getHeadersFilters(), exchange); final DefaultHttpHeaders httpHeaders = new DefaultHttpHeaders(); filtered.forEach(httpHeaders::set); boolean preserveHost = exchange .getAttributeOrDefault(PRESERVE_HOST_HEADER_ATTRIBUTE, false); Route route = exchange.getAttribute(GATEWAY_ROUTE_ATTR); // 把请求转发的逻辑 Flux&lt;HttpClientResponse&gt; responseFlux = getHttpClient(route, exchange) .headers(headers -&gt; &#123; headers.add(httpHeaders); // Will either be set below, or later by Netty headers.remove(HttpHeaders.HOST); if (preserveHost) &#123; String host = request.getHeaders().getFirst(HttpHeaders.HOST); headers.add(HttpHeaders.HOST, host); &#125; &#125;).request(method).uri(url).send((req, nettyOutbound) -&gt; &#123; if (log.isTraceEnabled()) &#123; nettyOutbound .withConnection(connection -&gt; log.trace(&quot;outbound route: &quot; + connection.channel().id().asShortText() + &quot;, inbound: &quot; + exchange.getLogPrefix())); &#125; return nettyOutbound.send(request.getBody().map(this::getByteBuf)); &#125;).responseConnection((res, connection) -&gt; &#123; // Defer committing the response until all route filters have run // Put client response as ServerWebExchange attribute and write // response later NettyWriteResponseFilter exchange.getAttributes().put(CLIENT_RESPONSE_ATTR, res); exchange.getAttributes().put(CLIENT_RESPONSE_CONN_ATTR, connection); ServerHttpResponse response = exchange.getResponse(); // put headers and status so filters can modify the response HttpHeaders headers = new HttpHeaders(); res.responseHeaders().forEach( entry -&gt; headers.add(entry.getKey(), entry.getValue())); String contentTypeValue = headers.getFirst(HttpHeaders.CONTENT_TYPE); if (StringUtils.hasLength(contentTypeValue)) &#123; exchange.getAttributes().put(ORIGINAL_RESPONSE_CONTENT_TYPE_ATTR, contentTypeValue); &#125; setResponseStatus(res, response); // make sure headers filters run after setting status so it is // available in response HttpHeaders filteredResponseHeaders = HttpHeadersFilter.filter( getHeadersFilters(), headers, exchange, Type.RESPONSE); if (!filteredResponseHeaders .containsKey(HttpHeaders.TRANSFER_ENCODING) &amp;&amp; filteredResponseHeaders .containsKey(HttpHeaders.CONTENT_LENGTH)) &#123; // It is not valid to have both the transfer-encoding header and // the content-length header. // Remove the transfer-encoding header in the response if the // content-length header is present. response.getHeaders().remove(HttpHeaders.TRANSFER_ENCODING); &#125; exchange.getAttributes().put(CLIENT_RESPONSE_HEADER_NAMES, filteredResponseHeaders.keySet()); response.getHeaders().putAll(filteredResponseHeaders); return Mono.just(res); &#125;); Duration responseTimeout = getResponseTimeout(route); if (responseTimeout != null) &#123; responseFlux = responseFlux .timeout(responseTimeout, Mono.error(new TimeoutException( &quot;Response took longer than timeout: &quot; + responseTimeout))) .onErrorMap(TimeoutException.class, th -&gt; new ResponseStatusException(HttpStatus.GATEWAY_TIMEOUT, th.getMessage(), th)); &#125; // 响应后传给下个Filter return responseFlux.then(chain.filter(exchange));&#125; 总结 源码中太多响应式编程的API了。没必要详细的看源码","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"20-微服务网关组件Gateway","slug":"springcloud/20-微服务网关组件Gateway","date":"2021-11-27T12:00:28.000Z","updated":"2022-03-23T09:03:57.249Z","comments":true,"path":"blog/springcloud/20-微服务网关组件Gateway/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/20-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%BD%91%E5%85%B3%E7%BB%84%E4%BB%B6Gateway/","excerpt":"","text":"Spring Cloud Gateway 什么是Spring Cloud Gateway网关作为流量的入口，常用的功能包括路由转发，权限校验，限流等。 Spring Cloud Gateway 是Spring Cloud官方推出的第二代网关框架，定位于取代 Netflix Zuul。相比 Zuul 来说，Spring Cloud Gateway 提供更优秀的性能，更强大的有功能。 Spring Cloud Gateway 是由 WebFlux + Netty + Reactor 实现的响应式的 API 网关。它不能在传统的 servlet 容器中工作，也不能构建成 war 包。 Spring Cloud Gateway 旨在为微服务架构提供一种简单且有效的 API 路由的管理方式，并基于 Filter 的方式提供网关的基本功能，例如说安全认证、监控、限流等等。 核心概念 路由（route) 路由是网关中最基础的部分，路由信息包括一个ID、一个目的URI、一组断言工厂、一组Filter组成。如果断言为真，则说明请求的URL和配置的路由匹配。 断言(predicates) Java8中的断言函数，SpringCloud Gateway中的断言函数类型是Spring5.0框架中的ServerWebExchange。断言函数允许开发者去定义匹配Http request中的任何信息，比如请求头和参数等。 过滤器（Filter) SpringCloud Gateway中的filter分为Gateway FilIer和Global Filter。Filter可以对请求和响应进行处理。 工作原理Spring Cloud Gateway 的工作原理跟 Zuul 的差不多，最大的区别就是 Gateway 的 Filter 只有 pre 和 post 两种。 客户端向 Spring Cloud Gateway 发出请求，如果请求与网关程序定义的路由匹配，则该请求就会被发送到网关 Web 处理程序，此时处理程序运行特定的请求过滤器链。 过滤器之间用虚线分开的原因是过滤器可能会在发送代理请求的前后执行逻辑。所有 pre 过滤器逻辑先执行，然后执行代理请求；代理请求完成后，执行 post 过滤器逻辑。 Spring Cloud Gateway快速开始基本配置1234567891011&lt;!-- gateway网关 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- nacos服务注册与发现 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 注意：会和spring-webmvc的依赖冲突，需要排除spring-webmvc 配置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849spring: application: name: mall-gateway #配置nacos注册中心地址 cloud: nacos: discovery: server-addr: 127.0.0.1:8848 gateway: # 是否开启网关 enabled: true globalcors: cors-configurations: &#x27;[/**]&#x27;: allowedOrigins: &quot;*&quot; allowedMethods: - GET - POST - DELETE - PUT - OPTION #设置路由：路由id、路由到微服务的uri、断言 routes: - id: order_route #路由ID，全局唯一，建议配合服务名# #uri: http://localhost:8020 #目标微服务的请求地址和端口 uri: lb://mall-order #lb 整合负载均衡器ribbon,loadbalancer # 下面的都是使用了Route Predicate Factories # 下面列举了一些官方提供的，可以自定，只需实现RoutePredicateFactory接口（一般都是继承AbstractRoutePredicateFactory） predicates:# #Path路径匹配 - Path=/order/** # 匹配在指定的日期时间之后发生的请求 入参是ZonedDateTime类型 #- After=2021-05-16T20:50:57.511+08:00[Asia/Shanghai] # Cookie匹配 #- Cookie=username, fox, \\d+ # Header匹配 请求中带有请求头名为 x-request-id，其值与 \\d+ 正则表达式匹配 #- Header=X-Request-Id, \\d+ filters: - AddRequestHeader=X-Request-color, red #添加请求头 - AddRequestParameter=color, blue # 添加请求参数 - PrefixPath=/mall-order # 添加前缀 对应微服务需要配置context-path #- RedirectTo=302, http://baidu.com #重定向到百度 详细配置看官方文档Spring Cloud Gateway 路由断言工厂（Route Predicate Factories）配置 这里的断言只的就是对请求按某种要求匹配，类似JDK8提供的Predicate接口 gateway-request-predicates-factories 网关启动日志： 上面这些都是官方提供的，可以自定，只需实现RoutePredicateFactory接口（一般都是继承AbstractRoutePredicateFactory） 路径匹配1234567891011spring: cloud: gateway: #设置路由：路由id、路由到微服务的uri、断言 routes: - id: order_route #路由ID，全局唯一 # uri: http://localhost:8020 #目标微服务的请求地址和端口 uri: lb://mall-order #lb 整合负载均衡器ribbon,loadbalancer predicates: # 测试：http://localhost:8888/order/findOrderByUserId/1 - Path=/order/** #Path路径匹配 时间匹配 123456789101112spring: cloud: gateway: #设置路由：路由id、路由到微服务的uri、断言 routes: - id: order_route #路由ID，全局唯一 # uri: http://localhost:8020 #目标微服务的请求地址和端口 uri: lb://mall-order #lb 整合负载均衡器ribbon,loadbalancer predicates: # 测试：http://localhost:8888/order/findOrderByUserId/1 # 匹配在指定的日期时间之后发生的请求 入参是ZonedDateTime类型 - After=2021-01-31T22:22:07.783+08:00[Asia/Shanghai] 这个时间使用 123ZonedDateTime zonedDateTime = ZonedDateTime.now();//默认时区// 用指定时区获取当前时间ZonedDateTime zonedDateTime2 = ZonedDateTime.now(ZoneId.of(&quot;Asia/Shanghai&quot;)); 获取 Cookie匹配1234567891011spring: cloud: gateway: #设置路由：路由id、路由到微服务的uri、断言 routes: - id: order_route #路由ID，全局唯一 # uri: http://localhost:8020 #目标微服务的请求地址和端口 uri: lb://mall-order #lb 整合负载均衡器ribbon,loadbalancer predicates: # Cookie匹配 - Cookie=username, xyz Cookie&#x3D;username, xyz的意思是Headers中的Cookie需要带username&#x3D;xyz Header匹配1234567891011spring: cloud: gateway: #设置路由：路由id、路由到微服务的uri、断言 routes: - id: order_route #路由ID，全局唯一 # uri: http://localhost:8020 #目标微服务的请求地址和端口 uri: lb://mall-order #lb 整合负载均衡器ribbon,loadbalancer predicates: # Header匹配 请求中带有请求头名为 x-request-id，其值与 \\d+ 正则表达式匹配 - Header=X-Request-Id, \\d+ Header中需要X-Request-Id&#x3D;整数 自定义路由断言工厂自定义路由断言工厂需要继承 AbstractRoutePredicateFactory 类，重写 apply 方法的逻辑。在 apply 方法中可以通过 ServerWebExchange.getRequest() 拿到 ServerHttpRequest 对象，从而可以获取到请求的参数、请求方式、请求头等信息。 注意： 命名需要以 RoutePredicateFactory 结尾，如果不以这个结尾，那predicates的值就是类的全名了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * 自定义的Predicate对类明有要求，需要以RoutePredicateFactory结尾 * 这样就能在配置中使用CheckAuth这个断言，否则就要用类明了 * */@Component@Slf4jpublic class CheckAuthRoutePredicateFactory extends AbstractRoutePredicateFactory&lt;CheckAuthRoutePredicateFactory.Config&gt; &#123; public CheckAuthRoutePredicateFactory() &#123; super(Config.class); &#125; @Override public Predicate&lt;ServerWebExchange&gt; apply(Config config) &#123; return new GatewayPredicate() &#123; @Override public boolean test(ServerWebExchange serverWebExchange) &#123; log.info(&quot;调用CheckAuthRoutePredicateFactory&quot; + config.getName()); if(&quot;xyz&quot;.equals(config.getName()))&#123; return true; &#125; return false; &#125; &#125;; &#125; /** * 快捷配置 * * 正常的配置是这样的 * predicates: * - name: Cookie * args: * name: mycookie * regexp: mycookievalue * 把args的参数传给Config对象。 * 而这个方法是用来定义快捷配置配置的，这种配置方式如下 * predicates: * - Cookie=mycookie,mycookievalue * @return */ @Override public List&lt;String&gt; shortcutFieldOrder() &#123; return Collections.singletonList(&quot;name&quot;); &#125; /** * 需要定义一个内部类，该类用于封装application.yml中的配置 * */ public static class Config &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125;&#125; 使用： 12345678910111213141516spring: cloud: gateway: #设置路由：路由id、路由到微服务的uri、断言 routes: - id: order_route #路由ID，全局唯一 # uri: http://localhost:8020 #目标微服务的请求地址和端口 uri: lb://mall-order #lb 整合负载均衡器ribbon,loadbalancer predicates: - Path=/order/** #Path路径匹配 #自定义CheckAuth断言工厂# - name: CheckAuth# args:# name: fox # 快捷配置 - CheckAuth=fox 过滤器工厂（ GatewayFilter Factories）配置SpringCloudGateway 内置了很多的过滤器工厂，我们通过一些过滤器工厂可以进行一些业务逻辑处理器，比如添加剔除响应头，添加去除参数等 gatewayfilter-factories 官网提供了很多过滤工厂，自定的的话可以实现GatewayFilterFactory接口，不过一般都是继承AbstractGatewayFilterFactory这个抽象类 这种过滤器确定很明显，就是需要一个一个route配置。 添加请求头1234567891011spring: cloud: gateway: #设置路由：路由id、路由到微服务的uri、断言 routes: - id: order_route #路由ID，全局唯一 # uri: http://localhost:8020 #目标微服务的请求地址和端口 uri: lb://mall-order #lb 整合负载均衡器ribbon,loadbalancer #配置过滤器工厂 filters: - AddRequestHeader=X-Request-color, red #添加请求头 在请求的Header中添加X-Request-color&#x3D;red 添加请求参数1234567891011spring: cloud: gateway: #设置路由：路由id、路由到微服务的uri、断言 routes: - id: order_route #路由ID，全局唯一 # uri: http://localhost:8020 #目标微服务的请求地址和端口 uri: lb://mall-order #lb 整合负载均衡器ribbon,loadbalancer #配置过滤器工厂 filters: - AddRequestParameter=color, blue # 添加请求参数 为匹配的路由统一添加前缀1234567891011spring: cloud: gateway: #设置路由：路由id、路由到微服务的uri、断言 routes: - id: order_route #路由ID，全局唯一 # uri: http://localhost:8020 #目标微服务的请求地址和端口 uri: lb://mall-order #lb 整合负载均衡器ribbon,loadbalancer #配置过滤器工厂 filters: - PrefixPath=/mall-order # 添加前缀 对应微服务需要配置context-path mall-order中需要配置 123server: servlet: context-path: /mall-order 测试：http://localhost:8888/order/findOrderByUserId/1 &#x3D;&#x3D;&#x3D;&#x3D;》 http://localhost:8020/mall-order/order/findOrderByUserId/1 重定向操作1234567891011spring: cloud: gateway: #设置路由：路由id、路由到微服务的uri、断言 routes: - id: order_route #路由ID，全局唯一 # uri: http://localhost:8020 #目标微服务的请求地址和端口 uri: lb://mall-order #lb 整合负载均衡器ribbon,loadbalancer #配置过滤器工厂 filters: - RedirectTo=302, https://www.baidu.com/ #重定向到百度 自定义过滤器工厂123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 自定义的GatewayFilter对类明有要求，需要以GatewayFilterFactory结尾 * 这样就能在配置中使用CheckAuth这个filters，否则就要用类明了 */@Component@Slf4jpublic class CheckAuthGatewayFilterFactory extends AbstractGatewayFilterFactory&lt;CheckAuthGatewayFilterFactory.Config&gt; &#123; @Override public GatewayFilter apply(Config config) &#123; return (exchange, chain) -&gt; &#123; log.info(&quot;调用CheckAuthGatewayFilterFactory===&quot; + config.getName() + &quot;:&quot; + config.getValue()); // TODO // 交给下一个fitler处理 return chain.filter(exchange); &#125;; &#125; @Override public List&lt;String&gt; shortcutFieldOrder() &#123; return super.shortcutFieldOrder(&quot;name&quot;, &quot;value&quot;); &#125; public static class Config &#123; private String name; private String value; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; &#125;&#125; 提供了一个AbstractNameValueGatewayFilterFactory，这个只是把Config和shortcutFieldOrder简略掉了。 1234567891011spring: cloud: gateway: #设置路由：路由id、路由到微服务的uri、断言 routes: - id: order_route #路由ID，全局唯一 # uri: http://localhost:8020 #目标微服务的请求地址和端口 uri: lb://mall-order #lb 整合负载均衡器ribbon,loadbalancer #配置过滤器工厂 filters: - CheckAuth=xyz,男 全局过滤器（Global Filters）配置global-filters GlobalFilter 接口和 GatewayFilter 有不一样的接口定义，但是作用都一样的，只不过， GlobalFilter 会作用于所有路由。而且GlobalFilter会在启动阶段通过GatewayFilterAdapter变成一个GatewayFilter 官方声明：GlobalFilter的接口定义以及用法在未来的版本可能会发生变化。 LoadBalancerClientFilterLoadBalancerClientFilter 会查看exchange的属性 ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR 的值（一个URI），如果该值的scheme是 lb，比如：lb:&#x2F;&#x2F;myservice ，它将会使用Spring Cloud的LoadBalancerClient 来将 myservice 解析成实际的host和port，并替换掉 ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR 的内容。 其实就是用来整合负载均衡器Ribbon的 12345678spring: cloud: gateway: routes: - id: order_route uri: lb://mall-order predicates: - Path=/order/** 自定义全局过滤器实现GlobalFilter接口 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Component@Order(-1)@Slf4jpublic class CheckAuthFilter implements GlobalFilter &#123; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; //校验请求头中的token List&lt;String&gt; token = exchange.getRequest().getHeaders().get(&quot;token&quot;); log.info(&quot;token:&quot;+ token); if (token.isEmpty())&#123; return chain.filter(exchange); &#125; // TODO token校验 return chain.filter(exchange); &#125;&#125;@Component@Slf4jpublic class CheckIPFilter implements GlobalFilter, Ordered &#123; @Override public int getOrder() &#123; return 0; &#125; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; HttpHeaders headers = exchange.getRequest().getHeaders(); //模拟对 IP 的访问限制，即不在 IP 白名单中就不能调用的需求 if (getIp(headers).equals(&quot;127.0.0.1&quot;)) &#123; log.info(&quot;======非法访问======&quot;); ServerHttpResponse response = exchange.getResponse(); byte[] bytes = new String(&quot;======非法访问======&quot;).getBytes(); response.setStatusCode(HttpStatus.NOT_ACCEPTABLE); DataBuffer buffer = response.bufferFactory().wrap(bytes); response.getHeaders().add(&quot;Content-Type&quot;, &quot;application/json;charset=UTF-8&quot;); return exchange.getResponse().writeWith(Mono.just(buffer)); &#125; return chain.filter(exchange); &#125; private String getIp(HttpHeaders headers) &#123; return headers.getHost().getHostName(); &#125;&#125; Gateway跨域配置（CORS Configuration）cors-configuration 跨越在nginx解决就好了，下面是后端使用响应式的解决方案 123456789101112131415@Configurationpublic class CorsConfig &#123; @Bean public CorsWebFilter corsFilter() &#123; CorsConfiguration config = new CorsConfiguration(); config.addAllowedMethod(&quot;*&quot;); config.addAllowedOrigin(&quot;*&quot;); config.addAllowedHeader(&quot;*&quot;); UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(new PathPatternParser()); source.registerCorsConfiguration(&quot;/**&quot;, config); return new CorsWebFilter(source); &#125;&#125; 下面是Gateway的解决方案 12345678910111213spring: cloud: gateway: globalcors: cors-configurations: &#x27;[/**]&#x27;: allowedOrigins: &quot;https://docs.spring.io&quot; allowedMethods: - GET - POST - DELETE - PUT - OPTION gateway整合sentinel限流从 1.6.0 版本开始，Sentinel 提供了 Spring Cloud Gateway 的适配模块，可以提供两种资源维度的限流： route 维度：即在 Spring 配置文件中配置的路由条目，资源名为对应的 routeId 自定义 API 维度：用户可以利用 Sentinel 提供的 API 来自定义一些 API 分组 快速开始使用时需引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-sentinel-gateway&lt;/artifactId&gt;&lt;/dependency&gt; 接入sentinel dashboard，添加yml配置 12345678910111213spring: application: name: mall-gateway-sentinel-demo #配置nacos注册中心地址 cloud: nacos: discovery: server-addr: 127.0.0.1:8848 sentinel: transport: # 添加sentinel的控制台地址 dashboard: 127.0.0.1:8080 引入依赖后，就能使用sentinel限流了，下面展示自定义 API 在代码是怎么定义的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@PostConstructpublic void doInit() &#123; //初始化自定义的API initCustomizedApis(); //初始化网关限流规则 initGatewayRules(); //自定义限流异常处理器 initBlockRequestHandler();&#125; //初始化自定义的APIprivate void initCustomizedApis() &#123; Set&lt;ApiDefinition&gt; definitions = new HashSet&lt;&gt;(); ApiDefinition api = new ApiDefinition(&quot;user_service_api&quot;) .setPredicateItems(new HashSet&lt;ApiPredicateItem&gt;() &#123;&#123; add(new ApiPathPredicateItem().setPattern(&quot;/user/**&quot;) .setMatchStrategy(SentinelGatewayConstants.URL_MATCH_STRATEGY_PREFIX)); &#125;&#125;); definitions.add(api); GatewayApiDefinitionManager.loadApiDefinitions(definitions);&#125;private void initGatewayRules() &#123; Set&lt;GatewayFlowRule&gt; rules = new HashSet&lt;&gt;(); //resource：资源名称，可以是网关中的 route 名称或者用户自定义的 API 分组名称。 //count：限流阈值 //intervalSec：统计时间窗口，单位是秒，默认是 1 秒。 rules.add(new GatewayFlowRule(&quot;order_route&quot;) .setCount(2) .setIntervalSec(1) ); rules.add(new GatewayFlowRule(&quot;user_service_api&quot;) .setCount(2) .setIntervalSec(1) ); // 加载网关规则 GatewayRuleManager.loadRules(rules);&#125;private void initBlockRequestHandler() &#123; BlockRequestHandler blockRequestHandler = new BlockRequestHandler() &#123; @Override public Mono&lt;ServerResponse&gt; handleRequest(ServerWebExchange exchange, Throwable t) &#123; HashMap&lt;String, String&gt; result = new HashMap&lt;&gt;(); result.put(&quot;code&quot;,String.valueOf(HttpStatus.TOO_MANY_REQUESTS.value())); result.put(&quot;msg&quot;, HttpStatus.TOO_MANY_REQUESTS.getReasonPhrase()); return ServerResponse.status(HttpStatus.TOO_MANY_REQUESTS) .contentType(MediaType.APPLICATION_JSON) .body(BodyInserters.fromValue(result)); &#125; &#125;; //设置自定义异常处理器 GatewayCallbackManager.setBlockHandler(blockRequestHandler);&#125; 或者使用sentinel控制台来设置自定义API 定义完了API后，在流控规则中就能对这些API进行限流了。 网关流控实现原理 源码看GatewayFlowSlot 网关高可用为了保证 Gateway 的高可用性，可以同时启动多个 Gateway 实例进行负载，在 Gateway 的上游使用 Nginx 或者 F5 进行负载转发以达到高可用。 基于服务发现的配置+Spel表达式上边的都是直接在配置文件上写的。Gateway还可以可以使用服务发现+Spel的方式来进行配置。 123456789101112131415161718spring: application: name: mall-gateway #配置nacos注册中心地址 cloud: nacos: discovery: server-addr: 127.0.0.1:8848 gateway: discovery: locator: enabled: true # 默认就是这个 # 基于Spel表达式，会从ServiceInstance对象中获取值 url-expression: &#x27;lb://&#x27;+serviceId # 使用Spel表达式，从ServiceInstance的metadata属性中拿值 predicates: - Path=&#x27;/&#x27;+metadata[contextPath]+&#x27;/**&#x27;","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"19-Sentinel规则持久化","slug":"springcloud/19-Sentinel规则持久化","date":"2021-11-27T12:00:27.000Z","updated":"2022-03-23T09:03:57.237Z","comments":true,"path":"blog/springcloud/19-Sentinel规则持久化/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/19-Sentinel%E8%A7%84%E5%88%99%E6%8C%81%E4%B9%85%E5%8C%96/","excerpt":"","text":"Sentinel规则推送模式 推送模式 说明 优点 缺点 原始模式 API 将规则推送至客户端并直接更新到内存中，扩展写数据源（WritableDataSource） 简单，无任何依赖 不保证一致性；规则保存在内存中，重启即消失。严重不建议用于生产环境 Pull 模式 扩展写数据源（WritableDataSource）， 客户端主动向某个规则管理中心定期轮询拉取规则，这个规则中心可以是 RDBMS、文件 等 简单，无任何依赖；规则持久化 不保证一致性；实时性不保证，拉取过于频繁也可能会有性能问题。 Push 模式 扩展读数据源（ReadableDataSource），规则中心统一推送，客户端通过注册监听器的方式时刻监听变化，比如使用 Nacos、Zookeeper 等配置中心。这种方式有更好的实时性和一致性保证。生产环境下一般采用 push 模式的数据源。 规则持久化；一致性；快速 引入第三方依赖 原始模式如果不做任何修改，Dashboard 的推送规则方式是通过 API 将规则推送至客户端并直接更新到内存中： 这种做法的好处是简单，无依赖；坏处是应用重启规则就会消失，仅用于简单测试，不能用于生产环境。 拉模式 这种模式服务与控制台通信，服务接收到控制台的数据后更新本地存储，并更新缓存。而且还需要定时的检查本地文件的变化 推模式 生产环境下一般更常用的是 push 模式的数据源。对于 push 模式的数据源，如远程配置中心（ZooKeeper, Nacos, Apollo等等），推送的操作不应由 Sentinel 客户端进行，而应该经控制台统一进行管理，直接进行推送，数据源仅负责获取配置中心推送的配置并更新到本地。因此推送规则正确做法应该是。 拉模式实现持久化前提 前提： 在自动配置类中SentinelAutoConfiguration的init()方法中会执行这个方法InitExecutor.doInit();。这个方法的核心逻辑 这个方法就是通过java的SPI加载InitFunc接口的实现，然后执行方法。从这里就可以发现，这个方法就是完成一些类的初始化的。 客户端与Sentinel控制台的通信是通过CommandCenter接口的实现类来完成初始化的，而CommandCenter接口的实现类的初始化又是通过CommandCenterInitFunc类来实现的。 在引入的包中： CommandCenterInitFunc的核心逻辑 CommandCenterProvider的核心逻辑 可以看到，CommandCenter的具体实现类还是通过javaSPI引入的。在引入的包中 所以可以确定SimpleHttpCommandCenter就是sentinel客户端的初始化的。并且这个类会在项目启动的时候启动。而这个类其实就是使用了javaIO的ServerSocket来完成的 看SimpleHttpCommandCenter源码会发现控制台发动的请求最终都会交给CommandHandler来完成的，而这个实现类又是通过javaSPI加载的 看ModifyRulesCommandHandler，在这里会会完成把规则写入到内存中的逻辑，并且有这个逻辑 就是提供了一个WritableDataSource扩展，来玩数据在客户端的持久化的。官方提供了一个FileWritableDataSource，就是用来写文件的。 而且官方还FileRefreshableDataSource实现，用来定时实现读物文件的。所以只要这两个配合使用拉模式实现持久化 实现 提供一个InitFunc实现，并添加SPI配置 在自定义的InitFunc的实现中提供 1234567891011121314151617181920private void dealFlowRules() throws FileNotFoundException &#123; String ruleFilePath = PersistenceRuleConstant.rulesMap.get(PersistenceRuleConstant.FLOW_RULE_PATH).toString(); //创建流控规则的可读数据源 ReadableDataSource&lt;String, List&lt;FlowRule&gt;&gt; flowRuleRDS = new FileRefreshableDataSource( ruleFilePath, RuleListConverterUtils.flowRuleListParser ); // 将可读数据源注册至FlowRuleManager 这样当规则文件发生变化时，就会更新规则到内存 FlowRuleManager.register2Property(flowRuleRDS.getProperty()); WritableDataSource&lt;List&lt;FlowRule&gt;&gt; flowRuleWDS = new FileWritableDataSource&lt;List&lt;FlowRule&gt;&gt;( ruleFilePath, RuleListConverterUtils.flowFuleEnCoding ); // 将可写数据源注册至 transport 模块的 WritableDataSourceRegistry 中. // 这样收到控制台推送的规则时，Sentinel 会先更新到内存，然后将规则写入到文件中. WritableDataSourceRegistry.registerFlowDataSource(flowRuleWDS);&#125; 最终实现sentinel-datasource-extension-file-pull.7z 推模式实现持久化通过官方demo-Sentinel&#x2F;sentinel-demo&#x2F;sentinel-demo-nacos-datasource)就能知道怎么通过nacos拉取配置并且刷新本地缓存了。其实就是创建一个NacosDataSource就好了。NacosDataSource会创建一个定时任务，定时的从nacos拉取配置。 1234ReadableDataSource&lt;String, List&lt;FlowRule&gt;&gt; flowRuleDataSource = new NacosDataSource&lt;&gt;(remoteAddress, groupId, dataId, source -&gt; JSON.parseObject(source, new TypeReference&lt;List&lt;FlowRule&gt;&gt;() &#123; &#125;));FlowRuleManager.register2Property(flowRuleDataSource.getProperty()); 现在就需要把上边的代码想办法集成到Spring中，方式有很多。在spring-cloud-alibaba中提供了基于配置的方式 12345678910111213141516171819202122232425262728293031spring: application: name: mall-user-sentinel-rule-push-demo #微服务名称 #配置nacos注册中心地址 cloud: nacos: discovery: server-addr: 127.0.0.1:8848 sentinel: transport: # 添加sentinel的控制台地址 dashboard: 127.0.0.1:8080 # 指定应用与Sentinel控制台交互的端口，应用本地会起一个该端口占用的HttpServer #port: 8719 datasource: flow-rules: #名称自定义，唯一 nacos: server-addr: 127.0.0.1:8848 dataId: $&#123;spring.application.name&#125;-flow-rules groupId: SENTINEL_GROUP # 注意groupId对应Sentinel Dashboard中的定义 data-type: json rule-type: flow #规范类型，一个枚举类 namespace: public converter-class: com.tuling.mall.sentinelrulepush.converter.MyConverterConfig # 自定义转换器 degrade-rules: nacos: server-addr: 127.0.0.1:8848 dataId: $&#123;spring.application.name&#125;-degrade-rules groupId: SENTINEL_GROUP data-type: json rule-type: degrade 在自动配置类SentinelAutoConfiguration中@Bean了这个类SentinelDataSourceHandler。这个类就是用来完成基于上边的规则来完成sentinel与nacos的整合的。zookeeper、redis等都可以，详细配置看SentinelProperties这个配置接收类就好了。 这种模式其实还是有问题，就是客户端链接了sentinel控制台，但sentinel控制台不支持与nacos通信。也就是说sentinel控制台的修改不会影响到nacos。这点在生产上是不行的。而且在微服务中与sentinel控制台控制链接并不优雅。这样就有两种优化： 不使用sentinel控制台，直接在nacos中修改 修改sentinel控制台，使其直接与nacos通信，客户端不与sentinel控制台通信 如果基于2的优化，最终的架构就如下图： 而修改sentinel控制台，在sentinel控制台源码其实提供了一个接口DynamicRulePublisher，该接口就是用来推消息的。不过没有实现，只是在测试类中实现了nacos、zookeeper的推送。 上边的代码只是退规则到配置中心，还有从配置中心中拉规则。源码也提供了接口DynamicRuleProvider。同样的，还是在测试中写了nacos、zookeeper的拉规则。需要自己修改。 实现完DynamicRulePublisher和DynamicRuleProvider后还需要在Controller中修改下。还有一点要注意，发布的时候，也就是DynamicRulePublisher的实现时，传入的的….RuleEntity（比如：FlowRuleEntity），这时需要转一下，通过RuleEntity#toRule()转成对应的Rule。 sentinel-dashboard(改造版源码) 总结","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"18-Sentinel核心源码","slug":"springcloud/18-Sentinel核心源码","date":"2021-11-27T12:00:26.000Z","updated":"2022-03-23T09:03:57.165Z","comments":true,"path":"blog/springcloud/18-Sentinel核心源码/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/18-Sentinel%E6%A0%B8%E5%BF%83%E6%BA%90%E7%A0%81/","excerpt":"","text":"前提Sentinel工作主流程Sentinel 工作主流程 在 Sentinel 里面，所有的资源都对应一个资源名称（resourceName），每次资源调用都会创建一个 Entry 对象。Entry 可以通过对主流框架的适配自动创建，也可以通过注解的方式或调用 SphU API 显式创建。Entry 创建的时候，同时也会创建一系列功能插槽（slot chain），这些插槽有不同的职责，例如: NodeSelectorSlot 负责收集资源的路径，并将这些资源的调用路径，以树状结构存储起来，用于根据调用路径来限流降级； ClusterBuilderSlot 则用于存储资源的统计信息以及调用者信息，例如该资源的 RT, QPS, thread count 等等，这些信息将用作为多维度限流，降级的依据； StatisticSlot 则用于记录、统计不同纬度的 runtime 指标监控信息； FlowSlot 则用于根据预设的限流规则以及前面 slot 统计的状态，来进行流量控制； AuthoritySlot 则根据配置的黑白名单和调用来源信息，来做黑白名单控制； DegradeSlot 则通过统计信息以及预设的规则，来做熔断降级； SystemSlot 则通过系统的状态，例如 load1 等，来控制总的入口流量； 总体的框架如下: Sentinel 将 ProcessorSlot 作为 SPI 接口进行扩展（1.7.2 版本以前 SlotChainBuilder 作为 SPI），使得 Slot Chain 具备了扩展的能力。您可以自行加入自定义的 slot 并编排 slot 间的顺序，从而可以给 Sentinel 添加自定义的功能。 Sentinel的API使用12345678910111213141516171819Entry entry = null;// 务必保证 finally 会被执行try &#123; // 资源名可使用任意有业务语义的字符串 开启资源的保护 entry = SphU.entry(&quot;自定义资源名&quot;); // ContextUtil 设置上下文 // 被保护的业务逻辑 method // do something...&#125; catch (BlockException ex) &#123; // 资源访问阻止，被限流或被降级 Sentinel定义异常 流控规则，降级规则，热点参数规则。。。服务降级(降级规则) // 进行相应的处理操作&#125; catch (Exception ex) &#123; // 若需要配置降级规则，需要通过这种方式记录业务异常 RuntimeException 服务降级 mock feign:fallback Tracer.traceEntry(ex, entry);&#125; finally &#123; // 务必保证 exit，务必保证每个 entry 与 exit 配对 if (entry != null) &#123; entry.exit();&#125; 引入的自动配置类看一个框架的客户端，重要的就是自动配置类。 对于Sentinel，在代码中引入了依赖 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt; 客户端的@SentinelResource注解在自动配置类SentinelAutoConfiguration中@Bean了这一个类 12345@Bean@ConditionalOnMissingBeanpublic SentinelResourceAspect sentinelResourceAspect() &#123; return new SentinelResourceAspect();&#125; 这类就是一个切面，其定义如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Aspectpublic class SentinelResourceAspect extends AbstractSentinelAspectSupport &#123; @Pointcut(&quot;@annotation(com.alibaba.csp.sentinel.annotation.SentinelResource)&quot;) public void sentinelResourceAnnotationPointcut() &#123; &#125; @Around(&quot;sentinelResourceAnnotationPointcut()&quot;) public Object invokeResourceWithSentinel(ProceedingJoinPoint pjp) throws Throwable &#123; Method originMethod = resolveMethod(pjp); SentinelResource annotation = originMethod.getAnnotation(SentinelResource.class); if (annotation == null) &#123; // Should not go through here. throw new IllegalStateException(&quot;Wrong state for SentinelResource annotation&quot;); &#125; String resourceName = getResourceName(annotation.value(), originMethod); EntryType entryType = annotation.entryType(); int resourceType = annotation.resourceType(); Entry entry = null; try &#123; entry = SphU.entry(resourceName, resourceType, entryType, pjp.getArgs()); Object result = pjp.proceed(); return result; &#125; catch (BlockException ex) &#123; return handleBlockException(pjp, annotation, ex); &#125; catch (Throwable ex) &#123; Class&lt;? extends Throwable&gt;[] exceptionsToIgnore = annotation.exceptionsToIgnore(); // The ignore list will be checked first. if (exceptionsToIgnore.length &gt; 0 &amp;&amp; exceptionBelongsTo(ex, exceptionsToIgnore)) &#123; throw ex; &#125; if (exceptionBelongsTo(ex, annotation.exceptionsToTrace())) &#123; traceException(ex); return handleFallback(pjp, annotation, ex); &#125; // No fallback function can handle the exception, so throw it out. throw ex; &#125; finally &#123; if (entry != null) &#123; entry.exit(1, pjp.getArgs()); &#125; &#125; &#125;&#125; 可以看到，就是为有SentinelResource注解的方法添加一个切面而已。而赠强的逻辑其实就是Sentinel的API使用的使用而已 Sentinel添加的HandlerInterceptor在使用的时候发现，每一个接口都组成成了Sentinel的资源了。这个是通过拦截器实现的，在自动配置类中SentinelWebAutoConfiguration. 123456789101112131415161718192021222324252627282930。。。。public class SentinelWebAutoConfiguration implements WebMvcConfigurer &#123; @Autowired private Optional&lt;SentinelWebInterceptor&gt; sentinelWebInterceptorOptional; @Override public void addInterceptors(InterceptorRegistry registry) &#123; if (!sentinelWebInterceptorOptional.isPresent()) &#123; return; &#125; SentinelProperties.Filter filterConfig = properties.getFilter(); registry.addInterceptor(sentinelWebInterceptorOptional.get()) .order(filterConfig.getOrder()) .addPathPatterns(filterConfig.getUrlPatterns()); log.info( &quot;[Sentinel Starter] register SentinelWebInterceptor with urlPatterns: &#123;&#125;.&quot;, filterConfig.getUrlPatterns()); &#125; @Bean @ConditionalOnProperty(name = &quot;spring.cloud.sentinel.filter.enabled&quot;, matchIfMissing = true) public SentinelWebInterceptor sentinelWebInterceptor( SentinelWebMvcConfig sentinelWebMvcConfig) &#123; return new SentinelWebInterceptor(sentinelWebMvcConfig); &#125; .....&#125; 添加了一个SentinelWebInterceptor拦截器。下面是这个拦截器的核心： 123456789101112131415161718192021222324252627282930@Overridepublic boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; try &#123; String resourceName = getResourceName(request); if (StringUtil.isEmpty(resourceName)) &#123; return true; &#125; if (increaseReferece(request, this.baseWebMvcConfig.getRequestRefName(), 1) != 1) &#123; return true; &#125; // Parse the request origin using registered origin parser. String origin = parseOrigin(request); String contextName = getContextName(request); ContextUtil.enter(contextName, origin); Entry entry = SphU.entry(resourceName, ResourceTypeConstants.COMMON_WEB, EntryType.IN); request.setAttribute(baseWebMvcConfig.getRequestAttributeName(), entry); return true; &#125; catch (BlockException e) &#123; try &#123; handleBlockException(request, response, e); &#125; finally &#123; ContextUtil.exit(); &#125; return false; &#125;&#125; 就是Sentinel的API使用 Sentinel的核心跟踪代码： 12345678910111213141516171819202122232425262728SphU.entry --&gt; CtSph.entryWithPriority// CtSph private Entry entryWithPriority(ResourceWrapper resourceWrapper, int count, boolean prioritized, Object... args) throws BlockException &#123; Context context = ContextUtil.getContext(); ... if (context == null) &#123; // Using default context. context = InternalContextUtil.internalEnter(Constants.CONTEXT_DEFAULT_NAME); &#125; ProcessorSlot&lt;Object&gt; chain = lookProcessChain(resourceWrapper); Entry e = new CtEntry(resourceWrapper, chain, context); try &#123; chain.entry(context, resourceWrapper, null, count, prioritized, args); &#125; catch (BlockException e1) &#123; e.exit(count, args); throw e1; &#125; catch (Throwable e1) &#123; // This should not happen, unless there are errors existing in Sentinel internal. RecordLog.info(&quot;Sentinel unexpected exception&quot;, e1); &#125; return e;&#125; 责任链ProcessorSlotChain构建从框架图可以看到，整个流程的处理就是一条链。在看上边的代码就能发现 1ProcessorSlot&lt;Object&gt; chain = lookProcessChain(resourceWrapper); 就是完成处ProcessorSlot的初始化。跟踪代码后发先，lookProcessChain会使用javaSPI来返回一个SlotChainBuilder对象，如果没有就使用默认实现DefaultSlotChainBuilder。这里默认就会使用DefaultSlotChainBuilder类来创建ProcessorSlot。 123456789101112131415161718192021public class DefaultSlotChainBuilder implements SlotChainBuilder &#123; @Override public ProcessorSlotChain build() &#123; ProcessorSlotChain chain = new DefaultProcessorSlotChain(); // Note: the instances of ProcessorSlot should be different, since they are not stateless. // 使用javaSPI引入ProcessorSlotChain中的Slot // 如果自己实现一个ProcessorSlot，一定要继承AbstractLinkedProcessorSlot List&lt;ProcessorSlot&gt; sortedSlotList = SpiLoader.loadPrototypeInstanceListSorted(ProcessorSlot.class); for (ProcessorSlot slot : sortedSlotList) &#123; if (!(slot instanceof AbstractLinkedProcessorSlot)) &#123; continue; &#125; chain.addLast((AbstractLinkedProcessorSlot&lt;?&gt;) slot); &#125; return chain; &#125;&#125; 这里也是使用了java的SPI，来加载ProcessorSlot的实现。下面就是sentinel默认的ProcessorSlot 12345678910# Sentinel default ProcessorSlotscom.alibaba.csp.sentinel.slots.nodeselector.NodeSelectorSlotcom.alibaba.csp.sentinel.slots.clusterbuilder.ClusterBuilderSlotcom.alibaba.csp.sentinel.slots.logger.LogSlotcom.alibaba.csp.sentinel.slots.statistic.StatisticSlotcom.alibaba.csp.sentinel.slots.block.authority.AuthoritySlotcom.alibaba.csp.sentinel.slots.system.SystemSlotcom.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowSlotcom.alibaba.csp.sentinel.slots.block.flow.FlowSlotcom.alibaba.csp.sentinel.slots.block.degrade.DegradeSlot 最终会形成下面的chain 这个链的顺序是通过@SpiOrder注解决定的，使用的是从小到大的顺序排序的 最后返回的chain对象的类型为DefaultProcessorSlotChain 责任链的Slot 根据这幅图就能知道每一个ProcessorSlot的作用了 这些类的整体逻辑很简单，但在里面涉及到滑动时间窗的算法、漏桶算法和令牌桶算法。还有涉及到熔断的设计。理解了这些，那Sentinel的代码就很简单了。 熔断设计——DegradeSlot熔断的处理流程如图： 限流算法计数器法计数器法是限流算法里最简单也是最容易实现的一种算法。比如我们规定，对于A接口来说，我们1分钟的访问次数不能超过100个。那么我们可以这么做：在一开始的时候，我们可以设置一个计数器counter，每当一个请求过来的时候，counter就加1，如果counter的值大于100并且该请求与第一个 请求的间隔时间还在1分钟之内，那么说明请求数过多；如果该请求与第一个请求的间隔时间大于1分钟，且counter的值还在限流范围内，那么就重置 counter。 简单的代码实现： 123456789101112131415161718192021public class Counter &#123; public long timeStamp = System.currentTimeMillis(); // 当前时间 public int reqCount = 0; // 初始化计数器 public final int limit = 100; // 时间窗口内最大请求数 public final long interval = 1000 * 60; // 时间窗口ms public synchronized boolean limit() &#123; long now = System.currentTimeMillis(); if (now &lt; timeStamp + interval) &#123; // 在时间窗口内 reqCount++; // 判断当前时间窗口内是否超过最大请求控制数 return reqCount &lt;= limit; &#125; else &#123; timeStamp = now; // 超时后重置 reqCount = 1; return true; &#125; &#125; &#125; 这种算法是有缺陷的，就是精度太低 12345[___A___][___B___]能限制A和B这个区间的请求数，但是[___A___][___B___] [____C___]C这个区间的请求数就不能限制了 滑动时间窗口滑动时间窗口，又称rolling window。为了解决计数器法统计精度太低的问题，引入了滑动窗口算法。下面这张图，很好地解释了滑动窗口算法： 简单的实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class SlideWindow &#123; /** 队列id和队列的映射关系，队列里面存储的是每一次通过时候的时间戳，这样可以使得程序里有多个限流队列 */ private volatile static Map&lt;String, List&lt;Long&gt;&gt; MAP = new ConcurrentHashMap&lt;&gt;(); private SlideWindow() &#123;&#125; public static void main(String[] args) throws InterruptedException &#123; while (true) &#123; // 任意10秒内，只允许2次通过 System.out.println(LocalTime.now().toString() + SlideWindow.isGo(&quot;ListId&quot;, 2, 10000L)); // 睡眠0-10秒 Thread.sleep(1000 * new Random().nextInt(10)); &#125; &#125; /** * 滑动时间窗口限流算法 * 在指定时间窗口，指定限制次数内，是否允许通过 * * @param listId 队列id * @param count 限制次数 * @param timeWindow 时间窗口大小 * @return 是否允许通过 */ public static synchronized boolean isGo(String listId, int count, long timeWindow) &#123; // 获取当前时间 long nowTime = System.currentTimeMillis(); // 根据队列id，取出对应的限流队列，若没有则创建 List&lt;Long&gt; list = MAP.computeIfAbsent(listId, k -&gt; new LinkedList&lt;&gt;()); // 如果队列还没满，则允许通过，并添加当前时间戳到队列开始位置 if (list.size() &lt; count) &#123; list.add(0, nowTime); return true; &#125; // 队列已满（达到限制次数），则获取队列中最早添加的时间戳 Long farTime = list.get(count - 1); // 用当前时间戳 减去 最早添加的时间戳 if (nowTime - farTime &lt;= timeWindow) &#123; // 若结果小于等于timeWindow，则说明在timeWindow内，通过的次数大于count // 不允许通过 return false; &#125; else &#123; // 若结果大于timeWindow，则说明在timeWindow内，通过的次数小于等于count // 允许通过，并删除最早添加的时间戳，将当前时间添加到队列开始位置 list.remove(count - 1); list.add(0, nowTime); return true; &#125; &#125;&#125; Sentinel中的滑动时间窗LeapArray就是在Sentinel设计的滑动时间窗抽象类，实现了滑动时间窗口的功能。 首先看它的初始化： 123456789101112131415161718192021222324252627public abstract class LeapArray&lt;T&gt; &#123; protected int windowLengthInMs; protected int sampleCount; protected int intervalInMs; private double intervalInSecond; protected final AtomicReferenceArray&lt;WindowWrap&lt;T&gt;&gt; array; /** * The conditional (predicate) update lock is used only when current bucket is deprecated. */ private final ReentrantLock updateLock = new ReentrantLock(); public LeapArray(int sampleCount, int intervalInMs) &#123; AssertUtil.isTrue(sampleCount &gt; 0, &quot;bucket count is invalid: &quot; + sampleCount); AssertUtil.isTrue(intervalInMs &gt; 0, &quot;total time interval of the sliding window should be positive&quot;); AssertUtil.isTrue(intervalInMs % sampleCount == 0, &quot;time span needs to be evenly divided&quot;); this.windowLengthInMs = intervalInMs / sampleCount; this.intervalInMs = intervalInMs; this.intervalInSecond = intervalInMs / 1000.0; this.sampleCount = sampleCount; this.array = new AtomicReferenceArray&lt;&gt;(sampleCount); &#125;&#125; 这里有3个重要属性 array：这就是时间格子，是一个数组结构，但从整体的看，是一个环形数组，随着时间流逝，旧的格子会被覆盖。 windowLengthInMs：每一个格子的时间长度 intervalInMs：这个就是时间窗口的长度 其中每一个每个格子的类型为WindowWrap，这个类有3个重要的属性： windowLengthInMs 每一个格子的时间长度 windowStart 格子的开始时间 value 格子里面存放的业务数据 了解了这3个属性后，构造方法的入场就很明白了 sampleCount：指定格子的数量 intervalInMs：指定时间窗口的大小 最终形成如下结构： 这里看两个重要的方法： 返回当前时间所在的格子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596// LeapArray// timeMillis=TimeUtil.currentTimeMillis()public WindowWrap&lt;T&gt; currentWindow(long timeMillis) &#123; if (timeMillis &lt; 0) &#123; return null; &#125; /* * * long timeId = timeMillis / windowLengthInMs; * return (int)(timeId % array.length()); * * timeMillis这个理解为现在的时间 * timeMillis=TimeUtil.currentTimeMillis() * * 把时间切割成了长度为windowLengthInMs的一个个小格子 * timeMillis / windowLengthInMs就返回了现在的实际所在的格子的下标 * 当不可能由无限多个格子，所以由array.length()个格子组成一个圆。就类似一个时钟的分针，1s为一个单位，60s一个周期 * 所以这个由array.length()个格子组成的圆，一个格子为一个单位，array.length()个周期。 * 所以timeId % array.length()就返回了当前时间所在格子的下标 * */ int idx = calculateTimeIdx(timeMillis); // Calculate current bucket start time. // timeMillis - timeMillis % windowLengthInMs; // timeMillis % windowLengthInMs返回当前时间在格子的位置 //timeMillis - timeMillis % windowLengthInMs 就是这个格子的开始时间 long windowStart = calculateWindowStart(timeMillis); /* * Get bucket item at given time from the array. * * (1) Bucket is absent, then just create a new bucket and CAS update to circular array. * (2) Bucket is up-to-date, then just return the bucket. * (3) Bucket is deprecated, then reset current bucket and clean all deprecated buckets. */ while (true) &#123; WindowWrap&lt;T&gt; old = array.get(idx); if (old == null) &#123; /* * B0 B1 B2 NULL B4 * ||_______|_______|_______|_______|_______||___ * 200 400 600 800 1000 1200 timestamp * ^ * time=888 * bucket is empty, so create new and update * 为空的话，就初始化就好了 */ WindowWrap&lt;T&gt; window = new WindowWrap&lt;T&gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); if (array.compareAndSet(idx, null, window)) &#123; // Successfully updated, return the created bucket. return window; &#125; else &#123; // Contention failed, the thread will yield its time slice to wait for bucket available. Thread.yield(); &#125; &#125; else if (windowStart == old.windowStart()) &#123; /* * B0 B1 B2 B3 B4 * ||_______|_______|_______|_______|_______||___ * 200 400 600 800 1000 1200 timestamp * ^ * time=888 * startTime of Bucket 3: 800, so it&#x27;s up-to-date * 如果不为空，而且当前所在格子的开始时间和array个对应格子的开始时间一样，就代表这两个格子是一样的 */ return old; &#125; else if (windowStart &gt; old.windowStart()) &#123; /* * (old) * B0 B1 B2 NULL B4 * |_______||_______|_______|_______|_______|_______||___ * ... 1200 1400 1600 1800 2000 2200 timestamp * ^ * time=1676 * startTime of Bucket 2: 400, deprecated, should be reset * 如果当前所在格子的开始时间大于array个对应格子的开始时间，代表着array个对应格子不是当前时间窗口的格子，需要丢弃 */ if (updateLock.tryLock()) &#123; try &#123; // Successfully get the update lock, now we reset the bucket. return resetWindowTo(old, windowStart); &#125; finally &#123; updateLock.unlock(); &#125; &#125; else &#123; // Contention failed, the thread will yield its time slice to wait for bucket available. Thread.yield(); &#125; &#125; else if (windowStart &lt; old.windowStart()) &#123; // 出现这种情况就代表timeMillis的值不是当前的时间，windowStart的格子已经被丢弃了，所以这里新建一个 // Should not go through here, as the provided time is already behind. return new WindowWrap&lt;T&gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); &#125; &#125;&#125; 返回当前时间的时间窗口中的格子 12345678910111213141516171819// LeapArray// validTime=TimeUtil.currentTimeMillis()public List&lt;WindowWrap&lt;T&gt;&gt; list(long validTime) &#123; int size = array.length(); List&lt;WindowWrap&lt;T&gt;&gt; result = new ArrayList&lt;WindowWrap&lt;T&gt;&gt;(size); for (int i = 0; i &lt; size; i++) &#123; WindowWrap&lt;T&gt; windowWrap = array.get(i); // isWindowDeprecated是整个方法的核心 // time - windowWrap.windowStart() &gt; intervalInMs; // time - windowWrap.windowStart()返回的是时间间隔，超过了时间窗口的间隔，就代表这是一个已经被丢弃的格子 if (windowWrap == null || isWindowDeprecated(validTime, windowWrap)) &#123; continue; &#125; result.add(windowWrap); &#125; return result;&#125; 如果理解了LeapArray，那统计这块就很好理解了。 漏桶算法漏桶算法，又称leaky bucket。 从图中我们可以看到，整个算法其实十分简单。首先，我们有一个固定容量的桶，有水流进来，也有水流出去。对于流进来的水来说，我们无法预计一共有多少水会流进来，也无法预计水流的速度。但是对于流出去的水来说，这个桶可以控制水流出的速率。而且，当桶满了之后，多余的水将会溢出。 我们将算法中的水换成实际应用中的请求，我们可以看到漏桶算法天生就限制了请求的速度。当使用了漏桶算法，我们可以保证接口会以一个常速速率来处理请求。所以漏桶算法天生不会出现临界问题。 简单实现： 123456789101112131415161718192021public class LeakyBucket &#123; private long timeStamp = System.currentTimeMillis(); // 当前时间 private long capacity; // 桶的容量 private long rate; // 水漏出的速度(每秒系统能处理的请求数) private long water; // 当前水量(当前累积请求数) public synchronized boolean limit() &#123; long now = System.currentTimeMillis(); // 先执行漏水，计算剩余水量 water = Math.max(0, water - ((now - timeStamp)/1000) * rate); timeStamp = now; if ((water + 1) &lt;= capacity) &#123; // 尝试加水,并且水还未满 water += 1; return true; &#125; else &#123; // 水满，拒绝加水 return false; &#125; &#125;&#125; Sentinel中的漏桶算法——RateLimiterController 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// RateLimiterController@Overridepublic boolean canPass(Node node, int acquireCount, boolean prioritized) &#123; // Pass when acquire count is less or equal than 0. if (acquireCount &lt;= 0) &#123; return true; &#125; // Reject when count is less or equal than 0. // Otherwise,the costTime will be max of long and waitTime will overflow in some cases. if (count &lt;= 0) &#123; return false; &#125; long currentTime = TimeUtil.currentTimeMillis(); // Calculate the interval between every two requests. // count就是配置的阈值，acquireCount是1 // 这里算出没两个请求之间的间隔 long costTime = Math.round(1.0 * (acquireCount) / count * 1000); // Expected pass time of this request. // latestPassedTime.get()上一个请求的执行时间 // expectedTime就是当前请求期望执行的时间 long expectedTime = costTime + latestPassedTime.get(); if (expectedTime &lt;= currentTime) &#123; // 进入到这里代表请求可以执行了 // Contention may exist here, but it&#x27;s okay. latestPassedTime.set(currentTime); return true; &#125; else &#123; // Calculate the time to wait. // 这里算出需要等待的时间 long waitTime = expectedTime - TimeUtil.currentTimeMillis(); // 判断是否超过最大的等待时间 if (waitTime &gt; maxQueueingTimeMs) &#123; // 超过后，需要限流 return false; &#125; else &#123; // oldTime和expectedTime其实是一回事 long oldTime = latestPassedTime.addAndGet(costTime); try &#123; waitTime = oldTime - TimeUtil.currentTimeMillis(); if (waitTime &gt; maxQueueingTimeMs) &#123; latestPassedTime.addAndGet(-costTime); return false; &#125; // in race condition waitTime may &lt;= 0 if (waitTime &gt; 0) &#123; Thread.sleep(waitTime); &#125; return true; &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125; return false;&#125; 上边的代码其实是有问题的，比如： 123456long expectedTime = costTime + latestPassedTime.get();if (expectedTime &lt;= currentTime) &#123; latestPassedTime.set(currentTime); return true;&#125; 这里如果有很多个线程进来，expectedTime很有可能获取到的值都是一样的，那么就有很多线程都被放行了 令牌桶算法令牌桶算法，又称token bucket。同样为了理解该算法，我们来看一下该算法的示意图： 从图中我们可以看到，令牌桶算法比漏桶算法稍显复杂。首先，我们有一个固定容量的桶，桶里存放着令牌（token）。桶一开始是空的，token以 一个固定的速率r往桶里填充，直到达到桶的容量，多余的令牌将会被丢弃。每当一个请求过来时，就会尝试从桶里移除一个令牌，如果没有令牌的话，请求无法通过。 简单实现： 123456789101112131415161718192021public class TokenBucket &#123; public long timeStamp = System.currentTimeMillis(); // 当前时间 public long capacity; // 桶的容量 public long rate; // 令牌放入速度 public long tokens; // 当前令牌数量 public boolean grant() &#123; long now = System.currentTimeMillis(); // 先添加令牌 tokens = Math.min(capacity, tokens + (now - timeStamp) * rate); timeStamp = now; if (tokens &lt; 1) &#123; // 若不到1个令牌,则拒绝 return false; &#125; else &#123; // 还有令牌，领取令牌 tokens -= 1; return true; &#125; &#125;&#125; Sentinel中的令牌桶算法——WarmUpController限流算法小结 计数器 VS 滑动窗口： 计数器算法是最简单的算法，可以看成是滑动窗口的低精度实现。 滑动窗口由于需要存储多份的计数器（每一个格子存一份），所以滑动窗口在实现上需要更多的存储空间。 也就是说，如果滑动窗口的精度越高，需要的存储空间就越大。 漏桶算法 VS 令牌桶算法： 漏桶算法和令牌桶算法最明显的区别是令牌桶算法允许流量一定程度的突发。 因为默认的令牌桶算法，取走token是不需要耗费时间的，也就是说，假设桶内有100个token时，那么可以瞬间允许100个请求通过。 当然我们需要具体情况具体分析，只有最合适的算法，没有最优的算法。 总结","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"17-Sentinel整合RestTemplate&openFegin&Dubbo","slug":"springcloud/17-Sentinel整合RestTemplate&openFegin&Dubbo","date":"2021-11-27T12:00:25.000Z","updated":"2022-03-23T09:03:57.036Z","comments":true,"path":"blog/springcloud/17-Sentinel整合RestTemplate&openFegin&Dubbo/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/17-Sentinel%E6%95%B4%E5%90%88RestTemplate&openFegin&Dubbo/","excerpt":"","text":"现在服务存在这样的关系： 如果对maill-order添加流控规则： 如果mall-order限流了，那在调用段会 这是Feign把mall-order的BolckException错误包装成HttpClientErrorException错误了。 那如果对mall-user-sentinel-ribbon-demo添加流控规则，由于ribbon的均衡负载规则就是轮询，请求能均摊到没一个服务。这种模式是sentinel集群规则的替代。 而现在有两种情况， 项目只使用Ribbon（也可以是SpringCloud的loadBalance），使用RestTemplate调用服务 项目使用Feign调用服务 项目中使用的是Dubbo RestTemplate整合Sentinel对RestTemplate的扩展，在均衡负载已经讲过，通过ClientHttpRequestInterceptor来实现的，这里的整合Sentinel也一样。Sentinel中提供了SentinelProtectInterceptor。 可以通过如下方式配置 1234#true开启sentinel对resttemplate的支持，false则关闭 默认trueresttemplate: sentinel: enabled: true java配置： 123456789101112@Configurationpublic class RestConfig &#123; @Bean @LoadBalanced //拦截器 @SentinelRestTemplate( fallbackClass = GlobalExceptionUtil.class,fallback = &quot;fallback&quot;, blockHandler = &quot;handleException&quot; ,blockHandlerClass = GlobalExceptionUtil.class ) public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 这样，在使用RestTemplate的时候就能通过SentinelProtectInterceptor来完成Sentinel的功能了。 而注解中对GlobalExceptionUtil中对应方法的入参是有要求的，必须是： 1234public static SentinelClientHttpResponse fallback(HttpRequest request, byte[] body, ClientHttpRequestExecution execution, BlockException ex) 可以看SentinelProtectInterceptor了解为什么。 原理可以看 Feign整合Sentinel这种方式肯定是最常用的 123feign: sentinel: enabled: true #开启sentinel对feign的支持 默认false 这样对于Feign就能支持Sentinel的功能了。 而要处理BlockException错误可以通过下面两种方式配置 使用fallback 1234@FeignClient(value = &quot;mall-order&quot;,path = &quot;/order&quot;, fallback = FallbackOrderFeignService.class)@Component //必须交给spring 管理public class FallbackOrderFeignService implements OrderFeignService 使用fallbackFactory 1234567891011121314151617181920@FeignClient(value = &quot;mall-order&quot;,path = &quot;/order&quot;, fallbackFactory = FallbackOrderFeignServiceFactory.class)@Component //必须交给spring管理public class FallbackOrderFeignServiceFactory implements FallbackFactory&lt;OrderFeignService&gt; &#123; @Override public OrderFeignService create(Throwable throwable) &#123; return new OrderFeignService() &#123; @Override public R findOrderByUserId(Integer userId) &#123; if (throwable instanceof FlowException) &#123; return R.error(100,&quot;接口限流了&quot;); &#125; return R.error(-1,&quot;=======服务降级了========&quot;); &#125; &#125;; &#125;&#125; 在通过上述配置后，Sentinel控制台的调用链路就变成这样了 可以根据需求来设置规则 原理： feign的架构图如下： sentinel是对代理机制进行了扩展，Feign是使用JDK的动态代理的，sentinel对这点没边，变的是换了一个InvocationHandler。在sentinel中提供了SentinelInvocationHandler。而为了使用这个InvocationHandler，sentinel扩展了Feign.Builder接口。详情看 Sentinel整合DubboSentinel 提供 Dubbo 的相关适配 Sentinel Dubbo Adapter，主要包括针对 Service Provider 和 Service Consumer 实现的 Filter。相关模块： sentinel-apache-dubbo-adapter（兼容 Apache Dubbo 2.7.x 及以上版本，自 Sentinel 1.5.1 开始支持） sentinel-dubbo-adapter（兼容 Dubbo 2.6.x 版本） 引入此依赖后，Dubbo 的服务接口和方法（包括调用端和服务端）就会成为 Sentinel 中的资源，在配置了规则后就可以自动享受到Sentinel 的防护能力。 Sentinel Dubbo Adapter 还支持配置全局的 fallback 函数，可以在 Dubbo 服务被限流&#x2F;降级&#x2F;负载保护的时候进行相应的 fallback 处理。用户只需要实现自定义的 DubboFallback 接口，并通过 DubboAdapterGlobalConfig注册即可。默认情况会直接将 BlockException 包装后抛出。同时，我们还可以配合 Dubbo 的 fallback 机制 来为降级的服务提供替代的实现。 Provider端 对服务提供方的流量控制可分为服务提供方的自我保护能力和服务提供方对服务消费方的请求分配能力两个维度。 Provider 用于向外界提供服务，处理各个消费者的调用请求。为了保护 Provider 不被激增的流量拖垮影响稳定性，可以给 Provider 配置 QPS 模式的限流，这样当每秒的请求量超过设定的阈值时会自动拒绝多的请求。限流粒度可以是 服务接口 和 服务方法 两种粒度。若希望整个服务接口的 QPS 不超过一定数值，则可以为对应服务接口资源（resourceName 为接口全限定名）配置 QPS 阈值；若希望服务的某个方法的 QPS 不超过一定数值，则可以为对应服务方法资源（resourceName 为接口全限定名:方法签名）配置 QPS 阈值。 限流粒度可以是服务接口和服务方法两种粒度： 服务接口：resourceName 为 接口全限定名，如 com.tuling.mall.service.UserService 服务方法：resourceName 为 接口全限定名:方法签名，如 com.tuling.mall.service.UserService:getById(java.lang.Integer) Consumer端 对服务提供方的流量控制可分为控制并发线程数和服务降级两个维度。 控制并发线程数 Service Consumer 作为客户端去调用远程服务。每一个服务都可能会依赖几个下游服务，若某个服务 A 依赖的下游服务 B 出现了不稳定的情况，服务 A 请求 服务 B 的响应时间变长，从而服务 A 调用服务 B 的线程就会产生堆积，最终可能耗尽服务 A 的线程数。我们通过用并发线程数来控制对下游服务 B 的访问，来保证下游服务不可靠的时候，不会拖垮服务自身。基于这种场景，推荐给 Consumer 配置线程数模式的限流，来保证自身不被不稳定服务所影响。采用基于线程数的限流模式后，我们不需要再显式地去进行线程池隔离，Sentinel 会控制资源的线程数，超出的请求直接拒绝，直到堆积的线程处理完成，可以达到信号量隔离的效果。 服务降级 当服务依赖于多个下游服务，而某个下游服务调用非常慢时，会严重影响当前服务的调用。这里我们可以利用 Sentinel 熔断降级的功能，为调用端配置基于平均 RT 的降级规则。这样当调用链路中某个服务调用的平均 RT 升高，在一定的次数内超过配置的 RT 阈值，Sentinel 就会对此调用资源进行降级操作，接下来的调用都会立刻拒绝，直到过了一段设定的时间后才恢复，从而保护服务不被调用端短板所影响。同时可以配合 fallback 功能使用，在被降级的时候提供相应的处理逻辑。 整合1.引入依赖 123456789&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--Sentinel 对 Dubbo的适配 Apache Dubbo 2.7.x 及以上版本--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-apache-dubbo-adapter&lt;/artifactId&gt;&lt;/dependency&gt; 2.接入sentinel dashboard，yml中增加配置 12345678910111213spring: cloud: sentinel: transport: # 添加sentinel的控制台地址 dashboard: 127.0.0.1:8080#暴露actuator端点 management: endpoints: web: exposure: include: &#x27;*&#x27; 3.consumer端配置流控规则测试 123456789101112131415161718@RequestMapping(&quot;/info/&#123;id&#125;&quot;)public User info(@PathVariable(&quot;id&quot;) Integer id) &#123; User user = null; try &#123; user = userService.getById(id); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return user;&#125;@PostConstructpublic void init() &#123; DubboAdapterGlobalConfig.setConsumerFallback( (invoker, invocation, ex) -&gt; AsyncRpcResult.newDefaultAsyncResult( new User(0,&quot;===fallback==&quot;), invocation));&#125; ​ 测试： http://localhost:8082/user/info/1 ​ 4.provider端配置流控规则测试 ​ 1234567891011121314151617@RequestMapping(&quot;/getById/&#123;id&#125;&quot;)@SentinelResource(&quot;getById&quot;)public User getById(@PathVariable(&quot;id&quot;) Integer id) &#123; User user = null; try &#123; user = userMapper.getById(id); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return user;&#125;@PostConstructpublic void init() &#123; DubboAdapterGlobalConfig.setProviderFallback( (invoker, invocation, ex) -&gt; AsyncRpcResult.newDefaultAsyncResult(new User(0,&quot;===provider fallback==&quot;), invocation));&#125; ​ 5.consumer中配置mock实现，关闭provider服务，测试mock降级 1234567891011121314@DubboReference(mock = &quot;com.tuling.mall.user.mock.UserServiceDubboMock&quot;)private UserService userService;public class UserServiceDubboMock implements UserService &#123; @Override public List&lt;User&gt; list() &#123; return null; &#125; @Override public User getById(Integer id) &#123; return new User(0,&quot;====mock===&quot;); &#125;&#125; ​","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"16-Sentinel详细使用","slug":"springcloud/16-Sentinel详细使用","date":"2021-11-27T12:00:24.000Z","updated":"2022-03-23T09:03:56.986Z","comments":true,"path":"blog/springcloud/16-Sentinel详细使用/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/16-Sentinel%E8%AF%A6%E7%BB%86%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Sentinel控制台介绍Sentinel 提供一个轻量级的开源控制台，它提供机器发现以及健康情况管理、监控（单机和集群），规则管理和推送的功能。 Sentinel 控制台包含如下功能: 查看机器列表以及健康情况：收集 Sentinel 客户端发送的心跳包，用于判断机器是否在线。 监控 (单机和集群聚合)：通过 Sentinel 客户端暴露的监控 API，定期拉取并且聚合应用监控信息，最终可以实现秒级的实时监控。 规则管理和推送：统一管理推送规则。 鉴权：生产环境中鉴权非常重要。这里每个开发者需要根据自己的实际情况进行定制。 BlockException异常统一处理——BlockExceptionHandlerspringwebmvc接口资源限流入口在HandlerInterceptor的实现类AbstractSentinelInterceptor的preHandle方法中，对异常的处理是BlockExceptionHandler的实现类 自定义BlockExceptionHandler 的实现类统一处理BlockException 12345678910111213141516171819202122232425262728293031323334@Componentpublic class MyBlockExceptionHandler implements BlockExceptionHandler &#123; @Override public void handle(HttpServletRequest request, HttpServletResponse response, BlockException e) throws Exception &#123; log.info(&quot;BlockExceptionHandler BlockException================&quot;+e.getRule()); R r = null; if (e instanceof FlowException) &#123; r = R.error(100,&quot;接口限流了&quot;); &#125; else if (e instanceof DegradeException) &#123; r = R.error(101,&quot;服务降级了&quot;); &#125; else if (e instanceof ParamFlowException) &#123; r = R.error(102,&quot;热点参数限流了&quot;); &#125; else if (e instanceof SystemBlockException) &#123; r = R.error(103,&quot;触发系统保护规则了&quot;); &#125; else if (e instanceof AuthorityException) &#123; r = R.error(104,&quot;授权规则不通过&quot;); &#125; //返回json数据 response.setStatus(500); response.setCharacterEncoding(&quot;utf-8&quot;); response.setContentType(MediaType.APPLICATION_JSON_VALUE); new ObjectMapper().writeValue(response.getWriter(), r); &#125;&#125; 流控规则限制请求核心服务提供者的流量，使大流量拦截在核心服务之外，这样可以更好的保证核心服务提供者不出问题，对于一些出问题的服务可以限制流量访问，只分配固定线程资源访问，这样能使整体的资源不至于被出问题的服务耗尽，进而整个系统雪崩。那么服务之间怎么限流，怎么资源隔离？例如可以通过线程池+队列的方式，通过信号量的方式。 流量控制（flow control），其原理是监控应用流量的 QPS 或并发线程数等指标，当达到指定的阈值时对流量进行控制，以避免被瞬时的流量高峰冲垮，从而保障应用的高可用性。 同一个资源可以创建多条限流规则。FlowSlot 会对该资源的所有限流规则依次遍历，直到有规则触发限流或者所有规则遍历完毕。一条限流规则主要由下面几个因素组成，我们可以组合这些元素来实现不同的限流效果。（FlowRule类的属性） Field 说明 默认值 resource 资源名，资源名是限流规则的作用对象 count 限流阈值 grade 限流阈值类型，QPS 模式（1）或并发线程数模式（0） QPS 模式 limitApp 流控针对的调用来源 default，代表不区分调用来源 strategy 调用关系限流策略：直接、链路、关联 根据资源本身（直接） controlBehavior 流控效果（直接拒绝&#x2F;WarmUp&#x2F;匀速+排队等待），不支持按调用关系限流 直接拒绝 clusterMode 是否集群限流 否 参考文档： 流量控制 · alibaba&#x2F;Sentinel Wiki (github.com) 限流阈值类型流量控制主要有两种统计类型，一种是统计并发线程数，另外一种则是统计 QPS。类型由 FlowRule 的 grade 字段来定义。其中，0 代表根据并发数量来限流，1 代表根据 QPS 来进行流量控制。 QPS（Query Per Second）：每秒请求数，就是说服务器在一秒的时间内处理了多少个请求。 并发线程数 并发数控制用于保护业务线程池不被慢调用耗尽。例如，当应用所依赖的下游应用由于某种原因导致服务不稳定、响应延迟增加，对于调用者来说，意味着吞吐量下降和更多的线程数占用，极端情况下甚至导致线程池耗尽。为应对太多线程占用的情况，业内有使用隔离的方案，比如通过不同业务逻辑使用不同线程池来隔离业务自身之间的资源争抢（线程池隔离）。这种隔离方案虽然隔离性比较好，但是代价就是线程数目太多，线程上下文切换的 overhead 比较大，特别是对低延时的调用有比较大的影响。 Sentinel 并发控制不负责创建和管理线程池，而是简单统计当前请求上下文的线程数目（正在执行的调用数目），如果超出阈值，新的请求会被立即拒绝，效果类似于信号量隔离。并发数控制通常在调用端进行配置。 流控模式基于调用关系的流量控制。调用关系包括调用方、被调用方；一个方法可能会调用其它方法，形成一个调用链路的层次关系。 直接（默认）资源调用达到设置的阈值后直接被流控抛出异常 关联当两个资源之间具有资源争抢或者依赖关系的时候，这两个资源便具有了关联。比如对数据库同一个字段的读操作和写操作存在争抢，读的速度过高会影响写得速度，写的速度过高会影响读的速度。如果放任读写操作争抢资源，则争抢本身带来的开销会降低整体的吞吐量。可使用关联限流来避免具有关联关系的资源之间过度的争抢。 举例来说，read_db 和 write_db 这两个资源分别代表数据库读写，我们可以给 read_db 设置限流规则来达到写优先的目的：设置 strategy 为 RuleConstant.STRATEGY_RELATE 同时设置 refResource 为 write_db。这样当写库操作过于频繁时，读数据的请求会被限流。 链路根据调用链路入口限流。 NodeSelectorSlot 中记录了资源之间的调用链路，这些资源通过调用关系，相互之间构成一棵调用树。这棵树的根节点是一个名字为 machine-root 的虚拟节点，调用链的入口都是这个虚节点的子节点。 1234567 machine-root / \\ / \\ Entrance1 Entrance2 / \\ / \\DefaultNode(nodeA) DefaultNode(nodeA) 比如存在这样的链路 从1.6.3版本开始，Sentinel Web filter默认收敛所有URL的入口context，导致链路限流不生效。 从1.7.0版本开始，官方在CommonFilter引入了WEB_CONTEXT_UNIFY参数，用于控制是否收敛context，将其配置为false即可根据不同的URL进行链路限流。 1.8.0 需要引入sentinel-web-servlet依赖 12345&lt;!--- 解决流控链路不生效的问题--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-web-servlet&lt;/artifactId&gt;&lt;/dependency&gt; 添加配置类，配置CommonFilter过滤器，指定WEB_CONTEXT_UNIFY&#x3D;false，禁止收敛URL的入口context @Configuration 1234567891011121314@Configurationpublic class SentinelConfig &#123; @Bean public FilterRegistrationBean sentinelFilterRegistration() &#123; FilterRegistrationBean registration = new FilterRegistrationBean(); registration.setFilter(new CommonFilter()); registration.addUrlPatterns(&quot;/*&quot;); // 入口资源关闭聚合 解决流控链路不生效的问题 registration.addInitParameter(CommonFilter.WEB_CONTEXT_UNIFY, &quot;false&quot;); registration.setName(&quot;sentinelFilter&quot;); registration.setOrder(1); return registration; &#125;&#125; 这里有坑。此时如果启动项目，配置了上图的规则被限流后会 出错 原因是调用getUser服务的类是代理服务，而该代理服务使用的是Cglib的代理，所以方法会进入到CglibAopRoxy.CglibMethodInvocation类中处理，而该类的proceed方法如下： 抛出了一个RuntimeException。如果不想走esle那只能在接口和实现类的方法上显示的抛出BlockException 12345678910// 接口UserEntity getUser(int id) throws BlockException;// 实现@Override@SentinelResource(value = &quot;getUser&quot;) //aoppublic UserEntity getUser(int id) throws BlockException &#123; UserEntity user = baseMapper.selectById(id); return user;&#125; 这样就能抛出BlockException。 而对于jdk的代理，会抛出实际的错误，所以不用做处理。 坑还没结束。因为抛出BlockException错误是要经过CommonFilter才能实现限流。但是啊，CommonFilter处理返回结果前是要经过Servlet的，这里的Servlet是DispatcherServlet，在该类的service方法中是会错误进行处理的 BlockException错误变成了NestedServletException错误。 所以到CommonFilter中时，限流就失效了。 下面有两个解决方法 自己提供Filter。 复制CommonFilter，然后添加这段逻辑 12345678910catch (NestedServletException e1) &#123; if (e1.getCause() instanceof BlockException) &#123; HttpServletResponse sResponse = (HttpServletResponse) response; // 注意这里使用的是UrlBlockHandler来处理 WebCallbackManager.getUrlBlockHandler().blocked(sRequest, sResponse, (BlockException) e1.getCause()); &#125; else &#123; Tracer.traceEntry(e1, urlEntry); throw e1; &#125;&#125; 使用blockHandler 1@SentinelResource(value = &quot;getUser&quot;, blockHandler = &quot;handleException&quot;) 如果使用的是第一种模式的话，坑还没结束，因为执行BlockException的接口不再是BlockExceptionHandler接口了，变成了UrlBlockHandler。所以还需要改 注入BlockExceptionHandler实现类，把WebCallbackManager.getUrlBlockHandler()改了 写一个MyUrlBlockHandler，然后在添加过滤器的时候手动执行WebCallbackManager.setUrlBlockHandler(new MyUrlBlockHandler()); 坑还没结束，因为CommonFilter还会导致授权规格失效，因为CommonFilter中通过WebCallbackManage来管理RequestOriginParser接口，不是通过spring管理的，所以如是自己提供CommonFilter的话，就在自己的类中手动注入一个。或者手动行WebCallbackManager.setRequestOriginParser。 如果是第一种，那只能手动执行方法了。 还有！就是这会影响到熔断，比如资源名是接口，最小请求数是6，异常数是3，这样配置后，请求3次后就直接熔断了。因为获取了两次资源， 所以客户端的一次请求，在Sentinel中由于获取了两次资源，被算了两次请求。 流控效果当 QPS 超过某个阈值的时候，则采取措施进行流量控制。流量控制的效果包括以下几种：快速失败（直接拒绝）、Warm Up（预热）、匀速排队（排队等待）。对应 FlowRule 中的 controlBehavior 字段。 快速失败（默认）RuleConstant.CONTROL_BEHAVIOR_DEFAULT）方式是默认的流量控制方式，&#x3D;&#x3D;当QPS超过任意规则的阈值后，新的请求就会被立即拒绝，拒绝方式为抛出FlowException&#x3D;&#x3D;。这种方式适用于对系统处理能力确切已知的情况下，比如通过压测确定了系统的准确水位时。 Warm UpWarm Up（RuleConstant.CONTROL_BEHAVIOR_WARM_UP）方式，即预热&#x2F;冷启动方式。 当系统长期处于低水位的情况下，当流量突然增加时，直接把系统拉升到高水位可能瞬间把系统压垮。 通过”冷启动”，让通过的流量缓慢增加，在一定时间内逐渐增加到阈值上限，给冷系统一个预热的时间，避免冷系统被压垮。 &#x3D;&#x3D;冷加载因子: codeFactor 默认是3，即请求 QPS 从 threshold &#x2F; 3 开始，经预热时长逐渐升至设定的 QPS 阈值。&#x3D;&#x3D; 通常冷启动的过程系统允许通过的 QPS 曲线如下图所示: 测试用例： 系统先从3开始，然后经过10s，达到了10. 匀速排队匀速排队（RuleConstant.CONTROL_BEHAVIOR_RATE_LIMITER）方式会严格控制请求通过的间隔时间，也即是让请求以均匀的速度通过，对应的是漏桶算法。 该方式的作用如下图所示： 这种方式主要用于处理间隔性突发的流量，例如消息队列。想象一下这样的场景，在某一秒有大量的请求到来，而接下来的几秒则处于空闲状态，我们希望系统能够在接下来的空闲期间逐渐处理这些请求，而不是在第一秒直接拒绝多余的请求。 注意：匀速排队模式暂时不支持 QPS &gt; 1000 的场景。 降级规则除了流量控制以外，对调用链路中不稳定的资源进行熔断降级也是保障高可用的重要措施之一。我们需要对不稳定的弱依赖服务调用进行熔断降级，暂时切断不稳定调用，避免局部不稳定因素导致整体的雪崩。熔断降级作为保护自身的手段，通常在客户端（调用端）进行配置。 熔断的概念现实世界的断路器大家肯定都很了解，断路器实时监控电路的情况，如果发现电路电流异常，就会跳闸，从而防止电路被烧毁。 软件世界的断路器可以这样理解：实时监测应用，如果发现在一定时间内失败次数&#x2F;失败率达到一定阈值，就“跳闸”，断路器打开——此时，请求直接返回，而不去调用原本调用的逻辑。跳闸一段时间后（例如10秒），断路器会进入半开状态，这是一个瞬间态，此时允许一次请求调用该调的逻辑，如果成功，则断路器关闭，应用正常调用；如果调用依然不成功，断路器继续回到打开状态，过段时间再进入半开状态尝试。通过”跳闸“，应用可以保护自己，而且避免浪费资源；而通过半开的设计，可实现应用的“自我修复“。 熔断降级规则说明熔断降级规则（DegradeRule）包含下面几个重要的属性： Field 说明 默认值 resource 资源名，即规则的作用对象 grade 熔断策略，支持慢调用比例&#x2F;异常比例&#x2F;异常数策略 慢调用比例 count 慢调用比例模式下为慢调用临界 RT（超出该值计为慢调用）；异常比例&#x2F;异常数模式下为对应的阈值 timeWindow 熔断时长，单位为 s minRequestAmount 熔断触发的最小请求数，请求数小于该值时即使异常比率超出阈值也不会熔断（1.7.0 引入） 5 statIntervalMs 统计时长（单位为 ms），如 60*1000 代表分钟级（1.8.0 引入） 1000 ms slowRatioThreshold 慢调用比例阈值，仅慢调用比例模式有效（1.8.0 引入） RT：响应时间 熔断策略 注意：异常降级仅针对业务异常，对 Sentinel 限流降级本身的异常（BlockException）不生效。 慢调用比例慢调用比例 (SLOW_REQUEST_RATIO)：&#x3D;&#x3D;选择以慢调用比例作为阈值，需要设置允许的慢调用 RT（即最大的响应时间），请求的响应时间大于该值则统计为慢调用。当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且慢调用的比例大于阈值&#x3D;&#x3D;，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求响应时间小于设置的慢调用 RT 则结束熔断，若大于设置的慢调用 RT 则会再次被熔断。 异常比例异常比例 (ERROR_RATIO)：当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且异常的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。异常比率的阈值范围是 [0.0, 1.0]，代表 0% - 100%。 查看实时监控，可以看到断路器熔断效果 异常数异常数 (ERROR_COUNT)：当单位统计时长内的异常数目超过阈值之后会自动进行熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。 热点参数限流何为热点？热点即经常访问的数据。很多时候我们希望统计某个热点数据中访问频次最高的 Top K 数据，并对其访问进行限制。比如： 商品 ID 为参数，统计一段时间内最常购买的商品 ID 并进行限制 用户 ID 为参数，针对一段时间内频繁访问的用户 ID 进行限制 &#x3D;&#x3D;热点参数限流会统计传入参数中的热点参数，并根据配置的限流阈值与模式，对包含热点参数的资源调用进行限流。&#x3D;&#x3D; 热点参数限流可以看做是一种特殊的流量控制，仅对包含热点参数的资源调用生效。 注意： 热点规则需要使用@SentinelResource(“resourceName”)注解，否则不生效 参数必须是7种基本数据类型才会生效 系统规则Sentinel 系统自适应限流从整体维度对应用入口流量进行控制，结合应用的 Load、CPU 使用率、总体平均 RT、入口 QPS 和并发线程数等几个维度的监控指标，通过自适应的流控策略，让系统的入口流量和系统的负载达到一个平衡，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定性。 Load 自适应（仅对 Linux&#x2F;Unix-like 机器生效）：系统的 load1 作为启发指标，进行自适应系统保护。当系统 load1 超过设定的启发值，且系统当前的并发线程数超过估算的系统容量时才会触发系统保护（BBR 阶段）。系统容量由系统的 maxQps * minRt 估算得出。设定参考值一般是 CPU cores * 2.5。 CPU usage（1.5.0+ 版本）：当系统 CPU 使用率超过阈值即触发系统保护（取值范围 0.0-1.0），比较灵敏。 平均 RT：当单台机器上所有入口流量的平均 RT 达到阈值即触发系统保护，单位是毫秒。 并发线程数：当单台机器上所有入口流量的并发线程数达到阈值即触发系统保护。 入口 QPS：当单台机器上所有入口流量的 QPS 达到阈值即触发系统保护。 授权控制规则很多时候，我们需要根据调用来源来判断该次请求是否允许放行，这时候可以使用 Sentinel 的来源访问控制（黑白名单控制）的功能。来源访问控制根据资源的请求来源（origin）限制资源是否通过，若配置白名单则只有请求来源位于白名单内时才可通过；若配置黑名单则请求来源位于黑名单时不通过，其余的请求通过。 来源访问控制规则（AuthorityRule）非常简单，主要有以下配置项： resource：资源名，即限流规则的作用对象。 limitApp：对应的黑名单&#x2F;白名单，不同 origin 用 , 分隔，如 appA,appB。 strategy：限制模式，AUTHORITY_WHITE 为白名单模式，AUTHORITY_BLACK 为黑名单模式，默认为白名单模式。 使用： 第一步：实现com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.RequestOriginParser接口，在parseOrigin方法中区分来源，并交给spring管理 注意：如果引入CommonFilter，此处会多出一个 12345678910111213141516171819202122import com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.RequestOriginParser;import org.springframework.stereotype.Component;import javax.servlet.http.HttpServletRequest;@Componentpublic class MyRequestOriginParser implements RequestOriginParser &#123; /** * 通过request获取来源标识，交给授权规则进行匹配 * @param request * @return */ @Override public String parseOrigin(HttpServletRequest request) &#123; // 标识字段名称可以自定义 String origin = request.getParameter(&quot;serviceName&quot;);// if (StringUtil.isBlank(origin))&#123;// throw new IllegalArgumentException(&quot;serviceName参数未指定&quot;);// &#125; return origin; &#125;&#125; 测试： 集群规则为什么要使用集群流控呢？假设我们希望给某个用户限制调用某个 API 的总 QPS 为 50，但机器数可能很多（比如有 100 台）。这时候我们很自然地就想到，找一个 server 来专门来统计总的调用量，其它的实例都与这台 server 通信来判断是否可以调用。这就是最基础的集群流控的方式。 另外集群流控还可以解决流量不均匀导致总体限流效果不佳的问题。假设集群中有 10 台机器，我们给每台机器设置单机限流阈值为 10 QPS，理想情况下整个集群的限流阈值就为 100 QPS。不过实际情况下流量到每台机器可能会不均匀，会导致总量没有到的情况下某些机器就开始限流。因此仅靠单机维度去限制的话会无法精确地限制总体流量。而集群流控可以精确地控制整个集群的调用总量，结合单机限流兜底，可以更好地发挥流量控制的效果。 集群流控 · alibaba&#x2F;Sentinel Wiki (github.com) 集群流控中共有两种身份： Token Client：集群流控客户端，用于向所属 Token Server 通信请求 token。集群限流服务端会返回给客户端结果，决定是否限流。 Token Server：即集群流控服务端，处理来自 Token Client 的请求，根据配置的集群规则判断是否应该发放 token（是否允许通过）。 Sentinel 集群流控支持限流规则和热点规则两种规则，并支持两种形式的阈值计算方式： 集群总体模式：即限制整个集群内的某个资源的总体 qps 不超过此阈值。 单机均摊模式：单机均摊模式下配置的阈值等同于单机能够承受的限额，token server 会根据连接数来计算总的阈值（比如独立模式下有 3 个 client 连接到了 token server，然后配的单机均摊阈值为 10，则计算出的集群总量就为 30），按照计算出的总的阈值来进行限制。这种方式根据当前的连接数实时计算总的阈值，对于机器经常进行变更的环境非常适合。 启动方式 Sentinel 集群限流服务端有两种启动方式： 独立模式（Alone），即作为独立的 token server 进程启动，独立部署，隔离性好，但是需要额外的部署操作。独立模式适合作为 Global Rate Limiter 给集群提供流控服务。 嵌入模式（Embedded），即作为内置的 token server 与服务在同一进程中启动。在此模式下，集群中各个实例都是对等的，token server 和 client 可以随时进行转变，因此无需单独部署，灵活性比较好。但是隔离性不佳，需要限制 token server 的总 QPS，防止影响应用本身。嵌入模式适合某个应用集群内部的流控。 这种模式其实有很多替代方案的，可以对feign限流或者对网关限流","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"15-Sentinel快速入门","slug":"springcloud/15-Sentinel快速入门","date":"2021-11-27T12:00:23.000Z","updated":"2022-03-23T09:03:56.910Z","comments":true,"path":"blog/springcloud/15-Sentinel快速入门/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/15-Sentinel%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/","excerpt":"","text":"Sentinel官网 (sentinelguard.io) 分布式系统遇到的问题在一个高度服务化的系统中,我们实现的一个业务逻辑通常会依赖多个服务,比如:商品详情展示服务会依赖商品服务, 价格 服务, 商品评论服务. 如图所示: 调用三个依赖服务会共享商品详情服务的线程池. 如果其中的商品评论服务不可用, 就会出现线程池里所有线程都因等待 响应而被阻塞, 从而造成服务雪崩. 如图所示: 服务雪崩效应:因服务提供者的不可用导致服务调用者的不可用,并将不可用逐渐放大的过程，就叫服务雪崩效应 导致服务不可用的原因: 程序Bug，大流量请求，硬件故障，缓存击穿 大流量请求：在秒杀和大促开始前,如果准备不充分,瞬间大量请求会造成服务提供者的不可用。 硬件故障：可能为硬件损坏造成的服务器主机宕机, 网络硬件故障造成的服务提供者的不可访问。 缓存击穿：一般发生在缓存应用重启, 缓存失效时高并发，所有缓存被清空时,以及短时间内大量缓存失效时。大量的缓存不命中, 使请求直击后端，造成服务提供者超负荷运行，引起服务不可用。 在服务提供者不可用的时候，会出现大量重试的情况:用户重试、代码逻辑重试，这些重试最终导致:进一步加大请求 流量。所以归根结底导致雪崩效应的最根本原因是:大量请求线程同步等待造成的资源耗尽。当服务调用者使用同步调 用时, 会产生大量的等待线程占用系统资源。一旦线程资源被耗尽，服务调用者提供的服务也将处于不可用状态, 于是服务雪崩效应产生了。 解决方案 超时机制 在不做任何处理的情况下，服务提供者不可用会导致消费者请求线程强制等待，而造成系统资源耗尽。加入超时机制，一旦超时，就释放资源。由于释放资源速度较快，一定程度上可以抑制资源耗尽的问题。 服务限流(资源隔离) 限制请求核心服务提供者的流量，使大流量拦截在核心服务之外，这样可以更好的保证核心服务提供者不出问题，对于一些出问题的服务可以限制流量访问，只分配固定线程资源访问，这样能使整体的资源不至于被出问题的服务耗尽，进而整个系统雪崩。那么服务之间怎么限流，怎么资源隔离？例如可以通过线程池+队列的方式，通过信号量的方式。 如下图所示, 当商品评论服务不可用时, 即使商品服务独立分配的20个线程全部处于同步等待状态,也不会影响其他依赖服务的调用。 服务熔断 远程服务不稳定或网络抖动时暂时关闭，就叫服务熔断。 现实世界的断路器大家肯定都很了解，断路器实时监控电路的情况，如果发现电路电流异常，就会跳闸，从而防止电路被烧毁。 软件世界的断路器可以这样理解：实时监测应用，如果发现在一定时间内失败次数&#x2F;失败率达到一定阈值，就“跳闸”，断路器打开——此时，请求直接返回，而不去调用原本调用的逻辑。跳闸一段时间后（例如10秒），断路器会进入半开状态，这是一个瞬间态，此时允许一次请求调用该调的逻辑，如果成功，则断路器关闭，应用正常调用；如果调用依然不成功，断路器继续回到打开状态，过段时间再进入半开状态尝试——通过”跳闸“，应用可以保护自己，而且避免浪费资源；而通过半开的设计，可实现应用的“自我修复“。 所以，同样的道理，当依赖的服务有大量超时时，在让新的请求去访问根本没有意义，只会无畏的消耗现有资源。比如我们设置了超时时间为1s,如果短时间内有大量请求在1s内都得不到响应，就意味着这个服务出现了异常，此时就没有必要再让其他的请求去访问这个依赖了，这个时候就应该使用断路器避免资源浪费。 服务降级 有服务熔断，必然要有服务降级。所谓降级，就是当某个服务熔断之后，服务将不再被调用，此时客户端可以自己准备一个本地的fallback（回退）回调，返回一个缺省值。 例如：(备用接口&#x2F;缓存&#x2F;mock数据) 。这样做，虽然服务水平下降，但好歹可用，比直接挂掉要强，当然这也要看适合的业务场景。 Sentinel: 分布式系统的流量防卫兵alibaba&#x2F;Sentinel Wiki (github.com) Sentinel 介绍(sentinelguard.io) 随着微服务的流行，服务和服务之间的稳定性变得越来越重要。Sentinel 是面向分布式服务架构的流量控制组件，主要以流量为切入点，从限流、流量整形、熔断降级、系统负载保护、热点防护等多个维度来帮助开发者保障微服务的稳定性。 Sentinel具有以下特征: 丰富的应用场景： Sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、实时熔断下游不可用应用等。 完备的实时监控： Sentinel 同时提供实时的监控功能。您可以在控制台中看到接入应用的单台机器秒级数据，甚至 500 台以下规模的集群的汇总运行情况。 广泛的开源生态： Sentinel 提供开箱即用的与其它开源框架&#x2F;库的整合模块，例如与 Spring Cloud、Dubbo、gRPC 的整合。您只需要引入相应的依赖并进行简单的配置即可快速地接入 Sentinel。 完善的 SPI 扩展点： Sentinel 提供简单易用、完善的 SPI 扩展点。您可以通过实现扩展点，快速的定制逻辑。例如定制规则管理、适配数据源等。 Sentinel 工作原理基本概念 资源 资源是 Sentinel 的关键概念。它可以是 Java 应用程序中的任何内容，例如，由应用程序提供的服务，或由应用程序调用的其它应用提供的服务，甚至可以是一段代码。 只要通过 Sentinel API 定义的代码，就是资源，能够被 Sentinel 保护起来。大部分情况下，可以使用方法签名，URL，甚至服务名称作为资源名来标示资源。 规则 围绕资源的实时状态设定的规则，可以包括流量控制规则、熔断降级规则以及系统保护规则。所有规则可以动态实时调整。 Sentinel工作主流程Sentinel 工作主流程 在 Sentinel 里面，所有的资源都对应一个资源名称（resourceName），每次资源调用都会创建一个 Entry 对象。Entry 可以通过对主流框架的适配自动创建，也可以通过注解的方式或调用 SphU API 显式创建。Entry 创建的时候，同时也会创建一系列功能插槽（slot chain），这些插槽有不同的职责，例如: NodeSelectorSlot 负责收集资源的路径，并将这些资源的调用路径，以树状结构存储起来，用于根据调用路径来限流降级； ClusterBuilderSlot 则用于存储资源的统计信息以及调用者信息，例如该资源的 RT, QPS, thread count 等等，这些信息将用作为多维度限流，降级的依据； StatisticSlot 则用于记录、统计不同纬度的 runtime 指标监控信息； FlowSlot 则用于根据预设的限流规则以及前面 slot 统计的状态，来进行流量控制； AuthoritySlot 则根据配置的黑白名单和调用来源信息，来做黑白名单控制； DegradeSlot 则通过统计信息以及预设的规则，来做熔断降级； SystemSlot 则通过系统的状态，例如 load1 等，来控制总的入口流量； 总体的框架如下: Sentinel 将 ProcessorSlot 作为 SPI 接口进行扩展（1.7.2 版本以前 SlotChainBuilder 作为 SPI），使得 Slot Chain 具备了扩展的能力。您可以自行加入自定义的 slot 并编排 slot 间的顺序，从而可以给 Sentinel 添加自定义的功能。 快速开始使用API引入依赖： 123456&lt;!-- spring boot不用引入 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-core&lt;/artifactId&gt; &lt;version&gt;1.8.3&lt;/version&gt;&lt;/dependency&gt; 通过这种模式来使用： 12345678910111213141516171819Entry entry = null;// 务必保证 finally 会被执行try &#123; // 资源名可使用任意有业务语义的字符串 开启资源的保护 entry = SphU.entry(&quot;自定义资源名&quot;); // ContextUtil 设置上下文 // 被保护的业务逻辑 method // do something...&#125; catch (BlockException ex) &#123; // 资源访问阻止，被限流或被降级 Sentinel定义异常 流控规则，降级规则，热点参数规则。。。服务降级(降级规则) // 进行相应的处理操作&#125; catch (Exception ex) &#123; // 若需要配置降级规则，需要通过这种方式记录业务异常 RuntimeException 服务降级 mock feign:fallback Tracer.traceEntry(ex, entry);&#125; finally &#123; // 务必保证 exit，务必保证每个 entry 与 exit 配对 if (entry != null) &#123; entry.exit();&#125; DEMO： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@RestController@Slf4jpublic class HelloController &#123; private static final String RESOURCE_NAME = &quot;hello&quot;; @RequestMapping(value = &quot;/hello&quot;) public String hello() &#123; Entry entry = null; try &#123; // 资源名可使用任意有业务语义的字符串，比如方法名、接口名或其它可唯一标识的字符串。 entry = SphU.entry(RESOURCE_NAME); // 被保护的业务逻辑 String str = &quot;hello world&quot;; log.info(&quot;=====&quot;+str); return str; &#125; catch (BlockException e1) &#123; // 资源访问阻止，被限流或被降级 //进行相应的处理操作 log.info(&quot;block!&quot;); return &quot;被流控了&quot;; &#125; catch (Exception ex) &#123; // 若需要配置降级规则，需要通过这种方式记录业务异常 Tracer.traceEntry(ex, entry); &#125; finally &#123; if (entry != null) &#123; entry.exit(); &#125; &#125; return null; &#125; /** * 定义流控规则 */ @PostConstruct private static void initFlowRules()&#123; List&lt;FlowRule&gt; rules = new ArrayList&lt;&gt;(); FlowRule rule = new FlowRule(); //设置受保护的资源 rule.setResource(RESOURCE_NAME); // 设置流控规则 QPS rule.setGrade(RuleConstant.FLOW_GRADE_QPS); // 设置受保护的资源阈值 // Set limit QPS to 20. rule.setCount(1); rules.add(rule); // 加载配置好的规则 FlowRuleManager.loadRules(rules); &#125;&#125; 业务侵入性很强，需要在controller中写入非业务代码. 配置不灵活 若需要添加新的受保护资源 需要手动添加 init方法来添加流控规则 使用@SentinelResource注解实现1234blockHandler: 定义当资源内部发生了BlockException应该进入的方法（捕获的是Sentinel定义的异常）fallback: 定义的是资源内部发生了Throwable应该进入的方法exceptionsToIgnore：配置fallback可以忽略的异常源码入口：com.alibaba.csp.sentinel.annotation.aspectj.SentinelResourceAspect 引入依赖： 123456&lt;!-- spring boot不用引入 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-annotation-aspectj&lt;/artifactId&gt; &lt;version&gt;1.8.3&lt;/version&gt;&lt;/dependency&gt; 配置切面支持： 如果是Spring Boot项目就不需要，自动配置类已经配置了 12345678@Configurationpublic class SentinelAspectConfiguration &#123; @Bean public SentinelResourceAspect sentinelResourceAspect() &#123; return new SentinelResourceAspect(); &#125;&#125; 使用： 1234567891011121314@RequestMapping(&quot;/list&quot;)@SentinelResource(value = &quot;userlist&quot;, blockHandlerClass = CommonBlockHandler.class, blockHandler = &quot;handleException&quot;, fallback = &quot;fallback&quot;)public R list(@RequestParam Map&lt;String, Object&gt; params)&#123; PageUtils page = userService.queryPage(params); return R.ok().put(&quot;page&quot;, page);&#125;public R fallback(@PathVariable(&quot;id&quot;) Integer id,Throwable e)&#123; return R.error(-1,&quot;===被熔断降级啦===&quot;+e.getMessage());&#125; CommonBlockHandler: 12345678910111213141516171819public class CommonBlockHandler &#123; /** * 注意： 必须为 static 函数，多个方法之间方法名不能一样 * @param exception * @return */ public static R handleException(Map&lt;String, Object&gt; params, BlockException exception)&#123; return R.error(-1,&quot;===被限流啦===&quot;+exception); &#125; public static R handleException2(Integer id, BlockException exception)&#123; return R.error(-1,&quot;===被限流啦===&quot;+exception); &#125; public static String handleException3(BlockException exception)&#123; return &quot;===被限流啦===&quot;+exception; &#125;&#125; 对于规则的定义，可以使用API的形式也可以使用sentinel dashboard定义规则。如果是后者，客户端需要引入 Transport 模块来与 Sentinel 控制台进行通信。 123456&lt;!-- spring boot不用引入 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-transport-simple-http&lt;/artifactId&gt; &lt;version&gt;1.8.3&lt;/version&gt;&lt;/dependency&gt; 启动sentinel dashboard控制台 · alibaba&#x2F;Sentinel Wiki (github.com) 1java -jar sentinel-dashboard-1.8.3.jar -Dserver.servlet.session.timeout=7200 123-Dsentinel.dashboard.auth.username=sentinel 用于指定控制台的登录用户名为 sentinel；-Dsentinel.dashboard.auth.password=123456 用于指定控制台的登录密码为 123456；如果省略这两个参数，默认用户和密码均为 sentinel；-Dserver.servlet.session.timeout=7200 用于指定 Spring Boot 服务端 session 的过期时间，如 7200 表示 7200 秒；60m 表示 60 分钟，默认为 30 分钟； 访问http://localhost:8080/#/login ,默认用户名密码： sentinel&#x2F;sentinel Spring Cloud Alibaba整合Sentinel123456789&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 添加yml配置，为微服务设置sentinel控制台地址 添加Sentinel后，需要暴露&#x2F;actuator&#x2F;sentinel端点,而Springboot默认是没有暴露该端点的，所以需要设置，测试http://localhost:8800/actuator/sentinel 123456789101112131415161718192021222324server: port: 8800spring: application: name: mall-user-sentinel-demo cloud: nacos: discovery: server-addr: 127.0.0.1:8848 sentinel: transport: # 添加sentinel的控制台地址 dashboard: 127.0.0.1:8080 # 指定应用与Sentinel控制台交互的端口，应用本地会起一个该端口占用的HttpServer # port: 8719 #暴露actuator端点 management: endpoints: web: exposure: include: &#x27;*&#x27; 在sentinel控制台中设置流控规则 项目启动后，在控制台就有服务的一些信息，比如在项目中这样定义了一个方法 1234567891011@RequestMapping(value = &quot;/findOrderByUserId/&#123;id&#125;&quot;)@SentinelResource(value = &quot;findOrderByUserId&quot;, blockHandlerClass = CommonBlockHandler.class, // CommonBlockHandler类中的handleException2方法必须为static blockHandler = &quot;handleException2&quot;)public R findOrderByUserId(@PathVariable(&quot;id&quot;) Integer id) &#123; R result = orderFeignService.findOrderByUserId(id); return result;&#125; 在sentinel就对于上边的方法生成了两个资源，也就是说方法findOrderByUserId和/user/findOrderByUserId/&#123;id&#125;接口也被保护了。对于接口，由于是SpringMVC的实现，它sentinel在这个接口的拦截器也定义了资源。所以上边的定义，findOrderByUserId方法有两条链，一条是/user/findOrderByUserId/&#123;id&#125;，一条就是findOrderByUserId方法。并且，如果客户端是通过http访问的，/user/findOrderByUserId/&#123;id&#125;链路会先生效。 添加规则 资源名: 接口的API 针对来源: 默认是default，当多个微服务都调用这个资源时，可以配置微服务名来对指定的微服务设置阈值 阈值类型: 分为QPS和线程数 假设阈值为10 QPS类型: 只得是每秒访问接口的次数&gt;10就进行限流 线程数: 为接受请求该资源分配的线程数&gt;10就进行限流 例子1: 例子2: 例子3: 通过上边的例子也说明了开头的优先级问题。 注意！通过上边的配置启动项目的话，在sentinel控制台配置的规则都是临时的，服务一重启规则就没了 访问http://localhost:8800/actuator/sentinel， 可以查看flowRules 微服务和Sentinel Dashboard通信原理Sentinel控制台与微服务端之间，实现了一套服务发现机制，集成了Sentinel的微服务都会将元数据传递给Sentinel控制台，架构图如下所示：","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"14-Nacos配置中心源码","slug":"springcloud/14-Nacos配置中心源码","date":"2021-11-27T12:00:22.000Z","updated":"2022-03-23T09:03:56.839Z","comments":true,"path":"blog/springcloud/14-Nacos配置中心源码/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/14-Nacos%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E6%BA%90%E7%A0%81/","excerpt":"","text":"配置中心架构 Nacos配置中心客户端使用这里看下配置中心客户端的时候 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class ConfigServerDemo &#123; public static void main(String[] args) throws NacosException, InterruptedException &#123; String serverAddr = &quot;localhost:8848&quot;; String dataId = &quot;nacos-config-demo.yaml&quot;; String group = &quot;DEFAULT_GROUP&quot;; Properties properties = new Properties(); properties.put(PropertyKeyConst.SERVER_ADDR, serverAddr); //获取配置中心服务 ConfigService configService = NacosFactory.createConfigService(properties); //从配置中心拉取配置 String content = configService.getConfig(dataId, group, 5000); System.out.println(content); //注册监听器 configService.addListener(dataId, group, new Listener() &#123; @Override public void receiveConfigInfo(String configInfo) &#123; System.out.println(&quot;感知配置变化:&quot; + configInfo); &#125; @Override public Executor getExecutor() &#123; return null; &#125; &#125;); //发布配置 //boolean isPublishOk = configService.publishConfig(dataId, group, &quot;content&quot;); //System.out.println(isPublishOk); //发送properties格式 configService.publishConfig(dataId, group,&quot;common.age=30&quot;, ConfigType.PROPERTIES.getType()); Thread.sleep(3000); //从配置中心拉取配置 content = configService.getConfig(dataId, group, 5000); System.out.println(content);// boolean isRemoveOk = configService.removeConfig(dataId, group);// System.out.println(isRemoveOk);// Thread.sleep(3000);// content = configService.getConfig(dataId, group, 5000);// System.out.println(content);// Thread.sleep(300000); Thread.sleep(Integer.MAX_VALUE); &#125;&#125; Nacos 配置中心客户端原理nacos配置中心客户端是使用了Spring cloud的 PropertySourceLocator扩展，来完成配置的拉取的。原理也在上边讲了。在该接口的执行阶段，ConfigurableEnvironment完成初始化了。在nacos-config中提供了NacosPropertySourceLocator，用来加载nacos server的配置，该类的核心代码： 1234567891011121314151617181920212223// NacosPropertySourceLocator// 加载共享配置文件loadSharedConfiguration(composite);// 加载扩展配置文件loadExtConfiguration(composite) &#123; for ( NacosConfigProperties.Config config : getExtensionConfigs()) &#123; loadNacosDataIfPresent() &#125;&#125;// 加载应用配置文件loadApplicationConfiguration(composite, dataIdPrefix, nacosConfigProperties, env) &#123; // 去加载$&#123;spring.application.name&#125; loadNacosDataIfPresent(compositePropertySource, dataIdPrefix, nacosGroup, // 去加载$&#123;spring.application.name&#125;.$&#123;spring.cloud.nacos.config.file-extension&#125; loadNacosDataIfPresent(compositePropertySource, dataIdPrefix + DOT + fileExtension, nacosGroup, fileExtension, true); // 去加载$&#123;spring.application.name&#125;.$&#123;profile&#125;.$&#123;spring.cloud.nacos.config.file-extension&#125; for (String profile : environment.getActiveProfiles()) &#123; String dataId = dataIdPrefix + SEP1 + profile + DOT + fileExtension; loadNacosDataIfPresent(compositePropertySource, dataId, nacosGroup, fileExtension, true); &#125;&#125; 方法的执行顺序就已经说明了Nacos 配置的优先级了，后面加载的会覆盖前面加载的的。 上边的方法的内部都是ConfigService的使用，最终都是调用了ConfigService.getConfig拉取配置，然后通过ConfigService.addListener来监听配置的变化。其内部还是和注册中心一样，都是通过HTTP进行交互的 获取配置获取配置的主要方法是 NacosConfigService 类的 getConfig 方法，通常情况下该方法直接从本地文件中取得配置的值，如果本地文件不存在或者内容为空，则再通过 HTTP GET 方法从远端拉取配置，并保存到本地快照中。当通过 HTTP 获取远端配置时，Nacos 提供了两种熔断策略，一是超时时间，二是最大重试次数，默认重试三次。 获取配置的http： 12URL: /nacos/v1/cs/configsMethod: GET 注册监听器配置中心客户端会通过对配置项注册监听器达到在配置项变更的时候执行回调的功能。 12NacosConfigService#getConfigAndSignListenerConfigService#addListener Nacos 可以通过以上方式注册监听器，它们内部的实现均是调用 ClientWorker 类的 addCacheDataIfAbsent。其中 CacheData 是一个维护配置项和其下注册的所有监听器的实例，所有的 CacheData 都保存在 ClientWorker 类中的原子 cacheMap 中，其内部的核心成员有： 在nacos-config-config中，是通过ApplicationListener来完成NACOS监听器的注册的。在Spring boot的上下文启动完成后，会发布一个ApplicationReadyEvent事件，Nacos通过NacosContextRefresher来处理该事件，来完成监听器的注册，NacosContextRefresher的核心源码如下： 12345678910111213141516Listener listener = listenerMap.computeIfAbsent(key, lst -&gt; new AbstractSharedListener() &#123; // 配置变化后会执行该方法 @Override public void innerReceive(String dataId, String group, String configInfo) &#123; refreshCountIncrement(); nacosRefreshHistory.addRefreshRecord(dataId, group, configInfo); // todo feature: support single refresh for listening // 发布RefreshEvent事件，刷新配置 applicationContext.publishEvent( new RefreshEvent(this, null, &quot;Refresh Nacos config&quot;)); &#125; &#125;);configService.addListener(dataKey, groupKey, listener); 监听 Nacos 上的配置，以便实时感知配置变更。如果配置变更，则用获取配置接口获取配置的最新值，动态刷新本地缓存。 注册监听采用的是异步 Servlet 技术。注册监听本质就是带着配置和配置值的 MD5 值和后台对比。如果 MD5 值不一致，就立即返回不一致的配置。如果值一致，就等待住 30 秒。返回值为空。 Nacos-config-client的监听，其实是一个长轮询 ClientWorker 通过其下的两个线程池完成配置长轮询的工作，一个是单线程的 executor，每隔 10ms 按照每 3000 个配置项为一批次捞取待轮询的 cacheData 实例，将其包装成为一个 LongPollingRunnable 提交进入第二个线程池 executorService 处理。 LongPollingRunnable最核心的就是执行了这个方法 检查服务端的配置是否变化了，其内部就是发起了一个异步HTTP请求 12URL: /nacos/v1/cs/configs/listenerMethod: POST 这里其实是一个异步http 异步http，有分 客户端异步（nio） 服务端异步（异步servlet） 这里是客户端异步（看情况，可能会是同步） 所以监听，其实就是一个长轮询，不断的检查配置是否发生变化。 如果发生变化了就会根据返回值调用获取配置的http请求。 项目中动态感知参数变化——@RefreshScope 刷新就是刷新本地的ConfigurableEnvironment对象，然后销毁refresh范围的真时bean对象，当通过refresh范围的某个代理对象调用某个方法时，会触发beanFactory.getBean方法重新生成一个对象。 刷新ConfigurableEnvironment对象是spring cloud的知识 后面的都是spring的知识 当nacos client通过监听，感知到配置变化后，项目中有@RefreshScope注解的对象的属性也要进行修改，这是怎么实现的？ nacos的配置变化是通过nacos监听器来监听的，而注册的nacos监听器是： 12345678910111213141516// NacosContextRefresher// ApplicationReadyEventListener listener = listenerMap.computeIfAbsent(key, lst -&gt; new AbstractSharedListener() &#123; // 配置变化后会执行该方法 @Override public void innerReceive(String dataId, String group, String configInfo) &#123; refreshCountIncrement(); nacosRefreshHistory.addRefreshRecord(dataId, group, configInfo); // todo feature: support single refresh for listening // 发布RefreshEvent事件，刷新配置 applicationContext.publishEvent( new RefreshEvent(this, null, &quot;Refresh Nacos config&quot;)); &#125; &#125;); 在配置变化后就会执行innerReceive，在这个方法内会发布一个RefreshEvent事件，该事件就是刷新配置事件，也就是说@RefreshScope注解的功能是通过ApplicationListener来实现的，这个ApplicationListener就是RefreshEventListener，它的执行流程如下： RefreshEventListener它会通过SpringApplication.run重新创建ConfigurableEnvironment(流程就是“Spring Cloud配置原理”中的流程图中的BootstrapApplicationListener)，这时新建的ConfigurableEnvironment就有配置的最新的信息了，然后用这个新的ConfigurableEnvironment更新旧的。应用的上下文的ConfigurableEnvironment的数据就是最新的了。 然后销毁有@RefreshScope注解的Bean（注入的是代理对象，销毁的是真实对象。从上下文中移除了真实对象）。之后如果有@RefreshScope的bean的方法调用，那就会触发beanFactory.getBean方法，从而新生成一个的bean，新bean就有新的数据了。 @RefreshScope实际上是@Scope(“refresh”)，把bean的作用域设置为refresh，而这个作用域的定义类为RefreshScope。 RefreshScope这个类BeanDefinition的注册阶段，会把refresh范围的类的RootBeanDefinition的class属性重新指向为LockedScopedProxyFactoryBean。而且在bean创建时会缓存refresh范围的bean。 refresh范围的bean在初始化时由于时LockedScopedProxyFactoryBean，会生成一个代理类，而该代理类的里面的TargetSource为DefaultScopedObject，会通过beanFactory.getBean(beanName)来获取目标类。 在配置信息发生变化后，刷新完本地的Environment对象后，会执行 123RefreshScope.refreshAll() &#123; super.destroy();&#125; 来销毁RefreshScope缓存的bean。 这是如有其他类通过代理对象调用了对应的方法，由于目标类已经被销毁了，所以beanFactory.getBean(beanName)会重新实例化refresh范围的bean Nacos 配置中心服务端原理 下面是以nacos集群+mysql存储 单机+嵌入式存储（Apache Derby）的就是去注解去数据库的 启动时拉取数据前面发现，nacos是直接读取本地文件数据的，那这些文件是怎么来的。 在服务启动时会启动一个服务DumpService，它是一个抽象类，有两个实现 ExternalDumpService就是我们的目标，另一个是使用内嵌存储才使用的。跟踪代码： 12// DumpServicedumpConfigInfo(dumpAllProcessor); 服务端启动时就会依赖 DumpService 的 init 方法，从数据库中 load 配置存储在本地磁盘上，并将一些重要的元信息例如 MD5 值缓存在内存中。服务端会根据心跳文件中保存的最后一次心跳时间，来判断到底是从数据库 dump 全量配置数据还是部分增量配置数据（如果机器上次心跳间隔是 6h 以内的话）。 全量 dump 当然先清空磁盘缓存，然后根据主键 ID 每次捞取一千条配置刷进磁盘和内存。增量 dump 就是捞取最近六小时的新增配置（包括更新的和删除的），先按照这批数据刷新一遍内存和文件，再根据内存里所有的数据全量去比对一遍数据库，如果有改变的再同步一次，相比于全量 dump 的话会减少一定的数据库 IO 和磁盘 IO 次数。 如果是增量的情况，最后会执行一个checkMd5Task任务，在该任务内部会发布这一个LocalDataChangeEvent事件，用来告诉客户端，配置发生变化了。 拉取数据服务前面已经知道，拉取配置信息是通过 12URL: /nacos/v1/cs/configsMethod: GET 获取的。这服务是ConfigController提供的。跟踪+debug就会发现，服务端获取配置不是直接查数据裤的，而是直接查询本地文件的。 12345678// ConfigServletInnerif (PropertyUtil.isDirectRead()) &#123; // 单机+嵌入式存储 直接查数据 configInfoBase = persistService.findConfigInfo(dataId, group, tenant);&#125; else &#123; file = DiskUtil.targetFile(dataId, group, tenant);&#125; 发布配置通过下面api发布配置： 12URL: /nacos/v1/cs/configsMethod: POST 发布配置的代码位于 ConfigController#publishConfig中。集群部署，请求一开始也只会打到一台机器，这台机器将配置插入Mysql中进行持久化。服务端并不是针对每次配置查询都去访问 MySQL ，而是会依赖 dump 功能在本地文件中将配置缓存起来。因此当单台机器保存完毕配置之后，需要通知其他机器刷新内存和本地磁盘中的文件内容，因此它会发布一个名为 ConfigDataChangeEvent 的事件，这个事件会通过 HTTP 调用通知所有集群节点（包括自身），触发本地文件和内存的刷新。 这里的发布事件和事件处理就涉及到nacos的 EventPublisher、Subscriber。详情看这里nacos事件 处理ConfigDataChangeEvent事件的Subscriber是在AsyncNotifyService类创建时订阅的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Autowiredpublic AsyncNotifyService(ServerMemberManager memberManager) &#123; this.memberManager = memberManager; // Register ConfigDataChangeEvent to NotifyCenter. NotifyCenter.registerToPublisher(ConfigDataChangeEvent.class, NotifyCenter.ringBufferSize); // Register A Subscriber to subscribe ConfigDataChangeEvent. NotifyCenter.registerSubscriber(new Subscriber() &#123; @Override public void onEvent(Event event) &#123; // Generate ConfigDataChangeEvent concurrently if (event instanceof ConfigDataChangeEvent) &#123; // 处理ConfigDataChangeEvent事件，用来通知集群中所有节点需要更新本地缓存 ConfigDataChangeEvent evt = (ConfigDataChangeEvent) event; long dumpTs = evt.lastModifiedTs; String dataId = evt.dataId; String group = evt.group; String tenant = evt.tenant; String tag = evt.tag; // ServerMemberManager在注册中心就讲过，他是用来管理集群中节点信息的 // 这里会返回整个集群中的节点信息，包括自己 Collection&lt;Member&gt; ipList = memberManager.allMembers(); // In fact, any type of queue here can be Queue&lt;NotifySingleTask&gt; queue = new LinkedList&lt;NotifySingleTask&gt;(); for (Member member : ipList) &#123; queue.add(new NotifySingleTask(dataId, group, tenant, tag, dumpTs, member.getAddress(), evt.isBeta)); &#125; // 这里会发起一个异步http请求 // 该请求为: // url: /nacos/v1/cs/communication/dataChange // method: GET ConfigExecutor.executeAsyncNotify(new AsyncTask(nacosAsyncRestTemplate, queue)); &#125; &#125; @Override public Class&lt;? extends Event&gt; subscribeType() &#123; return ConfigDataChangeEvent.class; &#125; &#125;);&#125; 这个流程就是向集群所有节点（包括自己）发起以异步的http请求，地址为： 12url: /nacos/v1/cs/communication/dataChangemethod: GET 异步http，有分 客户端异步（nio） 服务端异步（异步servlet） 这里是客户端异步 发布配置后上边已经讲了，发布配置后，数据库的配置已经是最新的了，而且会通过发布ConfigDataChangeEvent事件，使得整个集群中的节点，都会收到 12url: /nacos/v1/cs/communication/dataChangemethod: GET 这个请求，来处理变变更。下面看这个请求的处理 在CommunicationController#notifyConfigInfo，该方法非常的检查，就只是调用了DumpService#dump方法，而该方法也非常的简单 12345// DumpServicepublic void dump(String dataId, String group, String tenant, long lastModified, String handleIp, boolean isBeta) &#123; String groupKey = GroupKey2.getKey(dataId, group, tenant); dumpTaskMgr.addTask(groupKey, new DumpTask(groupKey, lastModified, handleIp, isBeta));&#125; 如果要了解dumpTaskMgr，就需要了解Nacos的AbstractNacosTaskExecuteEngine 下面是dumpTaskMgr的初始化 1234// DumpServicehis.processor = new DumpProcessor(this);this.dumpTaskMgr = new TaskManager(&quot;com.alibaba.nacos.server.DumpTaskManager&quot;);this.dumpTaskMgr.setDefaultTaskProcessor(processor); 所以这个DumpTask任务就交给了DumpProcessor来处理。该类主要有两个流程 调用DiskUtil.save...Disk来更新本地文件（缓存） 发起发起了一个LocalDataChangeEvent事件，来对客户端进行通知（采用servlet3.0的异步http） 这里的发布事件和事件处理就涉及到nacos的 EventPublisher、Subscriber。详情看这里nacos事件 处理LocalDataChangeEvent事件的Subscriber是在LongPollingService服务启动时注册的。 LongPollingService是用来管理通过 12URL: /nacos/v1/cs/configs/listenerMethod: POST 注册进来的客户端的，也就是调用过监听接口的客户端，该接口是异步http，使用了servlet3.0的异步 123456789101112131415161718192021// LongPollingService// Register A Subscriber to subscribe LocalDataChangeEvent.NotifyCenter.registerSubscriber(new Subscriber() &#123; @Override public void onEvent(Event event) &#123; if (isFixedPolling()) &#123; // Ignore. &#125; else &#123; if (event instanceof LocalDataChangeEvent) &#123; LocalDataChangeEvent evt = (LocalDataChangeEvent) event; ConfigExecutor.executeLongPolling(new DataChangeTask(evt.groupKey, evt.isBeta, evt.betaIps)); &#125; &#125; &#125; @Override public Class&lt;? extends Event&gt; subscribeType() &#123; return LocalDataChangeEvent.class; &#125;&#125;); 看DataChangeTask的run方法 123456789101112131415161718192021222324252627282930public void run() &#123; try &#123; ConfigCacheService.getContentBetaMd5(groupKey); // 这里是那些调用了POST /nacos/v1/cs/configs/listener接口的请求 // 该接口使用的是servlet3.0的异步请求（服务端异步）,其中ClientLongPolling保存客户端的AsyncContext // AsyncContext是servlet3.0的规范，用来异步的返回数据和请求响应 for (Iterator&lt;ClientLongPolling&gt; iter = allSubs.iterator(); iter.hasNext(); ) &#123; ClientLongPolling clientSub = iter.next(); if (clientSub.clientMd5Map.containsKey(groupKey)) &#123; // If published tag is not in the beta list, then it skipped. if (isBeta &amp;&amp; !CollectionUtils.contains(betaIps, clientSub.ip)) &#123; continue; &#125; // If published tag is not in the tag list, then it skipped. if (StringUtils.isNotBlank(tag) &amp;&amp; !tag.equals(clientSub.tag)) &#123; continue; &#125; getRetainIps().put(clientSub.ip, System.currentTimeMillis()); iter.remove(); // Delete subscribers&#x27; relationships. // 这里会对客户端做响应 clientSub.sendResponse(Arrays.asList(groupKey)); &#125; &#125; &#125; catch (Throwable t) &#123; LogUtil.DEFAULT_LOG.error(&quot;data change error: &#123;&#125;&quot;, ExceptionUtil.getStackTrace(t)); &#125;&#125; 要看懂这里的方法，所以先知道servlet3.0的异步http和异步http的使用 nacos客户端监听和nacos发布的原理其实就是使用了servlet3.0的异步http。 客户端会有一个长轮询任务，拉取服务端的配置变更，服务端处理逻辑在LongPollingService类中，其中有一个 Runnable 任务名为ClientLongPolling，服务端会将受到的轮询请求包装成一个 ClientLongPolling 任务，该任务持有一个 AsyncContext 响应对象，通过定时线程池延后 29.5s 执行。比客户端 30s 的超时时间提前 500ms 返回是为了最大程度上保证客户端不会因为网络延时造成超时。 客户端调用了POST /nacos/v1/cs/configs/listener请求后，在客户端会采用异步http，获取AsyncContext，并生成ClientLongPolling对象，保存进LongPollingService中 服务发布了LocalDataChangeEvent事件后，触发了LongPollingService的DataChangeTask任务，在该任务中就能对第一步的请求做响应 客户接收到响应后就重新获取配置 总结Nacos配置中心源码分析","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"13-Spring Cloud配置原理","slug":"springcloud/13-Spring Cloud配置原理","date":"2021-11-27T12:00:21.000Z","updated":"2022-03-23T09:03:56.620Z","comments":true,"path":"blog/springcloud/13-Spring Cloud配置原理/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/13-Spring%20Cloud%E9%85%8D%E7%BD%AE%E5%8E%9F%E7%90%86/","excerpt":"","text":"在Spring boot项目启动时，完成了默认配置文件的加载，会进行上下文初始化工作，这个初始化工作交给ApplicationContextInitializer来完成，而对应的实现类是通过Spring SPI引入的。 在Spring cloud中，也提供了一个PropertySourceBootstrapConfiguration，它是ApplicationContextInitializer的实现类，不过它不是通过SPI引入的，而是通过新建一个ApplicationContext引入的，流程如下图： 流程图 在Spring cloud中，新增了一个BootstrapConfiguration，这个在代码上没有任何实质的作用，它是作为SPI的key存在的，它的目是引入Spring cloud的配置类，其中一个重要的配置类就是PropertySourceBootstrapConfiguration。 而BootstrapApplicationListener作为作为spring cloud上下文的启动类，它会完成spring cloud上下文的初始化。BootstrapImportSelectorConfiguration作为spring cloud上下文的启动配置类，它会出发BootstrapConfiguration的SPI加载，使得能够引入扩展的配置类进入到spring cloud上下文中，PropertySourceBootstrapConfiguration就是通过这样引入到spring cloud上下文中的。在springcloud上下文完成了初始化后，还会把实现了ApplicationContextInitializer的对象加入到项目的SpringApplication对象中，使得这些被引入的ApplicationContextInitializer的对象能够参与到子上下文的准备工作中。 所以PropertySourceBootstrapConfiguration会在配置对象ConfigurableEnvironment完成了初始化后（默认配置文件的加载），会执行其属性成员PropertySourceLocator的方法调用，来完成一些额外的配置加载操作。 所以，想进行配置扩展的话可以 使用BootstrapConfiguration通过SPI引入PropertySourceLocator的实现类 或者像PropertySourceBootstrapConfiguration那样，实现ApplicationContextInitializer接口，然后使用BootstrapConfiguration通过SPI引入，这样能够不遵循spring cloud的规范","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"12-SpringBoot中的配置原理","slug":"springcloud/12-SpringBoot中的配置原理","date":"2021-11-27T12:00:20.000Z","updated":"2022-03-23T09:03:56.545Z","comments":true,"path":"blog/springcloud/12-SpringBoot中的配置原理/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/12-SpringBoot%E4%B8%AD%E7%9A%84%E9%85%8D%E7%BD%AE%E5%8E%9F%E7%90%86/","excerpt":"","text":"SpringBoot项目启动原理 配置类的初始化SpringBoot启动时，代码走到这进行配置的初始化: 12ApplicationArguments applicationArguments = new DefaultApplicationArguments(args);ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); prepareEnvironment方法如下： 1234567891011121314151617181920212223242526272829303132private ConfigurableEnvironment prepareEnvironment(SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) &#123; // 根据webApplicationType 创建Environment // 会读取环境变量和jvm的-D参数 // Servlet是StandardServletEnvironment // 反应式的是StandardReactiveWebEnvironment // 不是上边的，那就是StandardEnvironment ConfigurableEnvironment environment = getOrCreateEnvironment(); // 将启动命令参数读取环境变量中 configureEnvironment(environment, applicationArguments.getSourceArgs()); // 这里会把ConfigurationPropertySourcesPropertySource放在第一位 ConfigurationPropertySources.attach(environment); // 读取配置文件@PropertySource优先级是最低 // 发布了ApplicationEnvironmentPreparedEvent事件，让ConfigFileApplicationListener来处理 // 该类会做一系列的配置加载，比如环境变量，配置文件 listeners.environmentPrepared(environment); // 将所有spring.main 开头的配置信息绑定SpringApplication的属性中 bindToSpringApplication(environment); if (!this.isCustomEnvironment) &#123; environment = new EnvironmentConverter(getClassLoader()).convertEnvironmentIfNecessary(environment, deduceEnvironmentClass()); &#125; //更新PropertySources ConfigurationPropertySources.attach(environment); return environment;&#125; 这个方法的核心在 1listeners.environmentPrepared(environment); 这行代码，这行代码只是发布了ApplicationEnvironmentPreparedEvent事件，而SpringBoot会通过SpringSPI加载一系列的ApplicationListener实现，其中有： ConfigFileApplicationListener，这个类就是用来加载配置文件的，它的核心方法： 12345678private void onApplicationEnvironmentPreparedEvent(ApplicationEnvironmentPreparedEvent event) &#123; List&lt;EnvironmentPostProcessor&gt; postProcessors = loadPostProcessors(); postProcessors.add(this); AnnotationAwareOrderComparator.sort(postProcessors); for (EnvironmentPostProcessor postProcessor : postProcessors) &#123; postProcessor.postProcessEnvironment(event.getEnvironment(), event.getSpringApplication()); &#125;&#125; 其中loadPostProcessors方法只是用来SpringSPI来加载EnvironmentPostProcessor的实现对象而已，同时把ConfigFileApplicationListener对象也添加到这个返回的列表中，所以我们看ConfigFileApplicationListener的postProcessEnvironment方法就好了，最后会使用内部类Loader来完成配置文件的加载 在Loader创建时，会使用SpringSPI来加载PropertySourceLoader的实现类，SpringBoot会提供： 这两个类来完成对properties或者yaml文件的解析和处理。 比如，我的项目配置文件如下： 在启动完成后Environment对象的内容如下： application是spring boot项目默认会加载的 bootstrap是spring cloud项目添加，也会默认加载 配置的优先级配置的优先级就是 这个字段的顺序决定的，下标越大，优先级越低。 1234MutablePropertySources ps = (ConfigurableEnvironment)environment.getPropertySources();ps.addps.removeps.replace 来修改优先级 spring中的Environment都是在AbstractEnvironment的基础上创建的，其真个调用流程如下： 总结 spring boot项目中默认配置文件的加载是通过PropertySourceLoader来完成的，通过Spring spi引入 配置的优先级是通过其配置对象的内部的MutablePropertySources类型的属性决定的，MutablePropertySources内部有一个CopyOnWriteArrayList结构，用来保存配置，下标越小，优先级越高","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"11-nacos配置中心","slug":"springcloud/11-nacos配置中心","date":"2021-11-27T12:00:19.000Z","updated":"2022-03-23T09:03:56.507Z","comments":true,"path":"blog/springcloud/11-nacos配置中心/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/11-nacos%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/","excerpt":"","text":"官方文档: https://github.com/alibaba/spring­cloud­alibaba/wiki/Nacos­config Nacos 提供用于存储配置和其他元数据的 key&#x2F;value 存储，为分布式系统中的外部化配置提供服务器端和客户端支持。使用 Spring Cloud Alibaba Nacos Config，您可以在 Nacos Server 集中管理你 Spring Cloud 应用的外部属性配置。 Spring Cloud Alibaba Nacos Config 是 Config Server 和 Client 的替代方案，客户端和服务器上的概念与 Spring Environment 和 PropertySource 有着一致的抽象，在特殊的 bootstrap 阶段，配置被加载到 Spring 环境中。当应用程序通过部署管道从开发到测试再到生产时，您可以管理这些环境之间的配置，并确保应用程序具有迁移时需要运行的所有内容。 Nacos 数据模型 Key 由三元组唯一确定 Namespace默认是空串，公共命名空间(public) 分组默认是 DEFAULT_GROUP 快速开始准备配置，nacos server中新建nacos-config.yaml和nacos-config 两个配置文件中有一个同名的值default.user，但值不同。 搭建nacos-config服务通过 Nacos Server 和 spring-cloud-starter-alibaba-nacos-config 实现配置的动态变更 123456789101112131415161718192021222324&lt;!-- 父pom --&gt;&lt;dependencyManagement&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Hoxton.SR8&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.5.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencyManagement&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring‐cloud‐starter‐alibaba‐nacos‐config&lt;/artifactId&gt;&lt;/dependency&gt; 添加bootstrap.yaml配置 12345678910111213spring: main: allow-bean-definition-overriding: true application: name: nacos-config cloud: nacos: discovery: server-addr: 192.168.2.148:8848 config: #配置中心 file-extension: yaml server-addr: 192.168.2.148:8848 namespace: public 启动项目 12345678910111213@SpringBootApplicationpublic class NacosConfigApplication &#123; public static void main(String[] args) throws InterruptedException &#123; ConfigurableApplicationContext applicationContext = SpringApplication.run(NacosConfigApplication.class, args); String userName = applicationContext.getEnvironment().getProperty(&quot;common.name&quot;); String userAge = applicationContext.getEnvironment().getProperty(&quot;common.age&quot;); String defaultUser = applicationContext.getEnvironment().getProperty(&quot;default.user&quot;); String defaultData = applicationContext.getEnvironment().getProperty(&quot;default.data&quot;); System.err.println(&quot;common name :&quot; + userName + &quot;; age: &quot; + userAge); System.err.println(&quot;default user :&quot; + defaultUser + &quot;; data: &quot; + defaultData); &#125;&#125; Config相关配置 以yaml格式说明（或者yml） 默认会加载的配置服务的名字为nacos-config ， 启动后打印的结果： 可以看到，默认会加载nacos-config.yaml和nacos-config这两个配置文件的值。 spring‐cloud‐starter‐alibaba‐nacos‐config在加载配置的时候，默认会加载dataid为 $&#123;spring.application.name&#125; $&#123;spring.application.name&#125;.$&#123;file-extension:properties&#125; 前缀的基础配置 支持profile粒度的配置spring‐cloud‐starter‐alibaba‐nacos‐config除了默认的两个外，还会默认加载指定profile的配置，dataid前缀为： $&#123;spring.application.name&#125;-$&#123;spring.profiles.active&#125;.$&#123;file-extension&#125; 比如： 123spring: profiles: active: prod 自动的会加载nacos-config-prod.yaml这个配置 支持自定义 namespace 的配置支持资源隔离，可以按项目、数据库等方式隔离，在spring cloud项目配置中通过： $&#123;spring.cloud.nacos.config.namespace&#125;指定，默认情况下是： 也可以在nacos-web端添加。对于添加的，上边的配置填的值为： 命名空间的ID，不是名称 支持自定义 Group 的配置Group是组织配置的维度之一。通过一个有意义的字符串(如 Buy 或 Trade )对配置集进行分组，从而区分 Data ID 相同的配置集。当您在 Nacos 上创建一个配置时，如果未填写配置分组的名称，则配置分组的名称默认采用DEFAULT_GROUP 。 配置分组的常见场景：不同的应用或组件使用了相同的配置类型，如 database_url 配置和MQ_topic 配置。 想项目中通过 $&#123;spring.cloud.nacos.config.group&#125;指定，默认是DEFAULT_GROUP 。 支持自定义扩展的 Data Id 配置Data ID 是组织划分配置的维度之一。Data ID 通常用于组织划分系统的配置集。一个系统或者应用可以包含多个配置 集，每个配置集都可以被一个有意义的名称标识。 Data ID 通常采用类 Java 包(如 com.taobao.tc.refund.log.level)的命名规则保证全局唯一性。此命名规则非强制。 通过自定义扩展的 Data Id 配置，既可以解决多个应用间配置共享的问题，又可以支持一个应用有多个配置文件。 配置如下： 12345678910111213141516171819202122232425262728spring: profiles: active: prod application: name: nacos-config cloud: nacos: discovery: server-addr: 192.168.2.148:8848 # 对于nacos配置中心，这是必须的 config: #配置中心 file-extension: yaml server-addr: 192.168.2.148:8848 namespace: public group: DEFAULT_GROUP refresh-enabled: true # 是否支持刷新，默认支持 # 共享配置 shared-configs: - dataId: redis.yaml # 可以指定多个，用逗号隔开 group: DEFAULT_GROUP refresh: false # 是否支持刷新，默认不支持 # 多个配置文件 extension-configs: - dataId: common.yaml group: DEFAULT_GROUP refresh: false # 是否支持刷新，默认不支持 - dataId: ext-common.yaml group: REFRESH_GROUP refresh: true 支持配置的动态更新对于默认加载的配置，默认是支持刷新的。通过 1spring.cloud.nacos.config.refreshEnabled=true 配置 对于自定义扩展的 Data Id 配置，默认是不支持刷新的，通过如下配置： 1234567891011121314spring: application: name: nacos-config cloud: nacos: discovery: server-addr: 192.168.2.148:8848 # 对于nacos配置中心，这是必须的 config: #配置中心 file-extension: yaml server-addr: 192.168.2.148:8848 extension-configs: # 自定义扩展的 Data Id 配置 - dataId: redis.yaml group: REFRESH_GROUP refresh: true 配置的优先级spring‐cloud‐starter‐alibaba‐nacos‐config目前提供了三种配置能力，从nacos拉取相关的配置。 A: 通过 spring.cloud.nacos.config.sharedConfigs 支持多个共享 Data Id 的配置 B: 通过 spring.cloud.nacos.config.extensionConfigs 的方式支持多个扩展 Data Id 的配置 C: 通过内部相关规则(应用名、应用名+ Profile )自动生成相关的 Data Id 配置（默认会加载） 当三种方式共同使用时，他们的一个优先级关系是:A &lt; B &lt; C 优先级从高到低: ${spring.cloud.nacos.config.file-extension} &#x3D; ${file-extension} $&#123;spring.application.name&#125;-$&#123;spring.profiles.active&#125;.$&#123;file-extension&#125; $&#123;spring.application.name&#125;.$&#123;file-extension&#125; $&#123;spring.application.name&#125; extensionConfigs配置 extensionConfigs[1] extensionConfigs[0] sharedConfigs配置 sharedConfigs[1] sharedConfigs[0] 完全关闭配置通过设置 spring.cloud.nacos.config.enabled=false 来完全关闭 Spring Cloud Nacos Config 完整的配置123456789101112131415161718192021222324252627282930spring: profiles: active: prod main: allow-bean-definition-overriding: true application: name: nacos-config cloud: nacos: discovery: server-addr: 192.168.2.148:8848 config: #配置中心 # 是否启用Nacos配置，默认是true enabled: true file-extension: yaml server-addr: 192.168.2.148:8848 namespace: public group: DEFAULT_GROUP refresh-enabled: true # 是否支持刷新，默认支持 shared-configs: - dataId: common.yaml group: DEFAULT_GROUP refresh: false # 是否支持刷新，默认不支持 extension-configs: - dataId: common.yaml # group: DEFAULT_GROUP refresh: false # 是否支持刷新，默认不支持 - dataId: redis.yaml group: DEFAULT_GROUP refresh: false @RefreshScope注解123456789101112131415@RestController@RefreshScope // 替换配置beanpublic class TestController &#123; @Value(&quot;$&#123;common.age&#125;&quot;) private String age; @Value(&quot;$&#123;common.name&#125;&quot;) private String name; @GetMapping(&quot;/common&quot;) public String hello() &#123; return name+&quot;,&quot;+age; &#125;&#125; 如果需要动态的感知age和name的变化，在类上加上@RefreshScope注解即可。 @RefreshScope注解是Spring Cloud提供的 SpringBoot整合Nacos引入： 1234567891011&lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;nacos-config-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;nacos-config-spring-boot.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;nacos-config-spring-boot-actuator&lt;/artifactId&gt; &lt;version&gt;$&#123;nacos-config-spring-boot.version&#125;&lt;/version&gt;&lt;/dependency&gt; 配置： 123nacos: config: server-addr: 127.0.0.1:8848 启动类 123456789101112131415/** * Document: https://nacos.io/zh-cn/docs/quick-start-spring-boot.html */@SpringBootApplication@NacosPropertySources(value = &#123; @NacosPropertySource(dataId = &quot;example&quot;, autoRefreshed = true), @NacosPropertySource(dataId = &quot;example1&quot;, autoRefreshed = true)&#125;)@NacosPropertySource(dataId = &quot;example2&quot;, autoRefreshed = true)public class NacosConfigApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(NacosConfigApplication.class, args); &#125;&#125; 使用： 1234567891011121314151617@Controller@RequestMapping(&quot;config&quot;)public class ConfigController &#123; // 这是nacos提供的注解 @NacosValue(value = &quot;$&#123;useLocalCache:false&#125;&quot;, autoRefreshed = true) private boolean useLocalCache; @Value(value = &quot;$&#123;useLocalCache:false&#125;&quot;) private boolean useLocalCache; @RequestMapping(value = &quot;/get&quot;, method = GET) @ResponseBody public boolean get() &#123; return useLocalCache; &#125;&#125;","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"10-2Nacos注册中心CP架构Raft源码分析","slug":"springcloud/10-2Nacos注册中心CP架构Raft源码分析","date":"2021-11-27T12:00:18.000Z","updated":"2022-03-23T09:03:56.496Z","comments":true,"path":"blog/springcloud/10-2Nacos注册中心CP架构Raft源码分析/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/10-2Nacos%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83CP%E6%9E%B6%E6%9E%84Raft%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"raft 在1.4.0之后使用了sofa-jraft（从百度的 braft 移植而来）来代替nacos自己实现的raft。 这里看nacos自己实现的简化了的raft协议，注意了，是简化过的，不是严格的Raft协议，所以1.4.0之后已经被标记为弃用了,使用了sofa-jraft。 在Nacos是同时支持AP和CP的，默认情况下，注册的服务是使用AP架构，而如果要使用CP架构，只需要使用持久实例就好了，比如下面的配置： 12345678910spring: application: name: mall-user-ribbon-demo #微服务名称 #配置nacos注册中心地址 cloud: nacos: discovery: server-addr: 192.168.2.148:8848 # 是否临时实例，默认值为true，用的是AP架构；如果设置为false，就变成了持久实例，用的是CP架构 ephemeral: false 只需要改把ephemeral: false就好了。 nacos自己实现的简化了的raft协议 看的是nacos自己实现的raft协议 初始化每个节点启动时，都会执行的一些必要初始化逻辑 RaftCore#init 1234567891011121314151617181920212223242526272829303132333435363738// RaftCore// 把&#123;nacos.home&#125;/nacos/data/naming/data/中的文件加载到内存中// 数据保存到成员变量datums中，该结构为这个原子mapraftStore.loadDatums(notifier, datums);// 把&#123;nacos.home&#125;/nacos/data/naming/data/meta.properties的保存到Properties中// 读取term，这个是raft中的概念，和zk中的epoch的概念一样，表意选举周期// 这个值是在RaftPeerSet中维护的setTerm(NumberUtils.toLong(raftStore.loadMeta().getProperty(&quot;term&quot;), 0L));// 注册选举任务masterTask = GlobalExecutor.registerMasterElection(new MasterElection());// 注册心跳任务heartbeatTask = GlobalExecutor.registerHeartbeat(new HeartBeat());// 注册版本检查后的回调// 这个是在1.4.0加入的// 在1.4.0中使用sofa-jraft代替了nacos自己实现的raft协议// 所以如果集群中所有的节点版本都大于等于1.4.0，那么就会使用sofa-jraftversionJudgement.registerObserver(isAllNewVersion -&gt; &#123; stopWork = isAllNewVersion; if (stopWork) &#123; try &#123; shutdown(); raftListener.removeOldRaftMetadata(); &#125; catch (NacosException e) &#123; throw new NacosRuntimeException(NacosException.SERVER_ERROR, e); &#125; &#125;&#125;, 100);// 订阅ValueChangeEvent事件// 处理流程为 PersistentNotifier ---&gt; 对应的RecordListener// 这个对应的对应的RecordListener就是Service，在Service初始化时添加到PersistentNotifier中的// 其实就是和临时实例的一样，就是用来响应服务数据的变化的（更新注册表+udp通知客户端）NotifyCenter.registerSubscriber(notifier); RaftPeerSet 初始化Raft协议中的节点信息，依赖于ServerMemberManager。 Leader选举投票看MasterElection，该类主要做的事： MasterElection这个任务是放在一个定时任务中处理，每500毫秒执行一次 等待一段随机时间 123456789101112131415161718// MasterElection#run// 获取本机的RaftPeer local = peers.local();// local.leaderDueMs// 第一次是0~15s内// 之后是15s+(0~3s)// 这个投票时间还是挺长的// GlobalExecutor.TICK_PERIOD_MS = 500， 这个值也是MasterElection的执行周期local.leaderDueMs -= GlobalExecutor.TICK_PERIOD_MS;// 检查是否已经可以投票了if (local.leaderDueMs &gt; 0) &#123; return;&#125;// reset timeoutlocal.resetLeaderDue();local.resetHeartbeatDue(); 发起投票 12345678910111213141516171819// MasterElection#sendVoteRaftPeer local = peers.get(NetUtils.localServer());// 重制RaftPeer.voteFor = null// 这样做是为了能够统计票peers.reset();// term + 1local.term.incrementAndGet();// 表示投票给自己local.voteFor = local.ip;// 进入投票阶段local.state = RaftPeer.State.CANDIDATE;for (final String server : peers.allServersWithoutMySelf()) &#123; // 发起一个客户端异步http // POST /nacos/vi/ns/raft/vote local会作为参数 // 调用成功后RaftPeerSet.decideLeader(RaftPeer);来机票 // 入场是返回值，表示接收方投给谁了 // RaftPeerSet.decideLeader的核心就是选Leader，把leader赋值给RaftPeerSet.leader&#125; 当接收方收到一个投票后，其核心就是进行term的比较，比较规则是： term大的就投给谁 term相同时，谁先发起投票就投给谁 源码： 1234567891011121314151617181920212223242526// MasterElection#receivedVotepublic synchronized RaftPeer receivedVote(RaftPeer remote) &#123; // 由于可能存在并发，所以加锁了，这样就能保证term相同时谁先发起投票就投给谁 if (stopWork) &#123; throw new IllegalStateException(&quot;old raft protocol already stop work&quot;); &#125; if (!peers.contains(remote)) &#123; throw new IllegalStateException(&quot;can not find peer: &quot; + remote.ip); &#125; RaftPeer local = peers.get(NetUtils.localServer()); if (remote.term.get() &lt;= local.term.get()) &#123; // 投给term值大的那个 if (StringUtils.isEmpty(local.voteFor)) &#123; local.voteFor = local.ip; &#125; return local; &#125; local.resetLeaderDue(); local.state = RaftPeer.State.FOLLOWER; local.voteFor = remote.ip; local.term.set(remote.term.get()); return local;&#125; 整个投票的代码就这些，经过这写代码能保证只有一个节点成为Leader，其他都是Follower。但是，还有两个问题： 经过上述的代码只是在发起投票的节点知道哪个节点是leader，其他节点还不知道leader节点。 由于MasterElection是每500毫秒执行一次，所以要怎么在leader节点正常的时候，防止Follower节点发起投票。 为了解决这两个问题，Leader节点就必须在完成选举后每隔一段时间就向Follower发起心跳。 数据同步（心跳）Leadder发送心跳： 12345678910111213141516171819// HeartBeat#sendBeatprivate void sendBeat() &#123; RaftPeer local = peers.local(); // 只有Leader才需要发送心跳 if (EnvUtil.getStandaloneMode() || local.state != RaftPeer.State.LEADER) &#123; return; &#125; // 这个方法除了重置发起投票的时间外，还有阻止MasterElection任务执行的作用 // 在这里其实就是防止leader发起选举 local.resetLeaderDue(); ObjectNode packet.... packet.replace(&quot;peer&quot;, JacksonUtils.transferToJsonNode(local)); // array包含了一些持久实例的 packet.replace(&quot;datums&quot;, array); for (final String server : peers.allServersWithoutMySelf()) &#123; // 发起一个客户端异步http // POST /nacos/v1/ns/raft/beat &#125;&#125; Follower接收到Leader的心跳包: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// HeartBeat#receivedBeatpublic RaftPeer receivedBeat(JsonNode beat) &#123; RaftPeer local = peers.local(); // leader节点信息 RaftPeer remote .... // 有可能自己也发起其了选举（CANDIDATE） if (local.state != RaftPeer.State.FOLLOWER) &#123; // mk follower local.state = RaftPeer.State.FOLLOWER; local.voteFor = remote.ip; &#125; // 这个方法除了重置发起投票的时间外，还有阻止MasterElection任务执行的作用 // 阻止Follower节点发起选举 local.resetLeaderDue(); // 保存leader节点信息 peers.makeLeader(remote); // 判断哪些数据需要从leader拉取数据，哪些数据需要删除 // 删除的key只是那些不符合规则的可以而已，不会删除那些数据不一致的 for (Object object : beatDatums) &#123; // 这个的判断会导致Follower的数据会比Leader的新 if (datums.containsKey(datumKey) &amp;&amp; datums.get(datumKey).timestamp.get() &gt;= timestamp &amp;&amp; processedCount &lt; beatDatums.size()) &#123; continue; &#125; // 需要从leader拉取数据 // 发起一个客户端异步HTTP请求 GET /nacos/v1/ns/raft/datum 参数：key // 当请求完成时Callback执行逻辑如下： // 写到本地文件 raftStore.write(newDatum); // 更新内存缓存 datums.put(newDatum.key, newDatum); //更新注册表 notifier.notify(newDatum.key, DataOperation.CHANGE, newDatum.value); &#125; // 删除非法数据 // 这里删除都是那些不合法的 for (Map.Entry&lt;String, Integer&gt; entry : receivedKeysMap.entrySet()) &#123; if (entry.getValue() == 0) &#123; deleteDatum(entry.getKey()); &#125; &#125;&#125; 数据同步的代码也不复杂，从HeartBeat#receivedBeat的 12peers.makeLeader(remote);local.resetLeaderDue(); 这两行代码就解答的上边的2个问题了。 写操作在面已经知道了，服务注册都会走到这里 12345678910111213141516171819202122// ServiceManagerpublic void addInstance(String namespaceId, String serviceName, boolean ephemeral, Instance... ips) &#123; // 这里会创建一个Key // 对于临时节点key=com.alibaba.nacos.naming.iplist.ephemeral.$&#123;namespaceId&#125;##$&#123;serviceName&#125; // 对于持久节点key=com.alibaba.nacos.naming.iplist.$&#123;namespaceId&#125;##$&#123;serviceName&#125; // 区别就是有没有&quot;ephemeral.&quot;这个字符床 String key = KeyBuilder.buildInstanceListKey(namespaceId, serviceName, ephemeral); Service service = getService(namespaceId, serviceName); synchronized (service) &#123; List&lt;Instance&gt; instanceList = addIpAddresses(service, ephemeral, ips); Instances instances = new Instances(); instances.setInstanceList(instanceList); // 这里的consistencyService的实现是DelegateConsistencyServiceImpl // 该类是一个委托类，会根据是否持久实例和临时实例来选择不同的ConsistencyService consistencyService.put(key, instances); &#125;&#125; 对于临时实例来说consistencyService的实现类为PersistentConsistencyServiceDelegateImpl，但它是一个代理类。 在集群模式下，所有节点的版本大于等于1.4.0的consistencyService对应的实例为PersistentServiceProcessor。 如有有个节点低于1.4.0就用RaftConsistencyServiceImpl。 这两个的区别就是使用的raft协议实现不同，PersistentServiceProcessor用的是sofa-jraft而RaftConsistencyServiceImpl是Nacos自己实现。 这里看RaftConsistencyServiceImpl 12// RaftConsistencyServiceImpl的putraftCore.signalPublish(key, value); Leader节点12345678910// RaftCore#signalPublishDatum datum = new Datum();...// 从这里可以看出datum.timestamp这个字读只是一个持久化次数计数器if (getDatum(key) == null) &#123; datum.timestamp.set(1L);&#125; else &#123; datum.timestamp.set(getDatum(key).timestamp.incrementAndGet());&#125;onPublish(datum, peers.local()); onPublish方法没必要详细看了，该方法的逻辑如下： raftStore.write(datum)，写入文件完成持久化 datums.put(datum.key, datum)，更新文件缓存数据 local.term.addAndGet(PUBLISH_TERM_INCREASE_COUNT)，更新Leader的term（+100） raftStore.updateTerm(local.term.get())， 把term持久化到meta.properties文件中 NotifyCenter.publishEvent(ValueChangeEvent，发起一个ValueChangeEvent事件，用来触发Service的onChange方法，进行注册表的更新和发udp数据包给客户端 执行完onPublish后就同步到Follower节点了 12345678910111213141516// RaftCore#signalPublish// 设置CountDownLatch的数值为过半节点数final CountDownLatch latch = new CountDownLatch(peers.majorityCount());for (final String server : peers.allServersIncludeMyself()) &#123; if (isLeader(server)) &#123; latch.countDown(); continue; &#125; // 发起一个客户端异步的HTTP请求，POST /nacos/v1/ns/raft/datum/commit 参数：content // Callback的处理流程如下 latch.countDown();&#125;if (!latch.await(5000, TimeUnit.MILLISECONDS))&#123; throw new IllegalStateException&#125; 但Follower节点接收到POST raft/datum/commit 请求也会执行RaftCore#onPublish方法，在这里，第一步会先进行 123if (source.term.get() &lt; local.term.get()) &#123; throw new IllegalStateException(&#125; 如果请求的数据的中的term小于本地的term，那就拒绝请求 这逻辑在Leader也有，但是对于Leader来说是不会发生的而已 还有第3点： 12getLeader().term.set(source.term.get());local.term.set(getLeader().term.get()); 会同步leader的走起。其他就是一样的了。 从整个流程可以看出，term在这里的实现已经不止是leader任期了。而是变成了写操作的周期了。这样每次写时都增大该值也就保证了只有最新写入的节点才会成为Leader。 Follower节点12// RaftCore#signalPublishraftProxy.proxyPostLarge(leader.ip, API_PUB, params.toString(), parameters); Follower节点只是向leader节点发起了一个HTTP请求，让Leader完成写操作。HTTP地址如下 1POST /nacos/v1/ns/raft/datum 而这个请求最终还是调用到Leader的RaftCore#signalPublish方法而已。 如果Leader挂了leader挂了后，由于Follower没有收到Leader的心跳包，也就是说不会执行 1local.resetLeaderDue(); 来重置时间，所以过一段时间后，某些Follower就会发起选举重新选一台Leader。 存在的问题 允许Follower节点的数据比Leader新（数据会出现不一致） 每一次写都会更新Leader的term值，虽然这样能保证Leader都是有最新的数据，但会在出现重新选举后，之前的Leader重新加入集群时，term值可能会比新的Leader大 Leader写完后，向Follower发起同步时不使用2PC，而是直接发起一个完整的事物操作。也正是这样，如果这次写操作有一半的机器写失败了，但有一部分机器成功，这样就会出现数据不一致的情况 重新选Leader的时间过长 实现得不是很严谨，还是有很多问题的。 看nacos2.0再看sofa-jraft吧。 总结","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"10-1Nacos注册中心AP架构源码分析","slug":"springcloud/10-1Nacos注册中心AP架构源码分析","date":"2021-11-27T12:00:17.000Z","updated":"2022-03-23T09:03:56.472Z","comments":true,"path":"blog/springcloud/10-1Nacos注册中心AP架构源码分析/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/10-1Nacos%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83AP%E6%9E%B6%E6%9E%84%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"基于1.4.x 服务注册和调用图 Nacos注册中心核心功能点 服务注册：Nacos Client会通过发送REST请求的方式向Nacos Server注册自己的服务，提供自身的元数据，比如ip地址、端口等信息。Nacos Server接收到注册请求后，就会把这些元数据信息存储在一个双层的内存Map中。 服务心跳：在服务注册后，Nacos Client会维护一个定时心跳来持续通知Nacos Server，说明服务一直处于可用状态，防止被剔除。默认5s发送一次心跳。 服务健康检查：Nacos Server会开启一个定时任务用来检查注册服务实例的健康情况，对于超过15s没有收到客户端心跳的实例会将它的healthy属性置为false(客户端服务发现时不会发现)，如果某个实例超过30秒没有收到心跳，直接剔除该实例(被剔除的实例如果恢复发送心跳则会重新注册) 服务发现：服务消费者（Nacos Client）在调用服务提供者的服务时，会发送一个REST请求给Nacos Server，获取上面注册的服务清单，并且缓存在Nacos Client本地，同时会在Nacos Client本地开启一个定时任务定时拉取服务端最新的注册表信息更新到本地缓存 服务同步：Nacos Server集群之间会互相同步服务实例，用来保证服务信息的一致性。 启动类通过解压nacos的jar包，并解压jar包得到的MANIFEST.MF文件，这个文件定义了main方法在哪个类上。 Nacos就是一个SpringBoot工程，所以这个MANIFEST.MF文件，的启动类是SpringBoot提供的启动的类，通过这个类加载jar包中的jar里面的类。 最后会调用Start-Class指明的类的main方法。所以Nacos的启动类就是： 1com.alibaba.nacos.Nacos 单机运行： 12# 增加启动vm参数-Dnacos.standalone=true 从com.alibaba.nacos.Nacos所在的包中就能发现，这是一个Spring boot工程，并提供各种http服务，其中就包括服务的注册和服务的获取。 服务注册 Open API 指南 (nacos.io) 在Spring-cloud-nacos服务注册和发现核心原理就讲了，服务是通过Spring Cloud提供的3个标准来完成服务的注册的，这3个标准为 ServiceRegistry Registration AbstractAutoServiceRegistration&lt;R extends Registration&gt; 而ncos-discovery包分别提供了三个实现 NacosServiceRegistry NacosRegistration NacosAutoServiceRegistration 而且ServiceRegistry就是用来完成服务的注册的，所以NacosServiceRegistry就是Nacos服务注册的核心。其注册的源码也很简单，核心就是调用NamingService对应实现NacosNamingService的： 1registerInstance(String serviceName, String groupName, Instance instance) 方法，一直跟踪代码，最后看到其实现类发起了一个http请求： 12methoh POSTurl /nacos/v1/ns/instance 回到NACOS项目的源码，提供这个服务的Controller在naming模块的InstanceController。看源码 12345678910// InstanceController@CanDistro@PostMapping@Secured(parser = NamingResourceParser.class, action = ActionTypes.WRITE)public String register(HttpServletRequest request) throws Exception &#123; .... final Instance instance = parseInstance(request); serviceManager.registerInstance(namespaceId, serviceName, instance); return &quot;ok&quot;;&#125; @CanDistro注解是nacos定义的注解，表示该请求是能被转发到别的节点的。原理是通过Servlet的Filter实现 1234567891011121314151617181920// ServiceManagerpublic void registerInstance(String namespaceId, String serviceName, Instance instance) throws NacosException &#123; // 第一次注册的时候，创建一个空的服务实例 // 把服务实例保存到服务注册表中 // 该注册表就是serviceMap这个属性 // Map&lt;String, Map&lt;String, Service&gt;&gt; serviceMap = new ConcurrentHashMap&lt;&gt;(); createEmptyService(namespaceId, serviceName, instance.isEphemeral()); // 获取服务实例，第一次的时候会获取到一个空的服务实例 Service service = getService(namespaceId, serviceName); if (service == null) &#123; throw new NacosException(NacosException.INVALID_PARAM, &quot;service not found, namespace: &quot; + namespaceId + &quot;, service: &quot; + serviceName); &#125; addInstance(namespaceId, serviceName, instance.isEphemeral(), instance);&#125; 这里涉及到服务的注册表serviceMap，其结果如下： 1Map&lt;String, Map&lt;String, Service&gt;&gt; serviceMap = new ConcurrentHashMap&lt;&gt;(); 举例说明： 看addInstance 1234567891011121314151617181920public void addInstance(String namespaceId, String serviceName, boolean ephemeral, Instance... ips) throws NacosException &#123; // 这里会创建一个Key // 对于临时节点key=com.alibaba.nacos.naming.iplist.ephemeral.$&#123;namespaceId&#125;.##$&#123;serviceName&#125; // 对于持久节点key=com.alibaba.nacos.naming.iplist.$&#123;namespaceId&#125;.##$&#123;serviceName&#125; // 区别就是有没有&quot;ephemeral.&quot;这个字符床 String key = KeyBuilder.buildInstanceListKey(namespaceId, serviceName, ephemeral); Service service = getService(namespaceId, serviceName); synchronized (service) &#123; List&lt;Instance&gt; instanceList = addIpAddresses(service, ephemeral, ips); Instances instances = new Instances(); instances.setInstanceList(instanceList); consistencyService.put(key, instances); &#125;&#125; 首先生成了服务对应的kye，比如注册一个服务名字为demo_server，在命名空间123中的一个临时服务实例，其生产的key为 1com.alibaba.nacos.naming.iplist.ephemeral.123.##demo_server 然后从注册表中获取到服务实例Service后，就执行 1List&lt;Instance&gt; instanceList = addIpAddresses(service, ephemeral, ips); 这行代码涉及到集群，但有一点是能从该方法的源码中确定的，就是对于第一次服务注册，addIpAddresses方法的返回值是肯定包含了该次需要注册的服务。 继续看代码consistencyService.put(key, instances);，对于consistencyService所对应的实例，看该类的开头： 而BeanName为consistencyDelegate的对象对应的类为DelegateConsistencyServiceImpl，定义如下： 123456789101112131415@DependsOn(&quot;ProtocolManager&quot;)@Service(&quot;consistencyDelegate&quot;)public class DelegateConsistencyServiceImpl implements ConsistencyService &#123; private final PersistentConsistencyServiceDelegateImpl persistentConsistencyService; private final EphemeralConsistencyService ephemeralConsistencyService; public DelegateConsistencyServiceImpl(PersistentConsistencyServiceDelegateImpl persistentConsistencyService, EphemeralConsistencyService ephemeralConsistencyService) &#123; this.persistentConsistencyService = persistentConsistencyService; this.ephemeralConsistencyService = ephemeralConsistencyService; &#125; .......&#125; 从名字和属性对象可以知道，该类只是一个委托类，作用只是选择是用临时实例的ConsistencyService还是持久实例的ConsistencyService，选择的方法如下： 1234private ConsistencyService mapConsistencyService(String key) &#123; // 其实就是检查key中是否以com.alibaba.nacos.naming.iplist.ephemeral开头 return KeyBuilder.matchEphemeralKey(key) ? ephemeralConsistencyService : persistentConsistencyService;&#125; 临时实例nacos的服务默认情况下就是临时实例，这里就看临时实例的ConsistencyService——DistroConsistencyServiceImpl。看其put实现 123456@Overridepublic void put(String key, Record value) throws NacosException &#123; onPut(key, value); distroProtocol.sync(new DistroKey(key, KeyBuilder.INSTANCE_LIST_KEY_PREFIX), DataOperation.CHANGE, globalConfig.getTaskDispatchPeriod() / 2);&#125; 其中onPut就是注册逻辑了，其源码: 12345678910111213141516171819202122public void onPut(String key, Record value) &#123; if (KeyBuilder.matchEphemeralInstanceListKey(key)) &#123; Datum&lt;Instances&gt; datum = new Datum&lt;&gt;(); // 这个value是包含某个服务的全部的可用的实例 datum.value = (Instances) value; datum.key = key; datum.timestamp.incrementAndGet(); // 保存到dataStore中 // 其实是一个Map&lt;key, Datum&gt;中 // 这个结构很重要，当了解了Distro协议后，就明白这个字段的作用了 dataStore.put(key, datum); &#125; // 这时检查是有有监听对象RecordListener， if (!listeners.containsKey(key)) &#123; return; &#125; // 如果服务有监听对象RecordListener，那这里就需要添加一个DataOperation.CHANGE来触发RecordListener对应方法的调用 // 其原理就是一个线程里面包含一个堵塞队列，遍历队列得到得到服务的key和DataOperation后，获取对应的RecordListener来调用对应的方法 notifier.addTask(key, DataOperation.CHANGE);&#125; distroProtocol.sync方法涉及到集群同步，在后面的集群中讲。 临时实例的DistroConsistencyServiceImpl只是往dataStore结构保存一些数据。但是没有往注册表的Service添加服务实例。 往注册表添加实例（更新）是通过异步执行的，在onPut中有这段代码 1234567// 这时检查是有有监听对象RecordListener，if (!listeners.containsKey(key)) &#123; return;&#125;// 如果服务有监听对象RecordListener，那这里就需要添加一个DataOperation.CHANGE来触发RecordListener对应方法的调用// 其原理就是一个线程里面包含一个堵塞队列，遍历队列得到得到服务的key和DataOperation后，获取对应的RecordListener来调用对应的方法notifier.addTask(key, DataOperation.CHANGE); 对于服务注册来说，在服务的Service对象创建的时候就会添加一个RecordListener，这个默认添加的RecordListener就是负责完成更新注册表的数据的。其代码在ServicManger#createEmptyService中 12345678910// 服务注册部分流程代码ServicManger#createEmptyService --&gt; ServicManger#createServiceIfAbsent .... service = new Service(); ....// 这是临时实例consistencyService.listen(KeyBuilder.buildInstanceListKey(service.getNamespaceId(),service.getName(), true), service); 其RecordListener的实现为Service，所以更新注册表的逻辑还是在Service类中的。 所以notifier.addTask(key, DataOperation.CHANGE);这行代码是会执行到的，这里的notifier是Notifier。它只是一个Runner，在类DistroConsistencyServiceImpl初始化时把这个Notifier添加到一个ScheduledExecutorService中。而Notifier的在执行后，会执行到RecordListener的对应方法，其中对于DataOperation.CHANGE，会执行RecordListener#onChange，所以更新注册表的方法就在Service的onChange方法中。 持久实例对于持久化实例，ConsistencyService对应的实现类为PersistentConsistencyServiceDelegateImpl，这个其实也是一个代理剋，在单机下，ConsistencyService的所有当法调用都会由StandalonePersistentServiceProcessor完成。 这个类的作用就是进行数据持久化，会把数据写到 12$&#123;nacos.home&#125;/nacos/data/naming/data/com.alibaba.nacos.naming.iplist.$&#123;namespaceId&#125;##$&#123;serviceName&#125;// 默认是user.home，环境变量 这个文件下 更新注册表时使用了写时复制跟踪Service#onChange代码到Service#updateIPs的: 1234567// Service#updateIPsfor (Map.Entry&lt;String, List&lt;Instance&gt;&gt; entry : ipMap.entrySet()) &#123; //make every ip mine List&lt;Instance&gt; entryIPs = entry.getValue(); // Map&lt;String, Cluster&gt; clusterMap = new HashMap&lt;&gt;(); clusterMap.get(entry.getKey()).updateIps(entryIPs, ephemeral);&#125; 这里就是更新注册表的地方。这里查看下Cluster#updateIps 1234567891011121314public void updateIps(List&lt;Instance&gt; ips, boolean ephemeral) &#123; Set&lt;Instance&gt; toUpdateInstances = ephemeral ? ephemeralInstances : persistentInstances; // 进行一些对ips和toUpdateInstances的一些操作 .... toUpdateInstances = new HashSet&lt;&gt;(ips); if (ephemeral) &#123; ephemeralInstances = toUpdateInstances; &#125; else &#123; persistentInstances = toUpdateInstances; &#125;&#125; 其中ephemeralInstances和persistentInstances是Service对象的成员变量，分别用来保存临时实例和持久实例，同时他们的定义如下： 123private Set&lt;Instance&gt; persistentInstances = new HashSet&lt;&gt;();private Set&lt;Instance&gt; ephemeralInstances = new HashSet&lt;&gt;(); 如果跟完Service#onChange，在更新注册表的时候是完全没有添加任何锁的，设置是volatile。 服务的实例是共享的资源，读和写操作肯定会存在并发的，而为了保证并发执行，最简单的就是加锁，这样会导致读和写会互斥，会牺牲了读的性能，但能保证读的实时性。而在spring cloud的项目中，因为有重试和负载均衡机制，服务的实时性其实不是很重要。所以nacos就采用了这种写时复制的思想，并且写操作只有一个线程执行，这样能提高读操作的性能。 客户端拉取服务如果项目（Spring Cloud项目）是使用Ribbon做均衡负载的（默认），nacos-discovery包提供的NacosServerList类来完成服务的拉取，这是使用Ribbon的标准 如果项目是使用Spring Cloud提供的LoadBalancer来做均衡负载的，nacos-discovery包提供了 NacosDiscoveryClient来完成服务的拉取，它是实现了Spring Cloud提供的DiscoveryClient接口。 无论是使用哪种均衡负载，最终都是通过NacosNamingService.getAllInstances(serviceName)方法来获取服务列表的。跟踪代码到： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// HostReactorpublic ServiceInfo getServiceInfo(final String serviceName, final String clusters) &#123; NAMING_LOGGER.debug(&quot;failover-mode: &quot; + failoverReactor.isFailoverSwitch()); String key = ServiceInfo.getKey(serviceName, clusters); if (failoverReactor.isFailoverSwitch()) &#123; return failoverReactor.getService(key); &#125; // 从缓存中获取 ServiceInfo serviceObj = getServiceInfo0(serviceName, clusters); if (null == serviceObj) &#123; // 第一次如果为空，就新建一个空的ServiceInfo，并放入缓存中 serviceObj = new ServiceInfo(serviceName, clusters); serviceInfoMap.put(serviceObj.getKey(), serviceObj); // updatingMap是一个ConcurrentHashMap // 这个字段的作用时用来表示是否第一次拉取服务 updatingMap.put(serviceName, new Object()); // 通过 GET /nacos/v1/ns/instance/list 来获取服务列表 // 获取并更新完服务列表后会调用 serviceInfo.notifyAll 来唤醒等待的线程 // 这里采用的是写时复制的思想来提供读的性能 updateServiceNow(serviceName, clusters); updatingMap.remove(serviceName); &#125; else if (updatingMap.containsKey(serviceName)) &#123; // 这里表示有读线程在第一次拉取服务列表，需要等待下，最大等待时间为5s // 这里初始化的时候才有可能进入到这里 if (UPDATE_HOLD_INTERVAL &gt; 0) &#123; // hold a moment waiting for update finish synchronized (serviceObj) &#123; try &#123; serviceObj.wait(UPDATE_HOLD_INTERVAL); &#125; catch (InterruptedException e) &#123; NAMING_LOGGER .error(&quot;[getServiceInfo] serviceName:&quot; + serviceName + &quot;, clusters:&quot; + clusters, e); &#125; &#125; &#125; &#125; // 往定时任务中添加添加一个UpdateTask任务，做定时更新缓存的操作 // 会在1s后执行第一次 // 同时，这个方法保证了只会添加一次 scheduleUpdateIfAbsent(serviceName, clusters); // 从缓存中获取服务列表，由于更新缓采用的是写时复制，这里返回的不一定是最新的值 return serviceInfoMap.get(serviceObj.getKey());&#125; 这段代码清晰的描述了nacos客户端拉取服务的流程。看注释就好了。 下面看下拉取服务的任务的执行逻辑。添加的任务类UpdateTask，看其run方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public void run() &#123; // 下次执行的时间，默认是1s，但会根据返回的cacheMillis和错误次数来修改这个值 long delayTime = DEFAULT_DELAY; try &#123; // 服务缓存 ServiceInfo serviceObj = serviceInfoMap.get(ServiceInfo.getKey(serviceName, clusters)); if (serviceObj == null) &#123; updateService(serviceName, clusters); return; &#125; // ServiceInfo.lastRefTime表示上次拉取到服务的时间 // UpdateTask.lastRefTime也是上次拉取到服务的时间 // 这里之所以做这个判断，是因为ServiceInfo有可能随着push发生更新了 if (serviceObj.getLastRefTime() &lt;= lastRefTime) &#123; // 这里会去nacos server拉取服务 // 同时也会更新服务缓存，采用的是写时复制思想 updateService(serviceName, clusters); serviceObj = serviceInfoMap.get(ServiceInfo.getKey(serviceName, clusters)); &#125; else &#123; // 如果服务缓存的时间大于任务的时间，就表示通过push更新过一次了 // 比如：server段的心跳检查时发现过期了，会 // refreshOnly其实就是只发起一次 GET /nacos/v1/ns/instance/list 请求 // if serviceName already updated by push, we should not override it // since the push data may be different from pull through force push refreshOnly(serviceName, clusters); &#125; lastRefTime = serviceObj.getLastRefTime(); .... delayTime = serviceObj.getCacheMillis(); resetFailCount(); &#125; catch (Throwable e) &#123; // 这里做的只是failCount++, 最大为6次 incFailCount(); NAMING_LOGGER.warn(&quot;[NA] failed to update serviceName: &quot; + serviceName, e); &#125; finally &#123; // 最后成功往定时任务再次添加一次UpdateTask任务，不过这个延迟时间（下次执行时间） // 会跟失败次数和nacos server返回的CacheMillis的值友关 executor.schedule(this, Math.min(delayTime &lt;&lt; failCount, DEFAULT_DELAY * 60), TimeUnit.MILLISECONDS); &#125;&#125; 服务端的返回任务列表服务通过上边知道，Nacos server提供了GET /nacos/v1/ns/instance/list来完成服务的拉取，提供该服务的Controller为naming模块的InstanceController 1234567@GetMapping(&quot;/list&quot;)@Secured(parser = NamingResourceParser.class, action = ActionTypes.READ)public ObjectNode list(HttpServletRequest request) throws Exception &#123; ..... return doSrvIpxt(namespaceId, serviceName, agent, clusters, clientIP, udpPort, env, isCheck, app, tenant, healthyOnly);&#125; doSrvIpxt方法中最终会从服务注册表中获取对应的Service后，然后通过Service.srvIPs来获取服务的实例 12345678910111213141516171819202122// Service// clusters是集群名字，从客户端传过来，默认值为DEFAULTpublic List&lt;Instance&gt; srvIPs(List&lt;String&gt; clusters) &#123; if (CollectionUtils.isEmpty(clusters)) &#123; clusters = new ArrayList&lt;&gt;(); clusters.addAll(clusterMap.keySet()); &#125; return allIPs(clusters);&#125;public List&lt;Instance&gt; allIPs(List&lt;String&gt; clusters) &#123; List&lt;Instance&gt; result = new ArrayList&lt;&gt;(); for (String cluster : clusters) &#123; Cluster clusterObj = clusterMap.get(cluster); if (clusterObj == null) &#123; continue; &#125; result.addAll(clusterObj.allIPs()); &#125; return result;&#125; 可以看到，最终会调用Cluster.allIPs来获取服务实例列表： 1234567// Clusterpublic List&lt;Instance&gt; allIPs() &#123; List&lt;Instance&gt; allInstances = new ArrayList&lt;&gt;(); allInstances.addAll(persistentInstances); allInstances.addAll(ephemeralInstances); return allInstances;&#125; 客户端的心跳客户端在服务注册时有这一段代码： 12345678910// NacosNamingService.registerInstance@Overridepublic void registerInstance(String serviceName, String groupName, Instance instance) throws NacosException &#123; ..... if (instance.isEphemeral()) &#123; BeatInfo beatInfo = beatReactor.buildBeatInfo(groupedServiceName, instance); beatReactor.addBeatInfo(groupedServiceName, beatInfo); &#125; .....&#125; 会为临时实例添加一个心跳任务。 先看beatReactor.buildBeatInfo 12345678910111213// BeatReacto.buildBeatInfopublic BeatInfo buildBeatInfo(String groupedServiceName, Instance instance) &#123; BeatInfo beatInfo = new BeatInfo(); beatInfo.setServiceName(groupedServiceName); beatInfo.setIp(instance.getIp()); beatInfo.setPort(instance.getPort()); beatInfo.setCluster(instance.getClusterName()); beatInfo.setWeight(instance.getWeight()); beatInfo.setMetadata(instance.getMetadata()); beatInfo.setScheduled(false); beatInfo.setPeriod(instance.getInstanceHeartBeatInterval()); return beatInfo;&#125; 这里instance.getInstanceHeartBeatInterval()是获取心跳的执行间隔，默认是5s。 这参数是通过metadata来设置的，key=preserved.heart.beat.interval 比如： 123456789spring: application: name: mall-order cloud: nacos: discovery: server-addr: 192.168.2.148:8848 metadata: preserved.heart.beat.interval: 10 然后就执行beatReactor.addBeatInfo，往定时任务中添加BeatTask任务了。在BeatTask的run方法会执行下面这行代码 12// BeatTaskJsonNode result = serverProxy.sendBeat(beatInfo, BeatReactor.this.lightBeatEnabled); 这样代码就是发起一个PUT /nacos/v1/ns/instance/beat请求。 服务端处理心跳服务PUT /nacos/v1/ns/instance/beat是在naming模块的InstanceController提供的。 代码也很简单，处理的核心代码如下： 12345678910111213141516171819202122232425// InstanceController @PutMapping(&quot;/beat&quot;) @Secured(parser = NamingResourceParser.class, action = ActionTypes.WRITE) public ObjectNode beat(HttpServletRequest request) throws Exception &#123; ...... // 获取发送心跳的实例 Instance instance = serviceManager.getInstance(namespaceId, serviceName, clusterName, ip, port); // 如果注册表不存在，就进行服务重新注册 // 发生在之前注册过，但由于失联了，把服务实例下线了。然后客户端的网路突然好了，就重新注册了 if (instance == null) &#123; instance = new Instance(); ..... serviceManager.registerInstance(namespaceId, serviceName, instance); &#125; Service service = serviceManager.getService(namespaceId, serviceName); // 这里就是处理心跳的核心位置 // 创建一个ClientBeatProcessor任务，放到一个executor中 service.processClientBeat(clientBeat); ..... &#125; 心跳的处理就在service.processClientBeat(clientBeat);中完成： 123456public void processClientBeat(final RsInfo rsInfo) &#123; ClientBeatProcessor clientBeatProcessor = new ClientBeatProcessor(); clientBeatProcessor.setService(this); clientBeatProcessor.setRsInfo(rsInfo); HealthCheckReactor.scheduleNow(clientBeatProcessor);&#125; 就是创建了一个ClientBeatProcessor任务，做异步处理。 ClientBeatProcessor很简单，就不看了。 心跳检查在第一次服务注册时，会为注册的服务开启一个定时任务，看代码 12345678910111213141516InstanceController.register --&gt; ServiceManager.registerInstance --&gt; ServiceManager.createEmptyService --&gt; ..... Service.init // Servicepublic void init() &#123; // ClientBeatCheckTask clientBeatCheckTask = new ClientBeatCheckTask(this); // 开启一个定时任务 HealthCheckReactor.scheduleCheck(clientBeatCheckTask); for (Map.Entry&lt;String, Cluster&gt; entry : clusterMap.entrySet()) &#123; entry.getValue().setService(this); entry.getValue().init(); &#125;&#125; HealthCheckReactor.scheduleCheck(clientBeatCheckTask);就是开启一个ClientBeatCheckTask定时任务 ClientBeatCheckTask任务会每5s执行一次。而ClientBeatCheckTask任务就是执行心跳检查的】 一个服务一个定时任务 看其run方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@Overridepublic void run() &#123; try &#123; // 检查该服务是否需要该节点负责 if (!getDistroMapper().responsible(service.getName())) &#123; return; &#125; if (!getSwitchDomain().isHealthCheckEnabled()) &#123; return; &#125; // 获取该服务下的所有临时实例 List&lt;Instance&gt; instances = service.allIPs(true); // 这是第一次心跳检查 // first set health status of instances: for (Instance instance : instances) &#123; // instance.getLastBeat()为实例上一次的心跳时间 // System.currentTimeMillis() - instance.getLastBeat() 是否大于心跳的超时时间 if (System.currentTimeMillis() - instance.getLastBeat() &gt; instance.getInstanceHeartBeatTimeOut()) &#123; // 走到这就意味这服务可能失联了，这里要再次检查 if (!instance.isMarked()) &#123; // 如果是健康的，就需要修改健康状态为不健康了 if (instance.isHealthy()) &#123; instance.setHealthy(false); // 发布一个ServiceChangeEvent事件 // 这里先是使用了Spring的事件监听，然后会把这个事件发送到nacos客户端中 getPushService().serviceChanged(service); // 发布一个InstanceHeartbeatTimeoutEvent事件 ApplicationUtils.publishEvent(new InstanceHeartbeatTimeoutEvent(this, instance)); &#125; &#125; &#125; &#125; if (!getGlobalConfig().isExpireInstance()) &#123; return; &#125; // then remove obsolete instances: for (Instance instance : instances) &#123; if (instance.isMarked()) &#123; continue; &#125; // 检查是否超过需要移除实例的事件，默认是30s if (System.currentTimeMillis() - instance.getLastBeat() &gt; instance.getIpDeleteTimeout()) &#123; // delete instance // 这个方法就是调用本地的移除实例服务 // 如果是集群，在移除完本机数据后会主动的调用其他节点的http请求来完成数据同步 deleteIp(instance); &#125; &#125; &#125; catch (Exception e) &#123; Loggers.SRV_LOG.warn(&quot;Exception while processing client beat time out.&quot;, e); &#125;&#125; 就是检查失联时间是否超过15s，超过的话就将健康状态改为false，然后再检查是否超过30s，超过的话就需要把服务下面了。还有一点，这两个时间是能修改的，可以通过在客户端中配置 1234567891011121314spring: application: name: mall-order cloud: nacos: discovery: server-addr: 192.168.2.148:8848 metadata: # 客户端发心跳的间隔 preserved.heart.beat.interval: 10 # 心跳超时时间 preserved.heart.beat.timeout: 20 # 实例下线时间 preserved.ip.delete.timeout: 40 现在看下是怎么下线服务的。 超过下线时间后，会执行deleteIp(instance);方法，其核心代码如下： 123456String url = &quot;http://&quot; + IPUtil.localHostIP() + IPUtil.IP_PORT_SPLITER + EnvUtil.getPort() + EnvUtil.getContextPath() + UtilsAndCommons.NACOS_NAMING_CONTEXT + &quot;/instance?&quot; + request.toUrl();// delete instance asynchronously:HttpClient.asyncHttpDelete(url,.... 它发起了一个HTTP请求，不过是一个本地的调用，http api为DELETE /nacos/v1/ns/instance，所以看InstanceController代码： 12345678@CanDistro@DeleteMapping@Secured(parser = NamingResourceParser.class, action = ActionTypes.WRITE)public String deregister(HttpServletRequest request) throws Exception &#123; .... serviceManager.removeInstance(namespaceId, serviceName, instance.isEphemeral(), instance); return &quot;ok&quot;;&#125; 跟踪代码后 1234567891011121314151617// ServiceManager private void removeInstance(String namespaceId, String serviceName, boolean ephemeral, Service service, Instance... ips) throws NacosException &#123; String key = KeyBuilder.buildInstanceListKey(namespaceId, serviceName, ephemeral); // 这里把需要移除的实例移除掉，剩下的就是可用的 // 如果是集群的话，也会从其他其他机器中获取实例，然后移除 List&lt;Instance&gt; instanceList = substractIpAddresses(service, ephemeral, ips); Instances instances = new Instances(); instances.setInstanceList(instanceList); // 这里就是更新服务实例和通过 // 更新是通过单线程来更新的，看服务注册 consistencyService.put(key, instances);&#125; 服务实例推送当Server段的服务实例发生变更后，Nacos server会推送消息到Ncaos client，让客户端更新其内部缓存。在服务注册中，这个push操作是在DistroConsistencyServiceImpl.Notifier中通过调用RecordListener.onChange触发的，而代码在Service.updateIPs： 123// getPushService// getPushService = ApplicationUtils.getBean(PushService.class)getPushService().serviceChanged(this); 看PushService.serviceChanged: 12345678910// PushService.serviceChangedpublic void serviceChanged(Service service) &#123; // merge some change events to reduce the push frequency: if (futureMap .containsKey(UtilsAndCommons.assembleFullServiceName(service.getNamespaceId(), service.getName()))) &#123; return; &#125; // 这是Spring的事件监听 this.applicationContext.publishEvent(new ServiceChangeEvent(this, service));&#125; 实例移除同理，因为调用的是同一个方法。不过实例在移除前也会push一次实例变更消息，是在心跳检查中，第一次超过心跳超时时间时，会修改健康状态字段，修改完后就会调用PushService.serviceChanged，代码在ClientBeatCheckTask.run中 这个事件在PushService中处理的，看其onApplicationEvent方法，其核心逻辑如下 1234567// PushService// clients就是客户端列表，里面有ip和udp端口for (PushClient client : clients.values()) &#123; // 这里就是调用DatagramSocket.send(DatagramPacket) udpPush(ackEntry);&#125; 方法来发起一个udp请求，实际就是广播出去。使用的就是JDK的原生网络编程DatagramSocket发送的。 udp是不可靠的传输，可能存在丢包的情况，对于这种情况Naocs提供了重试机制来重新发送。看udpPush： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// PushServiceprivate static Receiver.AckEntry udpPush(Receiver.AckEntry ackEntry) &#123; if (ackEntry == null) &#123; Loggers.PUSH.error(&quot;[NACOS-PUSH] ackEntry is null.&quot;); return null; &#125; // 检查重试次数 // MAX_RETRY_TIMES = 1，也就是最多执行两次，第一次为第一次发送，第二次为重试 if (ackEntry.getRetryTimes() &gt; MAX_RETRY_TIMES) &#123; // 超过了两次，就意味着两次都发送失败了，那只能靠客户端的定时任务拉取新的服务实例了 ackMap.remove(ackEntry.key); udpSendTimeMap.remove(ackEntry.key); failedPush += 1; return ackEntry; &#125; try &#123; if (!ackMap.containsKey(ackEntry.key)) &#123; totalPush++; &#125; // 放入ackMap中，表示正在发送并等待ACK响应 ackMap.put(ackEntry.key, ackEntry); udpSendTimeMap.put(ackEntry.key, System.currentTimeMillis()); Loggers.PUSH.info(&quot;send udp packet: &quot; + ackEntry.key); // 发送 udpSocket.send(ackEntry.origin); ackEntry.increaseRetryTime(); // 加入一个Retransmitter任务，10s后运行 // 而Retransmitter很简单，检查下ackMap时候有对应的值，有就重新调用udpPush GlobalExecutor.scheduleRetransmitter(new Retransmitter(ackEntry), TimeUnit.NANOSECONDS.toMillis(ACK_TIMEOUT_NANOS), TimeUnit.MILLISECONDS); return ackEntry; &#125; catch (Exception e) &#123; // 这里错了，基本都是链接不通，所以就直接结束 ackMap.remove(ackEntry.key); udpSendTimeMap.remove(ackEntry.key); failedPush += 1; return null; &#125;&#125; 在发送完后，会在定时任务中添加一个Retransmitter任务，10S后运行。Retransmitter任务很简单，就是检查下ackMap时候有对应的值，有就重新调用udpPush 123456// PushService.Retransmitterpublic void run() &#123; if (ackMap.containsKey(ackEntry.key)) &#123; udpPush(ackEntry); &#125;&#125; Nacos在PushService类被加载后就会启动一个线程来接收udp的消息。 所以，Nacas server会获取客户端的ACK响应。如果接受到后就会移除PushService.ackMap中的记录，这样就不会重试了。 对于服务变更nacos server发生了服务变更，会通过udp推消息给nacos客户端，并提供重试机制（重试1次）。而且客户端也会通过定时任务定时的从服务端中拉取服务实例。 虽然udp是不可靠的协议，有可能丢包，但还有客户端的定时任务兜底。 nacos这种推送模式，对于zk那种通过tcp长链接来说会节省很多资源，而且推送速度也会更快，就算大量的节点更新也不会让Nacos出现太多的性能瓶颈，在Naocs中客户端如果接受到udp消息会返回一个ACK响应，如果Nacos server 10s内没有收到ACK响应，那会进行重发，超过一定的次数后（1次）就不会重发了，虽然UDP不能保证能真正的发送到订阅者，但是Nacos client还有定时轮训兜底，不需要担心数据不会更新的情况。 Nacos通过这种这两种手段，既保证了相对的实时性，又保证了数据更新了不会漏掉。 而ZK使用的长链接虽然能保证实时性，但对资源就相对要高些，而且如果客户端很多，那通知时的发送时间也相对要长。 不过Nacos这里的udp有个坑，就是不能指定端口！ 只能系统分配！这就不友好了 集群——AP架构集群架构图： 启动： nacos集群需要配置mysql存储，需要先创建一个数据，名字随便取，然后执行 distribution&#x2F;conf 目录下的 nacos-mysql.sql 脚本，然后修改 console\\src\\main\\resources 目录下的 application.properties 文件里的mysql配置，如下所示 12345678910### If use MySQL as datasource:spring.datasource.platform=mysql### Count of DB:db.num=1### Connect URL of DB:db.url.0=jdbc:mysql://127.0.0.1:3306/nacos?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useUnicode=true&amp;useSSL=false&amp;serverTimezone=UTCdb.user.0=rootdb.password.0=root 运行console模块里的 com.alibaba.nacos.Nacos.java，需要增加启动vm参数端口号和实例运行路径nacos.home(对应的目录需要自己提前创建好)，每台server的nacos.home目录里需要创建一个conf文件夹，里面放一个cluster.conf文件，文件里需要把所有集群机器ip和端口写入进去，见下图： AP架构的源码其实就围绕着几个类 ServiceManager ServerMemberManager DistroConsistencyServiceImpl Service NamingProxy DistroProtocol DistroMapper 很多重要的Runnable或者重要的接口调用都能在这些类中找到。一些重要的API主要集中在下面的Controller中 InstanceController 这个是客户主要调用的controller DistroController 和 NacosClusterController 这两个集群中节点间通信时调用的controller 集群模式下的客户端通过前面已经知道Nacos client都是通过NacosNamingService来完成对应的操作的，而NacosNamingService又是通过HTTP请求来完成与Nacos Server的通信的，而完成HTTP请求是通过其内部成员NamingProxy来完成的，而NamingProxy的核心方法就是reqApi，其代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445// NamingProxypublic String reqApi(String api, Map&lt;String, String&gt; params, Map&lt;String, String&gt; body, List&lt;String&gt; servers, String method) throws NacosException &#123; params.put(CommonParams.NAMESPACE_ID, getNamespaceId()); ..... if (StringUtils.isNotBlank(nacosDomain)) &#123; // 单机或者只给了一个nacos server地址的时候走到这里 // 这种情况下提供了重试机制，maxRetry默认值为3，通过maxRetry设置 for (int i = 0; i &lt; maxRetry; i++) &#123; try &#123; return callServer(api, params, body, nacosDomain, method); &#125; catch (NacosException e) &#123; exception = e; if (NAMING_LOGGER.isDebugEnabled()) &#123; NAMING_LOGGER.debug(&quot;request &#123;&#125; failed.&quot;, nacosDomain, e); &#125; &#125; &#125; &#125; else &#123; // 这种肯定是集群模式才会走了，提供多个nacos server的地址 // 随机选择一个nacos server Random random = new Random(System.currentTimeMillis()); int index = random.nextInt(servers.size()); for (int i = 0; i &lt; servers.size(); i++) &#123; String server = servers.get(index); try &#123; return callServer(api, params, body, server, method); &#125; catch (NacosException e) &#123; exception = e; if (NAMING_LOGGER.isDebugEnabled()) &#123; NAMING_LOGGER.debug(&quot;request &#123;&#125; failed.&quot;, server, e); &#125; &#125; // 如果失败了，就轮询剩下的机器，直到有一台成功为止 index = (index + 1) % servers.size(); &#125; &#125; ..... &#125; 该方法的servers是通过其内部的getServerList方法返回的，其代码如下： 1234567private List&lt;String&gt; getServerList() &#123; List&lt;String&gt; snapshot = serversFromEndpoint; if (!CollectionUtils.isEmpty(serverList)) &#123; snapshot = serverList; &#125; return snapshot;&#125; 这里serversFromEndpoint是通过配置endpoint提供的服务获取的，它会通过一个定时任务更新，而serverList配置serverAddr传入的地址。而nacosDomain是在serverAddr有值且只有一个地址的时候才会设置值。 回到reqApi可以看到，如果Naocs server是集群模式的，那最好在项目中配置多个Nacos server的地址，这样能避免单点问题。 nacos client 都是使用异步http请求的 集群中节点间的心跳 04-Server和Client动态获取nacos地址 在正确的部署集群后，集群间的节点会发互相发送心跳信息 在节点启动时，会创建ServerMemberManager对象，这个对象是用来管理集群中节点的信息的。 关于这个类可以看这里Server和Client动态获取nacos地址。 ServerMemberManager会在Servlet容器启动完成后开启一个定时任务： 1234567891011121314// ServerMemberManagerfinal MemberInfoReportTask infoReportTask = new MemberInfoReportTask();@Overridepublic void onApplicationEvent(WebServerInitializedEvent event) &#123; getSelf().setState(NodeState.UP); if (!EnvUtil.getStandaloneMode()) &#123; // 这里虽然只是执行一次，但是在MemberInfoReportTask的run方法执行完后，会重新把任务添加会executor中 GlobalExecutor.scheduleByCommon(this.infoReportTask, 5_000L); &#125; EnvUtil.setPort(event.getWebServer().getPort()); EnvUtil.setLocalAddress(this.localAddress); Loggers.CLUSTER.info(&quot;This node is ready to provide external services&quot;);&#125; 下面是这个任务的核心逻辑： 123456789101112131415161718192021222324252627// MemberInfoReportTaskprivate int cursor = 0;public void run() &#123; // 获取除了自己以外的节点 List&lt;Member&gt; members = ServerMemberManager.this.allMembersWithoutSelf(); // 遍历节点 this.cursor = (this.cursor + 1) % members.size(); Member target = members.get(cursor); String url = target.getAddress() + &quot;/nacos/v1/core/cluster/report&quot; // getSelf()返回代表自己的Member对象 asyncRestTemplate.post(url, getSelf()) // 异步处理 if (result.getCode() == 0 ｜｜ result.getCode() == 200) &#123; // 成功 MemberUtil.onSuccess(ServerMemberManager.this, target); &#125; else &#123; // 失败 MemberUtil.onFail(ServerMemberManager.this, target); &#125; finally &#123; // 重新进入exector，之后延迟2s GlobalExecutor.scheduleByCommon(this, 2_000L); &#125;&#125; 该任务只是发一个post请求到其他节点，告诉其他节点自己还活着，请求API如下： 12URL: /nacos/v1/core/cluster/reportMethod: POST 目标节点接收到请求后只是调用ServerMemberManager.update方法更新下信息而已。而请求方收到回应后就会执行MemberUtil.onSuccess，该方法如下： 12345678910// MemberUtilpublic static void onSuccess(final ServerMemberManager manager, final Member member) &#123; final NodeState old = member.getState(); manager.getMemberAddressInfos().add(member.getAddress()); member.setState(NodeState.UP); member.setFailAccessCnt(0); if (!Objects.equals(old, member.getState())) &#123; manager.notifyMemberChange(); &#125;&#125; 很简单，最终要就是把响应节点的Member的State修改为NodeState.UP。如果失败了执行MemberUtil.onFail，该方法如下： 12345678910111213141516171819202122public static void onFail(final ServerMemberManager manager, final Member member) &#123; // To avoid null pointer judgments, pass in one NONE_EXCEPTION onFail(manager, member, ExceptionUtil.NONE_EXCEPTION);&#125;public static void onFail(final ServerMemberManager manager, final Member member, Throwable ex) &#123; manager.getMemberAddressInfos().remove(member.getAddress()); final NodeState old = member.getState(); member.setState(NodeState.SUSPICIOUS); member.setFailAccessCnt(member.getFailAccessCnt() + 1); int maxFailAccessCnt = EnvUtil.getProperty(&quot;nacos.core.member.fail-access-cnt&quot;, Integer.class, 3); // 这里检查失败的次数是否超过3或者发生了Connection refused // 如果出现其中一种，就把对应节点DOWN掉（这是逻辑DOWN而已） if (member.getFailAccessCnt() &gt; maxFailAccessCnt || StringUtils .containsIgnoreCase(ex.getMessage(), TARGET_MEMBER_CONNECT_REFUSE_ERRMSG)) &#123; member.setState(NodeState.DOWN); &#125; if (!Objects.equals(old, member.getState())) &#123; manager.notifyMemberChange(); &#125;&#125; 如果心跳发生失败了，会把对应节点的State修改为NodeState.SUSPICIOUSP，然后检查是否失败了4次，或者发生了Connection refused，如果发生了其中一种，这样就会把对应节点的Member进行逻辑DOWN。 心跳任务任务仍然会往这些DOWN的Member发送心跳，只要这些机器能够和其正常通性，那么Member最终会变回UP。 接收方在的处理很简单，就是调用ServerMemberManager.update方法 节点加入集群同步数据 这里建议使用address-server模式，这种模式能动态的修改集群节点信息 当一台机器加入集群的时候，会向其他机器拉取全量的非持久化实例数据。 该过程会在启动阶段DistroProtocol对象创建时启动一个DistroLoadDataTask任务。在该任务下拉取全量数据的，看该任务的核心方法 1234567891011121314151617181920212223// DistroLoadDataTaskprivate boolean loadAllDataSnapshotFromRemote(String resourceType) &#123; // 这里获取到的是DistroHttpAgent DistroTransportAgent transportAgent = distroComponentHolder.findTransportAgent(resourceType); // 这里获取到的是DistroConsistencyServiceImpl DistroDataProcessor dataProcessor = distroComponentHolder.findDataProcessor(resourceType); for (Member each : memberManager.allMembersWithoutSelf()) &#123; try &#123; // DistroHttpAgent.getDatumSnapshot DistroData distroData = transportAgent.getDatumSnapshot(each.getAddress()); // DistroConsistencyServiceImpl.processSnapshot // 发起http请求 boolean result = dataProcessor.processSnapshot(distroData); if (result) &#123; return true; &#125; &#125; catch (Exception e) &#123; &#125; &#125; return false;&#125; 其中DistroHttpAgent.getDatumSnapshot方法中调用NamingProxy.getAllData方法来获取数据的，而该方法其实就是发起一个HTTP请求获取数据，其API如下： 12URL: /nacos/v1/ns/distro/datumsMethod: GET 最后调用DistroConsistencyServiceImpl.processSnapshot方法来完成注册表的初始化和注册表的更新 集群节点数据校验（同步）在 Distro 集群启动之后，各台机器会发送自己负责的Service的校验给其他机器，用来做数据校验，接收方发现收到的Service的校验和与本地的不同，就会发起一个HTTP请求，来拉取不同的Service的数据用来补全会更新自己的数据 下面从源码的角度看这个过程： 发送方在项目启动时会创建DistroProtocol，在这个对象创建时会启动一个DistroVerifyTask定时任务，该任务每5s运行一次 1234567891011DistroProtocol --&gt; DistroProtocol.startDistroTask --&gt; DistroProtocol.startVerifyTask private void startVerifyTask() &#123; GlobalExecutor.schedulePartitionDataTimedSync(new DistroVerifyTask(memberManager, distroComponentHolder), // 这个返回的值时5000，可以修改配置文件的 // nacos.core.protocol.distro.data.verify_interval_ms // 来指定值 distroConfig.getVerifyIntervalMillis());&#125; DistroVerifyTask的核心逻辑如下: 12345678// 伪代码List&lt;Member&gt; targetServer = serverMemberManager.allMembersWithoutSelf();for (Member member : targetServer) &#123; DistroData distroData = DistroDataStorageImpl.getVerifyData(); for (Member member : targetServer) &#123; NamingProxy.syncCheckSums(distroData.getContent(), targetServer); &#125;&#125; 又是到了NamingProxy，它的syncCheckSums方法就不看了，就是发起了一个HTTP异步请求而已，API如下： 1234URL: /nacos/v1/ns/distro/checksum?source=$&#123;发送方的地址&#125;Method: PUTbody Service 的 校验和JSON 接收方提供服务的是DistroController，跟踪代码： 123DistroProtocol.onVerify --&gt; DistroConsistencyServiceImpl.processVerifyData --&gt; DistroConsistencyServiceImpl.onReceiveChecksums 在DistroConsistencyServiceImpl.onReceiveChecksums中有这一段代码 123456789101112131415// DistroConsistencyServiceImpl#onReceiveChecksumsif (toUpdateKeys.isEmpty()) &#123; return;&#125;// 发现有Service的校验与接收的校验和不同，这里会发起一个HTTP请求来拉取数据，将本地的数据补全DistroHttpCombinedKey distroKey = new DistroHttpCombinedKey(KeyBuilder.INSTANCE_LIST_KEY_PREFIX, server);distroKey.getActualResourceTypes().addAll(toUpdateKeys);// 这里就是发送http请求，获取对应服务的数据的// distroProtocol为DistroProtocolDistroData remoteData = distroProtocol.queryFromRemote(distroKey);if (null != remoteData) &#123; // 这里就是进行注册数据的更新 processData(remoteData.getContent());&#125; 发现有Service的校验与接收的校验和不同，这里会发起一个HTTP请求来拉取数据，将本地的数据补全。发HTTP请求就在DistroProtocol.queryFromRemote，跟踪代码： 123DistroProtocol.queryFromRemote --&gt; DistroHttpAgent.getData --&gt; NamingProxy.getData 又是NamingProxy，它的getData方法就是调用HTTP而已，调用的HTTP API如下： 12URL: /nacos/v1/ns/distro/datumMethod: GET 而DistroConsistencyServiceImpl.processData就是完成注册表的更新的 写操作对于一个已经启动完成的 Distro 集群，在一次客户端发起写操作的流程中，当注册非持久化的实例的写请求到达某台 Nacos 服务器时，Distro 集群处理的流程图如下： 前置的 Filter 拦截请求，并根据请求中包含的 IP 和 port 信息计算其所属的 Distro 责任节点， 并将该请求转发到所属的 Distro 责任节点上。 责任节点上的 Controller 处理处理请求 Distro 协议定期执行 Sync 任务，将本机所负责的所有的实例信息同步到其他节点上。 通过上边的校验和方式 源码中就是使用@CanDistro来表示接口有路由转发能力的，比如服务注册接口： 1234567// InstanceController@CanDistro@PostMapping@Secured(parser = NamingResourceParser.class, action = ActionTypes.WRITE)public String register(HttpServletRequest request) throws Exception &#123; ....&#125; @Secured注解看着[02-Nacos Server中的Secured注解](.&#x2F;nacos中的一些细节&#x2F;02-Nacos Server中的Secured注解) 该注解是在DistroFilter处理的 其核心代码： 123456if (method.isAnnotationPresent(CanDistro.class) &amp;&amp; // distroMapper是DistroMapper !distroMapper.responsible(groupedServiceName)) &#123; String targetServer = distroMapper.mapSrv(groupedServiceName); ... &#125; DistroMapper.responsible就是用来检查一个服务是否需要负责的。该方法涉及到DistroMapper中的一个字段healthyList，该字段就是健康的节点，这个字段在在ServerMemberManager初始化时，初始化一次，然后就通过节点间的心跳来维护（ServerMemberManager 发布MembersChangeEvent事件到 DistroMapper），如果有修改就发起一个MembersChangeEvent事件，DistroMapper接收到该事件后就更新 读操作由于每台机器上都存放了全量数据，因此在每一次读操作中，Distro 机器会直接从本地拉取数据。 快速响应。 接口没有加@CanDistro 集群下健康实例状态同步在ServiceManager中会启动一个ServiceReporter任务 该任务会的核心代码如下： 123456789// ServiceManagerfinal Synchronizer synchronizer = new ServiceStatusSynchronizer();for (Member server : memberManager.allMembers()) &#123; if (server.getAddress().equals(NetUtils.localServer())) &#123; continue; &#125; // msg会包含校验和 synchronizer.send(server.getAddress(), msg);&#125; ServiceStatusSynchronizer.send就是发送一个HTTP请求，API如下： 12URL: /naocs/vi/ns/service/statusMethod: POST 接收方法在接收到请求后会进行校验和比对，不同的话会 1234// ServiceControllerif (!checksum.equals(service.getChecksum())) &#123; serviceManager.addUpdatedServiceToQueue(checksums.namespaceId, serviceName, serverIp, checksum);&#125; addUpdatedServiceToQueue添加消息到队列中，最后由线程处理，最终会执行到ServiceManager.updatedHealthStatus。该方法就是检查并更新实例的状态并通过udp通知客户端 集群下服务（客户端）的心跳检查一个客户端的心跳检查只会由一个节点负责，一旦客户端长时间失联，会在本地的对应实例移除并主动的往其他机器同步这次变化。而且还会由集群中检验和来完成被动的数据同步。 客户端的心跳检查是由ClientBeatCheckTask完的，在它的run方法开头有这一行代码： 123if (!getDistroMapper().responsible(service.getName())) &#123; return;&#125; 就是检查该节点是否服务处理该服务。 而主动往其他机器同步数据看下面 集群下服务信息变更后的数据同步以服务注册为例，代码最后到这里 123456789// Servicemanager.addInstancesynchronized (service) &#123; List&lt;Instance&gt; instanceList = addIpAddresses(service, ephemeral, ips); Instances instances = new Instances(); instances.setInstanceList(instanceList); consistencyService.put(key, instances);&#125; addIpAddresses方法的源码第一行如下： 1Datum datum = consistencyService.get(.....); 其他代码都很容易理解 在单机模式时，已经知道了临时实例的consistencyService的所有方法调用，最终都会调用DistroConsistencyServiceImpl的方法（临时实例）。所以这里的get源码如下： 12345// DistroConsistencyServiceImplDataStore dataStore;public Datum get(String key) throws NacosException &#123; return dataStore.get(key);&#125; 这段代码涉及到DataStore这个数据结构，这个结构用来存储什么的数据的呢？在单机中只是知道其内部有个Map。而通过集群源码的阅读，已经知道集群模式下的Distro模式是怎么工作的的。 在集群模式下的服务注册，第一步就是进行这个服务是否需要本机负责。所以能进入到服务注册的方法，就代表该服务是本机负责的，而服务注册的方法的最后会把该服务DataStore.put，然后异步的处理注册表的数据。所以DataStore的存储的数据已经很明了了，就是用来保存本机负责的服务数据的。所以上边的consistencyService.get就是用来获取本机的对应服务的实例列表的。所以在集群模式下，没个节点会维护两个重要的数据结构，一个是注册表，另一个就是DataStore。 consistencyService.put方法中在单机中已经讲过是怎么更新注册表的了。在该方法有行代码没有讲，就是: 12// DistroConsistencyServiceImpldistroProtocol.sync(new DistroKey(.....); 该方法就是本机节点主动的把变更推给集群中的其他机器的。 1234567891011// DistroProtocolpublic void sync(DistroKey distroKey, DataOperation action, long delay) &#123; for (Member each : memberManager.allMembersWithoutSelf()) &#123; DistroKey distroKeyWithTarget = new DistroKey(....); DistroDelayTask distroDelayTask = new DistroDelayTask(....); // DistroTaskEngineHolder distroTaskEngineHolder.getDelayTaskExecuteEngine() .addTask(distroKeyWithTarget, distroDelayTask); &#125;&#125; 这里的任务走向挺麻烦的 DistroDelayTask进入到DistroTaskEngineHolder.delayTaskExecuteEngine后，由一个线程执行了DistroDelayTaskProcessor的process，该方法内又创建了DistroSyncChangeTask，并交给DistroTaskEngineHolder.executeWorkersManager执行。经历了两次异步，第二次异步才同步 其实就是添加一个DistroDelayTask任务，该任务最终会创建DistroSyncChangeTask任务，在DistroExecuteTaskExecuteEngine中完成数据推送 DistroSyncChangeTask任务最终会通过NamingProxy.syncData来发起一个http请求，地址如下： 12URL: /nacos/v1/ns/distro/datumMethod: PUT DistroExecuteTaskExecuteEngine是一个任务执行器，是优化过的线程池。jdk的线程池只有一个队列，线程拿任务时会竞争的去拿，这是jdk的线程池的一个缺点。而DistroExecuteTaskExecuteEngine就优化了这个问题，其内部有多个队列，一个线程负责一个队列。 流程图","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"09-Nacos&Ribbon&Feign核心微服务架构图","slug":"springcloud/09-Nacos&Ribbon&Feign核心微服务架构图","date":"2021-11-27T12:00:16.000Z","updated":"2022-03-23T09:03:56.376Z","comments":true,"path":"blog/springcloud/09-Nacos&Ribbon&Feign核心微服务架构图/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/09-Nacos&Ribbon&Feign%E6%A0%B8%E5%BF%83%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E5%9B%BE/","excerpt":"","text":"微服务系统在启动时将自己注册到服务注册中心，同时外发布 Http 接口供其它系统调用(一般都是基于Spring MVC) 服务消费者基于 Feign 调用服务提供者对外发布的接口，先对调用的本地接口加上注解@FeignClient，Feign会针对加了该注解的接口生成动态代理，服务消费者针对 Feign 生成的动态代理去调用方法时，会在底层生成Http协议格式的请求，类似 &#x2F;stock&#x2F;deduct?productId&#x3D;100 Feign 最终会调用Ribbon从本地的Nacos注册表的缓存里根据服务名取出服务提供在机器的列表，然后进行负载均衡并选择一台机器出来，对选出来的机器IP和端口拼接之前生成的url请求，生成调用的Http接口地址 http://192.168.0.60:9000/stock/deduct?productId=100，最后基于HTTPClient调用请求","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"08-从Open Feign迁移到Dubbo","slug":"springcloud/08-从Open Feign迁移到Dubbo","date":"2021-11-27T12:00:15.000Z","updated":"2022-03-23T09:03:56.368Z","comments":true,"path":"blog/springcloud/08-从Open Feign迁移到Dubbo/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/08-%E4%BB%8EOpen%20Feign%E8%BF%81%E7%A7%BB%E5%88%B0Dubbo/","excerpt":"","text":"配置看上一章 Dubbo Spring Cloud 提供了方案，即 &#x3D;&#x3D;@DubboTransported&#x3D;&#x3D; 注解，支持在类，方法，属性上使用。能够帮助服务消费端的 Spring Cloud Open Feign 接口以及 @LoadBalanced RestTemplate Bean 底层走 Dubbo 调用（可切换 Dubbo 支持的协议），而服务提供方则只需在原有 @RestController 类上追加 Dubbo @Servce 注解（需要抽取接口）即可，换言之，在不调整 Feign 接口以及 RestTemplate URL 的前提下，实现无缝迁移。 修改服务提供者 12345678910111213141516171819202122feign的实现，启动类上添加@EnableFeignClients@Slf4j@RestController@RequestMapping(&quot;/user&quot;)public class UserServiceImpl implements UserService &#123; @Autowired private UserMapper userMapper; @Override @RequestMapping(&quot;/list&quot;) public List&lt;User&gt; list() &#123; log.info(&quot;查询user列表&quot;); return userMapper.list(); &#125; @Override @RequestMapping(&quot;/getById/&#123;id&#125;&quot;) public User getById(@PathVariable(&quot;id&quot;) Integer id) &#123; return userMapper.getById(id); &#125;&#125; 服务消费端引入依赖 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-dubbo&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; feign的实现，启动类上添加@EnableFeignClients 123456789@SpringBootApplication@EnableFeignClientspublic class SpringCloudDubboConsumerUserFeignApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringCloudDubboConsumerUserFeignApplication.class, args); &#125;&#125; feign接口添加 &#x3D;&#x3D;@DubboTransported&#x3D;&#x3D; 注解 123456789101112131415161718192021@FeignClient(value = &quot;spring-cloud-dubbo-provider-user-feign&quot;,path = &quot;/user&quot;)@DubboTransported(protocol = &quot;dubbo&quot;)public interface UserDubboFeignService &#123; @RequestMapping(&quot;/list&quot;) public List&lt;User&gt; list(); @RequestMapping(&quot;/getById/&#123;id&#125;&quot;) public User getById(@PathVariable(&quot;id&quot;) Integer id);&#125;@FeignClient(value = &quot;spring-cloud-dubbo-provider-user-feign&quot;,path = &quot;/user&quot;)public interface UserFeignService &#123; @RequestMapping(&quot;/list&quot;) public List&lt;User&gt; list(); @RequestMapping(&quot;/getById/&#123;id&#125;&quot;) public User getById(@PathVariable(&quot;id&quot;) Integer id);&#125; 调用对象添加 @DubboTransported 注解 12345678910111213141516171819202122232425262728293031323334353637383940@RestController@RequestMapping(&quot;/user&quot;)public class UserConstroller &#123; // dubbo的使用 @DubboReference private UserService userService; @RequestMapping(&quot;/info/&#123;id&#125;&quot;) public User info(@PathVariable(&quot;id&quot;) Integer id)&#123; return userService.getById(id); &#125; // fegin调用dubbo服务 @Autowired private UserDubboFeignService userDubboFeignService; @RequestMapping(&quot;/list2&quot;) public List&lt;User&gt; list2()&#123; return userDubboFeignService.list(); &#125; @Autowired private RestTemplate restTemplate; @Bean @LoadBalanced @DubboTransported public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125; @RequestMapping(&quot;/list3&quot;) public List&lt;User&gt; list3()&#123; String url = &quot;http://spring-cloud-dubbo-provider-user-feign/user/list&quot;; return restTemplate.getForObject(url, List.class); &#125;&#125;","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"07-Spring Cloud整合Dubbo","slug":"springcloud/07-Spring Cloud整合Dubbo","date":"2021-11-27T12:00:14.000Z","updated":"2022-03-23T09:03:56.367Z","comments":true,"path":"blog/springcloud/07-Spring Cloud整合Dubbo/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/07-Spring%20Cloud%E6%95%B4%E5%90%88Dubbo/","excerpt":"","text":"provider端配置12345678910&lt;!-- 不用的版本可能artifactId不同，这时需要去spring-cloud-alibaba-dependencies中看 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-dubbo&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; application.yml 12345678910111213141516171819202122232425262728dubbo: scan: # 指定 Dubbo 服务实现类的扫描基准包 base-packages: com.tuling.mall.user.service # application:# # 这里不用配置，使用spring.application.name就好了 # name: $&#123;spring.application.name&#125; protocol: # dubbo 协议 name: dubbo # dubbo 协议端口（ -1 表示自增端口，从 20880 开始） port: -1# registry:# #挂载到 Spring Cloud 注册中心 高版本可选（就是nacos）# #不建议这样写# address: spring-cloud://127.0.0.1:8848spring: application: name: spring-cloud-dubbo-provider-user main: # Spring Boot2.1及更高的版本需要设定 allow-bean-definition-overriding: true cloud: nacos: # Nacos 服务发现与注册配置 discovery: server-addr: 127.0.0.1:8848 服务实现类上配置@DubboService暴露服务 12345678910111213141516@DubboServicepublic class UserServiceImpl implements UserService &#123; @Autowired private UserMapper userMapper; @Override public List&lt;User&gt; list() &#123; return userMapper.list(); &#125; @Override public User getById(Integer id) &#123; return userMapper.getById(id); &#125;&#125; 启动后 重点 不要执行dubbo的注册中心，使用springcloud配置的注册中心。 consumer端配置123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 不用的版本可能artifactId不同，这时需要去spring-cloud-alibaba-dependencies中看 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-dubbo&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; application.yml 12345678910111213141516171819202122232425262728dubbo: cloud: # 指定需要订阅的服务提供方，默认值*，会订阅所有服务，不建议使用 subscribed-services: spring-cloud-dubbo-provider-user # application: # # 这里不用配置，使用spring.application.name就好了 # name: $&#123;spring.application.name&#125; protocol: # dubbo 协议 name: dubbo # dubbo 协议端口（ -1 表示自增端口，从 20880 开始） port: -1# registry:# #挂载到 Spring Cloud 注册中心 高版本可选（就是nacos）# #不建议这样写# address: spring-cloud://127.0.0.1:8848spring: application: name: spring-cloud-dubbo-consumer-user main: # Spring Boot2.1及更高的版本需要设定 allow-bean-definition-overriding: true cloud: nacos: # Nacos 服务发现与注册配置 discovery: server-addr: 127.0.0.1:8848 服务消费方通过@DubboReference引入服务 12345678910111213141516171819@RestController@RequestMapping(&quot;/user&quot;)public class UserConstroller &#123; @DubboReference private UserService userService; @RequestMapping(&quot;/info/&#123;id&#125;&quot;) public User info(@PathVariable(&quot;id&quot;) Integer id)&#123; return userService.getById(id); &#125; @RequestMapping(&quot;/list&quot;) public List&lt;User&gt; list()&#123; return userService.list(); &#125;&#125; 启动后 重点 不要执行dubbo的注册中心，使用springcloud配置的注册中心。 在dubbo中添加subscribed-services。订阅某个服务。","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"06-微服务调用组件Feign","slug":"springcloud/06-微服务调用组件Feign","date":"2021-11-27T12:00:13.000Z","updated":"2022-03-23T09:03:56.353Z","comments":true,"path":"blog/springcloud/06-微服务调用组件Feign/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/06-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%B0%83%E7%94%A8%E7%BB%84%E4%BB%B6Feign/","excerpt":"","text":"spring cloud 的openFeign还引入了ribbon的包 JAVA 项目中如何实现接口调用？ Httpclien HttpClient 是 Apache Jakarta Common 下的子项目，用来提供高效的、最新的、功能丰富的支持 Http 协议的客户端编程工具包，并且它支持 HTTP 协议最新版本和建议。HttpClient 相比传统 JDK 自带的 URLConnection，提升了易用性和灵活性，使客户端发送 HTTP 请求变得容易，提高了开发的效率 Okhttp 一个处理网络请求的开源项目，是安卓端最火的轻量级框架，由 Square 公司贡献，用于替代 HttpUrlConnection 和 Apache HttpClient。OkHttp 拥有简洁的 API、高效的性能，并支持多种协议（HTTP&#x2F;2 和 SPDY）。 HttpURLConnection HttpURLConnection 是 Java 的标准类，它继承自 URLConnection，可用于向指定网站发送 GET 请求、POST 请求。HttpURLConnection 使用比较复杂，不像 HttpClient 那样容易使用 RestTemplate RestTemplate 是 Spring 提供的用于访问 Rest 服务的客户端，RestTemplate 提供了多种便捷访问远程 HTTP 服务的方法，能够大大提高客户端的编写效率。 WebClient WebClient是从Spring WebFlux 5.0版本开始提供的一个非阻塞的基于响应式编程的进行Http请求的客户端工具。它的响应式编程的基于Reactor的。WebClient中提供了标准Http请求方式对应的get、post、put、delete等方法，可以用来发起相应的请求。 上面介绍的是最常见的几种调用接口的方法，我们下面要介绍的方法比上面的更简单、方便，它就是 Feign。 什么是FeignFeign是Netflix开发的声明式、模板化的HTTP客户端，其灵感来自Retrofit、JAXRS-2.0以及WebSocket。Feign可帮助我们更加便捷、优雅地调用HTTP API。 Feign支持多种注解，例如Feign自带的注解或者JAX-RS注解等 Feign与openfeign的区别Spring Cloud openfeign对Feign进行了增强，使其支持Spring MVC注解，另外还整合了Ribbon和Eureka，从而使得Feign的使用更加方便 Feignd的优势Feign可以做到使用 HTTP 请求远程服务时就像调用本地方法一样的体验，开发者完全感知不到这是远程方法，更感知不到这是个 HTTP 请求。它像 Dubbo 一样，consumer 直接调用接口方法调用 provider，而不需要通过常规的 Http Client 构造请求再解析返回数据。它解决了让开发者调用远程接口就跟调用本地方法一样，无需关注与远程的交互细节，更无需关注分布式环境开发。 Feign的设计架构 Feign的调用方式在上节中讲了 Ribbon，这是一个负载均衡框架，通过 123456789@Bean@LoadBalancedpublic RestTemplate restTemplate() &#123; return new RestTemplate();&#125;//调用方式String url = &quot;http://mall-order/order/findOrderByUserId/&quot;+id;R result = restTemplate.getForObject(url,R.class); 这种方式调用的。而Feign关注的点是服务的调用，也就是怎么调用一个服务（可以看成是一个HTTP客户端）。通过RestTemplate的调用方式虽然可以完成服务的调用。但这种方式并不优雅，涉及到了太多调用细节了。而Feign进行微服务调用方式如下（准确来讲，是openFeign）： 服务提供方配置： 12345678spring: application: name: mall-order #微服务名称 #配置nacos注册中心地址 cloud: nacos: discovery: server-addr: 127.0.0.1:8848 服务： 123456789101112131415161718@RestController@RequestMapping(&quot;/order&quot;)public class OrderController &#123; @Autowired private OrderService orderService; /** * 根据用户id查询订单信息 * @param userId * @return */ @RequestMapping(&quot;/findOrderByUserId/&#123;userId&#125;&quot;) public R findOrderByUserId(@PathVariable(&quot;userId&quot;) Integer userId) &#123; log.info(&quot;根据userId:&quot;+userId+&quot;查询订单信息&quot;); List&lt;OrderEntity&gt; orderEntities = orderService.listByUserId(userId); return R.ok().put(&quot;orders&quot;, orderEntities); &#125;&#125; 启动： 12345678910@SpringBootApplication// 完成服务的注册@EnableDiscoveryClientpublic class MallOrderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MallOrderApplication.class, args); &#125;&#125; 服务消费方服务定义： 12345678// 在项目中，该接口是作为一个共工包被服务提供者和消费者引入// 对于服务提供者，能知道提供什么服务和服务的uri（实现类的@RequestMapping必须和接口的一致）// 对于服务消费者，@FeignClient的注解就有用了，能在启动接口完成代理类的生成@FeignClient(value = &quot;mall-order&quot;,path = &quot;/order&quot;)public interface OrderFeignService &#123; @RequestMapping(&quot;/findOrderByUserId/&#123;userId&#125;&quot;) public R findOrderByUserId(@PathVariable(&quot;userId&quot;) Integer userId);&#125; 项目启动： 123456789@SpringBootApplication(exclude = &#123;DataSourceAutoConfiguration.class, DruidDataSourceAutoConfigure.class&#125;)@EnableFeignClientspublic class MallUserFeignDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MallUserFeignDemoApplication.class, args); &#125;&#125; 服务调用： 12345// 服务调用@AutowiredOrderFeignService orderFeignService;//feign调用R result = orderFeignService.findOrderByUserId(id); Feign可以做到使用 HTTP 请求远程服务时就像调用本地方法一样的体验，开发者完全感知不到这是远程方法，更感知不到这是个 HTTP 请求。它像 Dubbo 一样，consumer 直接调用接口方法调用 provider。 Feign单独使用1234567891011&lt;dependency&gt; &lt;groupId&gt;com.netflix.feign&lt;/groupId&gt; &lt;artifactId&gt;feign-core&lt;/artifactId&gt; &lt;version&gt;8.18.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 编码解码 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.netflix.feign&lt;/groupId&gt; &lt;artifactId&gt;feign-jackson&lt;/artifactId&gt; &lt;version&gt;8.18.0&lt;/version&gt;&lt;/dependency&gt; 编写接口 12345678// 这里定义的是服务接口// 服务提供方不一定要实现这个接口，可以只提供一个对应的http请求即可。参数也不一定要完全一样，只要能被SpringMVC的消息转换器能转换就可以了public interface RemoteService &#123; @Headers(&#123;&quot;Content-Type: application/json&quot;,&quot;Accept: application/json&quot;&#125;) @RequestLine(&quot;GET /order/findOrderByUserId/&#123;userId&#125;&quot;) public R findOrderByUserId(@Param(&quot;userId&quot;) Integer userId);&#125; 调用 12345678910111213141516171819202122232425262728293031public class FeignDemo &#123; public static void main(String[] args) &#123; // 就是获取动态代理对象而已 // 基于json // encoder指定对象编码方式，decoder指定对象解码方式 RemoteService service = Feign.builder() .logger(new Slf4jLogger(RemoteService.class)) .encoder(new JacksonEncoder()) .decoder(new JacksonDecoder()) // 支持Spring mvc注解 .contract(new SpringMvcContract()) // 支持Feign原生注解 //.contract(new Contract.Default()) // 超时时间，连接超时10s，读取超时60s .options(new Request.Options(10000, TimeUnit.MILLISECONDS, 60000, TimeUnit.MILLISECONDS, true)) // 设置重试策略 // 这里不是必须的，如果这里不设置，默认采用的就是这个， // 第一个参数是间隔，第二个就是最大的重试间隔，第三个就是最大的重试次数 // 每次的重试间隔的计算如下 // long interval = (long)((double)this.period * Math.pow(1.5D, (double)(this.attempt - 1))); // interval &gt; this.maxPeriod ? this.maxPeriod : interval; // 其实就是 interval = period * 1.5^(重试次数-1) // 在Spring Cloud项目中，默认是不开启重试的，可以根据需要是否使用Retryer.Default，还是自己重写 .retryer(new Retryer.Default(5000, 5000, 3)) // 指明接口和服务 .target(RemoteService.class, &quot;http://localhost:8020/&quot;); &#125;&#125; Spring Cloud Alibaba快速整合Feign——也就是openFeign引入依赖12345&lt;!-- openfeign 远程调用 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 编写调用接口+@FeignClient注解123456789// 定义服务接口// 同样的，服务提供方不一定要实现这个接口，可以只提供一个对应的http请求即可。// 这是SpringCloud的标准。上边使用了Feign的标准@FeignClient(value = &quot;mall-order&quot;,path = &quot;/order&quot;)public interface OrderFeignService &#123; @RequestMapping(&quot;/findOrderByUserId/&#123;userId&#125;&quot;) public R findOrderByUserId(@PathVariable(&quot;userId&quot;) Integer userId);&#125; 在@FeignClient注解中，有个url属性，这个属性的值就是指定具体的服务的url。指定有，底层调用就不走ribbon的均衡负载了。这种模式通常使用在测试时。 调用端在启动类上添加@EnableFeignClients注解1234567@SpringBootApplication@EnableFeignClientspublic class MallUserFeignDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MallUserFeignDemoApplication.class, args); &#125;&#125; @EnableFeignClients注解就是为了引入FeignClientsRegistrar类和指明服务接口类的包而已。 FeignClientsRegistrar这个类的为了扫描有@FeignClient注解的接口而已，最终会通过FeignClientFactoryBean这个FactoryBean来获取到真实的代理对象。 发起调用，像调用本地方式一样调用远程服务1234567891011121314@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @Autowired private OrderFeignService orderFeignService; @RequestMapping(value = &quot;/findOrderByUserId/&#123;id&#125;&quot;) public R findOrderByUserId(@PathVariable(&quot;id&quot;) Integer id) &#123; //feign调用 R result = orderFeignService.findOrderByUserId(id); return result; &#125;&#125; Spring Cloud Feign的自定义配置及使用 Feign 提供了很多的扩展机制，让用户可以更加灵活的使用。 feign的yaml配置可以看类FeignClientProperties. feign通过配置类的配置可以看类FeignClientsConfiguration来了解扩展点有哪些 配置模式openFeigin的配置方式和ribbon的配置模式很像 全局配置 1234567@Configuration // 全局配置public class FeignConfig &#123; @Bean public Logger.Level feignLoggerLevel() &#123; return Logger.Level.FULL; &#125;&#125; 全局配置的优先级是最高的 局部配置 配置类 12345678910111213public class FeignConfig &#123; @Bean public Logger.Level feignLoggerLevel() &#123; return Logger.Level.FULL; &#125;&#125;@FeignClient(value = &quot;mall-order&quot;,path = &quot;/order&quot;,configuration = FeignConfig.class)public interface OrderFeignService &#123; @RequestMapping(&quot;/findOrderByUserId/&#123;userId&#125;&quot;) R findOrderByUserId(@PathVariable(&quot;userId&quot;) Integer userId);&#125; FeignConfig不能被spring自动扫描到，否则会变成全局配置 yaml配置 123456feign: client: config: mall-order: #对应微服务 loggerLevel: FULL ...... 推荐使用这种方式 默认配置 @EnableFeignClients注解的basePackageClasses属性 1@EnableFeignClients(defaultConfiguration = &#123;FeignConfig.class&#125;) yaml配置 123456feign: client: config: default: # defaule是能修改的，通过feign.client.defaultConfig修改，它的默认值为default loggerLevel: FULL ...... 这3种配置方式又可以细分为注解和yaml文件，所以一共有5种配置方式。 在注解的配置方式下，全局配置的优先级是最高的，然后是局部配置，最后是默认配置。这是由Spring cloud项目在类NamedContextFactory中所定义的。这是Spring cloud的一个规范。而yaml配置就没有强制要求了，这要看要具体项目的定义。但有一点是确定的，就是在使用yaml配置时，某个服务的上下文已经根据注解方式的优先级完成了对起上下文的初始化，这时某些对象的配置已经确定了。 上面要求配置类都没有实现Order接口和没有@Order注解 在openFeign中yaml的配置与注解配置的优先级要分3中情况讨论，其代码在FeignClientFactoryBean类的configureFeign方法中。这里就只看默认情况下： 优先级最高的是局部配置的yaml方式 其次默认配置的yaml方式 最低的就是注解方式了。注解方式又按优先级高低，分为全局配置、局部配置最后是默认配置。 日志配置有时候我们遇到 Bug，比如接口调用失败、参数没收到等问题，或者想看看调用性能，就需要配置 Feign 的日志了，以此让 Feign 把请求信息输出来。 定义一个配置类，指定日志级别 12345678910111213// 注意： 此处配置@Configuration注解就会全局生效，如果想指定对应微服务生效，就不能配置// 加了@Configuration就是全局配置了public class FeignConfig &#123; /** * 日志级别 * * @return */ @Bean public Logger.Level feignLoggerLevel() &#123; return Logger.Level.FULL; &#125;&#125; 通过源码可以看到日志等级有 4 种，分别是： NONE【性能最佳，适用于生产】：不记录任何日志（默认值）。 BASIC【适用于生产环境追踪问题】：仅记录请求方法、URL、响应状态代码以及执行时间。 HEADERS：记录BASIC级别的基础上，记录请求和响应的header。 FULL【比较适用于开发及测试环境定位问题】：记录请求和响应的header、body和元数据。 局部配置 让调用的微服务生效，在@FeignClient 注解中指定使用的配置类 12345678910@FeignClient(value = &quot;mall-order&quot;,path = &quot;/order&quot;,configuration = FeignConfig.class)public interface OrderFeignService &#123; @RequestMapping(&quot;/findOrderByUserId/&#123;userId&#125;&quot;) R findOrderByUserId(@PathVariable(&quot;userId&quot;) Integer userId); @RequestMapping(value = &quot;/save&quot;,consumes = MediaType.APPLICATION_JSON_VALUE) R save(@RequestBody OrderVo order);&#125; 或者在yml中配置 12345feign: client: config: mall-order: #对应微服务 loggerLevel: FULL 建议使用配置文件的方式配置。 在yml配置文件添加执行 Client 的日志级别才能正常输出日志，格式是”logging.level.feign接口包路径&#x3D;debug” 1234logging: level: # 包名 com.tuling.mall.feigndemo.feign: debug 测试 契约配置 这里的契约指的是feign的Contract接口，该接口的作用就是指定Feign使用的是哪些注解。 Spring Cloud 在 Feign 的基础上做了扩展，可以让 Feign 支持 Spring MVC 的注解来调用。原生的 Feign 是不支持 Spring MVC 注解的，如果你想在 Spring Cloud 中使用原生的注解方式来定义客户端也是可以的，通过配置契约来改变这个配置，Spring Cloud 中默认的是 SpringMvcContract。 修改契约配置，支持Feign原生的注解 12345678/** * 修改契约配置，支持Feign原生的注解 * @return */@Beanpublic Contract feignContract() &#123; return new Contract.Default();&#125; 或者 通过yml配置契约 123456feign: client: config: mall-order: #对应微服务 loggerLevel: FULL contract: feign.Contract.Default #指定Feign原生注解契约配置 推荐使用配置文件 &#x3D;&#x3D;注意：修改契约配置后，OrderFeignService 不再支持springmvc的注解，需要使用Feign原生的注解&#x3D;&#x3D; OrderFeignService 中配置使用Feign原生的注解 12345@FeignClient(value = &quot;mall-order&quot;,path = &quot;/order&quot;)public interface OrderFeignService &#123; @RequestLine(&quot;GET /findOrderByUserId/&#123;userId&#125;&quot;) public R findOrderByUserId(@Param(&quot;userId&quot;) Integer userId);&#125; 既然都使用Spring Cloud了，就使用SpringMVC的就好了。 通过拦截器实现认证——feign.RequestInterceptor通常我们调用的接口都是有权限控制的，很多时候可能认证的值是通过参数去传递的，还有就是通过请求头去传递认证信息，完成接口鉴权。比如 Basic 认证方式： Feign 中我们可以直接配置 1234567@Configuration // 全局配置public class FeignConfig &#123; @Bean public BasicAuthRequestInterceptor basicAuthRequestInterceptor() &#123; return new BasicAuthRequestInterceptor(&quot;fox&quot;, &quot;123456&quot;); &#125;&#125; 扩展点： feign.RequestInterceptor上边展示了生成Basic认证信息的拦截器，这个拦截器实现了feign.RequestInterceptor接口。而这个就是feign架构图中的Interceptors部分。每次 feign 发起http调用之前，会去执行拦截器中的逻辑。 1234567public interface RequestInterceptor &#123; /** * Called for every request. Add data using methods on the supplied &#123;@link RequestTemplate&#125;. */ void apply(RequestTemplate template);&#125; 使用场景 统一添加 header 信息； 对 body 中的信息做修改或替换； 自定义拦截器实现认证逻辑12345678public class FeignAuthRequestInterceptor implements RequestInterceptor &#123; @Override public void apply(RequestTemplate template) &#123; // 业务逻辑 String access_token = UUID.randomUUID().toString(); template.header(&quot;Authorization&quot;, access_token); &#125;&#125; 配置 全局配置 123456789101112131415@Configuration // 全局配置public class FeignConfig &#123; @Bean public Logger.Level feignLoggerLevel() &#123; return Logger.Level.FULL; &#125; /** * 自定义拦截器 * @return */ @Bean public FeignAuthRequestInterceptor feignAuthRequestInterceptor()&#123; return new FeignAuthRequestInterceptor(); &#125;&#125; 在yml中配置 123456feign: client: config: mall-order: #对应微服务 或者使用default作为默认的配置 requestInterceptors[0]: #配置拦截器 com.tuling.mall.feigndemo.interceptor.FeignAuthRequestInterceptor 推荐这种模式 超时时间配置Java配置方式通过 Options 可以配置连接超时时间和读取超时时间，Options 的第一个参数是连接的超时时间（ms），默认值是 2s；第二个是请求处理的超时时间（ms），默认值是 5s。 1234567@Configurationpublic class FeignConfig &#123; @Bean public Request.Options options() &#123; return new Request.Options(5000, 10000); &#125;&#125; yml中配置12345678feign: client: config: mall-order: #对应微服务 # 连接超时时间，默认2s connectTimeout: 5000 # 请求处理超时时间，默认5s readTimeout: 10000 补充说明： Feign的的Client部分会使用到Ribbon的均衡负载，而Ribbon的均衡负载有超时时间，这个超时时间以Feign配置为准 客户端组件配置——feign.Client#executeFeign 中默认使用 JDK 原生的 URLConnection 发送 HTTP 请求，我们可以集成别的组件来替换掉 URLConnection，比如 Apache HttpClient，OkHttp。 Feign发起调用真正执行逻辑：feign.Client#execute （扩展点） 到底使用哪个实现，可以看FeignRibbonClientAutoConfiguration配置类（使用Ribbon做均衡负载的时候，如果是使用Spring cloud标准的loadBlance，那就看FeignLoadBalancerAutoConfiguration） 其中@Import的顺序决定Client对象的优先级，优先级最高的是HttpClient，其次是OkHttp，最低的是Default也就是URLConnection 配置Apache HttpClient1234567891011&lt;!-- Apache HttpClient --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.7&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt; &lt;artifactId&gt;feign-httpclient&lt;/artifactId&gt; &lt;version&gt;10.1.0&lt;/version&gt;&lt;/dependency&gt; 然后修改yml配置，将 Feign 的Apache HttpClient启用 ： 1234feign: #feign 使用 Apache HttpClient 可以忽略，默认开启 httpclient: enabled: true 关于配置可参考源码： org.springframework.cloud.openfeign.FeignAutoConfiguration 调用会进入feign.httpclient.ApacheHttpClient#execute 配置 OkHttp1234&lt;dependency&gt; &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt; &lt;artifactId&gt;feign-okhttp&lt;/artifactId&gt;&lt;/dependency&gt; 然后修改yml配置，将 Feign 的 HttpClient 禁用，启用 OkHttp，配置如下： 123456feign: #feign 使用 okhttp httpclient: enabled: false okhttp: enabled: true 关于配置可参考源码： org.springframework.cloud.openfeign.FeignAutoConfiguration http client的配置无论使用的是Apache HttpClient还是OkHttp，这两个客户端的其他配置都是用了FeignHttpClientProperties类作为配置接收类。到底哪些属性生效，看HttpClientFeignLoadBalancedConfiguration中的初始化过程。 GZIP 压缩配置开启压缩可以有效节约网络资源，提升接口性能，我们可以配置 GZIP 来压缩数据： 1234567891011feign: # 配置 GZIP 来压缩数据 compression: request: enabled: true # 配置压缩的类型 mime-types: text/xml,application/xml,application/json # 最小压缩值 min-request-size: 2048 response: enabled: true 注意：只有当 Feign 的 Http Client 不是 okhttp3 的时候，压缩才会生效，配置源码在FeignAcceptGzipEncodingAutoConfiguration 核心代码就是 @ConditionalOnMissingBean（type&#x3D;”okhttp3.OkHttpClient”），表示 Spring BeanFactory 中不包含指定的 bean 时条件匹配，也就是没有启用 okhttp3 时才会进行压缩配置。 编码器解码器配置——feign.Encoder &amp; feign.DecoderFeign 中提供了自定义的编码解码器设置，同时也提供了多种编码器的实现，比如 Gson、Jaxb、Jackson。我们可以用不同的编码解码器来处理数据的传输。如果你想传输 XML 格式的数据，可以自定义 XML 编码解码器来实现获取使用官方提供的 Jaxb。 扩展点：Encoder &amp; Decoder 123456public interface Encoder &#123; void encode(Object object, Type bodyType, RequestTemplate template) throws EncodeException;&#125;public interface Decoder &#123; Object decode(Response response, Type type) throws IOException, DecodeException, FeignException;&#125; Java配置方式12345678@Beanpublic Decoder decoder() &#123; return new JacksonDecoder();&#125;@Beanpublic Encoder encoder() &#123; return new JacksonEncoder();&#125; yml配置方式1234567feign: client: config: mall-order: #对应微服务 # 配置编解码器 encoder: feign.jackson.JacksonEncoder decoder: feign.jackson.JacksonDecoder 比较全的yaml配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657server: port: 8055spring: application: name: mall-user-feign-demo #微服务名称 #配置nacos注册中心地址 cloud: nacos: discovery: server-addr: 127.0.0.1:8848ribbon: eager-load: # 开启ribbon饥饿加载 enabled: true # 配置mall-user使用ribbon饥饿加载，多个使用逗号分隔 clients: mall-orderlogging: level: com.tuling.mall.feigndemo.feign: debugfeign: client: config: mall-order: #对应微服务 loggerLevel: FULL contract: feign.Contract.Default #指定Feign原生注解契约配置 requestInterceptors[0]: #配置拦截器 com.tuling.mall.feigndemo.interceptor.FeignAuthRequestInterceptor 连接超时时间，默认2s connectTimeout: 3000 请求处理超时时间，默认5s readTimeout: 10000 配置编解码器 encoder: feign.jackson.JacksonEncoder decoder: feign.jackson.JacksonDecoder #feign 使用 okhttp httpclient: enabled: false okhttp: enabled: true # 配置 GZIP 来压缩数据 compression: request: enabled: true # 配置压缩的类型 mime-types: text/xml,application/xml,application/json # 最小压缩值 min-request-size: 2048 response: enabled: true 关于配置可参考源码： 12org.springframework.cloud.openfeign.FeignAutoConfigurationorg.springframework.cloud.openfeign.FeignRibbonClientAutoConfiguration feign.clent配置的接受类为FeignClientProperties。 feign.clent.config的配置的接受类为FeignClientConfiguration 而feign的client如论使用那个http客户端，其配置的接受类都为FeignHttpClientProperties 而GZIP配置的接受类为FeignAcceptGzipEncodingAutoConfiguration","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"06-spring cloud中Feign的源码","slug":"springcloud/06-spring cloud中Feign的源码","date":"2021-11-27T12:00:12.000Z","updated":"2022-03-23T09:03:56.329Z","comments":true,"path":"blog/springcloud/06-spring cloud中Feign的源码/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/06-spring%20cloud%E4%B8%ADFeign%E7%9A%84%E6%BA%90%E7%A0%81/","excerpt":"","text":"启动 LoadBalancer使用的是Ribbon 要使用openFeign，需要引入 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 启动的关键就是@EnableFeignClients注解和FeignRibbonClientAutoConfiguration、FeignAutoConfiguration自动配置类。 自动配置类FeignRibbonClientAutoConfigurationFeignRibbonClientAutoConfiguration自动配置类的主要目的有两个： 初始化feign.Client对象LoadBalancerFeignClient 初始化CachingSpringLoadBalancerFactory对象。 FeignAutoConfigurationFeignAutoConfiguration自动配置类的主要目的看下面的代码： 12345678910111213141516171819202122232425262728@Configuration(proxyBeanMethods = false)@ConditionalOnClass(Feign.class)@EnableConfigurationProperties(&#123; FeignClientProperties.class, FeignHttpClientProperties.class &#125;)public class FeignAutoConfiguration &#123; @Autowired(required = false) private List&lt;FeignClientSpecification&gt; configurations = new ArrayList&lt;&gt;(); @Bean public FeignContext feignContext() &#123; FeignContext context = new FeignContext(); context.setConfigurations(this.configurations); return context; &#125; @Configuration(proxyBeanMethods = false) @ConditionalOnMissingClass(&quot;feign.hystrix.HystrixFeign&quot;) protected static class DefaultFeignTargeterConfiguration &#123; @Bean @ConditionalOnMissingBean public Targeter feignTargeter() &#123; return new DefaultTargeter(); &#125; &#125;&#125; 可以总结为3点： 收集FeignClientSpecification的实现，这个FeignClientSpecification类是NamedContextFactory.Specification的实现类。其作用就是保存名字为name的客户端的配置类。这个是NamedContextFactory提供的扩展。 初始化FeignContext对象，它是NamedContextFactory的子类。NamedContextFactory在前面就讲过了，按名字来管理上下文（按名字来隔离配置），到Spring cloud的服务注册和发现中就变成按服务进行配置的隔离和某些配置的共享。FeignContext类的功能其实很明了了，就是管理每个服务的客户端的配置（或者是ApplicationContext）。这个类也是openFeign的核心。而且从FeignContext的初始化中可以知，所有服务客户端的公用配置类为FeignClientsConfiguration. 初始化Targeter对象，这个对象所关注的核心功能生成代理对象。 @EnableFeignClients注解的处理 这个注解就是引入FeignClientsRegistrar类的，下面是FeignClientsRegistrar类的类图： 可以看到，它实现了ImportBeanDefinitionRegistrar接口，所以registerBeanDefinitions方法就是该类的核心 123456@Overridepublic void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) &#123; registerDefaultConfiguration(metadata, registry); registerFeignClients(metadata, registry);&#125; registerDefaultConfiguration源码： 12345678910111213141516171819202122232425262728private void registerDefaultConfiguration(AnnotationMetadata metadata, BeanDefinitionRegistry registry) &#123; Map&lt;String, Object&gt; defaultAttrs = metadata .getAnnotationAttributes(EnableFeignClients.class.getName(), true); if (defaultAttrs != null &amp;&amp; defaultAttrs.containsKey(&quot;defaultConfiguration&quot;)) &#123; String name; if (metadata.hasEnclosingClass()) &#123; name = &quot;default.&quot; + metadata.getEnclosingClassName(); &#125; else &#123; name = &quot;default.&quot; + metadata.getClassName(); &#125; registerClientConfiguration(registry, name, defaultAttrs.get(&quot;defaultConfiguration&quot;)); &#125;&#125;private void registerClientConfiguration(BeanDefinitionRegistry registry, Object name, Object configuration) &#123; BeanDefinitionBuilder builder = BeanDefinitionBuilder .genericBeanDefinition(FeignClientSpecification.class); builder.addConstructorArgValue(name); builder.addConstructorArgValue(configuration); registry.registerBeanDefinition( name + &quot;.&quot; + FeignClientSpecification.class.getSimpleName(), builder.getBeanDefinition());&#125; 就是为了处理EnableFeignClients注解的defaultConfiguration属性，把该属性的值注册成FeignClientSpecification的BeanDefinition，对应的beanName为default.com.tuling.mall.feigndemo.MallUserFeignDemoApplication.FeignClientSpecification。 com.tuling.mall.feigndemo.MallUserFeignDemoApplication是有@EnableFeignClients注解的类的全限定名 FeignClientSpecification是用来保存某个服务的配置类的。这里注册的FeignClientSpecification在自动配置类FeignAutoConfiguration中会收集起来，并把它们保存到FeignContext对象中，而且由于beanName是以default.开头的，所以这里注册的FeignClientSpecification会作为公用配置类，让每一个服务都共享，这个规则是由NamedContextFactory所定义的 NamedContextFactory的createContext方法就定义了一个配置的优先级。不过这个生效的前提就是配置类没有排序相关的注解或接口。 在没有排序等接口时，在解析这些类时是按register的顺序解析的。 registerFeignClients 在该方法中首先创建了一个ClassPathScanningCandidateComponentProvider对象 12345678910111213ClassPathScanningCandidateComponentProvider scanner = new ClassPathScanningCandidateComponentProvider(false, this.environment) &#123; @Override protected boolean isCandidateComponent( AnnotatedBeanDefinition beanDefinition) &#123; boolean isCandidate = false; if (beanDefinition.getMetadata().isIndependent()) &#123; if (!beanDefinition.getMetadata().isAnnotation()) &#123; isCandidate = true; &#125; &#125; return isCandidate; &#125;&#125;; 这个对象的作用就是进行包扫描的，接着 12AnnotationTypeFilter annotationTypeFilter = new AnnotationTypeFilter(FeignClient.class);scanner.addIncludeFilter(annotationTypeFilter); 就是限定了能被扫描到的类必须有FeignClient注解。 剩下的代码完成了两个功能 获取需要扫描的包 完成对应的BeanDefinition的注册 先看获取需要扫描的包，这里分两种情况： @EnableFeignClients的clients属性为空 这种情况下走到这段代码 123456789101112131415161718final Set&lt;String&gt; clientClasses = new HashSet&lt;&gt;();basePackages = new HashSet&lt;&gt;();for (Class&lt;?&gt; clazz : clients) &#123; basePackages.add(ClassUtils.getPackageName(clazz)); clientClasses.add(clazz.getCanonicalName());&#125;// 这里多了个限制，就是扫描到的类必须是@EnableFeignClients的`clients`属性所指定的类AbstractClassTestingTypeFilter filter = new AbstractClassTestingTypeFilter() &#123; @Override protected boolean match(ClassMetadata metadata) &#123; String cleaned = metadata.getClassName().replaceAll(&quot;\\\\$&quot;, &quot;.&quot;); return clientClasses.contains(cleaned); &#125;&#125;;// annotationTypeFilter在前边创建了// 而AllTypeFilter的作用其实就是通知执行filter和annotationTypeFilter的match方法scanner.addIncludeFilter( new AllTypeFilter(Arrays.asList(filter, annotationTypeFilter))); 这里段代码的作用其实就是加了限制，就是扫描到的类必须是@EnableFeignClients的clients属性所指定的类。 @EnableFeignClients的clients属性不为空 12345678910111213141516171819202122232425262728scanner.addIncludeFilter(annotationTypeFilter);basePackages = getBasePackages(metadata);protected Set&lt;String&gt; getBasePackages(AnnotationMetadata importingClassMetadata) &#123; Map&lt;String, Object&gt; attributes = importingClassMetadata .getAnnotationAttributes(EnableFeignClients.class.getCanonicalName()); Set&lt;String&gt; basePackages = new HashSet&lt;&gt;(); for (String pkg : (String[]) attributes.get(&quot;value&quot;)) &#123; if (StringUtils.hasText(pkg)) &#123; basePackages.add(pkg); &#125; &#125; for (String pkg : (String[]) attributes.get(&quot;basePackages&quot;)) &#123; if (StringUtils.hasText(pkg)) &#123; basePackages.add(pkg); &#125; &#125; for (Class&lt;?&gt; clazz : (Class[]) attributes.get(&quot;basePackageClasses&quot;)) &#123; basePackages.add(ClassUtils.getPackageName(clazz)); &#125; if (basePackages.isEmpty()) &#123; basePackages.add( ClassUtils.getPackageName(importingClassMetadata.getClassName())); &#125; return basePackages;&#125; 处理@EnableFeignClients剩下的属性，value和basePackages作用都是一样的，传入的值也是包，而basePackageClasses属性传入的Class对象数组，在处理该属性时，只是获取了该Class对象的所在的包，不管其Class对象。 最后，如果没有这些属性，会把@EnableFeignClients注解的类所在的包作为扫描包 获取了需要扫描的包basePackages后，就是进行遍历了，把有FeignClient注解的类扫描出来后会执行下面的代码来 1234Map&lt;String, Object&gt; attributes = annotationMetadata.getAnnotationAttributes(FeignClient.class.getCanonicalName());String name = getClientName(attributes);registerClientConfiguration(registry, name, attributes.get(&quot;configuration&quot;)); 处理FeignClient注解的name和configuration属性，往Spring上下文注解对应服务的FeignClientSpecification。 接着就是要注册一个FeignClientFactoryBean，一些属性从FeignClient注解中获取。而这个FeignClientFactoryBean是FactoryBean接口的实现类。 12345678910111213141516171819202122232425262728293031323334353637383940private void registerFeignClient(BeanDefinitionRegistry registry, AnnotationMetadata annotationMetadata, Map&lt;String, Object&gt; attributes) &#123; String className = annotationMetadata.getClassName(); BeanDefinitionBuilder definition = BeanDefinitionBuilder .genericBeanDefinition(FeignClientFactoryBean.class); validate(attributes); definition.addPropertyValue(&quot;url&quot;, getUrl(attributes)); definition.addPropertyValue(&quot;path&quot;, getPath(attributes)); String name = getName(attributes); definition.addPropertyValue(&quot;name&quot;, name); // 上边的name是服务的名字，这个contextId属性是服务对应的上下的id，如果在FeignClient没有设置contextId属性，那该值就是name // 如果contextId和name属性都没有，那么在bean的初始化阶段会报错 String contextId = getContextId(attributes); definition.addPropertyValue(&quot;contextId&quot;, contextId); definition.addPropertyValue(&quot;type&quot;, className); definition.addPropertyValue(&quot;decode404&quot;, attributes.get(&quot;decode404&quot;)); definition.addPropertyValue(&quot;fallback&quot;, attributes.get(&quot;fallback&quot;)); definition.addPropertyValue(&quot;fallbackFactory&quot;, attributes.get(&quot;fallbackFactory&quot;)); definition.setAutowireMode(AbstractBeanDefinition.AUTOWIRE_BY_TYPE); String alias = contextId + &quot;FeignClient&quot;; AbstractBeanDefinition beanDefinition = definition.getBeanDefinition(); // 这里设置一个属性，用来确定FactoryBean 的 bean 类型 beanDefinition.setAttribute(FactoryBean.OBJECT_TYPE_ATTRIBUTE, className); // has a default, won&#x27;t be null boolean primary = (Boolean) attributes.get(&quot;primary&quot;); beanDefinition.setPrimary(primary); // 别名 String qualifier = getQualifier(attributes); if (StringUtils.hasText(qualifier)) &#123; alias = qualifier; &#125; BeanDefinitionHolder holder = new BeanDefinitionHolder(beanDefinition, className, new String[] &#123; alias &#125;); BeanDefinitionReaderUtils.registerBeanDefinition(holder, registry);&#125; 服务代理类的生成——FeignClientFactoryBean通过上边的准备，服务环境的管理类FeignContext已经初始化完成，包含了每一个服务客户端所需的对象。 最终服务客户端的代理类是由FeignClientFactoryBean来完成的，他是一个FactoryBean 12345678910111213141516171819202122232425262728293031323334353637383940public Object getObject() throws Exception &#123; return getTarget();&#125;// 获取动态代理类&lt;T&gt; T getTarget() &#123; FeignContext context = applicationContext.getBean(FeignContext.class); Feign.Builder builder = feign(context); if (!StringUtils.hasText(url)) &#123; // 这是需要均衡负载的情况 if (!name.startsWith(&quot;http&quot;)) &#123; url = &quot;http://&quot; + name; &#125; else &#123; url = name; &#125; url += cleanPath(); return (T) loadBalance(builder, context, new HardCodedTarget&lt;&gt;(type, name, url)); &#125; // 这是不需要均衡负载的情况 if (StringUtils.hasText(url) &amp;&amp; !url.startsWith(&quot;http&quot;)) &#123; url = &quot;http://&quot; + url; &#125; String url = this.url + cleanPath(); Client client = getOptional(context, Client.class); if (client != null) &#123; if (client instanceof LoadBalancerFeignClient) &#123; client = ((LoadBalancerFeignClient) client).getDelegate(); &#125; if (client instanceof FeignBlockingLoadBalancerClient) &#123; client = ((FeignBlockingLoadBalancerClient) client).getDelegate(); &#125; builder.client(client); &#125; Targeter targeter = get(context, Targeter.class); return (T) targeter.target(this, builder, context, new HardCodedTarget&lt;&gt;(type, name, url));&#125; 这里只考虑需要均衡负载的情况 上边的代码就是做了下面的工作 12345678910111213141516171819202122RemoteService service = Feign.builder() .logger(new Slf4jLogger(RemoteService.class)) .encoder(new JacksonEncoder()) .decoder(new JacksonDecoder()) // 支持Spring mvc注解 .contract(new SpringMvcContract()) // 支持Feign原生注解 //.contract(new Contract.Default()) // 超时时间，连接超时10s，读取超时60s .options(new Request.Options(10000, TimeUnit.MILLISECONDS, 60000, TimeUnit.MILLISECONDS, true)) // 设置重试策略 // 这里不是必须的，如果这里不设置，默认采用的就是这个， // 第一个参数是间隔，第二个就是最大的重试间隔，第三个就是最大的重试次数 // 每次的重试间隔的计算如下 // long interval = (long)((double)this.period * Math.pow(1.5D, (double)(this.attempt - 1))); // interval &gt; this.maxPeriod ? this.maxPeriod : interval; // 其实就是 interval = period * 1.5^(重试次数-1) // 在Spring Cloud项目中，默认是不开启重试的，可以根据需要是否使用Retryer.Default，还是自己重写 .retryer(new Retryer.Default(5000, 5000, 3)) // 指明接口和服务 .target(RemoteService.class, &quot;http://localhost:8020/&quot;); 除了上边的代码，有2个方法和一组默认值需要说名下： configureFeign，该方法就是根据配置来修改Feign.Builder对象。 这里定义的yaml配置和注解配置的优先级，看下面的注释就好了： 1234567891011121314151617181920212223242526272829303132333435protected void configureFeign(FeignContext context, Feign.Builder builder) &#123; // 获取feign的yaml配置接收对象 FeignClientProperties properties = applicationContext.getBean(FeignClientProperties.class); // 获取FeignClientConfigurer，该接口的作用就是是否启用feign的yaml配置的功能，默认是启动的 FeignClientConfigurer feignClientConfigurer = getOptional(context, FeignClientConfigurer.class); setInheritParentContext(feignClientConfigurer.inheritParentConfiguration()); if (properties != null &amp;&amp; inheritParentContext) &#123; if (properties.isDefaultToProperties()) &#123; // 如果配置文件有feign.client.defaultToPropertie=true, 就会走到这里，默认是进入到该代码段的 // 下面配置顺序是：自己服务的spring上下--feign.client.default--feign.client.服务名 configureUsingConfiguration(context, builder); configureUsingProperties( properties.getConfig().get(properties.getDefaultConfig()), builder); configureUsingProperties(properties.getConfig().get(contextId), builder); &#125; else &#123; // 如果配置文件有feign.client.defaultToPropertie=false, 就会走到这这里 // 下面配置顺序是feign.client.default--feign.client.服务名--自己服务的spring上下 configureUsingProperties( properties.getConfig().get(properties.getDefaultConfig()), builder); configureUsingProperties(properties.getConfig().get(contextId), builder); configureUsingConfiguration(context, builder); &#125; &#125; else &#123; // 走到这里，意味着初始化了一个FeignClientConfigurer对象，该对象返回了false // 表示不使用配置文件的配置，只使用spring上下的 configureUsingConfiguration(context, builder); &#125;&#125; 上边的配置feign.client.default中的default是能修改的，通过feign.client.defaultConfig修改，看FeignClientProperties就明白了 loadBalance，该方法添加支持均衡负载的feign.Client对象，并得到对应服务客户端的代理对象 123456789101112protected &lt;T&gt; T loadBalance(Feign.Builder builder, FeignContext context, HardCodedTarget&lt;T&gt; target) &#123; Client client = getOptional(context, Client.class); if (client != null) &#123; builder.client(client); Targeter targeter = get(context, Targeter.class); return targeter.target(this, builder, context, target); &#125; throw new IllegalStateException( &quot;No Feign Client for loadBalancing defined. Did you forget to include spring-cloud-starter-netflix-ribbon?&quot;);&#125; 代码很简单，这里要说名的是默认情况下feign.Client就是LoadBalancerFeignClient。这个类就是处理负载均衡和HTTP请求的核心，后面讲 Targeter接口其实目的很简单，就是获取代理对象，使用的是JDK的动态代理。 feign调用时的connectTimeout和readTimeout 这两个值在FeignClientFactoryBean初始化时给了一组默认值 123456private int readTimeoutMillis = new Request.Options().readTimeoutMillis();private int connectTimeoutMillis = new Request.Options().connectTimeoutMillis();public Options() &#123; this(10L, TimeUnit.SECONDS, 60L, TimeUnit.SECONDS, true);&#125; 默认情况下是connectTimeout&#x3D;10s，readTimeout&#x3D;60s。 可以往Spring注入一个Request.Options类来完成全局配置，或者通过feign.client.default配置，还可以通过feign.client.服务名来对服务单独配置 服务调用从上边知道，服务代理类是通过JDK的动态代理生成的，通过Feign.Builder.target就很容易的知道代理对象的InvocationHandler是哪一类。看Feign.Builder.target： 12345678910111213141516171819202122public &lt;T&gt; T target(Target&lt;T&gt; target) &#123; return this.build().newInstance(target);&#125;public Feign build() &#123; Client client = (Client)Capability.enrich(this.client, this.capabilities); Retryer retryer = (Retryer)Capability.enrich(this.retryer, this.capabilities); ..... Options options = (Options)Capability.enrich(this.options, this.capabilities); // 这段代码的作用就是为了执行Capability对象中的 // Encoder enrich(Encoder encoder) &#123; // return encoder; // &#125; // 方法。其他都差不多，看下Capability的定义就能明白了 Encoder encoder = (Encoder)Capability.enrich(this.encoder, this.capabilities); Decoder decoder = (Decoder)Capability.enrich(this.decoder, this.capabilities); // this.invocationHandlerFactory = new feign.InvocationHandlerFactory.Default(); InvocationHandlerFactory invocationHandlerFactory = (InvocationHandlerFactory)Capability.enrich(this.invocationHandlerFactory, this.capabilities); ..... return new ReflectiveFeign(handlersByName, invocationHandlerFactory, queryMapEncoder);&#125; 创建代理类的代码在ReflectiveFeign#newInstance中，这里不看了源码了，说下流程 把接口的default方法封装成DefaultMethodHandler，把接口的抽象方法封装成SynchronousMethodHandler。并把这些保存到集合: 1Map&lt;Method, MethodHandler&gt; methodToHandler = new LinkedHashMap(); 中，最后把这个Map作为参数传入ReflectiveFeign.FeignInvocationHandler这个InvocationHandler对象中。这个InvocationHandler代码也很简单，整个方法调用的逻辑都在对应的MethodHandler中。DefaultMethodHandler就是直接调用对象的方法，没什么要讲的，而SynchronousMethodHandler就是服务调用的核心。调用流程如下： 而Client又是服务调用的重中之重，通过前面知道了Client的对象为LoadBalancerFeignClient。下面看LoadBalancerFeignClient feign.Client——LoadBalancerFeignClient下面是LoadBalancerFeignClient对象的构造方法 1234567public LoadBalancerFeignClient(Client delegate, CachingSpringLoadBalancerFactory lbClientFactory, SpringClientFactory clientFactory) &#123; this.delegate = delegate; this.lbClientFactory = lbClientFactory; this.clientFactory = clientFactory;&#125; LoadBalancerFeignClient对象就是Feign的服务代理对象处理HTTP请求的Client对象，从它的构造方法可以看出，它其实就是装饰类，最终处理HTTP请求的就是构造时传入的Client对象，类型通过配置决定，默认是Client.Default，它是通过HttpURLConnection完成http请求的 LoadBalancerFeignClient作为装饰类，提供了使用Ribbon实现负载均衡的功能。 123456789101112131415161718192021222324// LoadBalancerFeignClient@Overridepublic Response execute(Request request, Request.Options options) throws IOException &#123; try &#123; URI asUri = URI.create(request.url()); String clientName = asUri.getHost(); URI uriWithoutHost = cleanUrl(request.url(), clientName); FeignLoadBalancer.RibbonRequest ribbonRequest = new FeignLoadBalancer.RibbonRequest( this.delegate, request, uriWithoutHost); // 这里是将Feign配置 IClientConfig requestConfig = getClientConfig(options, clientName); // 这里就实现了负载均衡和处理HTTP return lbClient(clientName) .execueWithLoadBalancer(ribbonRequest, requestConfig).toResponse(); &#125; catch (ClientException e) &#123; IOException io = findIOException(e); if (io != null) &#123; throw io; &#125; throw new RuntimeException(e); &#125;&#125; 而处理HTTP请求的Client，可以通过配置来选择和配置，详细配置看配置类FeignRibbonClientAutoConfiguration和FeignAutoConfiguration即可. 1lbClient(clientName).executeWithLoadBalancer(ribbonRequest, requestConfig).toResponse(); 其中lbClient方法如下 123private FeignLoadBalancer lbClient(String clientName) &#123; return this.lbClientFactory.create(clientName);&#125; lbClientFactory是CachingSpringLoadBalancerFactory对象，它只做了一件事，就是创建并缓存FeignLoadBalancer对象。CachingSpringLoadBalancerFactory对象会作为参数，传入LoadBalancerFeignClient对象中。上边的LoadBalancerFeignClient#execute方法的最后调用FeignLoadBalancer对象。而FeignLoadBalancer#executeWithLoadBalancer方法的作用就是完成均衡负载和HTTP请求的处理。 FeignLoadBalancerFeignLoadBalancer是符合Ribbon的规范 FeignLoadBalancer对象的类图如下： 其中IClient是ribbon的接口，其作用只是关注的是HTTP请求的处理。而AbstractLoadBalancerAwareClient抽象类作为IClient的抽象实现类，添加了均衡负载的功能。AbstractLoadBalancerAwareClient类中的executeWithLoadBalancer方法完成两件事 先做均衡负载 根据第一步得到的服务实例，完成HTTP请求的处理，也就是调用IClient实现类的对象的execute方法 其源码如下： 123456789101112131415161718192021222324252627282930public T executeWithLoadBalancer(final S request, final IClientConfig requestConfig) throws ClientException &#123; LoadBalancerCommand&lt;T&gt; command = buildLoadBalancerCommand(request, requestConfig); try &#123; return command.submit( new ServerOperation&lt;T&gt;() &#123; @Override public Observable&lt;T&gt; call(Server server) &#123; URI finalUri = reconstructURIWithServer(server, request.getUri()); S requestForServer = (S) request.replaceUri(finalUri); try &#123; return Observable.just(AbstractLoadBalancerAwareClient.this.execute(requestForServer, requestConfig)); &#125; catch (Exception e) &#123; return Observable.error(e); &#125; &#125; &#125;) .toBlocking() .single(); &#125; catch (Exception e) &#123; Throwable t = e.getCause(); if (t instanceof ClientException) &#123; throw (ClientException) t; &#125; else &#123; throw new ClientException(e); &#125; &#125; &#125; 而FeignLoadBalancer继承了AbstractLoadBalancerAwareClient类，均衡负载的代码已经从AbstractLoadBalancerAwareClient继承了，剩下的只是如何完成HTTP请求的处理，所以看它的execute方法 123456789101112131415@Overridepublic RibbonResponse execute(RibbonRequest request, IClientConfig configOverride) throws IOException &#123; Request.Options options; if (configOverride != null) &#123; RibbonProperties override = RibbonProperties.from(configOverride); options = new Request.Options(override.connectTimeout(this.connectTimeout), override.readTimeout(this.readTimeout)); &#125; else &#123; options = new Request.Options(this.connectTimeout, this.readTimeout); &#125; Response response = request.client().execute(request.toRequest(), options); return new RibbonResponse(request.getUri(), response);&#125; 代码很简单，最后只是调用Client.Default了 总结","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"05-微服务均衡负载SpringCloud原生LoadBalancer","slug":"springcloud/05-微服务均衡负载SpringCloud原生LoadBalancer","date":"2021-11-27T12:00:11.000Z","updated":"2022-03-23T09:03:56.302Z","comments":true,"path":"blog/springcloud/05-微服务均衡负载SpringCloud原生LoadBalancer/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/05-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%9D%87%E8%A1%A1%E8%B4%9F%E8%BD%BDSpringCloud%E5%8E%9F%E7%94%9FLoadBalancer/","excerpt":"","text":"Spring Cloud LoadBalancer是Spring Cloud官方自己提供的客户端负载均衡器, 用来替代Ribbon。 Spring官方提供了两种负载均衡的客户端： RestTemplate RestTemplate是Spring提供的用于访问Rest服务的客户端，RestTemplate提供了多种便捷访问远程Http服务的方法，能够大大提高客户端的编写效率。默认情况下，RestTemplate默认依赖jdk的HTTP连接工具。 WebClient WebClient是从Spring WebFlux 5.0版本开始提供的一个非阻塞的基于响应式编程的进行Http请求的客户端工具。它的响应式编程的基于Reactor的。WebClient中提供了标准Http请求方式对应的get、post、put、delete等方法，可以用来发起相应的请求。 RestTemplate整合LoadBalancer引入依赖12345678910111213141516171819202122232425&lt;!-- LoadBalancer --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-loadbalancer&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 提供了RestTemplate支持 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- nacos服务注册与发现 移除ribbon支持--&gt;&lt;!-- nacos-discovery中引入了ribbon，需要移除ribbon的包 --&gt;&lt;!-- 不过，不推荐这种方式。建议使用配置关闭rebbon --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 注意： nacos-discovery中引入了ribbon，需要移除ribbon的包 如果不移除，也可以在yml中配置不使用ribbon 1234567891011spring:application: name: mall-user-loadbalancer-democloud: nacos: discovery: server-addr: 127.0.0.1:8848 # 不使用ribbon loadbalancer: ribbon: enabled: false 默认情况下BlockingLoadBalancerClient的自动配置类也会被加载，如果项目中引入了RibbonLoadBalancerClient，那么BlockingLoadBalancerClient的自动配置类就不会被实例化，而是使用RibbonLoadBalancerClient 要使用BlockingLoadBalancerClient的话，可以设置spring.cloud.loadbalancer.ribbon.enabled属性为false。 使用@LoadBalanced注解配置RestTemplate12345678@Configurationpublic class RestConfig &#123; @Bean @LoadBalanced public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 也可以 1234567891011121314@Configurationpublic class RestConfig &#123; @Autowired private LoadBalancerClient loadBalancer; @Bean @LoadBalanced public RestTemplate restTemplate() &#123; RestTemplate restTemplate = new RestTemplate(); restTemplate.setInterceptors(Collections.singletonList(new LoadBalancerInterceptor(loadBalancer))); return restTemplate; &#125;&#125; 使用@LoadBalanced的最终目的就是执行这样的代码 使用123456789101112131415@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @Autowired private RestTemplate restTemplate; @RequestMapping(value = &quot;/findOrderByUserId/&#123;id&#125;&quot;) public R findOrderByUserId(@PathVariable(&quot;id&quot;) Integer id) &#123; // 这样通过LoadBalancerInterceptor就能把mall-order解析成具体的IP:PORT地址。 String url = &quot;http://mall-order/order/findOrderByUserId/&quot;+id; R result = restTemplate.getForObject(url,R.class); return result; &#125;&#125; WebClient整合LoadBalancerSpringCloud 官方文档Load-Balancing with Spring Cloud LoadBalancer 引入依赖1234567891011121314151617181920212223&lt;!-- LoadBalancer --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-loadbalancer&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- webflux --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- nacos服务注册与发现 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 这里建议通过配置文件不使用Ribbon，看上一节 配置WebClient作为负载均衡器的client1234567891011121314@Configurationpublic class WebClientConfig &#123; @LoadBalanced @Bean private WebClient.Builder webClientBuilder() &#123; return WebClient.builder(); &#125; @Bean private WebClient webClient() &#123; return webClientBuilder().build(); &#125;&#125; 使用123456789101112@Autowiredprivate WebClient webClient;@RequestMapping(value = &quot;/findOrderByUserId2/&#123;id&#125;&quot;)public Mono&lt;R&gt; findOrderByUserIdWithWebClient(@PathVariable(&quot;id&quot;) Integer id) &#123; String url = &quot;http://mall-order/order/findOrderByUserId/&quot;+id; //基于WebClient Mono&lt;R&gt; result = webClient.get().uri(url) .retrieve().bodyToMono(R.class); return result;&#125; 原理： 底层会使用ReactiveLoadBalancer 1234567891011121314151617@Autowiredprivate ReactorLoadBalancerExchangeFilterFunction lbFunction;@RequestMapping(value = &quot;/findOrderByUserId3/&#123;id&#125;&quot;)public Mono&lt;R&gt; findOrderByUserIdWithWebFlux(@PathVariable(&quot;id&quot;) Integer id) &#123; String url = &quot;http://mall-order/order/findOrderByUserId/&quot;+id; //基于WebClient+webFlux Mono&lt;R&gt; result = WebClient.builder() .filter(lbFunction) .build() .get() .uri(url) .retrieve() .bodyToMono(R.class); return result;&#125; 配置使用@LoadBalancerClients注解和看LoadBalancerClientFactory类来了解有什么扩展 与Ribbon的LoadBalancer的区别相同： ribbon的LoadBalancer与Spring cloud提供的LoadBalancer都遵循了spring cloud的标准，都实现了LoadBalancerClient接口。Ribbon的LoadBalancerClient是RibbonLoadBalancerClient。而Spring cloud的LoadBalancerClient是BlockingLoadBalancerClient。 不同点： ribbon的LoadBalancer在获取服务上是使用了ribbon自己提供的标准的，比如在nacos-discovery中是实现了ribbon提供的ServerList接口来完成服务的获取。而在Spring cloud的LoadBalancer中，是使用了接口DiscoveryClient来完成服务的获取的。","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"04-微服务负载均衡器Ribbon","slug":"springcloud/04-微服务负载均衡器Ribbon","date":"2021-11-27T12:00:10.000Z","updated":"2022-03-23T09:03:56.296Z","comments":true,"path":"blog/springcloud/04-微服务负载均衡器Ribbon/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/04-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8Ribbon/","excerpt":"","text":"什么是Ribbon目前主流的负载方案分为以下两种: 集中式负载均衡，在消费者和服务提供方中间使用独立的代理方式进行负载，有硬件的(比如F5)，也有软件的(比如 Nginx)。 客户端根据自己的请求情况做负载均衡，Ribbon 就属于客户端自己做负载均衡 Spring Cloud Ribbon是基于Netflix Ribbon 实现的一套客户端的负载均衡工具，Ribbon客户端组件提 供一系列的完善的配置，如超时，重试等。通过Load Balancer获取到服务提供的所有机器实例， Ribbon会自动基于某种规则(轮询，随机)去调用这些服务。Ribbon也可以实现我们自己的负载均衡算 法。 服务端的负载均衡例如Nginx，通过Nginx进行负载均衡，先发送请求，然后通过负载均衡算法，在多个服务器之间选择一 个进行访问;即在服务器端再进行负载均衡算法分配。 客户端的负载均衡例如spring cloud中的ribbon，客户端会有一个服务器地址列表，在发送请求前通过负载均衡算法选择 一个服务器，然后进行访问，这是客户端负载均衡;即在客户端（调用方）就进行负载均衡算法分配。 常见负载均衡算法 随机，通过随机选择服务进行执行，一般这种方式使用较少; 轮训，负载均衡默认实现方式，请求来之后排队处理; 加权轮训，通过对服务器性能的分型，给高配置，低负载的服务器分配更高的权重，均衡各个服务器的压力; 地址Hash，通过调用方地址的Hash值与服务数量进行取模，使得调用方与某个服务地址绑定。 最小链接数，即使请求均衡了，压力不一定会均衡，最小连接数法就是根据服务器的情况，比如请求积压数等参数，将请求分配到当前压力最小的服务器上。 Ribbon模块 名 称 说 明 ribbon-loadbalancer 负载均衡模块，可独立使用，也可以和别的模块一起使用。 Ribbon 内置的负载均衡算法都实现在其中。 ribbon-eureka 基于 Eureka 封装的模块，能够快速、方便地集成 Eureka。 ribbon-transport 基于 Netty 实现多协议的支持，比如 HTTP、Tcp、Udp 等。 ribbon-httpclient 基于 Apache HttpClient 封装的 REST 客户端，集成了负载均衡模块，可以直接在项目中使用来调用接口。 ribbon-example Ribbon 使用代码示例，通过这些示例能够让你的学习事半功倍。 ribbon-core 一些比较核心且具有通用性的代码，客户端 API 的一些配置和其他 API 的定义。 Ribbon单独使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// 服务列表List&lt;Server&gt; serverList = Lists.newArrayList( new Server(&quot;localhost&quot;, 8020), new Server(&quot;localhost&quot;, 8021));DefaultClientConfigImpl config = new DefaultClientConfigImpl();config.loadProperties(&quot;xyz_test_server&quot;);config.set(CommonClientConfigKey.ConnectTimeout, 10000);config.set(CommonClientConfigKey.ReadTimeout, 10000);config.set(CommonClientConfigKey.GZipPayload, true);// 构建负载实例ILoadBalancer loadBalancer = LoadBalancerBuilder.newBuilder() .withClientConfig(config) .buildFixedServerListLoadBalancer(serverList);LoadBalancerContext loadBalancerContext = new LoadBalancerContext(loadBalancer);URI uri = new URI(&quot;http://xyz_test_server/order/findOrderByUserId/1&quot;);// 调用 5 次来测试效果for (int i = 0; i &lt; 5; i++) &#123; String result = LoadBalancerCommand.&lt;String&gt;builder() .withRetryHandler(new DefaultLoadBalancerRetryHandler(4, 4, true)) .withLoadBalancerContext(loadBalancerContext) .withLoadBalancerURI(uri) .build() .submit(new ServerOperation&lt;String&gt;() &#123; @Override // 已经选择了一个服务 public Observable&lt;String&gt; call(Server server) &#123; try &#123; URI finalUri = loadBalancerContext.reconstructURIWithServer(server, uri); HttpURLConnection conn = (HttpURLConnection) finalUri.toURL().openConnection(); conn.setRequestMethod(&quot;GET&quot;); conn.connect(); InputStream in = conn.getInputStream(); byte[] data = new byte[in.available()]; in.read(data); return Observable.just(new String(data)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; &#125;).toBlocking().first(); System.out.println(&quot; 调用结果：&quot; + result);&#125; 上边的代码还是挺复杂的。还有一种方式就是使用IClient接口，完成均衡负载和HTTP请求的调用。但这里强调一下，如果只是单纯的实现了IClient接口，那么就必须自己完善均衡负载的代码，因为IClient接口只关注的是http请求的调用。所以ribbon提供了一个抽象类AbstractLoadBalancerAwareClient完善了均衡负载的代码。所以一般情况下都是继承AbstractLoadBalancerAwareClient类的 在Spring cloud项目中多了一个选择。在spring-cloud-ribbon中还提供了AbstractLoadBalancingClient类，该类继承了AbstractLoadBalancerAwareClient，添加了ServerIntrospector这个扩展。 下面就是AbstractLoadBalancerAwareClient对象，完成均衡负载和http调用的例子 123456789101112131415161718192021222324252627// 服务列表List&lt;Server&gt; serverList = Lists.newArrayList( new Server(&quot;localhost&quot;, 8020), new Server(&quot;localhost&quot;, 8021));DefaultClientConfigImpl config = new DefaultClientConfigImpl();config.loadProperties(&quot;xyz_test_server&quot;);config.set(CommonClientConfigKey.ConnectTimeout, 10000);config.set(CommonClientConfigKey.ReadTimeout, 10000);config.set(CommonClientConfigKey.GZipPayload, true);// 构建负载实例ILoadBalancer loadBalancer = LoadBalancerBuilder.newBuilder() .withClientConfig(config) .buildFixedServerListLoadBalancer(serverList);RibbonLoadBalancingHttpClient loadBalancingHttpClient = new RibbonLoadBalancingHttpClient(config, null);loadBalancingHttpClient.setLoadBalancer(loadBalancer);// String serviceId, String method, String uri,// Boolean retryable, MultiValueMap&lt;String, String&gt; headers,// MultiValueMap&lt;String, String&gt; params, InputStream requestEntity,// List&lt; RibbonRequestCustomizer &gt; requestCustomizers, Long contenRibbonApacheHttpRequest request = new RibbonApacheHttpRequest(new RibbonCommandContext(&quot;xyz_test_server&quot;, &quot;GET&quot;, &quot;http://xyz_test_server/order/findOrderByUserId/1&quot;, null, null, null, null, null, 1000L ));RibbonApacheHttpResponse response = loadBalancingHttpClient.executeWithLoadBalancer(request, config); Spring Cloud快速整合Ribbon 引入依赖 12345&lt;!--添加ribbon的依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;&lt;/dependency&gt; nacos-discovery依赖了ribbon，可以不用再引入ribbon依赖 这里需要看nacos-discovery的版本 添加@LoadBalanced注解 12345678@Configurationpublic class RestConfig &#123; @Bean @LoadBalanced public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125; &#125; 调用服务 1234567891011121314151617181920212223@Autowiredprivate RestTemplate restTemplate;@RequestMapping(value = &quot;/findOrderByUserId/&#123;id&#125;&quot;)public R findOrderByUserId(@PathVariable(&quot;id&quot;) Integer id) &#123; // RestTemplate调用 //String url = &quot;http://localhost:8020/order/findOrderByUserId/&quot;+id; //模拟ribbon实现 //String url = getUri(&quot;mall-order&quot;)+&quot;/order/findOrderByUserId/&quot;+id; // 添加@LoadBalanced String url = &quot;http://mall-order/order/findOrderByUserId/&quot;+id; R result = restTemplate.getForObject(url,R.class); return result;&#125;// 或者通过如下方式来获取到某个服务实例@Autowiredprivate LoadBalancerClient loadBalancerClient;//使用 LoadBalanceClient 和 RestTemolate 结合的方式来访问ServiceInstance serviceInstance = loadBalancerClient.choose(&quot;nacos-provider&quot;);String url = String.format(&quot;http://%s:%s/echo/%s&quot;, serviceInstance.getHost(), serviceInstance.getPort(), appName); 这种方式在项目中基本是不使用的。会使用feign来完成调用（方法调用） Spring Cloud中Ribbon配置12345678910111213141516spring: application: name: mall-user-ribbon-demo #微服务名称 #配置nacos注册中心地址 cloud: nacos: discovery: server-addr: 127.0.0.1:8848 cluster-name: SH metadata: version: v2 # 不使用ribbon loadbalancer: ribbon: enabled: false 按服务配置使用yaml文件配置： 12345678910# 被调用的微服务名 # 当需要使用局部配置的时候推荐使用这种方式mall-order: ribbon: # 指定使用Nacos提供的负载均衡策略（优先调用同一集群的实例，基于随机&amp;权重） NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule# 自定义的负载均衡策略（基于随机&amp;权重）# 实现AbstractLoadBalancerRule类# NFLoadBalancerRuleClassName: com.tuling.mall.ribbondemo.rule.NacosRandomWithWeightRule# 更多属性看org.springframework.cloud.netflix.ribbon.PropertiesFactory类 使用注解配置： 12345678910111213141516171819// 配置多个 // RibbonConfig不能被@SpringbootApplication的@CompentScan扫描到，否则就是全局配置的效果@RibbonClients(value = &#123; // 在SpringBoot主程序扫描的包外定义配置类 @RibbonClient(name = &quot;mall-order&quot;,configuration = com.tuling.mall.rule.RibbonConfig.class), @RibbonClient(name = &quot;mall-account&quot;,configuration = com.tuling.mall.rule.RibbonConfig.class)&#125;)public class RibbonClientsConfig &#123;&#125;@Configurationpublic class RibbonConfig &#123; @Bean public IRule ribbonRule() &#123; return new NacosRandomWithWeightRule(); &#125;&#125; 全局配置1234567# 全局设置ribbon: eager-load: # 开启ribbon饥饿加载 enabled: true # 配置mall-user使用ribbon饥饿加载，多个使用逗号分隔 clients: mall-order 如果要全局配置Ribbon的负载均衡策略使用配置类 1234567891011121314@Configurationpublic class RibbonConfig &#123; /** * 全局配置 * 指定负载均衡策略 * @return */ @Bean public IRule ribbonRule() &#123; // 指定使用Nacos提供的负载均衡策略（优先调用同一集群的实例，基于随机权重） return new NacosRule(); &#125;&#125; Ribbon详细介绍Ribbon原理图 这里的LoadBalanceInterceptor就是使用了RestTemplate的ClientHttpRequestInterceptor扩展 RestTemplate的方法调用RestTemplate的方法调用 LoadBalancerInterceptor的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// LoadBalancerInterceptorpublic class LoadBalancerInterceptor implements ClientHttpRequestInterceptor &#123; private LoadBalancerClient loadBalancer; private LoadBalancerRequestFactory requestFactory; public LoadBalancerInterceptor(LoadBalancerClient loadBalancer, LoadBalancerRequestFactory requestFactory) &#123; this.loadBalancer = loadBalancer; this.requestFactory = requestFactory; &#125; public LoadBalancerInterceptor(LoadBalancerClient loadBalancer) &#123; // for backwards compatibility this(loadBalancer, new LoadBalancerRequestFactory(loadBalancer)); &#125; @Override public ClientHttpResponse intercept(final HttpRequest request, final byte[] body, final ClientHttpRequestExecution execution) throws IOException &#123; final URI originalUri = request.getURI(); String serviceName = originalUri.getHost(); Assert.state(serviceName != null, &quot;Request URI does not contain a valid hostname: &quot; + originalUri); return this.loadBalancer.execute(serviceName, this.requestFactory.createRequest(request, body, execution)); &#125;&#125;// LoadBalancerRequestFactorypublic LoadBalancerRequest&lt;ClientHttpResponse&gt; createRequest( final HttpRequest request, final byte[] body, final ClientHttpRequestExecution execution) &#123; return instance -&gt; &#123; HttpRequest serviceRequest = new ServiceRequestWrapper(request, instance, this.loadBalancer); if (this.transformers != null) &#123; for (LoadBalancerRequestTransformer transformer : this.transformers) &#123; serviceRequest = transformer.transformRequest(serviceRequest, instance); &#125; &#125; return execution.execute(serviceRequest, body); &#125;;&#125; 其实就是LoadBalancerClient的使用。而这个LoadBalancerClient的类型为RibbonLoadBalancerClient。剩下的就是ribbon的调用了 模拟ribbon实现123456@Autowiredprivate LoadBalancerClient loadBalancerClient;//使用 LoadBalanceClient 和 RestTemolate 结合的方式来访问ServiceInstance serviceInstance = loadBalancerClient.choose(&quot;nacos-provider&quot;);String url = String.format(&quot;http://%s:%s/echo/%s&quot;, serviceInstance.getHost(), serviceInstance.getPort(), appName); 上边是Ribbon的使用，下面的代码就是Ribbon的实现思路 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @Autowired private RestTemplate restTemplate; @RequestMapping(value = &quot;/findOrderByUserId/&#123;id&#125;&quot;) public R findOrderByUserId(@PathVariable(&quot;id&quot;) Integer id) &#123; //模拟ribbon实现 从注册中心获取service list ----&gt; service String url = getUri(&quot;mall-order&quot;)+&quot;/order/findOrderByUserId/&quot;+id; R result = restTemplate.getForObject(url,R.class); return result; &#125; // 这里的是Nacos的实现 @Autowired private DiscoveryClient discoveryClient; public String getUri(String serviceName) &#123; // 这要缓存起来 List&lt;ServiceInstance&gt; serviceInstances = discoveryClient.getInstances(serviceName); if (serviceInstances == null || serviceInstances.isEmpty()) &#123; // 这里因该抛出错误 return null; &#125; int serviceSize = serviceInstances.size(); //轮询，这里可以使用接口+配置来实现 int indexServer = incrementAndGetModulo(serviceSize); return serviceInstances.get(indexServer).getUri().toString(); &#125; private AtomicInteger nextIndex = new AtomicInteger(0); private int incrementAndGetModulo(int modulo) &#123; for (;;) &#123; int current = nextIndex.get(); int next = (current + 1) % modulo; if (nextIndex.compareAndSet(current, next) &amp;&amp; current &lt; modulo)&#123; return current; &#125; &#125; &#125;&#125; @LoadBalanced 注解原理从原理图中可知，是使用了LoadBalancerInterceptor来实现对RestTemplate的均衡负载的。这个有两种方式设置 通过RestTemplate#setInterceptors方法 1234567891011121314@Configurationpublic class RestConfig &#123; @Autowired LoadBalancerClient loadBalancer; @Bean public RestTemplate restTemplate() &#123; RestTemplate restTemplate = new RestTemplate(); restTemplate.setInterceptors(Collections.singletonList(new LoadBalancerInterceptor(loadBalancer))); return restTemplate; &#125;&#125; 通过@LoadBalanced注解 123456789@Configurationpublic class RestConfig &#123; @Bean @LoadBalanced public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 这里看下@LoadBalanced注解的原理。 1234567@Target(&#123; ElementType.FIELD, ElementType.PARAMETER, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@Qualifierpublic @interface LoadBalanced &#123;&#125; 该注解的重点就是有@Qualifier。该注解起到的是限定注入的类。如果添加在注解中，就是起到了限定注入的类在定义时需要有对应的注解。可以看[《Spring中的@Qualifier》]( 这个注解的最终目的就是选择有@LoadBalanced注解的RestTemplate。找到后就执行RestTemplate#setInterceptors方法，为RestTemplate添加LoadBalancerInterceptor拦截器。这部分的逻辑在自动配置类LoadBalancerAutoConfiguration中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@Configuration(proxyBeanMethods = false)@ConditionalOnClass(RestTemplate.class)@ConditionalOnBean(LoadBalancerClient.class)@EnableConfigurationProperties(LoadBalancerRetryProperties.class)public class LoadBalancerAutoConfiguration &#123; // 这里就是起到了限制注入到list中的RestTemplate。 // 要求注入RestTemplate需要有@LoadBalanced注解描述（也就是定义时使用@LoadBalanced） @LoadBalanced @Autowired(required = false) private List&lt;RestTemplate&gt; restTemplates = Collections.emptyList(); @Autowired(required = false) private List&lt;LoadBalancerRequestTransformer&gt; transformers = Collections.emptyList(); // 这里可以扩展 @Bean public SmartInitializingSingleton loadBalancedRestTemplateInitializerDeprecated( final ObjectProvider&lt;List&lt;RestTemplateCustomizer&gt;&gt; restTemplateCustomizers) &#123; return () -&gt; restTemplateCustomizers.ifAvailable(customizers -&gt; &#123; for (RestTemplate restTemplate : LoadBalancerAutoConfiguration.this.restTemplates) &#123; for (RestTemplateCustomizer customizer : customizers) &#123; customizer.customize(restTemplate); &#125; &#125; &#125;); &#125; @Bean @ConditionalOnMissingBean public LoadBalancerRequestFactory loadBalancerRequestFactory( LoadBalancerClient loadBalancerClient) &#123; return new LoadBalancerRequestFactory(loadBalancerClient, this.transformers); &#125; // 当不存在RetryTemplate这个类时，默认是这个 @Configuration(proxyBeanMethods = false) @ConditionalOnMissingClass(&quot;org.springframework.retry.support.RetryTemplate&quot;) static class LoadBalancerInterceptorConfig &#123; @Bean public LoadBalancerInterceptor ribbonInterceptor( LoadBalancerClient loadBalancerClient, LoadBalancerRequestFactory requestFactory) &#123; return new LoadBalancerInterceptor(loadBalancerClient, requestFactory); &#125; @Bean @ConditionalOnMissingBean public RestTemplateCustomizer restTemplateCustomizer( final LoadBalancerInterceptor loadBalancerInterceptor) &#123; return restTemplate -&gt; &#123; List&lt;ClientHttpRequestInterceptor&gt; list = new ArrayList&lt;&gt;( restTemplate.getInterceptors()); list.add(loadBalancerInterceptor); restTemplate.setInterceptors(list); &#125;; &#125; &#125; 。。。。。。&#125; Ribbon扩展接口参考自动配置类： 1org.springframework.cloud.netflix.ribbon.RibbonClientConfiguration IClientConfig：Ribbon的客户端配置，默认采用DefaultClientConfigImpl实现。 IRule：Ribbon的负载均衡策略，默认采用ZoneAvoidanceRule实现，该策略能够在多区域环境下选出最佳区域的实例进行访问。 IPing：Ribbon的实例检查策略，默认采用DummyPing实现，该检查策略是一个特殊的实现，实际上它并不会检查实例是否可用，而是始终返回true，默认认为所有服务实例都是可用的。 ServerList：服务实例清单的维护机制，默认采用ConfigurationBasedServerList实现。Nacos的实现时NacosServerList，这个接口就是用来获取服务列表的 ServerListFilter：服务实例清单过滤机制，默认采ZonePreferenceServerListFilter，该策略能够优先过滤出与请求方处于同区域的服 务实例。 使用场景就是黑白名单、灰度发布（V1调V1，V2调V2） ILoadBalancer：负载均衡器，支持懒加载，默认采用ZoneAwareLoadBalancer实现，它具备区域感知的能力；该类的父类为DynamicServerListLoadBalancer；每个服务都对应一个ILoadBalancer（或者是都对应一个ApplicationContext）；而且支持定时更新服务列表 Ribbon负载均衡策略 RandomRule： 随机选择一个Server。 RetryRule： 对选定的负载均衡策略机上重试机制，在一个配置时间段内当选择Server不成功，则一直尝试使用subRule的方式选择一个可用的server。 RoundRobinRule： 轮询选择， 轮询index，选择index对应位置的Server。 AvailabilityFilteringRule： 过滤掉一直连接失败的被标记为circuit tripped的后端Server，并过滤掉那些高并发的后端Server或者使用一个AvailabilityPredicate来包含过滤server的逻辑，其实就是检查status里记录的各个Server的运行状态。 BestAvailableRule： 选择一个最小的并发请求的Server，逐个考察Server，如果Server被tripped了，则跳过。 WeightedResponseTimeRule： 根据响应时间加权，响应时间越长，权重越小，被选中的可能性越低。 ZoneAvoidanceRule： 默认的负载均衡策略，即复合判断Server所在区域的性能和Server的可用性选择Server，在没有区域的环境下，类似于轮询(RandomRule) NacosRule: Nacos的实现，同集群优先调用 123456789101112spring: application: name: mall-user-ribbon-demo #微服务名称 #配置nacos注册中心地址 cloud: nacos: discovery: server-addr: 127.0.0.1:8848 # 集群 cluster-name: SH metadata: version: v2 也就是会优先调用cluster-name相同的。 修改默认负载均衡策略 全局配置 1234567891011121314@Configurationpublic class RibbonConfig &#123; /** * 全局配置 * 指定负载均衡策略 * @return */ @Bean public IRule() &#123; // 指定使用Nacos提供的负载均衡策略（优先调用同一集群的实例，基于随机权重） return new NacosRule(); &#125;&#125; 局部配置 调用指定微服务提供的服务时，使用对应的负载均衡算法 修改application.yml 12345# 被调用的微服务名mall-order: ribbon: # 指定使用Nacos提供的负载均衡策略（优先调用同一集群的实例，基于随机&amp;权重） NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule 局部配置还可以使用@RibbonClient指定微服务及其负载均衡策略。 123456789101112131415@SpringBootApplication(exclude = &#123;DataSourceAutoConfiguration.class, DruidDataSourceAutoConfigure.class&#125;)//@RibbonClient(name = &quot;mall-order&quot;,configuration = RibbonConfig.class)//配置多个 RibbonConfig不能被@SpringbootApplication的@CompentScan扫描到，否则就是全局配置的效果@RibbonClients(value = &#123; // 在SpringBoot主程序扫描的包外定义配置类 @RibbonClient(name = &quot;mall-order&quot;,configuration = RibbonConfig.class), @RibbonClient(name = &quot;mall-account&quot;,configuration = RibbonConfig.class) &#125;)public class MallUserRibbonDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MallUserRibbonDemoApplication.class, args); &#125;&#125; 不过，这种方式有坑，上边RibbonClient的配置类不能放在@SpringbootApplication注解的@CompentScan扫描得到的地方。否则自定义的配置类就会被所有的 RibbonClients共享。 不建议这么使用，推荐yml方式 配置自定义的策略通过实现 IRule 接口可以自定义负载策略，主要的选择服务逻辑在 choose 方法中。 一般都是继承 AbstractLoadBalancerRule 1234567891011121314151617181920212223242526@Slf4jpublic class NacosRandomWithWeightRule extends AbstractLoadBalancerRule &#123; @Autowired private NacosDiscoveryProperties nacosDiscoveryProperties; @Override public Server choose(Object key) &#123; DynamicServerListLoadBalancer loadBalancer = (DynamicServerListLoadBalancer) getLoadBalancer(); String serviceName = loadBalancer.getName(); NamingService namingService = nacosDiscoveryProperties.namingServiceInstance(); try &#123; //nacos基于权重的算法 Instance instance = namingService.selectOneHealthyInstance(serviceName); return new NacosServer(instance); &#125; catch (NacosException e) &#123; log.error(&quot;获取服务实例异常：&#123;&#125;&quot;, e.getMessage()); e.printStackTrace(); &#125; return null; &#125; @Override public void initWithNiwsConfig(IClientConfig clientConfig) &#123; &#125;&#125; 饥饿加载在进行服务调用的时候，如果网络情况不好，第一次调用会超时。 Ribbon默认懒加载，意味着只有在发起调用的时候才会创建客户端。 开启饥饿加载，解决第一次调用慢的问题 123456ribbon: eager-load: # 开启ribbon饥饿加载 enabled: true # 配置mall-user使用ribbon饥饿加载，多个使用逗号分隔 clients: mall-order 源码对应属性配置类：RibbonEagerLoadProperties","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"04-Spring Cloud中Ribbon的启动和调用源码","slug":"springcloud/04-Spring Cloud中Ribbon的启动和调用源码","date":"2021-11-27T12:00:09.000Z","updated":"2022-03-23T09:03:56.255Z","comments":true,"path":"blog/springcloud/04-Spring Cloud中Ribbon的启动和调用源码/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/04-Spring%20Cloud%E4%B8%ADRibbon%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E8%B0%83%E7%94%A8%E6%BA%90%E7%A0%81/","excerpt":"","text":"这里只涉及Ribbon在Spring Cloud的主要调用流程 启动在Spring Cloud中使用Ribbon都需要引入 12345&lt;!--添加ribbon的依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;&lt;/dependency&gt; 而该包有个自动配置类RibbonAutoConfiguration 这个配置类配置了SpringClientFactory对象 1234567891011// RibbonAutoConfiguration@Autowired(required = false)private List&lt;RibbonClientSpecification&gt; configurations = new ArrayList&lt;&gt;();@Bean@ConditionalOnMissingBeanpublic SpringClientFactory springClientFactory() &#123; SpringClientFactory factory = new SpringClientFactory(); factory.setConfigurations(this.configurations); return factory;&#125; SpringClientFactorySpringClientFactory是netflix-ribbon包提供的，它的父类为NamedContextFactory 。而NamedContextFactory是Spring Cloud项目提供的。现在先看NamedContextFactory： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public abstract class NamedContextFactory&lt;C extends NamedContextFactory.Specification&gt; implements DisposableBean, ApplicationContextAware &#123; private final String propertySourceName; private final String propertyName; private Map&lt;String, AnnotationConfigApplicationContext&gt; contexts = new ConcurrentHashMap&lt;&gt;(); private Map&lt;String, C&gt; configurations = new ConcurrentHashMap&lt;&gt;(); private ApplicationContext parent; private Class&lt;?&gt; defaultConfigType; public NamedContextFactory(Class&lt;?&gt; defaultConfigType, String propertySourceName, String propertyName) &#123; this.defaultConfigType = defaultConfigType; this.propertySourceName = propertySourceName; this.propertyName = propertyName; &#125; protected AnnotationConfigApplicationContext getContext(String name) &#123; if (!this.contexts.containsKey(name)) &#123; synchronized (this.contexts) &#123; if (!this.contexts.containsKey(name)) &#123; this.contexts.put(name, createContext(name)); &#125; &#125; &#125; return this.contexts.get(name); &#125; protected AnnotationConfigApplicationContext createContext(String name) &#123; AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(); if (this.configurations.containsKey(name)) &#123; for (Class&lt;?&gt; configuration : this.configurations.get(name) .getConfiguration()) &#123; context.register(configuration); &#125; &#125; for (Map.Entry&lt;String, C&gt; entry : this.configurations.entrySet()) &#123; if (entry.getKey().startsWith(&quot;default.&quot;)) &#123; for (Class&lt;?&gt; configuration : entry.getValue().getConfiguration()) &#123; context.register(configuration); &#125; &#125; &#125; context.register(PropertyPlaceholderAutoConfiguration.class, this.defaultConfigType); context.getEnvironment().getPropertySources().addFirst(new MapPropertySource( this.propertySourceName, Collections.&lt;String, Object&gt;singletonMap(this.propertyName, name))); if (this.parent != null) &#123; // Uses Environment from parent as well as beans context.setParent(this.parent); // jdk11 issue // https://github.com/spring-cloud/spring-cloud-netflix/issues/3101 context.setClassLoader(this.parent.getClassLoader()); &#125; context.setDisplayName(generateDisplayName(name)); context.refresh(); return context; &#125; // 省略 ......&#125; 该类的作用就是按名字管理AnnotationConfigApplicationContext，从这也可以看到，Spring Cloud提供的NamedContextFactory其实是定义了一个规范，那就是一个服务都要自己的一个AnnotationConfigApplicationContext。而且，通过defaultConfigType属性来设置相同的配置，通过configurations属性的设置来对AnnotationConfigApplicationContext做定制化。而且不同服务的AnnotationConfigApplicationContext的父ApplicationContext都是相同的，都是项目中的ApplicationContext。最后还有一点，就是把服务的名字添加到了自己的ApplicationContext的Environment中，其key值就是属性propertyName定义的。 现在看回SpringClientFactory 1234567891011public class SpringClientFactory extends NamedContextFactory&lt;RibbonClientSpecification&gt; &#123; static final String NAMESPACE = &quot;ribbon&quot;; public SpringClientFactory() &#123; super(RibbonClientConfiguration.class, NAMESPACE, &quot;ribbon.client.name&quot;); &#125; // 省略 ......&#125; 从SpringClientFactory的构造方法可知，服务间的AnnotationConfigApplicationContext的共同配置类为RibbonClientConfiguration。服务名字所对应的key为ribbon.client.name。 自定义服务的ApplicationContext原理在RibbonClientConfiguration中配置了每一个服务的AnnotationConfigApplicationContext必须的Bean，在不同的服务中，有一些Bean是能够自定义的，比如IRule；而有一些Bean是希望共享的，比如ServerList。自定义功能的实现总的来说，还是使用了NamedContextFactory的特性。但有一些细节是在Spring cloud ribbon提供的 yaml的局部配置实现12345678910# 被调用的微服务名 # 当需要使用局部配置的时候推荐使用这种方式mall-order: ribbon: # 指定使用Nacos提供的负载均衡策略（优先调用同一集群的实例，基于随机&amp;权重） NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule# 自定义的负载均衡策略（基于随机&amp;权重）# 实现AbstractLoadBalancerRule类# NFLoadBalancerRuleClassName: com.tuling.mall.ribbondemo.rule.NacosRandomWithWeightRule# 更多属性看org.springframework.cloud.netflix.ribbon.PropertiesFactory类 这些变量都是保存在项目的Environment中的，而且在自动配置类RibbonAutoConfiguration中配置了PropertiesFactory 123456// RibbonAutoConfiguration@Bean@ConditionalOnMissingBeanpublic PropertiesFactory propertiesFactory() &#123; return new PropertiesFactory();&#125; PropertiesFactory的作用就是获取上边的配置的。 在RibbonClientConfiguration中，首先通过RibbonClientName注解来获取到服务的名字，在SpringClientFactory中已经说了。 1234567891011@RibbonClientNameprivate String name = &quot;client&quot;;@Target(&#123; ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER, ElementType.ANNOTATION_TYPE &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Value(&quot;$&#123;ribbon.client.name&#125;&quot;)public @interface RibbonClientName &#123;&#125; 然后，RibbonClientConfiguration会注入PropertiesFactory对象，由于每一个服务的AnnotationConfigApplicationContext的父ApplicationContext都是一样的，而且是项目的ApplicationContext，所以这里的PropertiesFactory对象就是在自动配置类RibbonAutoConfiguration中配置的PropertiesFactory对象。 而且，在RibbonClientConfiguration中有这一段代码： 123if (this.propertiesFactory.isSet(IRule.class, name)) &#123; return this.propertiesFactory.get(IRule.class, config, name);&#125; 比如IRule对象的配置 这段代码的作用就是根据服务名字，检查和获取对应的配置的，详细的可以看PropertiesFactory类，代码步难。 @RibbonClients注解的局部配置实现12345@RibbonClients(value = &#123; // 在SpringBoot主程序扫描的包外定义配置类 @RibbonClient(name = &quot;mall-order&quot;,configuration = com.tuling.mall.rule.RibbonConfig.class), @RibbonClient(name = &quot;mall-account&quot;,configuration = com.tuling.mall.rule.RibbonConfig.class)&#125;) RibbonConfig不能被@SpringbootApplication的@CompentScan扫描到，否则就是全局配置的效果 该注解就是为了引入类RibbonClientConfigurationRegistrar。该类的作用就是解析每一个@RibbonClient，生成RibbonClientSpecification的BeanDefinition，并把该BeanDefinition注册到项目的ApplicationContext中。 接着就是SpringClientFactory的事了。 全局配置原理 NamedContextFactory提供的特性：每一个服务的AnnotationConfigApplicationContext的父ApplicationContext都是项目的ApplicationContext RibbonClientConfiguration中使用了@ConditionalOnMissingBean注解 服务的ribbon饥饿加载默认情况下服务的ribbon都是使用懒加载的，只有在使用的时候才会完成服务的AnnotationConfigApplicationContext的创建和初始化。但通过如下配置后 1234567# 全局设置ribbon: eager-load: # 开启ribbon饥饿加载 enabled: true # 配置mall-user使用ribbon饥饿加载，多个使用逗号分隔 clients: mall-order 就能指定某些服务在启动的时候就完成ribbon的初始化了。 原理也很简单，就是使用了Spring的事件通知机制。在自动配置类RibbonAutoConfiguration中配置了如下对象： 123456789@Autowiredprivate RibbonEagerLoadProperties ribbonEagerLoadProperties;@Bean@ConditionalOnProperty(&quot;ribbon.eager-load.enabled&quot;)public RibbonApplicationContextInitializer ribbonApplicationContextInitializer() &#123; return new RibbonApplicationContextInitializer(springClientFactory(), ribbonEagerLoadProperties.getClients());&#125; RibbonEagerLoadProperties就是配置对应的对象 RibbonApplicationContextInitializer是ApplicationListener的实现，其关注的事件为ApplicationReadyEvent，这个事件是在spring boot中新增的事件，应用启动完成后就会触发，完成指定服务的ribbon的初始化，代码如下： 123456789101112protected void initialize() &#123; if (clientNames != null) &#123; for (String clientName : clientNames) &#123; this.springClientFactory.getContext(clientName); &#125; &#125;&#125;@Overridepublic void onApplicationEvent(ApplicationReadyEvent event) &#123; initialize();&#125; 调用 只看有注解@LoadBalanced修饰的RestTemplate 通过上节知道，均衡负载最终是通过LoadBalancerClient实现的，而具体的对象是通过自动配置类RibbonAutoConfiguration配置的 12345@Bean@ConditionalOnMissingBean(LoadBalancerClient.class)public LoadBalancerClient loadBalancerClient() &#123; return new RibbonLoadBalancerClient(springClientFactory());&#125; 所以看RibbonLoadBalancerClient的execute方法 12345678910111213public &lt;T&gt; T execute(String serviceId, LoadBalancerRequest&lt;T&gt; request, Object hint) throws IOException &#123; ILoadBalancer loadBalancer = getLoadBalancer(serviceId); Server server = getServer(loadBalancer, hint); if (server == null) &#123; throw new IllegalStateException(&quot;No instances available for &quot; + serviceId); &#125; RibbonServer ribbonServer = new RibbonServer(serviceId, server, isSecure(server, serviceId), serverIntrospector(serviceId).getMetadata(server)); // 这里就是调用 LoadBalancerRequest 的方法，不是重点 return execute(serviceId, ribbonServer, request);&#125; 上面的execute方法可以分为3步 获取ILoadBalancer 服务的均衡负载 目标机器的http调用 服务的均衡负载就是通过IRule来完成的，源码可以看BaseLoadBalancer#chooseServer。而目标机器的http调用在Spring Cloud项目中就不是ribbon关心的，这里最终会通过其他模块来完成调用，比如RestTemplate或者feign。下面简单的看下获取ILoadBalancer源码。 ILoadBalancer是Ribbon里面提供的接口，该接口的主要作用就是定义了一系列方法来获取服务。而且每一个服务都有自己的ILoadBalancer对象。 看回源码源码 1234567// RibbonLoadBalancerClient// serviceId就是服务的名字ILoadBalancer loadBalancer = getLoadBalancer(serviceId);protected ILoadBalancer getLoadBalancer(String serviceId) &#123; return this.clientFactory.getLoadBalancer(serviceId);&#125; clientFactory就是SpringClientFactory，所以看源码 1234// SpringClientFactorypublic ILoadBalancer getLoadBalancer(String name) &#123; return getInstance(name, ILoadBalancer.class);&#125; 通过前面的学习SpringClientFactory的getInstance方法就是从对应服务的AnnotationConfigApplicationContext中获取对应的对应。而且默认情况下ILoadBalancer的对象为ZoneAwareLoadBalancer，它具备区域感知的能力，而且支持定时的从注册中心拉取可用的服务列表并缓存起来，怎么拉取式是通过接口ServerList定义的。所以在nacos-discovery中就提供了NacosServerList实现，并在自动配置类&#96;RibbonNacosAutoConfiguration中完成创建。 流程图ILoadBalancer流程图 Ribbon的ILoadBalancer初始化和调用流程 总流程图","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"03-Spring-cloud-nacos服务注册和发现核心原理","slug":"springcloud/03-Spring-cloud-nacos服务注册和发现核心原理","date":"2021-11-27T12:00:08.000Z","updated":"2022-03-23T09:03:56.204Z","comments":true,"path":"blog/springcloud/03-Spring-cloud-nacos服务注册和发现核心原理/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/03-Spring-cloud-nacos%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%92%8C%E5%8F%91%E7%8E%B0%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86/","excerpt":"","text":"NamingServer进行服务注册nacos提供的API： 123456789101112131415161718192021222324// NamingService 注册中心核心服务NamingService namingService = NamingFactory.createNamingService(&quot;localhost:8848&quot;);String serviceName = &quot;mall-user&quot;;namingService.registerInstance(serviceName,&quot;localhost&quot;,8040);namingService.registerInstance(serviceName,&quot;localhost&quot;,8041);Instance instance = new Instance();instance.setEphemeral(false);instance.setIp(&quot;localhost&quot;);instance.setPort(8042);instance.setServiceName(serviceName);namingService.registerInstance(serviceName, instance);System.out.println(&quot;获取实例列表&quot;+namingService.selectInstances(serviceName,true));System.out.println(&quot;获取一个健康实例&quot;+namingService.selectOneHealthyInstance(serviceName));namingService.subscribe(serviceName, new EventListener() &#123; @Override public void onEvent(Event event) &#123; NamingEvent namingEvent = (NamingEvent) event; System.out.println(&quot;NamingEvent: &quot;+namingEvent.getInstances()); &#125;&#125;); 这是nacos服务注册的核心原理。 Spring Cloud服务注册源码服务注册的规范使用了Spring Cloud提供的ServiceRegistry、Registration和AbstractAutoServiceRegistration&lt;R extends Registration&gt;这三个标准实现的。这是Spring Cloud提供的服务注册规范，服务注册中间件都要实现这三个接口。 在Nacos提供了对应的实现，分别为NacosServiceRegistry、NacosRegistration和NacosAutoServiceRegistration看NacosServiceRegistryAutoConfiguration这个自动配置类。 而且服务注册还涉及到一个注解——@EnableDiscoveryClient。 EnableDiscoveryClient注解原理解析123456789101112131415161718/** * Annotation to enable a DiscoveryClient implementation. * @author Spencer Gibb */@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@Import(EnableDiscoveryClientImportSelector.class)public @interface EnableDiscoveryClient &#123; /** * If true, the ServiceRegistry will automatically register the local server. * @return - &#123;@code true&#125; if you want to automatically register. */ boolean autoRegister() default true;&#125; 从这个注解的注释看，是为了引入DiscoveryClient实现。但从源码上看，该注解是为了引入EnableDiscoveryClientImportSelector类，该类的父类为SpringFactoryImportSelector。而SpringFactoryImportSelector是SpringCloud工程中提供的，其实现了DeferredImportSelector接口。 其作用是通过Spring的SPI，获取某个接口的实现类，并且把这些实现类的对象的生命周期交给Spring容器管理（Spring的SPI并不要求一定是接口和实现类的关系，可以是完全没有任何关系，也就是可以是kev–&gt;valueg关系）。 SpringFactoryImportSelector的核心源码： 12345678910111213141516171819202122public abstract class SpringFactoryImportSelector&lt;T&gt; implements DeferredImportSelector, BeanClassLoaderAware, EnvironmentAware &#123; // 这里是获取到实现类中，范型T的具体类型。 private Class&lt;T&gt; annotationClass; protected SpringFactoryImportSelector() &#123; // 这里是获取到实现类中，范型T的具体类型。 // 对于EnableDiscoveryClientImportSelector，就是EnableDiscoveryClient this.annotationClass = (Class&lt;T&gt;) GenericTypeResolver .resolveTypeArgument(this.getClass(), SpringFactoryImportSelector.class); &#125; public String[] selectImports(AnnotationMetadata metadata) &#123; .... // 使用Spring的API，获取spring.factories文件中，EnableDiscoveryClient = 的类集合。 // Spring SPI 的API SpringFactoriesLoader.loadFactoryNames List&lt;String&gt; factories = new ArrayList&lt;&gt;(new LinkedHashSet&lt;&gt;(SpringFactoriesLoader .loadFactoryNames(this.annotationClass, this.beanClassLoader))); .... return factories.toArray(new String[factories.size()]); &#125;&#125; 从SpringFactoryImportSelector的核心源码可知，这个类的作用就是获取其范型对应类的实现类。 回到EnableDiscoveryClientImportSelector，就是就是为了引入 这种定义的类。不一定是DiscoveryClient的实现。不过还是建议按照注解@EnableDiscoveryClient的注释来。而且在该注解引入的EnableDiscoveryClientImportSelector类，重写了selectImports方法，添加了是否允许自动注册的判断 EnableDiscoveryClientImportSelector的核心源码 1234567891011121314151617181920212223242526272829public String[] selectImports(AnnotationMetadata metadata) &#123; String[] imports = super.selectImports(metadata); // 这里的AnnotationAttributes就是注解@EnableDiscoveryClient的元数据。 AnnotationAttributes attributes = AnnotationAttributes.fromMap( metadata.getAnnotationAttributes(getAnnotationClass().getName(), true)); boolean autoRegister = attributes.getBoolean(&quot;autoRegister&quot;); if (autoRegister) &#123; List&lt;String&gt; importsList = new ArrayList&lt;&gt;(Arrays.asList(imports)); importsList.add( &quot;org.springframework.cloud.client.serviceregistry.AutoServiceRegistrationConfiguration&quot;); imports = importsList.toArray(new String[0]); &#125; else &#123; Environment env = getEnvironment(); if (ConfigurableEnvironment.class.isInstance(env)) &#123; ConfigurableEnvironment configEnv = (ConfigurableEnvironment) env; LinkedHashMap&lt;String, Object&gt; map = new LinkedHashMap&lt;&gt;(); map.put(&quot;spring.cloud.service-registry.auto-registration.enabled&quot;, false); MapPropertySource propertySource = new MapPropertySource( &quot;springCloudDiscoveryClient&quot;, map); configEnv.getPropertySources().addLast(propertySource); &#125; &#125; return imports;&#125; 这里也涉及到了一个SpringCloud规范。对于服务的自动注册的前提就是需要引入AutoServiceRegistrationConfiguration类，而该类能够注册到Spring容器的前提就是配置spring.cloud.service-registry.auto-registration.enabled=true。 1234567@Configuration(proxyBeanMethods = false)@EnableConfigurationProperties(AutoServiceRegistrationProperties.class)@ConditionalOnProperty(value = &quot;spring.cloud.service-registry.auto-registration.enabled&quot;, matchIfMissing = true)public class AutoServiceRegistrationConfiguration &#123;&#125; 对于服务的自动注解，@EnableDiscoveryClient注解是必须的。而且如果自己实现的服务注册中心都建议遵循这个规范。就是在自定义的服务注册自动配置类中，需要加上 123@ConditionalOnProperty(value = &quot;spring.cloud.service-registry.auto-registration.enabled&quot;, matchIfMissing = true)@AutoConfigureAfter(&#123; AutoServiceRegistrationConfiguration.class&#125;) Spring Cloud服务注册标准实现——ServiceRegistr、Registration和AbstractAutoServiceRegistration&lt;R extends Registration&gt;AbstractAutoServiceRegistration定义了服务注册的模版方法。该类类图： 可以发现，它实现了ApplicationListener接口，也就是Spring的事件监听接口。关注的事件为WebServerInitializedEvent。 在之前的Spring boot学习中知道了，SpringBoot项目的启动在不同阶段会发布不同的事件来完成某些操作，其中就是tomcat容器的启动，当WebServer启动完成就会发布WebServerInitializedEvent事件。而且Spring Boot的启动阶段的事件处理器采用的是同步策略，也就是事件处理现场和在启动线程是同一个。 所以onApplicationEvent(WebServerInitializedEvent event)方法就是入口方法。跟踪代码后核心代码如下： 123456789101112131415161718// AbstractAutoServiceRegistrationpublic abstract class AbstractAutoServiceRegistration&lt;R extends Registration&gt; .... &#123; private final ServiceRegistry&lt;R&gt; serviceRegistry; // 这里规范了new protected AbstractAutoServiceRegistration(ServiceRegistry&lt;R&gt; serviceRegistry, AutoServiceRegistrationProperties properties) &#123; this.serviceRegistry = serviceRegistry; this.properties = properties; &#125; protected void register() &#123; this.serviceRegistry.register(getRegistration()); &#125; protected abstract R getRegistration();&#125; 通过上边的代码就能搞清楚Spring Cloud服务注册提供的三个标准实现的关系了 AbstractAutoServiceRegistration：就是为了触发服务注册逻辑，定义了服务注册在什么时候进行的 ServiceRegistry：服务是如何注册的 Registration：服务的元数据 从AbstractAutoServiceRegistration的注册源码看到，有一个扩展点 123456this.context.publishEvent(new InstancePreRegisteredEvent(this, getRegistration()));register();if (shouldRegisterManagement()) &#123; registerManagement();&#125;this.context.publishEvent(new InstanceRegisteredEvent&lt;&gt;(this, getConfiguration())); 就是在注册前和注册后都会发布相应的事件，这可以做一些监听或者服务注册前的参数设置或者服务注册后的通知等。 Nacos基于Spring Cloud规范的实现在Nacos提供了&#96;&#96;NacosServiceRegistry、NacosRegistration和NacosAutoServiceRegistration，分别对应的Spring Cloud服务注册标准实现。而这些类又通过SpringBoot的自动配置注册到Spring容器中。而Nacos提供的自动配置类为NacosServiceRegistryAutoConfiguration&#96; 12345678910111213141516171819202122232425262728293031323334353637@Configuration(proxyBeanMethods = false)@EnableConfigurationProperties@ConditionalOnNacosDiscoveryEnabled@ConditionalOnProperty(value = &quot;spring.cloud.service-registry.auto-registration.enabled&quot;, matchIfMissing = true)@AutoConfigureAfter(&#123; AutoServiceRegistrationConfiguration.class, AutoServiceRegistrationAutoConfiguration.class, NacosDiscoveryAutoConfiguration.class &#125;)public class NacosServiceRegistryAutoConfiguration &#123; @Bean public NacosServiceRegistry nacosServiceRegistry( NacosDiscoveryProperties nacosDiscoveryProperties) &#123; return new NacosServiceRegistry(nacosDiscoveryProperties); &#125; @Bean @ConditionalOnBean(AutoServiceRegistrationProperties.class) public NacosRegistration nacosRegistration( ObjectProvider&lt;List&lt;NacosRegistrationCustomizer&gt;&gt; registrationCustomizers, NacosDiscoveryProperties nacosDiscoveryProperties, ApplicationContext context) &#123; return new NacosRegistration(registrationCustomizers.getIfAvailable(), nacosDiscoveryProperties, context); &#125; @Bean @ConditionalOnBean(AutoServiceRegistrationProperties.class) public NacosAutoServiceRegistration nacosAutoServiceRegistration( NacosServiceRegistry registry, AutoServiceRegistrationProperties autoServiceRegistrationProperties, NacosRegistration registration) &#123; return new NacosAutoServiceRegistration(registry, autoServiceRegistrationProperties, registration); &#125;&#125; 详细代码就不看了，NacosServiceRegistry最后还是通过NamingServer来进行服务注册的过程和开头的代码差不过， NamingServer进行服务发现123// NamingService 注册中心核心服务NamingService namingService = NamingFactory.createNamingService(&quot;localhost:8848&quot;);List&lt;Instance&gt; list = namingService.selectInstances(&quot;xyz&quot;, true); Spring Cloud提供DiscoveryClient接口在Spring Cloud中提供了DiscoveryClient接口来规范服务发现。在Ribbon中没有使用该规范，而在Spring Cloud提供的LoadBlance就使用了该规范。 Nacos提供的DiscoveryClient在Nacos-discovery中提供了NacosDiscoveryClient","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"02-nacos注册中心简单使用","slug":"springcloud/02-nacos注册中心简单使用","date":"2021-11-27T12:00:07.000Z","updated":"2022-03-23T09:03:56.199Z","comments":true,"path":"blog/springcloud/02-nacos注册中心简单使用/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/02-nacos%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/","excerpt":"","text":"注册中心注册中心的总体设计 这里面有几个概念 服务续约：客户端定时像服务端发送心跳，服务端接收到心跳后修改服务的续约时间。服务端续约时间，来判断服务是否可能，当超过某个间隔后会把服务的状态修改为down，表示服务下线；当这个间隔继续增大，到达某个阈值时，移除该服务。 nacos注册中心使用引入依赖父Pom中支持spring cloud&amp;spring cloud alibaba, 引入依赖 12345678910111213141516171819&lt;!-- 父pom --&gt;&lt;dependencyManagement&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Hoxton.SR8&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.5.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencyManagement&gt; 当前项目pom中引入依赖 1234567891011 &lt;!-- nacos服务注册与发现 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- nacos配置中心 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;&lt;/dependency&gt; application.yml中配置123456789101112131415161718192021222324252627282930313233server: port: 8020spring: application: name: mall-order #微服务名称 cloud: nacos: #配置nacos注册中心地址 discovery: server-addr: 127.0.0.1:8848 # 命名空间，用来做资源隔离。不同项目用不用的namespace namespace: 39e1e969-15f9-46d2-832d-fa052da55377 # 分组，区分生成、测试、uat环境 group: DEFAULT_GROUP # 集群，不同的集群是可以互通的，但为了性能考虑的话，会优先使用指定的集群。指定集群没有节点可用才会用其他集群的资源 cluster-name: SH # true=临时实例; false=持久化实例，默认为true ephemeral: false #配置中心 config: server-addr: 172.26.59.53:8848 namespace: 603d3f39-e158-4e68-9773-fc7ec63bac23 #允许加载多个配置文件 extension-configs[0]: data-id: common.properties # 是否允许刷新配置，默认值为false refresh: true extension-configs[1]: data-id: airline.properties group: airline refresh: true 还有很多配置，可以看配置的接收类NacosDiscoveryProperties 配置项 Key 默认值 说明 服务端地址 spring.cloud.nacos.discovery.server-addr 无 Nacos Server 启动监听的ip地址和端口 服务名 spring.cloud.nacos.discovery.service $&#123;spring.application.name&#125; 给当前的服务命名 服务分组 spring.cloud.nacos.discovery.group DEFAULT_GROUP 设置服务所处的分组 权重 spring.cloud.nacos.discovery.weight 1 取值范围 1 到 100，数值越大，权重越大 网卡名 spring.cloud.nacos.discovery.network-interface 无 当IP未配置时，注册的IP为此网卡所对应的IP地址，如果此项也未配置，则默认取第一块网卡的地址 注册的IP地址 spring.cloud.nacos.discovery.ip 无 优先级最高 注册的端口 spring.cloud.nacos.discovery.port -1 默认情况下不用配置，会自动探测 命名空间 spring.cloud.nacos.discovery.namespace 无 常用场景之一是不同环境的注册的区分隔离，例如开发测试环境和生产环境的资源（如配置、服务）隔离等。 AccessKey spring.cloud.nacos.discovery.access-key 无 当要上阿里云时，阿里云上面的一个云账号名 SecretKey spring.cloud.nacos.discovery.secret-key 无 当要上阿里云时，阿里云上面的一个云账号密码 Metadata spring.cloud.nacos.discovery.metadata 无 使用Map格式配置，用户可以根据自己的需要自定义一些和服务相关的元数据信息 日志文件名 spring.cloud.nacos.discovery.log-name 无 集群 spring.cloud.nacos.discovery.cluster-name DEFAULT 配置成Nacos集群名称 接入点 spring.cloud.nacos.discovery.enpoint UTF-8 地域的某个服务的入口域名，通过此域名可以动态地拿到服务端地址 是否集成Ribbon ribbon.nacos.enabled true 一般都设置成true即可 是否开启Nacos Watch spring.cloud.nacos.discovery.watch.enabled true 可以设置成false来关闭 watch Nacos discovery · alibaba&#x2F;spring-cloud-alibaba Wiki · GitHub 启动 Provider123456789101112131415161718// 在另一个SpringCloud应用@SpringBootApplication@EnableDiscoveryClientpublic class NacosProviderDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(NacosProducerDemoApplication.class, args); &#125; @RestController public class EchoController &#123; @GetMapping(value = &quot;/echo/&#123;string&#125;&quot;) public String echo(@PathVariable String string) &#123; return &quot;Hello Nacos Discovery &quot; + string; &#125; &#125;&#125; 启动一个 Consumer 应用Consumer 的应用可能还没像启动一个 Provider 应用那么简单。因为在 Consumer 端需要去调用 Provider 端提供的REST 服务。例子中我们使用最原始的一种方式， 即显示的使用 LoadBalanceClient 和 RestTemolate 结合的方式来访问。 通过带有负载均衡的RestTemplate 和 FeignClient 也是可以访问的。 &#x3D;&#x3D;@EnableDiscoveryClient这个注解可以省略&#x3D;&#x3D; 1234567891011121314151617181920212223242526272829303132333435363738@SpringBootApplication@EnableDiscoveryClientpublic class NacosConsumerApp &#123; @RestController public class NacosController&#123; @Autowired private LoadBalancerClient loadBalancerClient; @Autowired private RestTemplate restTemplate; @Value(&quot;$&#123;spring.application.name&#125;&quot;) private String appName; @GetMapping(&quot;/echo/app-name&quot;) public String echoAppName()&#123; //使用 LoadBalanceClient 和 RestTemolate 结合的方式来访问 ServiceInstance serviceInstance = loadBalancerClient.choose(&quot;nacos-provider&quot;); String url = String.format(&quot;http://%s:%s/echo/%s&quot;, serviceInstance.getHost(), serviceInstance.getPort(), appName); System.out.println(&quot;request url:&quot;+url); return restTemplate.getForObject(url,String.class); &#125; &#125; //实例化 RestTemplate 实例 @Bean public RestTemplate restTemplate()&#123; return new RestTemplate(); &#125; public static void main(String[] args) &#123; SpringApplication.run(NacosConsumerApp.class,args); &#125;&#125; 这个例子中我们注入了一个 LoadBalancerClient 的实例，并且手动的实例化一个 RestTemplate。 也可以使用@LoadBalanced注解 1234567891011121314151617181920212223242526272829303132333435@SpringBootApplication@EnableDiscoveryClientpublic class NacosConsumerApp &#123; @RestController public class NacosController&#123; @Autowired private RestTemplate restTemplate; @Value(&quot;$&#123;spring.application.name&#125;&quot;) private String appName; @GetMapping(&quot;/echo/app-name&quot;) public String echoAppName()&#123; // 可以通过服务命来调用 String url = String.format(&quot;http://mall-order/echo/%s&quot;, appName); System.out.println(&quot;request url:&quot;+url); return restTemplate.getForObject(url,String.class); &#125; &#125; //实例化 RestTemplate 实例 @Bean @LoadBalanced public RestTemplate restTemplate()&#123; return new RestTemplate(); &#125; public static void main(String[] args) &#123; SpringApplication.run(NacosConsumerApp.class,args); &#125;&#125; 这里的RestTemplate调用在底层，就是使用了负载均衡器LoadBalancerClient通过 mall-order从注册中心中选一个去调用。这里使用了RestTemplate扩展点——ClientHttpRequestInterceptor。通过ribbon的LoadBalancerInterceptor，将mall-order转化为某个服务的ip:port这种形式。 除了使用@LoadBalanced注解，还可以手动添加LoadBalancerInterceptor: 12345678910111213@Configurationpublic class RestConfig &#123; @Autowired LoadBalancerClient loadBalancer; @Bean public RestTemplate restTemplate() &#123; RestTemplate restTemplate = new RestTemplate(); restTemplate.setInterceptors(Collections.singletonList(new LoadBalancerInterceptor(loadBalancer))); return restTemplate; &#125;&#125; @LoadBalanced注解其实是使用了@Qualifier注解，这里后面会介绍。@LoadBalanced注解的最终目的其实就是和上边的代码是一样的，都是为了创建LoadBalancerInterceptor。 使用NamingServer进行服务注册这里的NamingServer是Nacos提供的一个接口。里面有如下类型的方法。 12345678registerInstancederegisterInstancegetAllInstancesselectInstancesselectOneHealthyInstancegetServicesOfServersubscribeunsubscribe 下面就介绍如果通过NamingServer进行服务注册 12345678910111213141516171819202122232425262728293031// NamingService 注册中心核心服务// 这里是直接使用API来创建，但在SpringCloud应用中，不推荐使用。NamingService namingService = NamingFactory.createNamingService(&quot;localhost:8848&quot;);//服务注册String serviceName = &quot;mall-user&quot;;namingService.registerInstance(serviceName, &quot;localhost&quot;, 8040);namingService.registerInstance(serviceName, &quot;localhost&quot;, 8041);Instance instance = new Instance();// 是否临时节点，这和zk里面的节点概念是一样的instance.setEphemeral(false);instance.setIp(&quot;localhost&quot;);instance.setPort(8042);instance.setServiceName(serviceName);namingService.registerInstance(serviceName, instance);// 第二个参数就是表面返回的节点是否健康的System.out.println(&quot;获取实例列表&quot; + namingService.selectInstances(serviceName, true));System.out.println(&quot;获取一个健康实例&quot; + namingService.selectOneHealthyInstance(serviceName));// 订阅服务变更事件// 这个变更事件只是返回serviceName变更后的服务实例，而onEvent只是接受变化后的内容而已namingService.subscribe(serviceName, new EventListener() &#123; @Override public void onEvent(Event event) &#123; // 这里的Event的实现只有一个就是NamingEvent， NamingEvent namingEvent = (NamingEvent) event; System.out.println(&quot;NamingEvent: &quot;+namingEvent.getInstances()); &#125;&#125;); 在Spring cloud项目中可以通过下面方式获取NamingService对象 1234567891011121314@AutowiredNacosDiscoveryProperties nacosDiscoveryPropertiesCache @AutowiredNacosServiceManager nacosServiceManager// 因为NamingService是没有直接交给Spring管理的，而是通过NacosServiceManager。private NamingService namingService() &#123; // 这里其实逻辑很简单，NamingService作为NacosServiceManager的一个属性成员。 // 而getNamingService有两个作用，检查NamingService是否已经初始化 // 如果初始化了就返回，没有就通过反射和NacosDiscoveryProperties配置作为参数进行NamingService的实例化 return nacosServiceManager .getNamingService(nacosDiscoveryProperties.getNacosProperties());&#125;","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"01-nacos启动","slug":"springcloud/01-nacos启动","date":"2021-11-27T12:00:06.000Z","updated":"2022-03-23T09:03:56.178Z","comments":true,"path":"blog/springcloud/01-nacos启动/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/01-nacos%E5%90%AF%E5%8A%A8/","excerpt":"","text":"单机启动1bin/startup.sh‐m standalone 这种会使用内置的数据源。如果使用外置的数据源，看下面 集群集群部署说明 (nacos.io) 集群部署架构图： 搭建： 单机搭建伪集群，复制nacos安装包，修改为nacos8849，nacos8850，nacos8851 以nacos8849为例，进入nacos8849目录 修改conf\\application.properties的配置，使用外置数据源 12345678#使用外置mysql数据源spring.datasource.platform=mysql### Count of DB:db.num=1### Connect URL of DB:db.url.0=jdbc:mysql://192.168.2.148:3306/nacos-server?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useUnicode=true&amp;useSSL=false&amp;serverTimezone=UTCdb.user=rootdb.password=458233365 将conf\\cluster.conf.example改 cluster.conf,添加节点配置 1234#ip:port192.168.3.14:8849192.168.3.14:8850192.168.3.14:8851 nacos8850，nacos8851 按同样的方式配置。 创建mysql数据库，执行sql文件，文件位置:conf\\nacos­mysql.sql 修改启动脚本(bin\\startup.sh)的jvm参数 分别启动nacos8849，nacos8850，nacos8851 以nacos8849为例，进入nacos8849目录，启动nacos nginx反向代理 prometheus+grafana监控Nacos Nacos 监控手册 Nacos 0.8.0版本完善了监控系统，支持通过暴露metrics数据接入第三方监控系统监控Nacos运行状态。 nacos暴露metrics数据 12# conf\\application.properties的配置management.endpoints.web.exposure.include=* 访问{ip}:8848&#x2F;nacos&#x2F;actuator&#x2F;prometheus prometheus+grafana的安装看nacos文档","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"00-nacos源码启动","slug":"springcloud/00-nacos源码启动","date":"2021-11-27T12:00:05.000Z","updated":"2022-03-23T09:03:56.158Z","comments":true,"path":"blog/springcloud/00-nacos源码启动/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/00-nacos%E6%BA%90%E7%A0%81%E5%90%AF%E5%8A%A8/","excerpt":"","text":"在idea中 1mvn clean install package compile -Dmaven.test.skip=true nacos使用了protobuf 可以通过mvn compile来自动生成他们。 进入console模块，找到启动类 com.alibaba.nacos.Nacos，执行main方法。 启动前，配置启动参数为： 1-Dnacos.standalone=true -Dnacos.home=/Volumes/main/software/nacos/distribution 在执行前可以根据需要修改下application.properties文件 默认用户名: nacos 密码：nacos","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"00-SpringCloudAlibaba基础依赖","slug":"springcloud/00-SpringCloudAlibaba基础依赖","date":"2021-11-27T12:00:04.000Z","updated":"2022-03-23T09:03:56.158Z","comments":true,"path":"blog/springcloud/00-SpringCloudAlibaba基础依赖/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/00-SpringCloudAlibaba%E5%9F%BA%E7%A1%80%E4%BE%9D%E8%B5%96/","excerpt":"","text":"123456789101112131415161718&lt;!-- 父pom --&gt;&lt;dependencyManagement&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Hoxton.SR8&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.5.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencyManagement&gt; 剩下的就根据需求引入各个组件的依赖","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"00-RestTemplate的方法调用","slug":"springcloud/00-RestTemplate的方法调用","date":"2021-11-27T12:00:03.000Z","updated":"2022-03-23T09:03:56.157Z","comments":true,"path":"blog/springcloud/00-RestTemplate的方法调用/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/00-RestTemplate%E7%9A%84%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8/","excerpt":"","text":"通过RestTemplate的调用最终都会调用doExecute方法： 12345678910111213141516171819202122232425262728293031323334353637383940//RestTemplate /** * * @param url 包含了整个http协议的信息，包含host、请求参数等 * @param method GET、PUT等 * @param requestCallback * @param responseExtractor 用来处理response，解析返回的参数并做类型转换 * */protected &lt;T&gt; T doExecute(URI url, @Nullable HttpMethod method, @Nullable RequestCallback requestCallback, @Nullable ResponseExtractor&lt;T&gt; responseExtractor) throws RestClientException &#123; Assert.notNull(url, &quot;URI is required&quot;); Assert.notNull(method, &quot;HttpMethod is required&quot;); ClientHttpResponse response = null; try &#123; // 这里获取http client。默认情况下是jdk提供的、可以通过setRequestFactory(ClientHttpRequestFactory requestFactory) // 方法来选择不同的http client工厂，默认情况是SimpleClientHttpRequestFactory，这个是jdk提供http client的工厂。如果要使用OkHttp，那就使用OkHttp3ClientHttpRequestFactory， // 从后面的代码可知，整个doExecute的核心就是这行代码，会根据返回的类来完成请求调用 ClientHttpRequest request = createRequest(url, method); if (requestCallback != null) &#123; requestCallback.doWithRequest(request); &#125; response = request.execute(); handleResponse(url, method, response); return (responseExtractor != null ? responseExtractor.extractData(response) : null); &#125; catch (IOException ex) &#123; String resource = url.toString(); String query = url.getRawQuery(); resource = (query != null ? resource.substring(0, resource.indexOf(&#x27;?&#x27;)) : resource); throw new ResourceAccessException(&quot;I/O error on &quot; + method.name() + &quot; request for \\&quot;&quot; + resource + &quot;\\&quot;: &quot; + ex.getMessage(), ex); &#125; finally &#123; if (response != null) &#123; response.close(); &#125; &#125;&#125; 看createRequest： 123456789// HttpAccessor, RestTemplate的父类protected ClientHttpRequest createRequest(URI url, HttpMethod method) throws IOException &#123; ClientHttpRequest request = getRequestFactory().createRequest(url, method); initialize(request); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;HTTP &quot; + method.name() + &quot; &quot; + url); &#125; return request;&#125; getRequestFactory被子类InterceptingHttpAccessor重写 123456789101112131415// InterceptingHttpAccessorpublic ClientHttpRequestFactory getRequestFactory() &#123; List&lt;ClientHttpRequestInterceptor&gt; interceptors = getInterceptors(); if (!CollectionUtils.isEmpty(interceptors)) &#123; ClientHttpRequestFactory factory = this.interceptingRequestFactory; if (factory == null) &#123; factory = new InterceptingClientHttpRequestFactory(super.getRequestFactory(), interceptors); this.interceptingRequestFactory = factory; &#125; return factory; &#125; else &#123; return super.getRequestFactory(); &#125;&#125; 这里终于出现了ClientHttpRequestInterceptor。 这里有两种情况 ClientHttpRequestInterceptor列表为空，这种情况会调用其父类的getRequestFactory方法，直接放回RestTemplate的ClientHttpRequestFactory对象，这个对象可以通过RestTemplate.setRequestFactory设置，默认是SimpleClientHttpRequestFactory ClientHttpRequestInterceptor列表不为空，这里会new一个InterceptingClientHttpRequestFactory对象，并把restTemplate.requestFactory作为参数传入，最后返回这个InterceptingClientHttpRequestFactory对象 这里看第二种情况，InterceptingClientHttpRequestFactory这个对象的createRequest会在方法调用时被执行，返回了一个InterceptingClientHttpRequest类型的对象。该类重点看executeInternal方法就好了 12345678910111213141516171819// InterceptingClientHttpRequest// 属性都是创建时传入的// requestFactory属性是从InterceptingClientHttpRequestFactory传入// 而InterceptingClientHttpRequestFactory的这个属性是从RestTemplate传入private final ClientHttpRequestFactory requestFactory;// interceptors属性是从InterceptingClientHttpRequestFactory传入// 而InterceptingClientHttpRequestFactory的这个属性是从RestTemplate传入private final List&lt;ClientHttpRequestInterceptor&gt; interceptors;private HttpMethod method;private URI uri;@Overrideprotected final ClientHttpResponse executeInternal(HttpHeaders headers, byte[] bufferedOutput) throws IOException &#123; InterceptingRequestExecution requestExecution = new InterceptingRequestExecution(); return requestExecution.execute(this, bufferedOutput);&#125; 这里的InterceptingRequestExecution类是InterceptingClientHttpRequest的内部类。看源码 1234567891011121314151617181920212223242526272829303132333435363738private class InterceptingRequestExecution implements ClientHttpRequestExecution &#123; private final Iterator&lt;ClientHttpRequestInterceptor&gt; iterator; public InterceptingRequestExecution() &#123; this.iterator = interceptors.iterator(); &#125; @Override public ClientHttpResponse execute(HttpRequest request, byte[] body) throws IOException &#123; if (this.iterator.hasNext()) &#123; ClientHttpRequestInterceptor nextInterceptor = this.iterator.next(); // 这里会传入没一个拦截都需要调用 // 如果想请求能到达下一个拦截，并且请求能进入到这里的else， // 必须在自己的拦截器内部手动的调用ClientHttpRequestExecution.execut方法 return nextInterceptor.intercept(request, body, this); &#125; else &#123; // 当最后一个拦截器的内部调用了ClientHttpRequestExecution.execut方法会进入到这里发送http请求 HttpMethod method = request.getMethod(); Assert.state(method != null, &quot;No standard HTTP method&quot;); // 会调用RestTempalte的requestFactory来创建一个ClientHttpRequest来处理http请求 // 随着ClientHttpRequest不同，会使用apache http、或者JDK的Client。默认是使用jdk的 ClientHttpRequest delegate = requestFactory.createRequest(request.getURI(), method); request.getHeaders().forEach((key, value) -&gt; delegate.getHeaders().addAll(key, value)); if (body.length &gt; 0) &#123; if (delegate instanceof StreamingHttpOutputMessage) &#123; StreamingHttpOutputMessage streamingOutputMessage = (StreamingHttpOutputMessage) delegate; streamingOutputMessage.setBody(outputStream -&gt; StreamUtils.copy(body, outputStream)); &#125; else &#123; StreamUtils.copy(body, delegate.getBody()); &#125; &#125; return delegate.execute(); &#125; &#125;&#125; 假设每一个拦截器内部都有手动的调用ClientHttpRequestExecution.execut方法，那么整个RestTemplate的拦截器的处理流程如下：","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"00-Raft协议","slug":"springcloud/00-Raft协议","date":"2021-11-27T12:00:02.000Z","updated":"2022-03-23T09:03:56.156Z","comments":true,"path":"blog/springcloud/00-Raft协议/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/00-Raft%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"Raft 全流程动画演示 Raft github Raft算法概述 三种角色 Raft将系统中的角色分为领导者（Leader）、跟从者（Follower）和候选人（Candidate）： Leader：负责日志的同步管理，处理来自客户端读写请求，与Follower保持heartBeat的联系； Follower：响应 Leader 的日志同步请求，响应Candidate的邀票请求，负责客户端的读请求，以及把客户端请求到Follower的事务转发（重定向）给Leader； Candidate：Leader选举过程中的临时角色。 Raft算法角色状态转换如下： Raft要求系统在任意时刻最多只有一个Leader，正常工作期间只有Leader和Followers。Raft算法将时间分为一个个的任期（term），每一个term的开始都是Leader选举。在成功选举Leader之后，Leader会在整个term内管理整个集群。如果Leader选举失败，该term就会因为没有Leader而结束。 Term Raft 算法将时间划分成为任意不同长度的任期（term）。任期用连续的数字进行表示。每一个任期的开始都是一次选举（election），一个或多个候选人会试图成为领导人。如果一个候选人赢得了选举，它就会在该任期的剩余时间担任领导人。在某些情况下，选票会被瓜分，有可能没有选出领导人，那么，将会开始另一个任期，并且立刻开始下一次选举。Raft 算法保证在给定的一个任期最多只有一个领导人。 RPC Raft 算法中服务器节点之间通信使用远程过程调用（RPC），并且基本的一致性算法只需要两种类型的 RPC，为了在服务器之间传输快照增加了第三种 RPC。 RequestVote RPC：候选人在选举期间发起。 AppendEntries RPC：领导人发起的一种心跳机制，复制日志也在该命令中完成。 InstallSnapshot RPC: 领导者使用该RPC来发送快照给太落后的追随者。 Leader选举Leader选举的过程Raft 使用心跳（heartbeat）触发Leader选举。当服务器启动时，初始化为Follower。Leader向所有Followers周期性发送heartbeat。如果Follower在选举超时时间内没有收到Leader的heartbeat，就会等待一段随机的时间后发起一次Leader选举。 每一个follower都有一个时钟，是一个随机的值，表示的是follower等待成为leader的时间，谁的时钟先跑完，则发起leader选举。 Follower将其当前term加一然后转换为Candidate。它首先给自己投票并且给集群中的其他服务器发送 RequestVote RPC。结果有以下三种情况： 赢得了多数的选票，成功选举为Leader； 收到了Leader的消息，表示有其它服务器已经抢先当选了Leader； 没有服务器赢得多数的选票，Leader选举失败，等待选举时间超时后发起下一次选举。 Leader选举的限制在Raft协议中，所有的日志条目都只会从Leader节点往Follower节点写入，且Leader节点上的日志只会增加，绝对不会删除或者覆盖。 这意味着Leader节点必须包含所有已经提交的日志，即能被选举为Leader的节点一定需要包含所有的已经提交的日志。因为日志只会从Leader向Follower传输，所以如果被选举出的Leader缺少已经Commit的日志，那么这些已经提交的日志就会丢失，显然这是不符合要求的。 这就是Leader选举的限制：能被选举成为Leader的节点，一定包含了所有已经提交的日志条目。 日志复制（保证数据一致性）日志复制的过程Leader选出后，就开始接收客户端的请求。Leader把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起 AppendEntries RPC复制日志条目。当这条日志被复制到大多数服务器上，Leader将这条日志应用到它的状态机并向客户端返回执行结果。 客户端的每一个请求都包含被复制状态机执行的指令。 leader把这个指令作为一条新的日志条目添加到日志中，然后并行发起 RPC 给其他的服务器，让他们复制这条信息。 假如这条日志被安全的复制，领导人就应用这条日志到自己的状态机中，并返回给客户端。 如果 follower 宕机或者运行缓慢或者丢包，leader会不断的重试，直到所有的 follower 最终都复制了所有的日志条目。 日志的组成日志由有序编号（log index）的日志条目组成。每个日志条目包含它被创建时的任期号（term）和用于状态机执行的命令。如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了。 上图显示，共有 8 条日志，提交了 7 条。提交的日志都将通过状态机持久化到磁盘中，防止宕机。 日志的一致性 日志复制的两条保证 如果不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的（原因：leader 最多在一个任期里的一个日志索引位置创建一条日志条目，日志条目在日志的位置从来不会改变）。 如果不同日志中的两个条目有着相同的索引和任期号，则它们之前的所有条目都是完全一样的（原因：每次 RPC 发送附加日志时，leader 会把这条日志条目的前面的日志的下标和任期号一起发送给 follower，如果 follower 发现和自己的日志不匹配，那么就拒绝接受这条日志，这个称之为一致性检查）。 日志的不正常情况 一般情况下，Leader和Followers的日志保持一致，因此 AppendEntries 一致性检查通常不会失败。然而，Leader崩溃可能会导致日志不一致：旧的Leader可能没有完全复制完日志中的所有条目。 下图阐述了一些Followers可能和新的Leader日志不同的情况。一个Follower可能会丢失掉Leader上的一些条目，也有可能包含一些Leader没有的条目，也有可能两者都会发生。丢失的或者多出来的条目可能会持续多个任期。 如何保证日志的正常复制Leader通过强制Followers复制它的日志来处理日志的不一致，Followers上的不一致的日志会被Leader的日志覆盖。Leader为了使Followers的日志同自己的一致，Leader需要找到Followers同它的日志一致的地方，然后覆盖Followers在该位置之后的条目。 具体的操作是：Leader会从后往前试，每次AppendEntries失败后尝试前一个日志条目，直到成功找到每个Follower的日志一致位置点（基于上述的两条保证），然后向后逐条覆盖Followers在该位置之后的条目。 总结一下就是：当 leader 和 follower 日志冲突的时候，leader 将校验 follower 最后一条日志是否和 leader 匹配，如果不匹配，将递减查询，直到匹配，匹配后，删除冲突的日志。这样就实现了主从日志的一致性。 安全性Raft增加了如下两条限制以保证安全性： 拥有最新的已提交的log entry的Follower才有资格成为leader。 Leader只能推进commit index来提交当前term的已经复制到大多数服务器上的日志，旧term日志的提交要等到提交当前term的日志来间接提交（log index 小于 commit index的日志被间接提交）。 日志压缩在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响可用性。Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃（以前的数据已经落盘了）。 每个副本独立的对自己的系统状态进行snapshot，并且只能对已经提交的日志记录进行snapshot。 当Leader要发给某个日志给落后太多的Follower，Leader会将snapshot发给Follower。或者当新加进一台机器时，也会发送snapshot给它。发送snapshot使用InstalledSnapshot RPC。 做snapshot既不要做的太频繁，否则消耗磁盘带宽， 也不要做的太不频繁，否则一旦节点重启需要回放大量日志，影响可用性。推荐当日志达到某个固定的大小做一次snapshot。 做一次snapshot可能耗时过长，会影响正常日志同步。可以通过使用copy-on-write技术避免snapshot过程影响正常日志同步。 与ZAB不同 投票阶段 zab中，每个节点都需要进行投票，而且每个节点都需要进行票的pk","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"00-Nacos注册中心架构","slug":"springcloud/00-Nacos注册中心架构","date":"2021-11-27T12:00:01.000Z","updated":"2022-03-23T09:03:56.103Z","comments":true,"path":"blog/springcloud/00-Nacos注册中心架构/","link":"","permalink":"http://sv.pointcut.cc/blog/springcloud/00-Nacos%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E6%9E%B6%E6%9E%84/","excerpt":"","text":"nacos官方文档 服务注册 Nacos Client会通过发送REST请求的方式向Nacos Server注册自己的服务，提供自 身的元数据，比如ip地址、端口等信息。Nacos Server接收到注册请求后，就会把这些元数据信 息存储在一个双层的内存Map中。 服务心跳 在服务注册后，Nacos Client会维护一个定时心跳来持续通知Nacos Server，说明服务一直处于可用状态，防止被剔除。默认5s发送一次心跳。 服务同步 Nacos Server集群之间会互相同步服务实例，用来保证服务信息的一致性。 服务发现 服务消费者(Nacos Client)在调用服务提供者的服务时，会发送一个REST请求给 Nacos Server，获取上面注册的服务清单，并且缓存在Nacos Client本地，同时会在Nacos Client本地开启一个定时任务定时拉取服务端最新的注册表信息更新到本地缓存 服务健康检查 Nacos Server会开启一个定时任务用来检查注册服务实例的健康情况，对于超过15s没有收到客户端心跳的实例会将它的healthy属性置为false(客户端服务发现时不会发现)，如果某个实例超过30秒没有收到心跳，直接剔除该实例(被剔除的实例如果恢复发送心跳则会重新注册) 服务注册表结构 该结构在代码中使用了双Map结构 123456789// NameSpace, GroupMap&lt;String, Map&lt;String, Service&gt;&gt; serviceMap = new ConcurrentHashMap&lt;&gt;();// Service Map&lt;String, Cluster&gt; clusterMap = new HashMap&lt;&gt;();//Clusterprivate Set&lt;Instance&gt; persistentInstances = new HashSet&lt;&gt;();private Set&lt;Instance&gt; ephemeralInstances = new HashSet&lt;&gt;(); 服务领域模型","categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"}]},{"title":"SpringBoot很好用的一组 Condition —— AllNestedConditions AnyNestedCondition NoneNestedConditions","slug":"springboot/SpringBoot很好用的一组 Condition —— AllNestedConditions AnyNestedCondition NoneNestedConditions","date":"2021-11-26T12:00:09.000Z","updated":"2022-03-23T09:03:56.062Z","comments":true,"path":"blog/springboot/SpringBoot很好用的一组 Condition —— AllNestedConditions AnyNestedCondition NoneNestedConditions/","link":"","permalink":"http://sv.pointcut.cc/blog/springboot/SpringBoot%E5%BE%88%E5%A5%BD%E7%94%A8%E7%9A%84%E4%B8%80%E7%BB%84%20Condition%20%E2%80%94%E2%80%94%20AllNestedConditions%20AnyNestedCondition%20NoneNestedConditions/","excerpt":"","text":"【源码】Spring —— Condition 条件匹配解读 【源码】SpringBoot 对 Condition 的拓展 —— @ConditionalOnXxx系列注解 123456789101112关于什么叫做 SpringBoot 式 API 设计，这是我个人的理解：Spring 式 API，java API 设计的天花板，简单、优雅，最重要的是拥有无限的生命力（易拓展，不然也不会有 SpringBoot 了），“简单” 是指逻辑的实现相对简单（设计可一点都不简单）而且代码伴随着大量的注解，因此想潜心看还是有可能看懂的SpringBoot 式 API，同样是 java API 设计的天花板，但是逻辑实现是真的复杂，且伴随着 天马行空 的想象力，编码风格逐渐向 函数式编程 靠拢，且因为代码注解相对 Spring 少的多了，因此我认为逻辑能力如果不够强且没有足够的时间，真的看不懂一些 API 的实现逻辑（反正我看不懂，光一个Binder 我看了一周没看明白，放弃了） AllNestedConditions， 该类本身就是一个 Condition，因此可以直接作为条件注解 @Conditonal 的 value，但是它可以包含一组 Condition，且当前目标类要 match 这些所有 Conditon，才会被注册进去 如果不好理解，直接看示例 12345678910111213141516171819202122232425262728293031323334353637@Configurationpublic class ConditionConfig &#123; @Bean @Conditional(BorCCondition.class) public BorC borC() &#123; return new BorC(); &#125; static class BorCCondition extends AllNestedConditions &#123; public BorCCondition() &#123; super(ConfigurationPhase.REGISTER_BEAN); &#125; @ConditionalOnBean(B.class) class BCondition &#123; &#125; @ConditionalOnBean(C.class) class CCondition &#123; &#125; &#125; @Configuration static class Config &#123; @Bean public B b() &#123; return new B(); &#125; &#125;&#125; 示例中的 BorCCondition 包含了一组 Conditon，其中 BCondition 是容器中要有 B 的 bean实例，CConditon 亦然，同时 BorCCondition 是一个 AllNestedConditions，因此只有当 BCondition 和 CConditon 全都 match 时，BorC 的 bean实例 才会被注册到容器中 1至于为什么命名 BorCCondition 是因为我一开始写的 demo 是 AnyNestedCondition 如上示例中，BorC 的 bean实例 不会被注册，可以自己动手试试 AnyNestedCondition这就不难理解了，只要 match 其中任意 Condition，目标实例就会被注册 NoneNestedConditions只有不 match 所有 Conditon，目标实例才会被注册","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://sv.pointcut.cc/tags/springboot/"}]},{"title":"Spring Boot 2 官方指导手册译文","slug":"springboot/Spring Boot 2 官方指导手册译文","date":"2021-11-26T12:00:08.000Z","updated":"2022-03-23T09:03:56.062Z","comments":true,"path":"blog/springboot/Spring Boot 2 官方指导手册译文/","link":"","permalink":"http://sv.pointcut.cc/blog/springboot/Spring%20Boot%202%20%E5%AE%98%E6%96%B9%E6%8C%87%E5%AF%BC%E6%89%8B%E5%86%8C%E8%AF%91%E6%96%87/","excerpt":"","text":"官方文档 Spring Boot 2.3.9.RELEASE Reference Documentation Gradle 安装Spring Boot 与 Gradle 4 兼容。如果您还没有安装 Gradle，您可以按照 gradle.org 上的说明进行操作。 Spring Boot 依赖关系可以使用 org.springframework.boot group 声明。通常，你的项目向一个或多个启动器声明依赖关系。Spring Boot 提供了一个有用的 Gradle 插件，它可以用来简化依赖项声明并创建可执行的 jar。 Gradle Wrapper 当您需要构建一个项目时，Gradle Wrapper 提供了一种“获取”Gradle 的好方法。它是一个小脚本和库，您可以在代码旁边提交，以引导构建过程。有关详细信息，请参阅 docs.gradle.org&#x2F;4.2.1&#x2F;userguide&#x2F;gradle_wrapper.html。 下面的示例显示一个典型的 build.gradle 文件： 12345678910111213141516171819plugins &#123; id &#x27;org.springframework.boot&#x27; version &#x27;2.0.2.RELEASE&#x27; id &#x27;java&#x27;&#125;jar &#123; baseName = &#x27;myproject&#x27; version = &#x27;0.0.1-SNAPSHOT&#x27;&#125;repositories &#123; jcenter()&#125;dependencies &#123; compile(&quot;org.springframework.boot:spring-boot-starter-web&quot;) testCompile(&quot;org.springframework.boot:spring-boot-starter-test&quot;)&#125; 从 Spring Boot 的一个早期的版本升级如果您正在从早期的 Spring Boot 版本升级，请检查提供详细的升级说明的“迁移指南”。还要检查发行说明，以获得每个版本的“新的和值得注意的”特性。 开发你的第一个 Spring Boot 应用程序本节描述如何开发一个简单的“Hello World!”web 应用程序突出了 Spring Boot 的一些关键特性。我们使用 Maven 来构建这个项目，因为大多数 IDE 都支持它。 spring.io 网站包含许多使用 Spring Boot 的“入门”指南。如果你需要解决一个具体的问题，先检查一下。 要简化下面的步骤，你可以去 start.spring.io 并从依赖关系搜索器中选择“Web”启动器。这样做会生成一个新的项目结构，这样您就可以立即开始编写代码。查看 Spring Initializr 文档了解更多细节。 在开始之前，打开一个终端并运行以下命令，以确保安装了有效的 Java 和 Maven 版本： 1234$ java -versionjava version &quot;1.8.0_102&quot;Java(TM) SE Runtime Environment (build 1.8.0_102-b14)Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode) 1234$ mvn -vApache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T16:41:47+00:00)Maven home: /usr/local/Cellar/maven/3.3.9/libexecJava version: 1.8.0_102, vendor: Oracle Corporation 此示例需要在其自己的文件夹中创建。随后的指令假设您已经创建了一个合适的文件夹，并且它是您当前的目录。 创建 POM我们需要从创建一个 Maven pom.xml 文件开始。pom.xml 是用于构建你的项目的“食谱”。打开你喜欢的文本编辑器并添加如下内容： 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;myproject&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;!-- Additional lines to be added here... --&gt;&lt;/project&gt; 前面的清单应该给您一个生效的构建。您可以通过运行 mvn package 来测试它（现在，您可以忽略“jar will be empty - no content was marked for inclusion!”警告）。 此时，您可以将项目导入到 IDE 中（大多数现代 Java IDE 都包含对 Maven 的内置支持）。为了简单起见，我们继续使用这个示例的纯文本编辑器。 添加类路径依赖项Spring Boot 提供了一些“启动器”让你可以添加 jar 到你的类路径。我们的示例应用程序已经在 POM 的 parent 部分使用了 spring-boot-starter-parent。spring-boot-starter-parent 是一个特殊的启动器，它提供了有用的 Maven 默认值。它还提供了一个 dependency-management 部分，以便对于“有福的”依赖项你可以省略版本标记。 其它“启动器”提供在开发特定类型的应用程序时可能需要的依赖项。由于我们正在开发一个 web 应用程序，所以我们添加了一个 spring-boot-starter-web 依赖项。在此之前，我们可以通过运行以下命令查看当前所拥有的内容： 123$ mvn dependency:tree[INFO] com.example:myproject:jar:0.0.1-SNAPSHOT mvn dependency:tree 命令打印你的项目依赖项的树型表示。您可以看到 spring-boot-starter-parent 本身不提供依赖项。要添加必要的依赖项，请编辑 pom.xml 并将 spring-boot-starter-web 依赖项立即添加到 parent 部分下方： 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 如果你再次运行 mvn dependency:tree，您可以看到现在有许多额外的依赖项，包括 Tomcat web 服务器和 Spring Boot 本身。 编写你的代码要完成我们的应用程序，我们需要创建一个 Java 文件。默认情况下，Maven 从 src/main/java 编译源代码，因此您需要创建该文件夹结构，然后添加一个名为 src/main/java/Example.java 的文件，包含以下代码： 123456789101112131415161718import org.springframework.boot.*;import org.springframework.boot.autoconfigure.*;import org.springframework.web.bind.annotation.*;@RestController@EnableAutoConfigurationpublic class Example &#123; @RequestMapping(&quot;/&quot;) String home() &#123; return &quot;Hello World!&quot;; &#125; public static void main(String[] args) throws Exception &#123; SpringApplication.run(Example.class, args); &#125;&#125; 虽然这里没有多少代码，但仍有很多工作要做。我们将在接下来的几节中讨论重要的部分。 @RestController 和 @RequestMapping 注解 在我们的示例类上的第一个注解是 @RestController。这被称为构造型注解。它为阅读代码的人们提供了一些提示，并且为 Spring 提供了一个特定的角色。在本例中，我们的类是一个web @Controller，所以 Spring 在处理传入 web 请求时考虑它。 @RequestMapping 注解提供了“路由”信息。它告诉 Spring，任何带有 &#x2F; 路径的 HTTP 请求都应该映射到 home 方法。@RestController 注解告诉 Spring 将生成的字符串直接呈现给调用者。 @RestController 和 @RequestMapping 注解是 Spring MVC 注解。（它们不是特定于 Spring Boot 的。）有关更多详细信息，请参见 Spring 参考文档中的 MVC 部分。 @EnableAutoConfiguration 注解 第二个类级别注解是 @EnableAutoConfiguration。这个注解告诉 Spring Boot 去“猜测”您想如何配置 Spring，这基于您添加的 jar 依赖项。因为 spring-boot-starter-web 添加了 Tomcat 和 Spring MVC，所以自动配置假设您正在开发一个 web 应用程序，并相应地设置 Spring。 启动器和自动配置 自动配置被设计成与“启动器”很好地工作，但是这两个概念并没有直接关联。您可以自由地选择除启动器之外的 jar 依赖项。Spring Boot 仍然尽力地自动配置您的应用程序。 main 方法 我们的应用程序的最后一部分是 main 方法。这只是遵循应用程序入口点的 Java 约定的标准方法。通过调用 run，我们的 main 方法委托给 Spring Boot 的 SpringApplication 类。SpringApplication 引导我们的应用程序启动 Spring，而 Spring 又启动了自动配置的 Tomcat web 服务器。我们需要传递 Example.class 作为 run 方法的参数，以告诉 SpringApplication 它是主要的 Spring 组件。args 数组也被传递，以暴露任何命令行参数。 运行这个示例此时，您的应用程序应该可以工作了。因为您使用了 spring-boot-starter-parent POM，所以您有一个有用的 run 目标，您可以使用它来启动应用程序。 在根项目目录键入 mvn spring-boot:run 以启动应用程序。您应该会看到类似如下的输出： 12345678910111213$ mvn spring-boot:run . ____ _ __ _ _ /\\\\ / ___&#x27;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\( ( )\\___ | &#x27;_ | &#x27;_| | &#x27;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) &#x27; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.0.2.RELEASE)....... . . ........ . . . (log output here)....... . . ......... Started Example in 2.222 seconds (JVM running for 6.514) 如果你打开一个浏览器，访问 http://localhost:8080/，你应该会看见如下输出： 1Hello World! 要优雅地退出应用程序，按 ctrl-c。 创建一个可执行的 jar我们通过创建一个可以在生产中运行的完全自包含的可执行 jar 文件来完成我们的示例。可执行 jar（有时称为“fat jars”）是包含您的编译类的存档文件，以及您的代码需要运行所需的所有 jar 依赖项。 可执行的 jar 和 Java Java 没有提供加载嵌套 jar 文件的标准方法（jar 文件本身包含在一个 jar 中）。如果您希望分发一个自包含的应用程序，这可能会有问题。 为了解决这个问题，许多开发人员使用“uber”jar。一个 uber jar 将所有应用程序依赖项的所有类打包成一个归档文件。这种方法的问题在于，很难看到应用程序中有哪些库。如果在多个 jar 中使用相同的文件名（但使用不同的内容），也会有问题。 Spring Boot 采用了一种不同的方法，让您可以直接嵌套 jar。 要创建一个可执行 jar，我们需要将 spring-boot-maven-plugin 添加到我们的 pom.xml 中。要做到这一点，请在 dependencies 部分下面插入以下几行： 12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; spring-boot-starter-parent POM 包含 &lt;executions&gt; 配置以绑定 repackage 目标。如果您不使用父 POM，您需要自己声明这个配置。有关详细信息，请参见插件文档。 保存你的 pom.xml 并在命令行运行 mvn package，如下所示： 123456789101112131415$ mvn package[INFO] Scanning for projects...[INFO][INFO] ------------------------------------------------------------------------[INFO] Building myproject 0.0.1-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] .... ..[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ myproject ---[INFO] Building jar: /Users/developer/example/spring-boot-example/target/myproject-0.0.1-SNAPSHOT.jar[INFO][INFO] --- spring-boot-maven-plugin:2.0.2.RELEASE:repackage (default) @ myproject ---[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------ 如果你查看 target 目录内部，你应该会看到 myproject-0.0.1-SNAPSHOT.jar，这个文件应该差不多 10MB 大小。如果你想偷看里面的内容，你可以使用 jar tvf，如下所示： 1$ jar tvf target/myproject-0.0.1-SNAPSHOT.jar 你应该也会看见一个更小的名为 myproject-0.0.1-SNAPSHOT.jar.original 的文件在 target 目录下。这是在 Spring Boot 重新打包之前，Maven 创建的初始 jar 文件 。 要运行该应用程序，使用 java -jar 命令，如下所示： 12345678910111213$ java -jar target/myproject-0.0.1-SNAPSHOT.jar . ____ _ __ _ _ /\\\\ / ___&#x27;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\( ( )\\___ | &#x27;_ | &#x27;_| | &#x27;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) &#x27; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.0.2.RELEASE)....... . . ........ . . . (log output here)....... . . ......... Started Example in 2.536 seconds (JVM running for 2.864) 像前面一样，要退出该应用程序，按 ctrl-c。 下一步该读什么希望这一节提供了一些 Spring Boot 基础知识，并帮助您编写自己的应用程序。如果您是面向任务的开发人员，您可能想跳到 spring.io 并查看一些入门指南来解决具体的“我如何用 Spring 实现它？”问题。我们还有 Spring Boot 特有的“如何做”参考文档。 Spring Boot 存储库也有一些您可以运行的示例。这些示例与代码的其余部分无关（也就是说，您不需要构建其余的代码来运行或使用示例）。 否则，下一个逻辑步骤是阅读 *第三部分：“使用 Spring Boot”*。如果你真的很不耐烦，你也可以跳过，读一下 *Spring Boot 特性*。 使用 Spring Boot本节将详细讨论如何使用 Spring Boot。它涵盖了诸如构建系统、自动配置以及如何运行应用程序等主题。我们还介绍了一些 Spring Boot 最佳实践。尽管 Spring Boot 没有什么特别之处（它只是您可以使用的另一个库），但是有一些建议，在接下来的时候，使您的开发过程更容易一些。 如果您开始使用 Spring Boot，那么您应该在深入这一节之前阅读*入门*指南。 构建系统强烈建议您选择一个支持依赖管理的构建系统，并且可以使用发布到“Maven Central”存储库的工件。我们建议您选择 Maven 或 Gradle。可以让 Spring Boot 与其他构建系统（例如 Ant）一起工作，但是它们并不是特别受支持。 依赖管理Spring Boot 的每一个版本都提供了它所支持的一个被整理的依赖项列表。实际上，在构建配置中，您不需要为这些依赖项提供一个版本，因为 Spring Boot 为您管理这些依赖项。当您升级 Spring Boot 本身时，这些依赖项也会以一致的方式升级。 如果需要，您仍然可以指定一个版本并覆盖 Spring Boot 的建议。 策划列表包含了 Spring Boot 可以使用的所有 spring 模块以及第三方库的改进列表。这个列表可以作为一个标准的材料清单（spring-boot-dependencies），可以适用于 Maven 和 Gradle。 Spring Boot 的每个版本都与 Spring 框架的一个基本版本相关联。我们强烈建议您不要指定它的版本。 MavenMaven 用户可以从 spring-boot-starter-parent 项目继承来获得合理的默认值。父项目提供了以下特性： Java 1.8 作为默认编译等级 UTF-8 源码编码 一个依赖管理部分，继承自 spring-boot-dependencies pom，管理通用依赖项的版本。这个依赖管理允许你在自己的 pom 中使用这些依赖项时省略 &lt;version&gt; 标签 明智的资源过滤 明智的插件配置（exec plugin、Git commit ID 和 shade） 明智的用于 application.properties 和 application.yml 的包含特定 profile 文件的资源过滤（比如：application-dev.properties 和 application-dev.yml） 注意，由于 application.properties 和 application.yml 文件接受 Spring 风格的占位符 $&#123;…&#125;，Maven 过滤被更改为使用 @..@ 占位符。（你可以通过设置一个名为 resource.delimiter 的 Maven 属性来覆盖它。） 继承父启动器 要配置你的项目继承自 spring-boot-starter-parent，设置 parent 如下所示： 123456&lt;!-- Inherit defaults from Spring Boot --&gt;&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt;&lt;/parent&gt; 您应该仅在此依赖项上指定 Spring Boot 版本号。如果您导入额外的启动器，您可以安全地省略版本号。 通过这种设置，您还可以通过在自己的项目中覆盖一个属性来覆盖单个依赖项。例如，要升级到另一个 Spring Data release train，您需要将以下内容添加到您的 pom.xml： 123&lt;properties&gt; &lt;spring-data-releasetrain.version&gt;Fowler-SR2&lt;/spring-data-releasetrain.version&gt;&lt;/properties&gt; 检查 spring-boot-dependencies pom 获取支持的属性列表。 使用没有父 POM 的 Spring Boot 不是每个人都喜欢继承自 spring-boot-starter-parent POM。您可能有您自己的企业标准父类，您需要使用它们，或者您可能倾向于显式地声明所有的 Maven 配置。 如果你不想使用 spring-boot-starter-parent，您仍然可以使用 scope=import 依赖项来保持依赖管理（但不是插件管理）的好处： 123456789101112&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;!-- Import dependency management from Spring Boot --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 前面的示例设置不允许您使用属性来覆盖单个依赖项，如上所述。要实现相同的结果，您需要在 spring-boot-dependencies 条目之前在您的项目的依赖项管理中添加一个条目。例如，要升级到另一个 Spring Data release train，您可以将以下元素添加到您的 pom.xml： 12345678910111213141516171819&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- Override Spring Data release train provided by Spring Boot --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-releasetrain&lt;/artifactId&gt; &lt;version&gt;Fowler-SR2&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 在前面的示例中，我们指定了一个 BOM，但是任何依赖类型都可以以相同的方式被覆盖。 使用 Spring Boot Maven 插件 Spring Boot 包含了一个 Maven 插件可以将项目打包成一个可执行的 jar。如果你想使用它，添加该插件到你的 &lt;plugins&gt; 部分，如下示例所示： 12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 如果你使用 Spring Boot 启动器父 pom，你只需要添加该插件。没有其它需要配置的东西，除非你想修改父 pom 中定义的设置。 Gradle要了解搭配 Gradle 来使用 Spring Boot，请参考 Spring Boot 的 Gradle 插件文档： 参考手册（HTML 和 PDF） API 启动器启动器是一组方便的依赖关系描述符，您可以在应用程序中包括这些描述符。您可以获取为所有 Spring 和相关技术提供一站式服务，而无需通过示例代码和复制粘贴的依赖描述符来进行搜索。例如，如果您想要开始使用 Spring 和 JPA 进行数据库访问，请在项目中包含 spring-boot-starter-data-jpa 依赖项。 启动器包含大量的依赖项，您需要通过一个一致的、受支持的管理传递依赖集来快速地启动项目并运行。 名字里包含了什么 所有官方的启动器都遵循类似的命名模式：spring-boot-starter-*，其中 * 是一种特殊类型的应用程序。这种命名结构旨在帮助您找到启动器。许多 IDE 中的 Maven 集成让您可以通过名称搜索依赖项。例如，如果安装了适当的 Eclipse 或 STS 插件，您可以在 POM 编辑器中按下 ctrl-space，并在一个完整的列表中键入“spring-boot-starter”。 正如在“创建你自己的启动器”部分中所解释的，第三方启动器不应该从 spring-boot 开始，因为它是为官方 Spring Boot 工件预留的。相反，第三方启动器通常以项目的名称开始。例如，一个名为 thirdpartyproject 的第三方启动项目通常会被命名为 thirdpartyproject-spring-boot-starter。 下面列举的应用程序启动器，由 Spring Boot 提供，位于 org.springframework.boot group 下： 名称 描述 Pom spring-boot-starter 核心启动器，保留自动配置支持、日志和 YAML Pom spring-boot-starter-activemq 用于使用 Apache ActiveMQ 进行 JMS 消息传递 Pom spring-boot-starter-amqp 用于使用 Spring AMQP 和 Rabbit MQ Pom spring-boot-starter-aop 用于使用 Spring AOP 和 AspectJ 进行面向切面编程 Pom spring-boot-starter-artemis 用于使用 Apache Artemis 进行 JMS 消息传递 Pom spring-boot-starter-batch 用于使用 Spring Batch Pom spring-boot-starter-cache 用于使用 Spring Framework 的缓存支持 Pom spring-boot-starter-cloud-connectors 用于使用 Spring Cloud Connectors，它简化了与云平台里的服务的连接，如 Cloud Foundry 和 Heroku Pom spring-boot-starter-data-cassandra 用于使用 Cassandra 分布式数据库和 Spring Data Cassandra Pom spring-boot-starter-data-cassandra-reactive 用于使用 Cassandra 分布式数据库和 Spring Data Cassandra Reactive Pom spring-boot-starter-data-couchbase 用于使用 Couchbase 面向文档型数据库和 Spring Data Couchbase Pom spring-boot-starter-data-couchbase-reactive 用于使用 Couchbase 面向文档型数据库和 Spring Data Couchbase Reactive Pom spring-boot-starter-data-elasticsearch 用于使用 Elasticsearch 搜索分析引擎和 Spring Data Elasticsearch Pom spring-boot-starter-data-jpa 用于使用基于 Hibernate 的 Spring Data JPA Pom spring-boot-starter-data-ldap 用于使用 Spring Data LDAP Pom spring-boot-starter-data-mongodb 用于使用 MongoDB 面向文档型数据库和 Spring Data MongoDB Pom spring-boot-starter-data-mongodb-reactive 用于使用 MongoDB 面向文档型数据库和 Spring Data MongoDB Reactive Pom spring-boot-starter-data-neo4j 用于使用 Neo4j 图形数据库和 Spring Data Neo4j Pom spring-boot-starter-data-redis 用于使用 Redis 键值对数据存储，以及 Spring Data Redis 和 Lettuce 客户端 Pom spring-boot-starter-data-redis-reactive 用于使用 Redis 键值对数据存储，以及 Spring Data Redis reactive 和 Lettuce 客户端 Pom spring-boot-starter-data-rest 用于使用 Spring Data Rest 通过 REST 暴露 Spring Data 仓库 Pom spring-boot-starter-data-solr 用于使用 Apache Solr 搜索平台以及 Spring Data Solr Pom spring-boot-starter-freemarker 用于使用 FreeMarker 视图构建 MVC web 应用程序 Pom spring-boot-starter-groovy-templates 用于使用 Groovy Templates 视图构建 MVC web 应用程序 Pom spring-boot-starter-hateoas 用于构建基于超媒体的 RESTful web 应用程序，使用 Spring MVC 和 Spring HATEOAS Pom spring-boot-starter-integration 用于使用 Spring Integration Pom spring-boot-starter-jdbc 用于使用 JDBC 以及 HikariCP 连接池 Pom spring-boot-starter-jersey 用于构建 RESTful web 应用程序，使用 JAX-RS 和 Jersey。spring-boot-starter-web 的另一种选择。 Pom spring-boot-starter-jooq 用于使用 jOOQ 访问 SQL 数据库。spring-boot-starter-data-jpa 或 spring-boot-starter-jdbc 的另一种选择。 Pom spring-boot-starter-json 用于读写 json Pom spring-boot-starter-jta-atomikos 用于使用 Atomikos JTA 事务 Pom spring-boot-starter-jta-bitronix 用于使用 Bitronix JTA 事务 Pom spring-boot-starter-jta-narayana 用于使用 Narayana JTA 事务 Pom spring-boot-starter-mail 用于使用 Java Mail 和 Spring Framework 的 email 发送支持 Pom spring-boot-starter-mustache 用于使用 Mustache 视图构建 web 应用程序 Pom spring-boot-starter-quartz 用于使用 Quartz 调度程序 Pom spring-boot-starter-security 用于使用 Spring Security Pom spring-boot-starter-test 用于测试 Spring Boot 应用程序，包括 JUnit、Hamcrest 和 Mockito 库 Pom spring-boot-starter-thymeleaf 用于使用 Thymeleaf 视图构建 MVC web 应用程序 Pom spring-boot-starter-validation 用于使用基于 Hibernate Validator 的 Java Bean 校验程序 Pom spring-boot-starter-web 用于构建 web，包括 RESTful 和使用 Spring MVC 的应用程序。使用 Tomcat 作为默认内置容器 Pom spring-boot-starter-web-services 用于使用 Spring Web Services Pom spring-boot-starter-webflux 用于构建 WebFlux 应用程序，使用 Spring Framework 的 Reactive Web 支持 Pom spring-boot-starter-websocket 用于构建 WebSocket 应用程序，使用 Spring Framework 的 WebSocket 支持 Pom 除了应用程序启动器之外，下面的启动器还可以用于添加*生产就绪*特性： 名称 描述 Pom spring-boot-starter-actuator 用于使用 Spring Boot 的 Actuator 提供生产就绪特性，帮助您监视和管理应用程序 Pom 最后，Spring Boot 还包括以下启动器，如果您想要排除或交换特定的技术方面，可以使用： 名称 描述 Pom spring-boot-starter-jetty 用于使用 Jetty 作为内置 servlet 容器。spring-boot-starter-tomcat 的另一种选择。 Pom spring-boot-starter-log4j2 用于使用 Log4j2 记录日志。spring-boot-starter-logging 的另一种选择。 Pom spring-boot-starter-logging 用于logging 使用 Logback. Default logging starter Pom spring-boot-starter-reactor-netty 用于使用 Reactor Netty 作为内置响应式 HTTP 服务器。 Pom spring-boot-starter-tomcat 用于使用 Tomcat 作为内置 servlet 容器。spring-boot-starter-web 使用的默认 servlet 容器启动器。 Pom spring-boot-starter-undertow 用于使用 Undertow 作为内置 servlet 容器。spring-boot-starter-tomcat 的另一种选择。 Pom 有关附加社区贡献的启动器列表，请参阅 Github 上的 spring-boot-starters 模块中的自述文件。 组织你的代码Spring Boot 不需要任何特定的代码布局来工作。然而，有一些最佳实践可以提供帮助。 使用“default”包当一个类不包含包声明时，它被认为是在“default 包”中。使用“default 包”通常是不鼓励的，应该避免使用。它可能会导致使用 @ComponentScan、@EntityScan 或 @SpringBootApplication 注解的 Spring Boot 应用程序的特定问题，因为每个 jar 的每个类都被读取。 我们建议您遵循 Java 推荐的包命名约定，并使用一个反向的域名（例如，com.example.project）。 定位主应用程序类我们通常建议您在其他类之上的根包中定位主应用程序类。@SpringBootApplication 注解通常放在主类上，它隐式地为某些项定义了一个基本的“搜索包”。例如，如果您正在编写一个 JPA 应用程序，则使用 @SpringBootApplication 注解类的包来搜索 @Entity 项。使用根包也允许组件扫描只应用于您的项目。 如果您不想使用 @SpringBootApplication，那么 @EnableAutoConfiguration 和 @ComponentScan 注解将定义该行为，因此您也可以使用它。 下面的清单展示了一个典型的布局： 12345678910111213141516com +- example +- myapplication +- Application.java | +- customer | +- Customer.java | +- CustomerController.java | +- CustomerService.java | +- CustomerRepository.java | +- order +- Order.java +- OrderController.java +- OrderService.java +- OrderRepository.java Application.java 文件会声明 main 方法，以及基础的 @SpringBootApplication，如下所示： 12345678910111213package com.example.myapplication;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 配置类Spring Boot 支持基于 Java 的配置。虽然可以使用 XML 源码的 SpringApplication，但是我们通常建议您的主源是一个 @Configuration 类。通常，定义主方法的类是一个很好的候选者，作为主要的 @Configuration。 许多 Spring 配置示例已经在 Internet 上发布，它们使用 XML 配置。如果可能，始终尝试使用等效的基于 Java 的配置。搜索 Enable* 注解可能是一个很好的起点。 导入附加的配置类你不需要把你所有的 @Configuration 放入单一的类中。@Import 注解可用来导入附加的配置类。替代地，你可以使用 @ComponentScan 自动拾取所有的 Spring 组件，包括 @Configuration 类。 导入 XML 配置如果你必须使用基于 XML 的配置，我们推荐你仍然从 @Configuration 类开始。你可以使用 @ImportResource 注解加载 XML 配置文件。 自动配置Spring Boot 自动配置尝试根据您添加的 jar 依赖项自动配置 Spring 应用程序。例如，如果 HSQLDB 在您的类路径上，并且您没有手动配置任何数据库连接 bean，那么 Spring Boot 将自动配置内存数据库。 通过将 @EnableAutoConfiguration 或 @SpringBootApplication 注解添加到您的一个 @Configuration 类中，您需要选择加入到自动配置（原文：You need to opt-in to auto-configuration）。 您应该只添加一个 @SpringBootApplication 或 @EnableAutoConfiguration 注解。我们通常建议只在主 @Configuration 类中添加其中一个。 逐步取代自动配置自动配置是非侵入性的。在任何时候，您都可以开始定义自己的配置来替换自动配置的特定部分。例如，如果您添加了自己的 DataSource bean，默认的嵌入式数据库支持就会被取代。 如果您需要了解当前正在应用的自动配置，以及为什么。使用 --debug 开关启动应用程序。这样做可以为一些核心日志记录器开始调试日志，并将条件报告记录到控制台。 禁用特定的自动配置类如果你发现你不想应用的特定自动配置类，你可以使用 @EnableAutoConfiguration 的 exclude 属性禁用它们，如下示例所示： 12345678import org.springframework.boot.autoconfigure.*;import org.springframework.boot.autoconfigure.jdbc.*;import org.springframework.context.annotation.*;@Configuration@EnableAutoConfiguration(exclude=&#123;DataSourceAutoConfiguration.class&#125;)public class MyConfiguration &#123;&#125; 如果那个类不在类路径，你可以使用这个注解的 excludeName 属性，指定完整的全类名。最后，您还可以通过使用 spring.autoconfigure.exclude property 来控制需要排除的自动配置类的列表。 你可以同时使用注解级别和 property 定义排除项。 Spring Bean 和依赖注入您可以自由使用任何标准 Spring 框架技术来定义 bean 及其注入的依赖项。为了简单起见，我们经常发现使用 @ComponentScan（找到您的 bean）和使用 @Autowired（进行构造函数注入）工作得很好。 如果按照上面建议的方式构造代码（在根包中定位应用程序类），可以不带任何参数添加 @ComponentScan。所有应用程序组件（@Component、@Service、@Repository、@Controller 等）都自动注册为 Spring bean。 下面的示例显示了使用构造函数注入来获得所需的 RiskAssessor Bean 的 @Service Bean： 123456789101112131415161718package com.example.service;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;@Servicepublic class DatabaseAccountService implements AccountService &#123; private final RiskAssessor riskAssessor; @Autowired public DatabaseAccountService(RiskAssessor riskAssessor) &#123; this.riskAssessor = riskAssessor; &#125; // ...&#125; 如果一个 Bean 只有一个构造函数，你可以省略 @Autowired，如下示例所示： 123456789101112@Servicepublic class DatabaseAccountService implements AccountService &#123; private final RiskAssessor riskAssessor; public DatabaseAccountService(RiskAssessor riskAssessor) &#123; this.riskAssessor = riskAssessor; &#125; // ...&#125; 注意，使用构造函数注入时，可以将 riskAssessor 字段标记为 final，表示它不能在之后被修改。 使用 @SpringBootApplication 注解许多 Spring Boot 开发者喜欢在他们的应用中使用自动配置、组件扫描，并且能够在他们的“application class”上定义额外配置。单个 @SpringBootApplication 注解即可以用来开启上述三个特性，即： @EnableAutoConfiguration：开启 Spring Boot 的自动配置机制 @ComponentScan：在应用程序的包上开启 @Component 扫描（查看最佳实践） @Configuration：允许在上下文注册额外的 bean，或者导入额外的配置类 @SpringBootApplication 相当于使用默认属性的 @Configuration、@EnableAutoConfiguration 和 @ComponentScan，如下示例所示： 12345678910111213package com.example.myapplication;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplication // same as @Configuration @EnableAutoConfiguration @ComponentScanpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; @SpringBootApplication 也提供了别名去定制 @EnableAutoConfiguration 和 @ComponentScan 的属性。 这些特性都不是强制性的，您可以选择用它支持的任何特性来替换这个注解。例如，您可能不想在应用程序中使用组件扫描： 1234567891011121314151617package com.example.myapplication;import org.springframework.boot.SpringApplication;import org.springframework.context.annotation.ComponentScanimport org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Import;@Configuration@EnableAutoConfiguration@Import(&#123; MyConfig.class, MyAnotherConfig.class &#125;)public class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 在本例中，应用程序和其他 Spring Boot 应用程序一样，只是没有自动检测到 @Component 注解的类，而用户定义的 bean 是显式导入的（参见 @Import）。 运行你的应用程序将您的应用程序打包为 jar 并使用嵌入式 HTTP 服务器的最大优点之一是，您可以像对待其他应用程序一样运行应用程序。调试 Spring Boot 应用程序也很简单。您不需要任何特殊的 IDE 插件或扩展。 本节只讨论基于 jar 的打包。如果您选择将应用程序打包为 war 文件，则应该参考服务器和 IDE 文档。 从 IDE 运行您可以从 IDE 运行 Spring Boot 应用程序，作为简单的 Java 应用程序。但是，您首先需要导入您的项目。导入步骤取决于您的 IDE 和构建系统。大多数 IDE 可以直接导入 Maven 项目。例如，Eclipse 用户可以选择 File 菜单的 Import… → Existing Maven Projects。 如果不能直接将项目导入到 IDE 中，那么可以使用构建插件生成 IDE 元数据。Maven 包含 Eclipse 和 IDEA 的插件。Gradle 提供各种 IDE 的插件。 如果您不小心运行了两次 web 应用程序，您会看到一个“Port already in use”错误。STS 用户可以使用 Relaunch 按钮而不是 Run 按钮来确保任何现有实例都已关闭。 作为打包应用程序运行如果您使用 Spring Boot Maven 或 Gradle 插件来创建一个可执行 jar，您可以使用 java -jar 运行您的应用程序，如下面的例子所示： 1$ java -jar target/myapplication-0.0.1-SNAPSHOT.jar 还可以运行具有远程调试支持的打包应用程序。这样做可以将调试器附加到您的打包应用程序中，如下面的示例所示： 12$ java -Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=8000,suspend=n \\ -jar target/myapplication-0.0.1-SNAPSHOT.jar 使用 Maven 插件Spring Boot Maven 插件包含一个 run 目标，可以用来快速编译和启动你的应用程序。应用程序以展开的方式运行，正如它们在你的 IDE 中所做的那样。下面的示例显示一个典型的 Maven 命令来运行 Spring Boot 应用程序： 1$ mvn spring-boot:run 您可能还希望使用 MAVEN_OPTS 操作系统环境变量，如下例所示： 1$ export MAVEN_OPTS=-Xmx1024m 使用 Gradle 插件Spring Boot Gradle 插件还包括一个 bootRun 任务，它可以用来以一个展开的形式运行您的应用程序。当您应用 org.springframework.boot 和 java 插件时，将添加 bootRun 任务。如下例所示： 1$ gradle bootRun 您可能还想使用 JAVA_OPTS 操作系统环境变量，如下例所示： 1$ export JAVA_OPTS=-Xmx1024m 热交换由于 Spring Boot 应用程序只是普通的 Java 应用程序，所以 JVM 热交换应该可以开箱即用。JVM 热交换在一定程度上限制了它可以替换的字节码。对于更完整的解决方案，可以使用 JRebel。 spring-boot-devtools 模块还包括对快速应用程序重启的支持。查看本章后面的开发者工具部分和如何做热交换获取详细信息。 开发者工具Spring Boot 包括一组额外的工具，这些工具可以使应用程序开发体验变得更加愉快。spring-boot-devtools 模块可以包含在任何项目中，以提供额外的开发时特性。要包含 devtools 支持，请将模块依赖项添加到您的构建中，如下所示的 Maven 和 Gradle 列表： Maven 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Gradle 123dependencies &#123; compile(&quot;org.springframework.boot:spring-boot-devtools&quot;)&#125; 在运行完全打包的应用程序时，开发者工具会自动被禁用。如果您的应用程序是从 java -jar 启动的，或者是从一个特殊的类加载器开始的，那么它就被认为是一个“生产应用程序”。在 Maven 中将依赖项标记为 optional 或 在 Gradle 使用 compileOnly 中是一种最佳实践，它可以防止 devtools 被传递到其他使用您的项目的模块中。 默认情况下，重新打包的存档不包含 devtools。如果您想要使用某个远程 devtools 特性，您需要禁用 excludeDevtools 构建属性来包含它。该属性同时支持 Maven 和 Gradle 插件。 属性默认值Spring Boot 所支持的几个库都使用缓存来提高性能。例如，模板引擎缓存已编译的模板以避免重复解析模板文件。另外，Spring MVC 可以在服务静态资源时添加 HTTP 缓存头信息。 虽然缓存在生产中非常有益，但在开发过程中可能会产生相反的效果，使您无法看到您在应用程序中所做的更改。出于这个原因，spring-boot-devtools 在默认情况下禁用了缓存选项。 缓存选项通常在你的 application.properties 文件中配置。例如，Thymeleaf 提供了 spring.thymeleaf.cache 属性。spring-boot-devtools 模块不需要手动设置这些属性，而是自动应用合理的开发时配置。 有关 devtools 应用的属性的完整列表，请参见 DevToolsPropertyDefaultsPostProcessor。 自动重启使用 spring-boot-devtools 的应用程序在类路径更改时自动重新启动。当在 IDE 中工作时，这可能是一个有用的特性，因为它为代码更改提供了非常快速的反馈循环。默认情况下，指向一个文件夹的类路径上的任何条目都会被监控以进行更改。请注意，某些资源（如静态资产和视图模板）不需要重新启动应用程序。 触发重启 当 DevTools 监视类路径资源时，触发重启的惟一方法是更新类路径。您导致要更新的类路径的方式取决于您使用的 IDE。在 Eclipse 中，保存修改后的文件会导致类路径被更新并触发重新启动。在 IntelliJ IDEA 中，构建项目（Build -&gt; Build Project）具有相同的效果。 只要启用了 forking，您就可以使用支持的构建插件（Maven 和 Gradle）来启动应用程序，因为 DevTools 需要一个独立的应用程序类加载器才能正常运行。默认情况下，Gradle 和 Maven 在类路径上检测 DevTools 时是这样做的。 当与 LiveReload 一起使用时，自动重启非常有效。详情请参阅 LiveReload 一节。如果您使用 JRebel，自动重新启动将被禁用，以支持动态类重载。其他 devtools 特性（如 LiveReload 和 property overrides）仍然可以使用。 DevTools 依赖于应用程序上下文的关闭钩子在重新启动时关闭它。如果您已经禁用了关闭钩子（SpringApplication.setRegisterShutdownHook(false)），那么它将无法正常工作。 当决定是否在类路径上的条目发生更改时触发重新启动时，DevTools 会自动忽略名为 spring-boot、spring-boot-devtools、spring-boot-autoconfigure、spring-boot-actuator 和 spring-boot-starter 的项目。 DevTools 需要自定义 ApplicationContext 所使用的 ResourceLoader。如果您的应用程序已经提供了一个，那么它将被打包。不支持在 ApplicationContext 上直接覆盖 getResource 方法。 重启和重新加载 Spring Boot 提供的重启技术使用两个类加载器。不改变的类（例如，来自第三方 jar 的类）被加载到一个基类加载器中。正在积极开发的类被加载到重启类加载器中。当应用程序重新启动时，重启类加载器将被丢弃，并创建一个新的类加载器。这种方法意味着应用程序重新启动通常要比“冷启动”快得多，因为基类加载器已经可用并填充了。 如果您发现重新启动对应用程序不够快，或者遇到了类加载问题，那么您可以考虑重新加载技术，例如从 ZeroTurnaround 转向 JRebel。这些工作通过重写类，使它们更适合重载。 记录状态评估的变化（Logging changes in condition evaluation） 默认情况下，每次应用程序重新启动时，都会记录显示状态评估增量的报告。报告显示了在进行更改（如添加或删除 bean 和设置配置属性）时对应用程序的自动配置的更改。 若要禁用报告的日志记录，请设置以下属性： 1spring.devtools.restart.log-condition-evaluation-delta=false 排除资源 某些资源在更改时不一定需要触发重新启动。例如，Thymeleaf 模板可以就地编辑。默认情况下，/META-INF/maven、/META-INF/resources、/resources、/static、/public 或 /templates 不会触发重新启动，而是触发 live reload。如果你想要自定义这些排除项，你可以使用 spring.devtools.restart.exclude 属性。例如，只排除 /static 和 /public 可以这样设置： 1spring.devtools.restart.exclude=static/**,public/** 如果你想要保留默认设置并添加额外的排除项，使用 spring.devtools.restart.additional-exclude 替代它。 观察附加路径 你可能希望当你对不在类路径上的文件进行更改时，重新启动或加载你的应用程序。要这样，使用 spring.devtools.restart.additional-paths 属性配置要观察变化的附加路径。您可以使用上面提到的 spring.devtools.restart.exclude 属性来控制在附加路径下的更改是否会触发完全重启或 live reload。 禁用重启 如果您不想使用重启功能，您可以使用 spring.devtools.restart.enabled 属性禁用它。在大多数情况下，您可以在 application.properties 中设置此属性（这样做仍然初始化重启类加载器，但它不观察文件的更改）。 如果您需要完全禁用重新启动支持（例如，因为它不能与特定的库一起工作），那么您需要设置 spring.devtools.restart.enabled 系统属性为 false，然后调用 SpringApplication.run(…)，如下例所示： 1234public static void main(String[] args) &#123; System.setProperty(&quot;spring.devtools.restart.enabled&quot;, &quot;false&quot;); SpringApplication.run(MyApp.class, args);&#125; 使用一个触发文件 如果您使用一个持续编译已更改文件的 IDE，您可能只需要在特定的时间触发重新启动。为此，您可以使用一个“触发文件”，它是一个特殊的文件，当您想要实际触发重新启动检查时，必须对其进行修改。更改文件只会触发检查，只有当 Devtools 检测到它必须做某事时才会重新启动。触发器文件可以手动更新，也可以使用 IDE 插件进行更新。 要使用一个触发器文件，请将 spring.devtools.restart.trigger-file 属性设置为触发器文件的路径。 您可能想要设置 spring.devtools.restart.trigger-file 作为全局设置，以便所有的项目都以相同的方式运行。 自定义重启类加载器 如前所述，在“重启和重新加载”部分中，重新启动功能是通过使用两个类加载器实现的。对于大多数应用程序来说，这种方法运行良好。然而，它有时会导致类加载问题。 默认情况下，IDE 中的任何开放项目都包含“重启”类加载器，任何常规的 .jar 文件都装载了“基础”类加载器。如果您在一个多模块项目中工作，而不是每个模块都导入到您的 IDE 中，您可能需要定制一些东西。为此，您可以创建一个 META-INF/spring-devtools.properties 文件。 spring-devtools.properties 文件可以包含以 restart.exclude 和 restart.include 为前缀的属性。include 元素是应该被拉到“重启”类加载器中的项，而 exclude 元素则是应该被推入“基础”类加载器的项。属性的值是应用于类路径的正则表达式模式，如下例所示： 12restart.exclude.companycommonlibs=/mycorp-common-[\\\\w-]+\\.jarrestart.include.projectcommon=/mycorp-myproj-[\\\\w-]+\\.jar 所有属性的键必须是唯一的。只有属性以 restart.include. 或 restart.exclude. 开头，它才会被考虑。 所有类路径的 META-INF/spring-devtools.properties 都会被加载。您可以在项目中或项目使用的库中打包文件。 已知局限性 通过使用标准 ObjectInputStream 来反序列化的对象，重新启动功能不会很好地工作。如果需要反序列化数据，可能需要使用 Spring 的 ConfigurableObjectInputStream 和 Thread.currentThread().getcontextclassloader()。 不幸的是，一些第三方库在不考虑上下文类加载器的情况下反序列化。如果您发现这样的问题，您需要向原始作者请求修复。 LiveReloadspring-boot-devtools 模块包含一个嵌入式的 LiveReload 服务器，当资源被更改时，它可以用来触发浏览器刷新。LiveReload 浏览器扩展可以从 livereload.com 免费提供给 Chrome、Firefox 和 Safari。 如果您不想在应用程序运行时启动 LiveReload 服务器，则可以设置 spring.devtools.livereload.enabled 属性为 false。 您一次只能运行一个 LiveReload 服务器。在启动应用程序之前，确保没有其他的 LiveReload 服务器在运行。如果您在 IDE 中启动多个应用程序，那么只有第一个应用程序得到了 LiveReload 的支持。 全局设置您可以通过添加名为 .spring-boot-devtools.properties 的文件到 $HOME 文件夹（注意文件名以“.”开头）来配置全局 devtools 设置。添加到该文件的任何属性都适用于使用 devtools 的机器上的所有 Spring Boot 应用程序。例如，要配置重新启动以始终使用触发器文件，您需要添加以下属性： ~&#x2F;.spring-boot-devtools.properties. 1spring.devtools.reload.trigger-file=.reloadtrigger 远程应用程序Spring Boot 开发者工具并不局限于本地开发。在远程运行应用程序时，还可以使用几个特性。远程支持是可选的。要启用它，您需要确保将 devtools 包含在重新打包的归档文件中，如下面的清单所示： 1234567891011&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludeDevtools&gt;false&lt;/excludeDevtools&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 然后，你需要设置 spring.devtools.remote.secret 属性，如下示例所示： 1spring.devtools.remote.secret=mysecret 在远程应用程序上启用 spring-boot-devtools 是一种安全风险。您不应该在生产部署上启用支持。 远程 devtools 支持分两部分提供：一个服务器端端点接受连接，一个客户端应用程序在 IDE 中运行。当 spring.devtools.remote.remote.secret 属性被设置时，服务器组件自动启用。客户端组件必须手动启动。 运行远程客户端应用程序 远程客户端应用程序被设计为从 IDE 中运行。你需要运行 org.springframework.boot.devtools.RemoteSpringApplication，使用你链接的远程项目相同的类路径。应用程序的唯一必需参数是它连接的远程 URL。 例如，如果您正在使用 Eclipse 或 STS，并且您有一个名为 my-app 的项目，您已经部署到 Cloud Foundry，那么您将执行以下操作： 选择 Run 菜单的 Run Configurations… 创建一个新的 Java Application “launch configuration” 浏览 my-app 项目 使用 org.springframework.boot.devtools.RemoteSpringApplication 作为主类 添加 https://myapp.cfapps.io 到 Program arguments（或者你的任何远程 URL） 运行中的远程客户端可能类似于如下列表： 12345678910111213 . ____ _ __ _ _ /\\\\ / ___&#x27;_ __ _ _(_)_ __ __ _ ___ _ \\ \\ \\ \\( ( )\\___ | &#x27;_ | &#x27;_| | &#x27;_ \\/ _` | | _ \\___ _ __ ___| |_ ___ \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| []::::::[] / -_) &#x27; \\/ _ \\ _/ -_) ) ) ) ) &#x27; |____| .__|_| |_|_| |_\\__, | |_|_\\___|_|_|_\\___/\\__\\___|/ / / / =========|_|==============|___/===================================/_/_/_/ :: Spring Boot Remote :: 2.0.2.RELEASE2015-06-10 18:25:06.632 INFO 14938 --- [ main] o.s.b.devtools.RemoteSpringApplication : Starting RemoteSpringApplication on pwmbp with PID 14938 (/Users/pwebb/projects/spring-boot/code/spring-boot-devtools/target/classes started by pwebb in /Users/pwebb/projects/spring-boot/code/spring-boot-samples/spring-boot-sample-devtools)2015-06-10 18:25:06.671 INFO 14938 --- [ main] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@2a17b7b6: startup date [Wed Jun 10 18:25:06 PDT 2015]; root of context hierarchy2015-06-10 18:25:07.043 WARN 14938 --- [ main] o.s.b.d.r.c.RemoteClientConfiguration : The connection to http://localhost:8080 is insecure. You should use a URL starting with &#x27;https://&#x27;.2015-06-10 18:25:07.074 INFO 14938 --- [ main] o.s.b.d.a.OptionalLiveReloadServer : LiveReload server is running on port 357292015-06-10 18:25:07.130 INFO 14938 --- [ main] o.s.b.devtools.RemoteSpringApplication : Started RemoteSpringApplication in 0.74 seconds (JVM running for 1.105) 因为远程客户端使用与实际应用程序相同的类路径，它可以直接读取实际应用程序属性。这是 spring.devtools.remote.secret 属性被读取并传递给服务器进行身份验证的方法。 使用 https:// 作为连接协议总是明智的，这样就可以加密传输并不能截获密码。 如果需要使用代理访问远程应用程序，配置 spring.devtools.remote.proxy.host 和 spring.devtools.remote.proxy.port 属性。 远程更新 远程客户端监控您的应用程序类路径，以与本地重启相同的方式进行更改。任何更新的资源都被推送到远程应用程序，并且（如果需要的话）触发重启。如果您在一个使用不本地化的云服务的特性上进行迭代，这将是很有帮助的。一般来说，远程更新和重新启动比完整的重建和部署周期要快得多。 只在远程客户端运行时监视文件。如果在启动远程客户端之前更改一个文件，则不会将其推送到远程服务器。 打包用于生产环境的应用程序可执行 jar 可以用于生产部署。由于它们是自包含的，所以它们也非常适合基于云的部署。 对于额外的“生产就绪”特性，如健康、审计和度量 REST 或 JMX 端点，考虑添加 spring-boot-actuator。查看 *Spring Boot Actuator：生产就绪特性*获取详细信息。 下一步该读什么现在您应该了解如何使用 Spring Boot 和您应该遵循的一些最佳实践。现在，您可以深入了解特定的 *Spring Boot 特性*，或者您可以跳过，阅读 Spring Boot 的“生产就绪”方面的内容。 Spring Boot 特性本节将深入介绍 Spring Boot 的详细信息。在这里，您可以了解您可能想要使用和定制的关键特性。如果您还没有这样做，您可能希望阅读“入门”和“使用 Spring Boot”部分，这样您就有了良好基础。 SpringApplicationSpringApplication 类提供了一种方便的方法来引导从 main() 方法开始的 Spring 应用程序。在许多情况下，您可以委托给静态 SpringApplication.run 方法，如下例所示： 123public static void main(String[] args) &#123; SpringApplication.run(MySpringConfiguration.class, args);&#125; 当你的应用程序启动时，你应该看到类似于如下输出： 123456789101112 . ____ _ __ _ _ /\\\\ / ___&#x27;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\( ( )\\___ | &#x27;_ | &#x27;_| | &#x27;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) &#x27; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: v2.0.2.RELEASE2013-07-31 00:08:16.117 INFO 56603 --- [ main] o.s.b.s.app.SampleApplication : Starting SampleApplication v0.1.0 on mycomputer with PID 56603 (/apps/myapp.jar started by pwebb)2013-07-31 00:08:16.166 INFO 56603 --- [ main] ationConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@6e5a8246: startup date [Wed Jul 31 00:08:16 PDT 2013]; root of context hierarchy2014-03-04 13:09:54.912 INFO 41370 --- [ main] .t.TomcatServletWebServerFactory : Server initialized with port: 80802014-03-04 13:09:56.501 INFO 41370 --- [ main] o.s.b.s.app.SampleApplication : Started SampleApplication in 2.992 seconds (JVM running for 3.658) 默认情况下，显示 INFO 日志消息，包括一些相关的启动细节，比如启动应用程序的用户。如果您需要一个除 INFO 之外的日志级别，您可以设置它，如日志级别。 启动失败如果您的应用程序启动失败，注册的 FailureAnalyzers 将有机会提供专用的错误消息和解决问题的具体操作。例如，如果您在端口 8080 上启动 web 应用程序，并且该端口已经在使用，您应该会看到类似于以下消息的内容： 1234567891011***************************APPLICATION FAILED TO START***************************Description:Embedded servlet container failed to start. Port 8080 was already in use.Action:Identify and stop the process that&#x27;s listening on port 8080 or configure this application to listen on another port. Spring Boot 提供了大量的 FailureAnalyzer 实现，您可以添加自己的。 如果没有故障分析器能够处理异常，您仍然可以显示完整的情况报告，以便更好地理解错误。要做到这一点，您需要启用 debug 属性或为 org.springframework.boot.autoconfigure.logging.ConditionEvaluationReportLoggingListener 启用 DEBUG 日志记录。 例如，如果您正在使用 java -jar 运行您的应用程序，您可以使调试属性如下： 1$ java -jar myproject-0.0.1-SNAPSHOT.jar --debug 定制横幅可以通过向类路径中添加 banner.txt 文件或将 spring.banner.location 属性设置为此类文件的位置来更改在启动时打印的横幅。如果文件的编码不是 UTF-8，则可以设置 spring.banner.charset。除了文本文件之外，还可以向类路径中添加 banner.gif、banner.jpg 或 banner.png 图像文件，或者设置 spring.banner.image.location 属性。图像转换为 ASCII 艺术表示，并打印在任何文本横幅之上。 在 banner.txt 文件中, 您可以使用下列任何一个占位符： 变量 描述 $&#123;application.version&#125; 你的应用程序版本号，如 MANIFEST.MF 中声明的。比如，Implementation-Version: 1.0 打印为 1.0。 $&#123;application.formatted-version&#125; 你的应用程序版本号，如 MANIFEST.MF 中声明的，格式化显示（被括号包裹，用 v 作前缀）。比如 (v1.0)。 $&#123;spring-boot.version&#125; 你使用的 Spring Boot 版本。比如 2.0.2.RELEASE。 $&#123;spring-boot.formatted-version&#125; 你使用的 Spring Boot 版本，格式化显示（被括号包裹，用 v 作前缀）。比如 (v2.0.2.RELEASE)。 $&#123;Ansi.NAME&#125;（或 $&#123;AnsiColor.NAME&#125;, $&#123;AnsiBackground.NAME&#125;, $&#123;AnsiStyle.NAME&#125;） 其中 NAME 是 ANSI 转义代码的名称。查看 AnsiPropertySource 获取详细信息。 $&#123;application.title&#125; 你的应用程序的标题，如 MANIFEST.MF 中声明的。比如 Implementation-Title: MyApp 打印为 MyApp。 如果您想以编程的方式生成横幅，则可以使用 SpringApplication.setBanner(…) 方法。使用 org.springframework.boot.Banner 接口并实现您自己的 printBanner() 方法。 你也可以使用 spring.main.banner-mode 属性决定横幅是否打印在 System.out（console） 上、发送到配置的日志记录器（log）、完全不产生（off）。 在以下名称中，打印的横幅被注册为一个单例 bean：springBootBanner。 YAML 映射 off 到 false，所以如果您想要禁用应用程序中的横幅，请确保添加引号，如下例所示： 123spring: main: banner-mode: &quot;off&quot; 定制 SpringApplication如果 SpringApplication 的默认值不符合您的喜好，您可以创建一个本地实例并自定义它。例如，要关闭横幅，你可以写： 12345public static void main(String[] args) &#123; SpringApplication app = new SpringApplication(MySpringConfiguration.class); app.setBannerMode(Banner.Mode.OFF); app.run(args);&#125; 传递给 SpringApplication 的构造函数参数是 Spring bean 的配置源。在大多数情况下，这些都是对 @Configuration 类的引用，但是它们也可以是对 XML 配置的引用，或者对应该被扫描的包的引用。 也可以使用 application.properties 文件配置 SpringApplication。查看*外部化配置*以获取详细信息。 有关配置选项的完整列表，查看 SpringApplication Javadoc。 Fluent 风格构建器 API如果您需要构建 ApplicationContext 层次结构（包含父&#x2F;子关系的多个上下文），或者您更喜欢使用“fluent”构建器 API，那么您可以使用 SpringApplicationBuilder。 SpringApplicationBuilder 允许您将多个方法调用链接在一起，并包含让您创建层次结构的 parent 和 child 方法，如下例所示： 12345new SpringApplicationBuilder() .sources(Parent.class) .child(Application.class) .bannerMode(Banner.Mode.OFF) .run(args); 在创建 ApplicationContext 层次结构时，有一些限制。例如，Web 组件必须包含在子上下文内，并且在父和子上下文环境中都使用相同的 Environment。请参阅 SpringApplicationBuilder Javadoc 了解详细信息。 应用程序事件和监听器除了通常的 Spring Framework 事件（比如 ContextRefreshedEvent）之外，SpringApplication 还会发送一些附加的应用程序事件。 在创建 ApplicationContext 之前，实际上触发了一些事件，因此不能将侦听器注册为 @Bean。您可以使用 SpringApplication.addListeners(…) 方法或 SpringApplicationBuilder.listeners(…) 方法注册它们。 如果您希望这些侦听器自动注册，不管应用程序是如何创建的，您都可以添加一个 META-INF/spring.factories 文件到您的项目，并通过使用 org.springframework.context.ApplicationListener 键来引用您的侦听器，如下例所示： 1org.springframework.context.ApplicationListener = com.example.project.MyListener 应用程序事件按以下顺序发送： ApplicationStartingEvent，是在运行开始时发送的，但在任何处理之前，除了侦听器和初始化器的注册之外。 ApplicationEnvironmentPreparedEvent，当 Environment 被使用时，在上下文被创建之前被发送。 ApplicationPreparedEvent，在刷新之前发送，但是在加载 bean 定义之后。 ApplicationStartedEvent，在调用上下文之后发送，但是在调用任何应用程序和命令行运行程序之前。 ApplicationReadyEvent，在调用任何应用程序和命令行运行程序后发送。它表明应用程序已经准备好服务请求。 ApplicationFailedEvent，如果启动时出现异常，则发送。 您通常不需要使用应用程序事件，但是知道它们的存在是很方便的。在内部，Spring Boot 使用事件来处理各种任务。 使用 Spring Framework 的事件发布机制发送应用程序事件。该机制的一部分确保在子环境中发布给侦听器的事件也会在任何祖先上下文中被发布给侦听器。因此，如果您的应用程序使用了 SpringApplication 实例的层次结构，那么侦听器可能会接收到相同类型的应用程序事件的多个实例。 为了让您的侦听器区分事件的上下文和派生上下文的事件，它应该请求将其应用程序上下文注入，然后将注入的上下文与事件上下文进行比较。可以通过实现 ApplicationContextAware 或，如果侦听器是 bean，通过使用 @Autowired 来注入上下文。 Web 环境SpringApplication 试图为您创建合适的 ApplicationContext 类型。用于确定 WebEnvironmentType 的算法相当简单： 如果 Spring MVC 存在，则使用 AnnotationConfigServletWebServerApplicationContext 如果 Spring MVC 不存在，Spring WebFlux 是存在的，那么就使用一个 AnnotationConfigReactiveWebServerApplicationContext 否则，使用 AnnotationConfigApplicationContext 这意味着如果您使用 Spring MVC 和来自 Spring WebFlux 的新 WebClient 在相同的应用程序中，Spring MVC 将在默认情况下使用。您可以通过调用 setWebApplicationType(WebApplicationType) 来轻松覆盖它。 还可以完全控制调用 setApplicationContextClass(…) 所使用的 ApplicationContext 类型。 在 JUnit 测试中使用 SpringApplication 时，通常需要调用 setWebApplicationType(WebApplicationType.NONE)。 访问应用程序参数如果您需要访问传递到 SpringApplication.run(…) 的应用程序参数，您可以注入一个 org.springframework.boot.ApplicationArguments bean。ApplicationArguments 接口提供了对原始 String[] 参数以及解析 option 和 non-option 参数的访问，如下例所示： 123456789101112131415import org.springframework.boot.*;import org.springframework.beans.factory.annotation.*;import org.springframework.stereotype.*;@Componentpublic class MyBean &#123; @Autowired public MyBean(ApplicationArguments args) &#123; boolean debug = args.containsOption(&quot;debug&quot;); List&lt;String&gt; files = args.getNonOptionArgs(); // if run with &quot;--debug logfile.txt&quot; debug=true, files=[&quot;logfile.txt&quot;] &#125;&#125; Spring Boot 还会在 Spring Environment 中注册一个 CommandLinePropertySource。这允许您使用 @Value 注解注入单个应用程序参数。 使用 ApplicationRunner 或 CommandLineRunner如果您需要在 SpringApplication 启动之后运行一些特定的代码，您可以实现 ApplicationRunner 或 CommandLineRunner 接口。两个接口都以相同的方式工作，并提供了一个单独的运行方法，在 SpringApplication.run(…) 完成之前调用。 CommandLineRunner 接口提供对应用程序参数的访问作为一个简单的字符串数组，而 ApplicationRunner 使用前面讨论的 ApplicationArguments 接口。下面的示例展示了一个使用 run 方法的 CommandLineRunner： 1234567891011import org.springframework.boot.*;import org.springframework.stereotype.*;@Componentpublic class MyBean implements CommandLineRunner &#123; public void run(String... args) &#123; // Do something... &#125;&#125; 如果定义了多个 CommandLineRunner 或 ApplicationRunner bean，必须以特定的顺序调用它们，那么您可以额外地实现 org.springframework.core.Ordered 接口或使用 org.springframework.core.annotation.Order 注解。 应用程序退出每个 SpringApplication 都向 JVM 注册一个关闭钩子，以确保 ApplicationContext 在退出时优雅地关闭。可以使用所有标准的 Spring 生命周期回调函数（如 DisposableBean bean 接口或 @PreDestroy 注解）。 此外，bean 可以实现 org.springframework.boot.ExitCodeGenerator 接口，如果希望当 SpringApplication.exit() 被调用时，返回特定的退出代码。然后可以将此退出代码传递给 System.exit()，以将其作为状态代码返回，如下面的示例所示： 1234567891011121314@SpringBootApplicationpublic class ExitCodeApplication &#123; @Bean public ExitCodeGenerator exitCodeGenerator() &#123; return () -&gt; 42; &#125; public static void main(String[] args) &#123; System.exit(SpringApplication .exit(SpringApplication.run(ExitCodeApplication.class, args))); &#125;&#125; 此外，ExitCodeGenerator 接口也可以由异常来实现。当遇到这样的异常时，Spring Boot 返回由实现的 getExitCode() 方法提供的退出代码。 管理员特性通过指定 spring.application.admin.enabled 属性，可以为应用程序启用与 admin 相关的特性。这将在平台 MBeanServer 上公开 SpringApplicationAdminMXBean。您可以使用该特性远程管理您的 Spring Boot 应用程序。这个特性还可以用于任何服务包装器实现。 如果您想知道应用程序正在运行哪个 HTTP 端口，请使用 local.server.port 的键获取该属性。 谨慎 在启用该特性时要注意，因为 MBean 公开了关闭应用程序的方法。 外部化配置（Externalized Configuration）Spring Boot 允许您外部化您的配置，这样您就可以在不同的环境中使用相同的应用程序代码。您可以使用 properties 文件、YAML 文件、环境变量和命令行参数来外部化配置。属性值可以通过使用 @Value 注解直接注入到您的 bean 中，通过 Spring 的 Environment 抽象访问，或者通过 @ConfigurationProperties 绑定到结构化对象。 Spring Boot 使用一种非常特殊的 PropertySource 命令，该命令旨在允许对值进行合理的覆盖。属性按以下顺序考虑： 主目录上的 Devtools 全局设置属性（~/.spring-boot-devtools.properties，当 devtools 激活时） 测试上的 @TestPropertySource 注解 测试上的 @SpringBootTest#properties 注解 命令行参数 来自 SPRING_APPLICATION_JSON 的属性（嵌入在环境变量或系统属性中的内联 JSON） ServletConfig 初始化参数 ServletContext 初始化参数 来自 java:comp/env 的 JNDI 属性 Java 系统属性（System.getProperties()） OS 环境变量 RandomValuePropertySource，只在 random.* 中的属性 指定 Profile 的应用程序 properties，在打包好的 jar 之外（application-&#123;profile&#125;.properties 和 YAML variants） 指定 Profile 的应用程序 properties，打包在 jar 中（application-&#123;profile&#125;.properties 和 YAML variants） 应用程序属性，在打包好的 jar 之外（application.properties 和 YAML variants） 应用程序属性，打包在 jar 中（application.properties 和 YAML variants） @Configuration 类上的 @PropertySource 注解 默认属性（通过设置 SpringApplication.setDefaultProperties 明确规定） 为了提供一个具体的示例，假设您开发了一个使用 name 属性的 @Component，如下例所示： 123456789101112import org.springframework.stereotype.*;import org.springframework.beans.factory.annotation.*;@Componentpublic class MyBean &#123; @Value(&quot;$&#123;name&#125;&quot;) private String name; // ...&#125; 在您的应用程序类路径（例如，在 jar 中）您可以有一个 application.properties 文件，为 name 提供一个合理的默认属性值。在新环境中运行时，可以在您的 jar 之外提供 application.properties 文件，以覆盖 name。对于一次性测试，您可以使用特定的命令行开关启动（例如，java -jar app.jar --name=&quot;Spring&quot;）。 SPRING_APPLICATION_JSON 属性可以在命令行上提供环境变量。例如：你可以在 UN*X shell 中使用如下行： 1$ SPRING_APPLICATION_JSON=&#x27;&#123;&quot;acme&quot;:&#123;&quot;name&quot;:&quot;test&quot;&#125;&#125;&#x27; java -jar myapp.jar 在前面的示例中，您在 Spring Environment 中最终得到了 acme.name=test。您也可以提供 JSON 如 spring.application.json 在系统属性中，如下例所示： 1$ java -Dspring.application.json=&#x27;&#123;&quot;name&quot;:&quot;test&quot;&#125;&#x27; -jar myapp.jar 您还可以使用命令行参数来提供 JSON，如下面的示例所示： 1$ java -jar myapp.jar --spring.application.json=&#x27;&#123;&quot;name&quot;:&quot;test&quot;&#125;&#x27; 您还可以将 JSON 作为 JNDI 变量提供，如下所示：java:comp/env/spring.application.json。 配置随机值RandomValuePropertySource 用于注入随机值（例如，在机密或测试用例中）。它可以生成 integer、long、uuid 或 string，如下面的示例所示： 123456my.secret=$&#123;random.value&#125;my.number=$&#123;random.int&#125;my.bignumber=$&#123;random.long&#125;my.uuid=$&#123;random.uuid&#125;my.number.less.than.ten=$&#123;random.int(10)&#125;my.number.in.range=$&#123;random.int[1024,65536]&#125; random.int* 语法是 OPEN value (,max) CLOSE，其中 OPEN,CLOSE 为任意字符、value,max 为 integer。如果 max 被提供，value 便是最小值、max 便是最大值（不包含）。 访问命令行属性在默认情况下，SpringApplication 会转换任何命令行选项参数（也就是说，参数以 -- 开始，如 --server.port=9000）到一个属性，并将它们添加到 Spring Environment 中。如前所述，命令行属性总是优先于其他属性源。 如果您不希望将命令行属性添加到 Environment 中，您可以使用 SpringApplication.setAddCommandLineProperties(false) 禁用它们。 应用程序属性文件SpringApplication 从以下位置的 application.properties 文件加载属性，并将它们添加到Spring Environment： 当前目录的 /config 子目录 当前目录 类路径的 /config 包 类路径根 列表按优先顺序排序（在列表中较高的位置定义的属性覆盖在较低位置定义的属性）。 你也可以使用 YAML (‘.yml’) 文件代替“.properties”。 如果您不喜欢 application.properties 作为配置文件名，可以通过指定 spring.config.name 环境属性切换到另一个文件名。您还可以使用 spring.config.location 环境属性来引用一个显式的位置（它是一个以逗号分隔的目录位置或文件路径列表）。下面的示例演示如何指定不同的文件名： 1$ java -jar myproject.jar --spring.config.name=myproject 下面的示例演示如何指定两个位置： 1$ java -jar myproject.jar --spring.config.location=classpath:/default.properties,classpath:/override.properties spring.config.name 和 spring.config.location 很早就被用于确定哪些文件必须被加载，因此它们必须被定义为环境属性（通常是一个 OS 环境变量、一个系统属性或一个命令行参数）。 如果 spring.config.location 包含目录（相对于文件），它们应该以 &#x2F; 结束（并且在运行时，被附加到 spring.config.name 生成的名称，包括特定于 profile 的文件名)。spring.config.location 中指定的文件是按原样使用的，不支持特定于 profile 的变体，并且被任何特定于 profile 的属性覆盖。 配置位置按相反顺序搜索。默认情况下，配置的位置是 classpath:/,classpath:/config/,file:./,file:./config/。由此产生的搜索顺序如下： file:./config/ file:./ classpath:/config/ classpath:/ 当自定义配置位置使用 spring.config.location 配置时，它们替换默认的位置。例如，如果 spring.config.location 配置为值：classpath:/custom-config/,file:./custom-config/，搜索顺序如下： file:./custom-config/ classpath:custom-config/ 或者，当自定义配置位置使用 spring.config.additional-location 配置时，除了默认位置外，还使用它们。在默认位置之前搜索额外的位置。例如，如果额外的位置配置为 classpath:/custom-config/,file:./custom-config/，搜索顺序如下： file:./custom-config/ classpath:custom-config/ file:./config/ file:./ classpath:/config/ classpath:/ 这个搜索排序允许您在一个配置文件中指定默认值，然后在另一个配置文件中选择性地覆盖这些值。您可以在位于默认位置之一的 application.properties 中为应用程序提供默认值（或您在 spring.config.name 中选择的其他 basename）。这些默认值可以在运行时被定制的位置中放置的一个不同的文件重写。 如果您使用环境变量而不是系统属性，大多数操作系统都不允许使用句号分隔的键名，但是您可以使用下划线（例如，SPRING_CONFIG_NAME 而不是 spring.config.name）。 如果应用程序在容器中运行，那么可以使用 JNDI 属性（在 java:comp/env 中）或 servlet 上下文初始化参数，而不是环境变量或系统属性。 特定于 Profile 属性除了 application.properties 文件，特定于 profile 的属性也可以通过使用以下命名约定来定义：application-&#123;profile&#125;.properties。Environment 中有一组默认 profile（默认情况下是 [default]），如果没有设置激活的 profile，则使用默认 profile。换句话说，如果没有显式地激活 profile，那么就会加载 application-default.properties。 特定于 profile 的属性从相同的位置加载到标准 application.properties 中，特定于 profile的文件总是覆盖非特定的文件，无论特定于 profile 的文件是否在您的打包 jar 内或外部。 如果指定了多个 profile，则应用最后的策略。例如，spring.profiles.active 属性指定的 profile，在通过 SpringApplication API 配置后，被添加，因此优先。 如果您在 spring.config.location 中指定了任何文件，这些文件特定于 profile 的变体不会被考虑。如果您还想使用特定于 profile 的属性，使用在 spring.config.location 中的目录。 属性中的占位符在 application.properties 中的值在使用时通过现有 Environment 进行过滤，因此您可以引用之前定义的值（例如，从系统属性）。 12app.name=MyAppapp.description=$&#123;app.name&#125; is a Spring Boot application 您还可以使用此技术创建存在于 Spring Boot 属性中的“短”变体。请参阅*使用“短”命令行参数*来获取详细信息。 使用 YAML 代替 PropertiesYAML 是 JSON 的超集，因此，它是一种用于指定分层配置数据的方便格式。当您的类路径上有 SnakeYAML 库时，SpringApplication 类会自动支持 YAML 作为属性的替代品。 如果你使用“启动器”，SnakeYAML 是由 spring-boot-starter 自动提供的。 加载 YAML Spring 框架提供了两个方便的类，可以用来加载 YAML 文档。YamlPropertiesFactoryBean 将 YAML 加载为 Properties，而 YamlMapFactoryBean 将 YAML 加载为 Map。 例如，考虑以下 YAML 文档： 1234567environments: dev: url: http://dev.example.com name: Developer Setup prod: url: http://another.example.com name: My Cool App 前面的示例将转换为以下属性： 1234environments.dev.url=http://dev.example.comenvironments.dev.name=Developer Setupenvironments.prod.url=http://another.example.comenvironments.prod.name=My Cool App YAML 列表表示为 [index] 引用的属性键。例如，考虑以下 YAML： 1234my:servers: - dev.example.com - another.example.com 前面的示例将转换为这些属性： 12my.servers[0]=dev.example.commy.servers[1]=another.example.com 要使用 Spring Boot 的 Binder 工具（这是 @ConfigurationProperties 所做的）来绑定到这样的属性，您需要在 java.util.List（或 Set）类型的目标 bean 中拥有一个属性，您要么需要提供一个 setter，要么用一个可变值初始化它。例如，下面的示例绑定到前面显示的属性： 123456789@ConfigurationProperties(prefix=&quot;my&quot;)public class Config &#123; private List&lt;String&gt; servers = new ArrayList&lt;String&gt;(); public List&lt;String&gt; getServers() &#123; return this.servers; &#125;&#125; 在 Spring Environment 中暴露 YAML 作为 Properties YamlPropertySourceLoader 类可以用于在 Spring Environment 中将 YAML 作为 PropertySource 公开。这样做可以让您使用带有占位符语法的 @Value 注解来访问 YAML 属性。 多 profile 的 YAML 文档 您可以使用 spring.profiles 键在单个文件中指定多个特定于 profile 的 YAML 文档，以指示文档何时应用，如以下示例所示： 123456789101112server: address: 192.168.1.100---spring: profiles: developmentserver: address: 127.0.0.1---spring: profiles: productionserver: address: 192.168.1.120 在前面的示例中，如果 development profile 是激活的，则 server.address 属性是 127.0.0.1。类似地，如果 production profile 是激活的，则 server.address 属性是 192.168.1.120。如果未启用 development 和 production profile，则该属性的值为 192.168.1.100。 如果在应用程序上下文启动时没有显式激活，则会激活默认 profile。因此，在接下来的 YAML 中，我们为 spring.security.user.password 设置了一个值，仅在 “default” profile 中可用： 12345678server: port: 8000---spring: profiles: default security: user: password: weak 然而，在下面的例子中，密码总是被设置，因为它没有附加到任何 profile，而且必须在必要时显式地重置所有其他 profile： 123456server: port: 8000spring: security: user: password: weak 使用 spring.profiles 元素指定的 Spring profile 可以选择性地使用 ! 字符否定。如果为单个文档指定了否定和非否定的 profile，那么至少有一个非否定的 profile 必须匹配，并且没有任何被否定的 profile 可能匹配。 YAML 缺陷 YAML 文件不能通过使用 @PropertySource 注解来加载。因此，在需要以这种方式加载值的情况下，需要使用 properties 文件。 类型安全的配置属性使用 @Value(&quot;$&#123;property&#125;&quot;) 注解注入配置属性有时会很麻烦，特别是如果您使用的是多个属性，或者您的数据在本质上是分层的。Spring Boot 提供了一种处理属性的替代方法，可以让强类型 bean 管理和验证应用程序的配置，如下面的示例所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.example;import java.net.InetAddress;import java.util.ArrayList;import java.util.Collections;import java.util.List;import org.springframework.boot.context.properties.ConfigurationProperties;@ConfigurationProperties(&quot;acme&quot;)public class AcmeProperties &#123; private boolean enabled; private InetAddress remoteAddress; private final Security security = new Security(); public boolean isEnabled() &#123; ... &#125; public void setEnabled(boolean enabled) &#123; ... &#125; public InetAddress getRemoteAddress() &#123; ... &#125; public void setRemoteAddress(InetAddress remoteAddress) &#123; ... &#125; public Security getSecurity() &#123; ... &#125; public static class Security &#123; private String username; private String password; private List&lt;String&gt; roles = new ArrayList&lt;&gt;(Collections.singleton(&quot;USER&quot;)); public String getUsername() &#123; ... &#125; public void setUsername(String username) &#123; ... &#125; public String getPassword() &#123; ... &#125; public void setPassword(String password) &#123; ... &#125; public List&lt;String&gt; getRoles() &#123; ... &#125; public void setRoles(List&lt;String&gt; roles) &#123; ... &#125; &#125;&#125; 前面的 POJO 定义了以下属性： acme.enabled，默认值为 false acme.remote-address，一个可以从 String 强转的类型 acme.security.username，使用嵌套的“security”对象，其名称由属性的名称决定。特别是，返回类型在那里没有使用，并且可能是 SecurityProperties acme.security.password acme.security.roles，String 集合 getter 和 setter 通常是强制的，因为绑定是通过标准的 Java bean 属性描述符，就像在 Spring MVC 中一样。在下列情况下，可以省略 setter： Map，只要它们被初始化，就需要一个 getter，但不一定需要 setter，因为它们可以由绑定器进行更改。 可以通过索引（通常是 YAML）或使用单个逗号分隔值（属性）来访问集合和数组。在后一种情况下，setter 是强制的。我们建议总是为这种类型添加一个 setter。如果您初始化一个集合，请确保它不是不可变的（就像前面的例子）。 如果嵌套的 POJO 属性被初始化（比如前面示例中的 Security 字段），则不需要 setter。如果您希望绑定器使用它的默认构造函数来动态创建实例，那么您需要一个 setter。 有些人使用项目 Lombok 自动添加 getter 和 setter。确保 Lombok 不会为这种类型生成任何特定的构造函数，因为它是由容器自动使用来实例化对象的。 最后，只考虑标准的 Java Bean 属性，不支持对静态属性的绑定。 也可参考 @Value 和 @ConfigurationProperties 的区别。 您还需要列出在 @EnableConfigurationProperties 注解中注册的属性类，如下例所示： 1234@Configuration@EnableConfigurationProperties(AcmeProperties.class)public class MyConfiguration &#123;&#125; 当 @ConfigurationProperties bean 以这种方式注册时，bean 有一个常规名称： &lt;prefix&gt;-&lt;fqn&gt;，其中 &lt;prefix&gt; 是 @ConfigurationProperties 注解中指定的环境键前缀，&lt;fqn&gt; 是 bean 的完全限定名称。如果注解没有提供任何前缀，则只使用 bean 的完全限定名。 上面示例中的 bean 名称是 acme-com.example.AcmeProperties。 即使前面的配置为 AcmeProperties 创建了一个常规 bean，我们建议 @ConfigurationProperties 只处理环境，特别是不从上下文注入其他 bean。已经说过，@EnableConfigurationProperties 注解也会自动地应用到您的项目中，这样就可以从 Environment 中配置任何带有 @ConfigurationProperties 的现有 bean。您可以通过确保 AcmeProperties 已经是一个 bean 来快捷地进行 MyConfiguration，如下面的示例所示： 1234567@Component@ConfigurationProperties(prefix=&quot;acme&quot;)public class AcmeProperties &#123; // ... see the preceding example&#125; 这种类型的配置与 SpringApplication 外部 YAML 配置特别有效，如下例所示： 1234567891011# application.ymlacme: remote-address: 192.168.1.1 security: username: admin roles: - USER - ADMIN# additional configuration as required 要使用 @ConfigurationProperties bean，您可以像其他 bean 一样注入它们，如下例所示： 12345678910111213141516171819@Servicepublic class MyService &#123; private final AcmeProperties properties; @Autowired public MyService(AcmeProperties properties) &#123; this.properties = properties; &#125; //... @PostConstruct public void openConnection() &#123; Server server = new Server(this.properties.getRemoteAddress()); // ... &#125;&#125; 使用 @ConfigurationProperties 还可以生成可以被 IDE 使用的 metadata 文件，为您自己的键提供自动完成。详见附录 B 配置 Metadata。 第三方配置 除了使用 @ConfigurationProperties 来注解一个类之外，还可以在 public @Bean 方法上使用它。当您希望将属性绑定到控件之外的第三方组件时，这样做尤其有用。 要从 Environment 属性配置 bean，请将 @ConfigurationProperties 添加到它的 bean 注册，如下例所示： 12345@ConfigurationProperties(prefix = &quot;another&quot;)@Beanpublic AnotherComponent anotherComponent() &#123; ...&#125; 用 another 前缀定义的任何属性都被映射到与前面的 AcmeProperties 示例类似的 AnotherComponent bean。 松散绑定 Spring Boot 使用一些松散的规则将 Environment 属性绑定到 @ConfigurationProperties bean，因此 Environment 属性名和 bean 属性名之间不需要精确匹配。常见的示例中，这是有用的，其中包括短横线分隔的环境属性（例如，context-path 绑定到 contextPath），以及大写的环境属性（例如，PORT 绑定到 port）。 例如，考虑以下 @ConfigurationProperties 类： 1234567891011121314@ConfigurationProperties(prefix=&quot;acme.my-project.person&quot;)public class OwnerProperties &#123; private String firstName; public String getFirstName() &#123; return this.firstName; &#125; public void setFirstName(String firstName) &#123; this.firstName = firstName; &#125;&#125; 在前面的示例中，可以使用以下属性名称： 属性 注释 acme.my-project.person.first-name Kebab 风格，推荐使用在 .properties 和 .yml 文件中 acme.myProject.person.firstName 标准驼峰语法 acme.my_project.person.first_name 下划线符号，使用在 .properties 和 .yml 文件中的替代格式 ACME_MYPROJECT_PERSON_FIRSTNAME 大写格式，推荐系统环境变量使用 注解的 prefix 必须是 kebab 风格（小写字母、- 分隔，如 acme.my-project.person）。 属性源 简单的 List Properties Files Camel case、kebab case 或 underscore notation Standard list syntax using [ ] or comma-separated values YAML Files Camel case、kebab case 或 underscore notation Standard YAML list syntax or comma-separated values Environment Variables 使用下划线作为定界符的大写格式。_ 不应该在属性名称中使用 Numeric values surrounded by underscores, such as MY_ACME_1_OTHER &#x3D; my.acme[1].other System properties Camel case、kebab case 或 underscore notation Standard list syntax using [ ] or comma-separated values","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://sv.pointcut.cc/tags/springboot/"}]},{"title":"04-自定义start","slug":"springboot/04-自定义start","date":"2021-11-26T12:00:07.000Z","updated":"2022-03-23T09:03:56.059Z","comments":true,"path":"blog/springboot/04-自定义start/","link":"","permalink":"http://sv.pointcut.cc/blog/springboot/04-%E8%87%AA%E5%AE%9A%E4%B9%89start/","excerpt":"","text":"自定义strat命名规范:官方命名空间: 123前缀:spring-boot-starter- 模式:spring-boot-starter-模块名 举例:spring-boot-starter-web、spring-boot-starter-jdbc 自定义命名空间: 123后缀:-spring-boot-starter模式:模块-spring-boot-starter举例:mybatis-spring-boot-starter 自定义strat定制strat的配置 12345678@Data@ConfigurationProperties(prefix = &quot;spring.redis&quot;)public class RedisConfig &#123; private String host; private Integer port;&#125; 定制配置类，并且引入@EnableConfigurationProperties注解，使得配置和类进行绑定 1234567891011121314@Configuration@ConditionalOnClass(JackTemplate.class)@EnableConfigurationProperties(RedisConfig.class)public class CustomStarterRun &#123; @Autowired private RedisConfig redisConfig; @Bean public JackTemplate jackTemplate() &#123; JackTemplate jackTemplate = new JackTemplate(redisConfig); return jackTemplate; &#125;&#125; 通过上边其实这个strat也可以使用了，在传统的spring项目中可以通过@Import注解引入CustomStarterRun类就可以了。在SpringBoot可以添加一个META-INF/spring.factories文件 并在该文件中添加如下内容： 123org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\com.xx.jack.start.CustomStarterRun0#多个用逗号“，”隔开 这样就可以实现自动启动了。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://sv.pointcut.cc/tags/springboot/"}]},{"title":"03-SpringBoot项目启动原理","slug":"springboot/03-SpringBoot项目启动原理","date":"2021-11-26T12:00:06.000Z","updated":"2022-03-23T09:03:56.058Z","comments":true,"path":"blog/springboot/03-SpringBoot项目启动原理/","link":"","permalink":"http://sv.pointcut.cc/blog/springboot/03-SpringBoot%E9%A1%B9%E7%9B%AE%E5%90%AF%E5%8A%A8%E5%8E%9F%E7%90%86/","excerpt":"","text":"在SpringBoot项目中都会有类似的代码 1234567@SpringBootApplicationpublic class ConfigurationFileApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigurationFileApplication.class, args); &#125;&#125; 创建SpringApplication该类就是SpringBoot应用的启动类。这里就需要看SpringApplication.run方法。该方法也很简单，就是 123public static ConfigurableApplicationContext run(Class&lt;?&gt;[] primarySources, String[] args) &#123; return new SpringApplication(primarySources).run(args);&#125; 这里会创建一个SpringApplication对象 1234567891011121314151617181920public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) &#123; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, &quot;PrimarySources must not be null&quot;); // 将启动类放入primarySources this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); // 推算当前web应用类型(webFlux, servlet) this.webApplicationType = WebApplicationType.deduceFromClasspath(); // 下面都是Spring SPI的应用， // 就是去spring.factories 中去获取所有key=org.springframework.context.ApplicationContextInitializer的类 setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class)); // 就是去spring.factories 中去获取所有key: org.springframework.context.ApplicationListener setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); // 获取main的class对象 this.mainApplicationClass = deduceMainApplicationClass();&#125; 通过上边可以了解到，这个SpringApplication对象会有一个我们传入的应用启动类的class对象，会把这个class对象放入到primarySources集合中。集合开头的，现在这个primarySources里面有ConfigurationFileApplication.class对象。之所以需要传入这个，是因为需要使用这个class对象来告诉spring的上下文，那些类需要spring容器管理。 这里通过Spring SPI收集ApplicationContextInitializer和ApplicationListener的实现类，然后分别放入集合initializers和listeners中。在SpringBoot中，会默认提供很多实现 总结 获取启动类:根据启动类加载ioc容器 获取web应用类型 Spring SPI读取ApplicationContextInitializer ,ApplicationListener 的实现类， 通过SPI进行解耦（实现全局配置文件、热部署插件） 根据main推算出所在的类 启动看SpringApplication对象的调用的run方法。该方法包含了启动springboot最核心的逻辑 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public ConfigurableApplicationContext run(String... args) &#123; // 用来记录当前springboot启动耗时 StopWatch stopWatch = new StopWatch(); // 就是记录了启动开始时间 stopWatch.start(); // 它是任何spring上下文的接口， 所以可以接收任何ApplicationContext实现 ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); // 开启了Headless模式： configureHeadlessProperty(); // 去spring.factroies中读取了SpringApplicationRunListener 的组件， 就是用来发布事件或者运行监听器 SpringApplicationRunListeners listeners = getRunListeners(args); // 发布1.ApplicationStartingEvent事件，在运行开始时发送 listeners.starting(); try &#123; // 根据命令行参数 实例化一个ApplicationArguments ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); // 预初始化环境： 读取环境变量，读取配置文件信息（基于监听器） ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); // 忽略beaninfo的bean configureIgnoreBeanInfo(environment); // 打印Banner 横幅 Banner printedBanner = printBanner(environment); // 根据webApplicationType创建Spring上下文 context = createApplicationContext(); exceptionReporters = getSpringFactoriesInstances(SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); //预初始化spring上下文 prepareContext(context, environment, listeners, applicationArguments, printedBanner); // 加载spring ioc 容器 **相当重要 由于是使用AnnotationConfigServletWebServerApplicationContext 启动的spring容器所以springboot对它做了扩展： // 加载自动配置类：invokeBeanFactoryPostProcessors ， 创建servlet容器onRefresh refreshContext(context); afterRefresh(context, applicationArguments); stopWatch.stop(); if (this.logStartupInfo) &#123; new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), stopWatch); &#125; listeners.started(context); callRunners(context, applicationArguments); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, listeners); throw new IllegalStateException(ex); &#125; try &#123; listeners.running(context); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, null); throw new IllegalStateException(ex); &#125; return context;&#125; SpringApplicationRunListener实现类收集123// 去spring.factroies中读取了SpringApplicationRunListener 的组件， 就是用来发布事件或者运行监听器SpringApplicationRunListeners listeners = getRunListeners(args); 首先通过SpringSPI获取SpringApplicationRunListener的实现类，实例化后把这些对象封装成SpringApplicationRunListeners对象，而SpringBoot默认提供了一个实现，这个实现为EventPublishingRunListener。 这里看EventPublishingRunListener的构造函数 1234567891011121314private final SpringApplication application;private final String[] args;private final SimpleApplicationEventMulticaster initialMulticaster;public EventPublishingRunListener(SpringApplication application, String[] args) &#123; this.application = application; this.args = args; this.initialMulticaster = new SimpleApplicationEventMulticaster(); for (ApplicationListener&lt;?&gt; listener : application.getListeners()) &#123; this.initialMulticaster.addApplicationListener(listener); &#125;&#125; 可以看到，这里的使用了Spring提供的事件多播器SimpleApplicationEventMulticaster来完成事件的发布和订阅。而且把在创建SpringApplication对象收集到的ApplicationListener，添加到该多播器中。 从SpringApplicationRunListener这个接口的方法就能猜到这个接口的作用了，就是在SpringBoot的不同时机发布不同的事件的，然后通过这些SpringApplicationRunListener来完成事件的处理。 发布事件是由SpringApplicationRunListeners的方法进行发布。所以spring boot启动阶段的事件处理如下图： 发布ApplicationStartingEvent事件123// 发布1.ApplicationStartingEvent事件，在运行开始时发送// listeners=SpringApplicationRunListenerslisteners.starting(); 收集完SpringApplicationRunListener并完成订阅后，会调用starting方法，看EventPublishingRunListener的该方法 1234public void starting() &#123; // 这里的application，就是前面创建的SpringApplication，args就是main方法的args this.initialMulticaster.multicastEvent(new ApplicationStartingEvent(this.application, this.args));&#125; 这里就是发布一个ApplicationStartingEvent事件，可以更具需要在spring.factories文件中添加对应的ApplicationListener。 配置对象的初始化 读取完配置后ApplicationEnvironmentPreparedEvent事件 代码继续走，看 1234 // 根据命令行参数 实例化一个ApplicationArguments ApplicationArguments applicationArguments = new DefaultApplicationArguments(args);// 预初始化环境： 读取环境变量，读取配置文件信息（基于ApplicationListener）ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); 源码： 1234567891011121314151617181920212223242526272829303132private ConfigurableEnvironment prepareEnvironment(SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) &#123; // 根据webApplicationType 创建Environment // 会读取环境变量和jvm的-D参数 // Servlet是StandardServletEnvironment // 反应式的是StandardReactiveWebEnvironment // 不是上边的，那就是StandardEnvironment ConfigurableEnvironment environment = getOrCreateEnvironment(); // 将启动命令参数读取环境变量中 configureEnvironment(environment, applicationArguments.getSourceArgs()); // 这里会把ConfigurationPropertySourcesPropertySource放在第一位 ConfigurationPropertySources.attach(environment); // 读取配置文件@PropertySource优先级是最低 // 发布了ApplicationEnvironmentPreparedEvent事件，让ConfigFileApplicationListener来处理 // 该类会做一系列的配置加载，比如环境变量，配置文件 listeners.environmentPrepared(environment); // 将所有spring.main 开头的配置信息绑定SpringApplication的属性中 bindToSpringApplication(environment); if (!this.isCustomEnvironment) &#123; environment = new EnvironmentConverter(getClassLoader()).convertEnvironmentIfNecessary(environment, deduceEnvironmentClass()); &#125; //更新PropertySources ConfigurationPropertySources.attach(environment); return environment;&#125; 这里先创建了ConfigurableEnvironment对象来接收会读取环境变量、jvm的-D参数和main方法传入的值。然后调用下面方法 123456// EventPublishingRunListener@Overridepublic void environmentPrepared(ConfigurableEnvironment environment) &#123; this.initialMulticaster .multicastEvent(new ApplicationEnvironmentPreparedEvent(this.application, this.args, environment));&#125; 这里发布了一个ApplicationEnvironmentPreparedEvent事件。 之前有一批ApplicationListener添加到进了事件多播器中，而 ConfigFileApplicationListener就是读取SpringBoot项目的全局配置文件的. 这里个方法会去读 12345678910// 读config文件夹下的file:./config/ // 读项目的根目录下的file:./// 读类路径下的config文件夹classpath:config/// 读类路径下classpath: 这4个路径下的以application开头的配置文件（properties或者yaml文件） spring cloud 还是读取bootstrap 而ConfigFileApplicationListener是通过PropertySourceLoader来完成配置文件的加载的，会通过SpringSPI加载到ConfigFileApplicationListener中： 然后 打印横幅12345678910111213141516// 打印Banner 横幅Banner printedBanner = printBanner(environment);private Banner printBanner(ConfigurableEnvironment environment) &#123; if (this.bannerMode == Banner.Mode.OFF) &#123; return null; &#125; ResourceLoader resourceLoader = (this.resourceLoader != null) ? this.resourceLoader : new DefaultResourceLoader(getClassLoader()); SpringApplicationBannerPrinter bannerPrinter = new SpringApplicationBannerPrinter(resourceLoader, this.banner); if (this.bannerMode == Mode.LOG) &#123; return bannerPrinter.print(environment, this.mainApplicationClass, logger); &#125; return bannerPrinter.print(environment, this.mainApplicationClass, System.out);&#125; 横幅的核心代码在 12345678910111213// SpringApplicationBannerPrinterprivate Banner getBanner(Environment environment) &#123; Banners banners = new Banners(); banners.addIfNotNull(getImageBanner(environment)); banners.addIfNotNull(getTextBanner(environment)); if (banners.hasAtLeastOneBanner()) &#123; return banners; &#125; if (this.fallbackBanner != null) &#123; return this.fallbackBanner; &#125; return DEFAULT_BANNER;&#125; 横幅支持图片和文字，而且这两种可以同时出现 图片横幅对于图片，可以看getImageBanner方法 12345678910111213141516static final String BANNER_IMAGE_LOCATION_PROPERTY = &quot;spring.banner.image.location&quot;;static final String[] IMAGE_EXTENSION = &#123; &quot;gif&quot;, &quot;jpg&quot;, &quot;png&quot; &#125;;private Banner getImageBanner(Environment environment) &#123; String location = environment.getProperty(BANNER_IMAGE_LOCATION_PROPERTY); if (StringUtils.hasLength(location)) &#123; Resource resource = this.resourceLoader.getResource(location); return resource.exists() ? new ImageBanner(resource) : null; &#125; for (String ext : IMAGE_EXTENSION) &#123; Resource resource = this.resourceLoader.getResource(&quot;banner.&quot; + ext); if (resource.exists()) &#123; return new ImageBanner(resource); &#125; &#125; return null;&#125; 这里的意思就是如果环境变量或者配置文件或者启动参数设置了spring.banner.image.location=图片，这个参数的话，那么在启动时就打印这张图片。 如果没有，就看下有没有resource目录下有没有banner.jpg、banner.png、banner.gif&#96;文件。 文字横幅12345678private Banner getTextBanner(Environment environment) &#123; String location = environment.getProperty(&quot;spring.banner.location&quot;, &quot;banner.txt&quot;); Resource resource = this.resourceLoader.getResource(location); if (resource.exists()) &#123; return new ResourceBanner(resource); &#125; return null;&#125; 这个也很简答，就是如果环境变量或者配置文件或者启动参数设置了spring.banner.location=文件 ，那么就答应文件的内容，如果没有就看下resource目录下有没有banner.txt，如果有的话就答应这个banner.txt文件的内容 创建Spring上下文 AnnotationConfigServletWebServerApplicationContext 123ConfigurableApplicationContext context = null; // 创建上下文对象context = createApplicationContext(); 看源码： 12345678910111213141516171819202122232425262728293031323334// Servletpublic static final String DEFAULT_SERVLET_WEB_CONTEXT_CLASS = &quot;org.springframework.boot.&quot; + &quot;web.servlet.context.AnnotationConfigServletWebServerApplicationContext&quot;;// 反应式public static final String DEFAULT_REACTIVE_WEB_CONTEXT_CLASS = &quot;org.springframework.&quot; + &quot;boot.web.reactive.context.AnnotationConfigReactiveWebServerApplicationContext&quot;;// 默认public static final String DEFAULT_CONTEXT_CLASS = &quot;org.springframework.context.&quot; + &quot;annotation.AnnotationConfigApplicationContext&quot;;protected ConfigurableApplicationContext createApplicationContext() &#123; Class&lt;?&gt; contextClass = this.applicationContextClass; if (contextClass == null) &#123; try &#123; switch (this.webApplicationType) &#123; case SERVLET: contextClass = Class.forName(DEFAULT_SERVLET_WEB_CONTEXT_CLASS); break; case REACTIVE: contextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS); break; default: contextClass = Class.forName(DEFAULT_CONTEXT_CLASS); &#125; &#125; catch (ClassNotFoundException ex) &#123; throw new IllegalStateException( &quot;Unable create a default ApplicationContext, please specify an ApplicationContextClass&quot;, ex); &#125; &#125; return (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass);&#125; 预初始化上下文 最后发布ApplicationContextInitializedEvent事件 接着执行了 123// context = AnnotationConfigServletWebServerApplicationContext// environment = ConfigurableEnvironment 包含了全部的变量prepareContext(context, environment, listeners, applicationArguments, printedBanner); 源码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private void prepareContext(ConfigurableApplicationContext context, ConfigurableEnvironment environment, SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) &#123; context.setEnvironment(environment); postProcessApplicationContext(context); // 拿到之前通过SpringSPI获取到的所有ApplicationContextInitializer对象 // 循环调用initialize方法.完成初始化 // 扩展点：可以修改下上下的一些值 applyInitializers(context); // 发布ApplicationContextInitializedEvent事件 listeners.contextPrepared(context); if (this.logStartupInfo) &#123; logStartupInfo(context.getParent() == null); logStartupProfileInfo(context); &#125; // 获取当前spring上下文beanFactory (负责创建bean) ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); beanFactory.registerSingleton(&quot;springApplicationArguments&quot;, applicationArguments); if (printedBanner != null) &#123; beanFactory.registerSingleton(&quot;springBootBanner&quot;, printedBanner); &#125; // 在Spring下如果出现2个重名的bean, 则后读取到的会覆盖前面 // 在SpringBoot 在这里设置了不允许覆盖， 当出现2个重名的bean 会抛出异常 if (beanFactory instanceof DefaultListableBeanFactory) &#123; ((DefaultListableBeanFactory) beanFactory) .setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); &#125; // 设置当前spring容器是不是要将所有的bean设置为懒加载 if (this.lazyInitialization) &#123; context.addBeanFactoryPostProcessor(new LazyInitializationBeanFactoryPostProcessor()); &#125; // Load the sources Set&lt;Object&gt; sources = getAllSources(); Assert.notEmpty(sources, &quot;Sources must not be empty&quot;); // 读取主启动类,将它注册为BD、就像我们以前register(启动类);一个意思 （因为后续要根据配置类解析配置的所有bean) load(context, sources.toArray(new Object[0])); // 读取完配置类后发布ApplicationPreparedEvent事件。 listeners.contextLoaded(context);&#125; 启动Spring容器 AbstractApplicationContext.refresh 12345678910111213141516171819202122// SpringApplication// 核心方法，启动spring容器refreshContext(context);private void refreshContext(ConfigurableApplicationContext context) &#123; // 调用AbstractApplicationContext.refresh // 这里的ApplicationContext是AnnotationConfigServletWebServerApplicationContext refresh(context); // 这里会注册jvm关闭前执行的钩子方法 // 就是和close方法差不多，只是执行的时机不同 // close是由别的地方主动调用的，比如Servlet容器关闭前 // 而这里是jvm关闭前，由jvm主动调用的 if (this.registerShutdownHook) &#123; try &#123; context.registerShutdownHook(); &#125; catch (AccessControlException ex) &#123; // Not allowed in some environments. &#125; &#125;&#125; 这里就是调用AbstractApplicationContext.refresh方法，这个方法在Spring中就已经讲过了。 发起启动完成事件——ApplicationStartedEvent12// SpringApplicationlisteners.started(context); 内嵌Tomcat使用00-嵌入式Tomcat SpringBoot的ApplicationContext的onRefresh方法——Servlet容器初始化SpringBoot内嵌Tomcat启动时发生在AbstractApplicationContext.refresh方法中的，在这个方法中会调用onRefresh，该方法的调用时机发生在BeanDefinition已经创建完成，在Spring Bean的bean初始化前调用的。而这个方法中会启动内嵌tomcat。 从上边的源码分析可知，这里的ApplicationContext是AnnotationConfigServletWebServerApplicationContext，该类重写了onRefresh。看源码 12345678910111213// ServletWebServerApplicationContext// AnnotationConfigServletWebServerApplicationContext的父类@Overrideprotected void onRefresh() &#123; super.onRefresh(); try &#123; createWebServer(); &#125; catch (Throwable ex) &#123; throw new ApplicationContextException(&quot;Unable to start web server&quot;, ex); &#125;&#125; creaetWebServer方法源码 12345678910111213141516171819202122/* * 创建servlet容器 */// ServletWebServerApplicationContextprivate void createWebServer() &#123; WebServer webServer = this.webServer; ServletContext servletContext = getServletContext(); if (webServer == null &amp;&amp; servletContext == null) &#123; ServletWebServerFactory factory = getWebServerFactory(); // 主要看这个方法 this.webServer = factory.getWebServer(getSelfInitializer()); &#125; else if (servletContext != null) &#123; try &#123; getSelfInitializer().onStartup(servletContext); &#125; catch (ServletException ex) &#123; throw new ApplicationContextException(&quot;Cannot initialize servlet context&quot;, ex); &#125; &#125; initPropertySources();&#125; 该方法需要分两种情况 内置的tomcat 外置的tomcat 如果ServletContext servletContext = getServletContext();这行代码方会null就走内置tomcat 内置Tomcat1234// ServletWebServerApplicationContextServletWebServerFactory factory = getWebServerFactory();// 主要看这个方法this.webServer = factory.getWebServer(getSelfInitializer()); 这里的getWebServerFactory就是从beanFactory中获取一个实现了ServletWebServerFactory接口的类。这个接口是SpringBoot提供的，而且也提供了下面的实现类。 而TomcatServletWebServerFactory就是用来创建内嵌Tomcat的。 而这个类是是通过自动配置类ServletWebServerFactoryAutoConfiguration引入的。 看TomcatServletWebServerFactory的getWebServer 12345678910111213141516171819202122@Overridepublic WebServer getWebServer(ServletContextInitializer... initializers) &#123; if (this.disableMBeanRegistry) &#123; Registry.disableRegistry(); &#125; Tomcat tomcat = new Tomcat(); File baseDir = (this.baseDirectory != null) ? this.baseDirectory : createTempDir(&quot;tomcat&quot;); tomcat.setBaseDir(baseDir.getAbsolutePath()); Connector connector = new Connector(this.protocol); connector.setThrowOnFailure(true); tomcat.getService().addConnector(connector); customizeConnector(connector); tomcat.setConnector(connector); tomcat.getHost().setAutoDeploy(false); configureEngine(tomcat.getEngine()); for (Connector additionalConnector : this.additionalTomcatConnectors) &#123; tomcat.getService().addConnector(additionalConnector); &#125; prepareContext(tomcat.getHost(), initializers); // new TomcatWebServer(tomcat, getPort() &gt;= 0) return getTomcatWebServer(tomcat);&#125; 会在创建TomcatWebServer对象的同时，启动tomcat 123456public TomcatWebServer(Tomcat tomcat, boolean autoStart) &#123; Assert.notNull(tomcat, &quot;Tomcat Server must not be null&quot;); this.tomcat = tomcat; this.autoStart = autoStart; initialize();&#125; 外置的tomcat使用： 下载tomcat服务 设置当前maven项目的打包方式 12&lt;!--打包方式 默认是jar--&gt;&lt;packaging&gt;war&lt;/packaging&gt; 让tomcat相关的依赖不参与打包部署 ，因为外置tomcat服务器已经有这些jar包 123456&lt;!--让它不参与打包部署--&gt;&lt;dependency&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 为了让它支持springboot需要加上： 才能启动springboot应用 123456public class TomcatStartSpringBoot extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) &#123; return builder.sources(Application.class); &#125;&#125; 放到tomcat运行 这里讲的重点在第4步，这一步其实就和单纯的使用SpringMVC一样。单纯使用Spring的话，我们在项目中需要提供一个继承AbstractAnnotationConfigDispatcherServletInitializer的类。而SpringBoot是SpringBootServletInitializer。 SpringMVC详情看springMVC启动讲解.md 所以看SpringBootServletInitializer的onStartup方法， 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public void onStartup(ServletContext servletContext) throws ServletException &#123; // Logger initialization is deferred in case an ordered // LogServletContextInitializer is being used this.logger = LogFactory.getLog(getClass()); WebApplicationContext rootAppContext = createRootApplicationContext(servletContext); if (rootAppContext != null) &#123; servletContext.addListener(new ContextLoaderListener(rootAppContext) &#123; @Override public void contextInitialized(ServletContextEvent event) &#123; // no-op because the application context is already initialized &#125; &#125;); &#125; else &#123; this.logger.debug(&quot;No ContextLoaderListener registered, as createRootApplicationContext() did not &quot; + &quot;return an application context&quot;); &#125;&#125;protected WebApplicationContext createRootApplicationContext(ServletContext servletContext) &#123; SpringApplicationBuilder builder = createSpringApplicationBuilder(); builder.main(getClass()); ApplicationContext parent = getExistingRootWebApplicationContext(servletContext); if (parent != null) &#123; this.logger.info(&quot;Root context already created (using as parent).&quot;); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, null); builder.initializers(new ParentContextApplicationContextInitializer(parent)); &#125; builder.initializers(new ServletContextApplicationContextInitializer(servletContext)); builder.contextClass(AnnotationConfigServletWebServerApplicationContext.class); builder = configure(builder); builder.listeners(new WebEnvironmentPropertySourceInitializer(servletContext)); SpringApplication application = builder.build(); if (application.getAllSources().isEmpty() &amp;&amp; MergedAnnotations.from(getClass(), SearchStrategy.TYPE_HIERARCHY).isPresent(Configuration.class)) &#123; application.addPrimarySources(Collections.singleton(getClass())); &#125; Assert.state(!application.getAllSources().isEmpty(), &quot;No SpringApplication sources have been defined. Either override the &quot; + &quot;configure method or add an @Configuration annotation&quot;); // Ensure error pages are registered if (this.registerErrorPageFilter) &#123; application.addPrimarySources(Collections.singleton(ErrorPageFilterConfiguration.class)); &#125; return run(application);&#125; 这里很简答，就是 1234SpringApplication springApplication = new SpringApplication(Application.class);// 有很多属性可以设置// springApplication.setspringApplication.run(args); 而在springApplication启动的时候，会通过Sping SPI引入DispatcherServletAutoConfiguration自动配置类，@Bean了DispatcherServletRegistrationBean进去。 回到ServletWebServerApplicationContext.createWebServer方法，外置Tomcat会走到这里 1234567891011121314getSelfInitializer().onStartup(servletContext);private org.springframework.boot.web.servlet.ServletContextInitializer getSelfInitializer() &#123; return this::selfInitialize;&#125;private void selfInitialize(ServletContext servletContext) throws ServletException &#123; prepareWebApplicationContext(servletContext); registerApplicationScope(servletContext); WebApplicationContextUtils.registerEnvironmentBeans(getBeanFactory(), servletContext); for (ServletContextInitializer beans : getServletContextInitializerBeans()) &#123; beans.onStartup(servletContext); &#125;&#125; getServletContextInitializerBeans方法就会获取到DispatcherServletRegistrationBean，然后执行onStartup。 最后 1servletContext.addServlet(servletName, dispatcherServlet);","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://sv.pointcut.cc/tags/springboot/"}]},{"title":"02-为什么SpringBoot的jar可以直接运行","slug":"springboot/02-为什么SpringBoot的jar可以直接运行","date":"2021-11-26T12:00:05.000Z","updated":"2022-03-23T09:03:55.991Z","comments":true,"path":"blog/springboot/02-为什么SpringBoot的jar可以直接运行/","link":"","permalink":"http://sv.pointcut.cc/blog/springboot/02-%E4%B8%BA%E4%BB%80%E4%B9%88SpringBoot%E7%9A%84jar%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E8%BF%90%E8%A1%8C/","excerpt":"","text":"SpringBoot项目我们可以只通过jar -jar一行命令便启动一个web项目。 java -jar做了什么先要弄清楚java -jar命令做了什么，在oracle官网找到了该命令的描述： 1If the -jar option is specified, its argument is the name of the JAR file containing class and resource files for the application. The startup class must be indicated by the Main-Class manifest header in its source code. 翻译 1如果指定了-jar参数，后面的参数就是jar文件名，这个jar文件包含了应用的class文件和资源文件。启动类必须由源码中的manifest的Main-Class指定 其实就是java -jar中会去jar包中找META-INF/MANIFEST.MF文件，在这个文件中找到真正的启动类 这是某个SpringBoot项目的jar包中的文件 这里就是通过 1Main-Class: org.springframework.boot.loader.JarLauncher 指定了真正的启动类。 疑惑出现在MANIFEST.MF文件中有这么一行内容： 1Start-Class: com.tulingxueyuan.Application 前面的java官方文档中，只提到过Main-Class ，并没有提到Start-Class。 Start-Class的值是com.tulingxueyuan.Application，这是我们的java代码中的唯一类，也是应用的真正的应用启动类； 所以问题就来了：理论上看，执行java -jar命令时JarLauncher类会被执行，但实际上是com.tulingxueyuan.Application被执行了，这其中发生了什么呢？为什么要这么做呢？ 其中，Java没有提供任何标准的方式来加载嵌套的jar文件（即，它们本身包含在jar中的jar文件）。 Jar包的打包插件及核心方法Spring Boot项目的pom.xml文件中默认使用如下插件进行打包： 12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 执行maven clean package之后，会生成两个文件： 12springboot_starter-0.0.1-SNAPSHOT.jarspringboot_starter-0.0.1-SNAPSHOT.jar.original spring-boot-maven-plugin项目存在于spring-boot-tools目录中。spring-boot-maven-plugin默认有5个goals：repackage、run、start、stop、build-info。 在打包的时候默认使用的是repackage。 spring-boot-maven-plugin的repackage能够将mvn package生成的软件包，再次打包为可执行的软件包，并将mvn package生成的软件包重命名为*.original。 spring-boot-maven-plugin的repackage在代码层面调用了RepackageMojo的execute方法，而在该方法中又调用了repackage方法。repackage方法代码及操作解析如下： 123456789101112131415161718192021222324private void repackage() throws MojoExecutionException &#123; // maven生成的jar，最终的命名将加上.original后缀 Artifact source = getSourceArtifact(); // 最终为可执行jar，即fat jar File target = getTargetFile(); // 获取重新打包器，将maven生成的jar重新打包成可执行jar Repackager repackager = getRepackager(source.getFile()); // 查找并过滤项目运行时依赖的jar Set&lt;Artifact&gt; artifacts = filterDependencies(this.project.getArtifacts(), getFilters(getAdditionalFilters())); // 将artifacts转换成libraries Libraries libraries = new ArtifactsLibraries(artifacts, this.requiresUnpack, getLog()); try &#123; // 获得Spring Boot启动脚本 LaunchScript launchScript = getLaunchScript(); // 执行重新打包，生成fat jar repackager.repackage(target, libraries, launchScript); &#125;catch (IOException ex) &#123; throw new MojoExecutionException(ex.getMessage(), ex); &#125; // 将maven生成的jar更新成.original文件 updateArtifact(source, target, repackager.getBackupFile());&#125; 执行以上命令之后，便生成了打包结果对应的两个文件。下面针对文件的内容和结构进行一探究竟。 jar包目录结构首先来看看jar的目录结构，都包含哪些目录和文件，解压jar包可以看到如下结构： 12345678910111213springboot_starter-0.0.1-SNAPSHOT.jar├── META-INF│ └── MANIFEST.MF├── BOOT-INF│ ├── classes│ │ └── 应用程序类│ └── lib│ └── 第三方依赖jar└── org └── springframework └── boot └── loader └── springboot启动程序 META-INF内容1234567891011Manifest-Version: 1.0Created-By: Maven Jar Plugin 3.2.0Build-Jdk-Spec: 17Implementation-Title: 12_springboot_starterImplementation-Version: 0.0.1-SNAPSHOTMain-Class: org.springframework.boot.loader.JarLauncherStart-Class: com.tulingxueyuan.ApplicationSpring-Boot-Version: 2.3.6.RELEASESpring-Boot-Classes: BOOT-INF/classes/Spring-Boot-Lib: BOOT-INF/lib/Spring-Boot-Classpath-Index: BOOT-INF/classpath.idx 可以看到有Main-Class是org.springframework.boot.loader.JarLauncher ，这个是jar启动的Main函数。 还有一个Start-Class是com.tulingxueyuan.Application，这个是我们应用自己的Main函数。 Archive的概念在继续了解底层概念和原理之前，我们先来了解一下Archive的概念： archive即归档文件，这个概念在linux下比较常见。 通常就是一个tar&#x2F;zip格式的压缩包。 jar是zip格式。 SpringBoot抽象了Archive的概念，一个Archive可以是jar（JarFileArchive），可以是一个文件目录（ExplodedArchive），可以抽象为统一访问资源的逻辑层。关于Spring Boot中Archive的源码如下： 12345678public interface Archive extends Iterable&lt;Archive.Entry&gt; &#123; // 获取该归档的url URL getUrl() throws MalformedURLException; // 获取jar!/META-INF/MANIFEST.MF或[ArchiveDir]/META-INF/MANIFEST.MF Manifest getManifest() throws IOException; // 获取jar!/BOOT-INF/lib/*.jar或[ArchiveDir]/BOOT-INF/lib/*.jar List&lt;Archive&gt; getNestedArchives(EntryFilter filter) throws IOException;&#125; SpringBoot定义了一个接口用于描述资源，也就是org.springframework.boot.loader.archive.Archive。该接口有两个实现，分别是org.springframework.boot.loader.archive.ExplodedArchive和org.springframework.boot.loader.archive.JarFileArchive。前者用于在文件夹目录下寻找资源，后者用于在jar包环境下寻找资源。而在SpringBoot打包的fatJar中，则是使用后者。 JarFile：对jar包的封装，每个JarFileArchive都会对应一个JarFile。JarFile被构造的时候会解析内部结构，去获取jar包里的各个文件或文件夹，这些文件或文件夹会被封装到Entry中，也存储在JarFileArchive中。如果Entry是个jar，会解析成JarFileArchive。 比如一个JarFileArchive对应的URL为： 1jar:file:/Users/format/Develop/gitrepository/springboot-analysis/springboot-executable-jar/target/executable-jar-1.0-SNAPSHOT.jar!/ 它对应的JarFile为： 1/Users/format/Develop/gitrepository/springboot-analysis/springboot-executable-jar/target/executable-jar-1.0-SNAPSHOT.jar 这个JarFile有很多Entry，比如： 123456789META-INF/META-INF/MANIFEST.MFspring/spring/study/....spring/study/executablejar/ExecutableJarApplication.classlib/spring-boot-starter-1.3.5.RELEASE.jarlib/spring-boot-1.3.5.RELEASE.jar... JarFileArchive内部的一些依赖jar对应的URL(SpringBoot使用org.springframework.boot.loader.jar.Handler处理器来处理这些URL)： 123jar:file:/Users/Format/Develop/gitrepository/springboot-analysis/springboot-executable-jar/target/executable-jar-1.0-SNAPSHOT.jar!/lib/spring-boot-starter-web-1.3.5.RELEASE.jar!/jar:file:/Users/Format/Develop/gitrepository/springboot-analysis/springboot-executable-jar/target/executable-jar-1.0-SNAPSHOT.jar!/lib/spring-boot-loader-1.3.5.RELEASE.jar!/org/springframework/boot/loader/JarLauncher.class 我们看到如果有jar包中包含jar，或者jar包中包含jar包里面的class文件，那么会使用 !&#x2F; 分隔开，这种方式只有org.springframework.boot.loader.jar.Handler能处理，它是SpringBoot内部扩展出来的一种URL协议。 ### JarLauncher 从MANIFEST.MF可以看到Main函数是JarLauncher，下面来分析它的工作流程。JarLauncher类的继承结构是： 12class JarLauncher extends ExecutableArchiveLauncherclass ExecutableArchiveLauncher extends Launcher 按照定义，JarLauncher可以加载内部&#x2F;BOOT-INF&#x2F;lib下的jar及&#x2F;BOOT-INF&#x2F;classes下的应用class，其实JarLauncher实现很简单： 123456public class JarLauncher extends ExecutableArchiveLauncher &#123; public JarLauncher() &#123;&#125; public static void main(String[] args) throws Exception &#123; new JarLauncher().launch(args); &#125;&#125; 其主入口新建了JarLauncher并调用父类Launcher中的launch方法启动程序。在创建JarLauncher时，父类ExecutableArchiveLauncher找到自己所在的jar，并创建archive。 JarLauncher继承于org.springframework.boot.loader.ExecutableArchiveLauncher。该类的无参构造方法最主要的功能就是构建了当前main方法所在的FatJar的JarFileArchive对象。下面来看launch方法。该方法主要是做了2个事情： 以FatJar为file作为入参，构造JarFileArchive对象。获取其中所有的资源目标，取得其Url，将这些URL作为参数，构建了一个URLClassLoader。 以第一步构建的ClassLoader加载MANIFEST.MF文件中Start-Class指向的业务类，并且执行静态方法main。进而启动整个程序。 1234567891011121314151617181920212223242526272829303132public abstract class ExecutableArchiveLauncher extends Launcher &#123; private final Archive archive; public ExecutableArchiveLauncher() &#123; try &#123; // 找到自己所在的jar，并创建Archive this.archive = createArchive(); &#125; catch (Exception ex) &#123; throw new IllegalStateException(ex); &#125; &#125;&#125; public abstract class Launcher &#123; protected final Archive createArchive() throws Exception &#123; ProtectionDomain protectionDomain = getClass().getProtectionDomain(); CodeSource codeSource = protectionDomain.getCodeSource(); URI location = (codeSource == null ? null : codeSource.getLocation().toURI()); String path = (location == null ? null : location.getSchemeSpecificPart()); if (path == null) &#123; throw new IllegalStateException(&quot;Unable to determine code source archive&quot;); &#125; File root = new File(path); if (!root.exists()) &#123; throw new IllegalStateException( &quot;Unable to determine code source archive from &quot; + root); &#125; return (root.isDirectory() ? new ExplodedArchive(root) : new JarFileArchive(root)); &#125;&#125; 在Launcher的launch方法中，通过以上archive的getNestedArchives方法找到&#x2F;BOOT-INF&#x2F;lib下所有jar及&#x2F;BOOT-INF&#x2F;classes目录所对应的archive，通过这些archives的url生成LaunchedURLClassLoader，并将其设置为线程上下文类加载器，启动应用。 1234567891011121314151617181920212223protected void launch(String[] args) throws Exception &#123; JarFile.registerUrlProtocolHandler(); ClassLoader classLoader = createClassLoader(getClassPathArchives()); launch(args, getMainClass(), classLoader);&#125;protected List&lt;Archive&gt; getClassPathArchives() throws Exception &#123; List&lt;Archive&gt; archives = new ArrayList&lt;&gt;(this.archive.getNestedArchives(this::isNestedArchive)); postProcessClassPathArchives(archives); return archives;&#125;protected ClassLoader createClassLoader(List&lt;Archive&gt; archives) throws Exception &#123; List&lt;URL&gt; urls = new ArrayList&lt;&gt;(archives.size()); for (Archive archive : archives) &#123; urls.add(archive.getUrl()); &#125; return createClassLoader(urls.toArray(new URL[0]));&#125;protected ClassLoader createClassLoader(URL[] urls) throws Exception &#123; return new LaunchedURLClassLoader(urls, getClass().getClassLoader());&#125; 至此，才执行我们应用程序主入口类的main方法，所有应用程序类文件均可通过&#x2F;BOOT-INF&#x2F;classes加载，所有依赖的第三方jar均可通过&#x2F;BOOT-INF&#x2F;lib加载。 123456protected void launch(String[] args, String mainClass, ClassLoader classLoader) throws Exception &#123; // 关键步骤，这时了启动线程的ClassLoader。 Thread.currentThread().setContextClassLoader(classLoader); // 这里知识通过反射调用main方法 createMainMethodRunner(mainClass, args, classLoader).run();&#125; URLStreamHandlerjava中描述资源常使用URL。而URL有一个方法用于打开链接java.net.URL#openConnection()。由于URL用于表达各种各样的资源，打开资源的具体动作由java.net.URLStreamHandler这个类的子类来完成。根据不同的协议，会有不同的handler实现。而JDK内置了相当多的handler实现用于应对不同的协议。比如jar、file、http等等。URL内部有一个静态HashTable属性，用于保存已经被发现的协议和handler实例的映射。 获得URLStreamHandler有三种方法： 实现URLStreamHandlerFactory接口，通过方法URL.setURLStreamHandlerFactory设置。该属性是一个静态属性，且只能被设置一次。 直接提供URLStreamHandler的子类，作为URL的构造方法的入参之一。但是在JVM中有固定的规范要求： 子类的类名必须是Handler，同时最后一级的包名必须是协议的名称。比如自定义了Http的协议实现，则类名必然为xx.http.Handler； 具体的需要看JDK提供的URL类 JVM启动的时候，需要设置java.protocol.handler.pkgs系统属性，如果有多个实现类，那么中间用|隔开。因为JVM在尝试寻找Handler时，会从这个属性中获取包名前缀，最终使用包名前缀.协议名.Handler，使用Class.forName方法尝试初始化类，如果初始化成功，则会使用该类的实现作为协议实现。而SpringBoot就是使用了这种模式 为了实现这个目标，SpringBoot首先从支持jar in jar中内容读取做了定制，也就是支持多个!&#x2F;分隔符的url路径。SpringBoot定制了以下两个方面： （1）实现了一个java.net.URLStreamHandler的子类org.springframework.boot.loader.jar.Handler。该Handler支持识别多个!&#x2F;分隔符，并且正确的打开URLConnection。打开的Connection是SpringBoot定制的org.springframework.boot.loader.jar.JarURLConnection实现。 （2）实现了一个java.net.JarURLConnection的子类org.springframework.boot.loader.jar.JarURLConnection。该链接支持多个!&#x2F;分隔符，并且自己实现了在这种情况下获取InputStream的方法。而为了能够在org.springframework.boot.loader.jar.JarURLConnection正确获取输入流，SpringBoot自定义了一套读取ZipFile的工具类和方法。 在JarLauncher的launch方法中有这一行代码 1JarFile.registerUrlProtocolHandler(); 这样就是注册SpringBoot定义的Handler 123456789private static final String PROTOCOL_HANDLER = &quot;java.protocol.handler.pkgs&quot;;private static final String HANDLERS_PACKAGE = &quot;org.springframework.boot.loader&quot;;public static void registerUrlProtocolHandler() &#123; String handlers = System.getProperty(PROTOCOL_HANDLER, &quot;&quot;); System.setProperty(PROTOCOL_HANDLER, (&quot;&quot;.equals(handlers) ? HANDLERS_PACKAGE : handlers + &quot;|&quot; + HANDLERS_PACKAGE)); resetCachedUrlHandlers();&#125; Spring Boot的Jar应用启动流程总结Spring Boot应用打包之后，生成一个Fat jar，包含了应用依赖的jar包和Spring Boot loader相关的类。 Fat jar的启动Main函数是JarLauncher，它负责创建一个LaunchedURLClassLoader来加载&#x2F;lib下面的jar，并以一个新线程调用应用设置的的Main函数。这个main函数所在的类就是通过META-INF/MANIFEST.MF文件的Start-Class指定的。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://sv.pointcut.cc/tags/springboot/"}]},{"title":"01-自动配置原理","slug":"springboot/01-自动配置原理","date":"2021-11-26T12:00:04.000Z","updated":"2022-03-23T09:03:55.988Z","comments":true,"path":"blog/springboot/01-自动配置原理/","link":"","permalink":"http://sv.pointcut.cc/blog/springboot/01-%E8%87%AA%E5%8A%A8%E9%85%8D%E7%BD%AE%E5%8E%9F%E7%90%86/","excerpt":"","text":"依赖Spring的知识 简答介绍SpringBoot使用了Spring的DeferredImportSelector接口和SpringSPI来完成自动配置类的引入；然后在引入类中，通过各种注解（@Bean、各种@Conditional注解等）来完成类的引入。 自动配置类的引入流程分析项目中都是有这样的一个启动类 1234567@SpringBootApplicationpublic class ConfigurationFileApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigurationFileApplication.class, args); &#125;&#125; 该启动类有@SpringBootApplication注解。看该注解的定义 1234567891011@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication &#123; 。。。&#125; 这里重点看@EnableAutoConfiguration 123456789@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import(AutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123; ....&#125; 该注解的作用就是@Import一个类，该类为AutoConfigurationImportSelector。也就意味着该类会被注册到Spring的容器中。看该类的类图： 该类实现了DeferredImportSelector接口。 DeferredImportSelector接口的作用、初始化和执行原理 而且AutoConfigurationImportSelector重写了getImportGroup方法，并返回了AutoConfigurationGroup的Class对象。所以现在的重点看AutoConfigurationGroup的process方法和selectImports方法 SpringSPI的运用——获取自动配置类看AutoConfigurationGroup.process方法。 12345678910111213// AutoConfigurationGrouppublic void process(AnnotationMetadata annotationMetadata, DeferredImportSelector deferredImportSelector) &#123; Assert.state(deferredImportSelector instanceof AutoConfigurationImportSelector, () -&gt; String.format(&quot;Only %s implementations are supported, got %s&quot;, AutoConfigurationImportSelector.class.getSimpleName(), deferredImportSelector.getClass().getName())); AutoConfigurationEntry autoConfigurationEntry = ((AutoConfigurationImportSelector) deferredImportSelector) .getAutoConfigurationEntry(getAutoConfigurationMetadata(), annotationMetadata); this.autoConfigurationEntries.add(autoConfigurationEntry); for (String importClassName : autoConfigurationEntry.getConfigurations()) &#123; this.entries.putIfAbsent(importClassName, annotationMetadata); &#125;&#125; 这里会调用AutoConfigurationImportSelectord的deferredImportSelector方法，看该方法 12345678910111213141516171819// AutoConfigurationImportSelectorprotected AutoConfigurationEntry getAutoConfigurationEntry(AutoConfigurationMetadata autoConfigurationMetadata, AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return EMPTY_ENTRY; &#125; AnnotationAttributes attributes = getAttributes(annotationMetadata); // SPI获取EnableAutoConfiguration为key的所有类 List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); // 把某些自动配置类过滤掉 configurations = filter(configurations, autoConfigurationMetadata); fireAutoConfigurationImportEvents(configurations, exclusions); // 包装成自动配置实体类 return new AutoConfigurationEntry(configurations, exclusions);&#125; 首先，该方法通过Sping的SPI获取到一批全限定名的列表 12345678910111213141516List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes);// AutoConfigurationImportSelectorprotected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123; // SPI获取EnableAutoConfiguration为key的所有实现类 List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, &quot;No auto configuration classes found in META-INF/spring.factories. If you &quot; + &quot;are using a custom packaging, make sure that file is correct.&quot;); return configurations;&#125;// AutoConfigurationImportSelectorprotected Class&lt;?&gt; getSpringFactoriesLoaderFactoryClass() &#123; return EnableAutoConfiguration.class;&#125; 上边的代码就是获取项目中META-INF/spring.factories文件定义的 自动配置类列表 而这些类在项目中不一定存在和不一定用到，所以在后面的代码就是对其进行过滤的，这里有两种过滤方式： 通过@SpringBootApplication注解的exclude或者excludeName或者配置spring.autoconfigure.exclude 1234// AutoConfigurationImportSelector.getAutoConfigurationEntrySet&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes);checkExcludedClasses(configurations, exclusions);configurations.removeAll(exclusions); 通过AutoConfigurationImportFilter来过滤。 12345678910111213141516171819202122// AutoConfigurationImportSelector.getAutoConfigurationEntryconfigurations = getConfigurationClassFilter().filter(configurations);// AutoConfigurationImportSelectorprivate ConfigurationClassFilter getConfigurationClassFilter() &#123; if (this.configurationClassFilter == null) &#123; // 这里通过SPI获取key为AutoConfigurationImportFilter的实现类 List&lt;AutoConfigurationImportFilter&gt; filters = getAutoConfigurationImportFilters(); for (AutoConfigurationImportFilter filter : filters) &#123; // 检查是否有实现这些接口。 // ResourceLoaderAware、EnvironmentAware、BeanFactoryAware、BeanClassLoaderAware invokeAwareMethods(filter); &#125; this.configurationClassFilter = new ConfigurationClassFilter(this.beanClassLoader, filters); &#125; return this.configurationClassFilter;&#125;// AutoConfigurationImportSelectorprotected List&lt;AutoConfigurationImportFilter&gt; getAutoConfigurationImportFilters() &#123; return SpringFactoriesLoader.loadFactories(AutoConfigurationImportFilter.class, this.beanClassLoader);&#125; 这里的过滤规则就是根据根据AutoConfigurationImportFilter的返回结果boolean[]，如果这个数组的某个位置为false，就把configurations的对应位置清除掉。 过滤完后就发布一个AutoConfigurationImportEvent事件，该事件由AutoConfigurationImportListener的实现类处理。 1234567891011121314151617// AutoConfigurationImportSelector.getAutoConfigurationEntryfireAutoConfigurationImportEvents(configurations, exclusions);// AutoConfigurationImportSelectorprivate void fireAutoConfigurationImportEvents(List&lt;String&gt; configurations, Set&lt;String&gt; exclusions) &#123; // 用Spring SPI获取AutoConfigurationImportListener的实现类 List&lt;AutoConfigurationImportListener&gt; listeners = getAutoConfigurationImportListeners(); if (!listeners.isEmpty()) &#123; AutoConfigurationImportEvent event = new AutoConfigurationImportEvent(this, configurations, exclusions); for (AutoConfigurationImportListener listener : listeners) &#123; // 检查是否有实现这些接口。 // ResourceLoaderAware、EnvironmentAware、BeanFactoryAware、BeanClassLoaderAware invokeAwareMethods(listener); listener.onAutoConfigurationImportEvent(event); &#125; &#125;&#125; 最后返回AutoConfigurationEntry对象。返回后就是把AutoConfigurationEntry对象放到autoConfigurationEntries集合中。这个集合就在AutoConfigurationGroup.selectImports中遍历处理。 AutoConfigurationGroup.selectImports方法不看了很简单。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://sv.pointcut.cc/tags/springboot/"}]},{"title":"00-注解说明","slug":"springboot/00-注解说明","date":"2021-11-26T12:00:03.000Z","updated":"2022-03-23T09:03:55.966Z","comments":true,"path":"blog/springboot/00-注解说明/","link":"","permalink":"http://sv.pointcut.cc/blog/springboot/00-%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E/","excerpt":"","text":"jdk注解定义使用@interface，表示该类是一个注解 123456@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inheritedpublic @interface AnnotationDemo &#123;&#125; 这里又涉及了几个jdk提供的注解 @Target 设置当前注解可以标记在哪，常用的有 1234ElementType.TYPE //对象ElementType.METHOD //方法ElementType.FIELD //字段ElementType.PARAMETER //参数 @Retention 当注解标注的类编译以什么方式保留，RetentionPolicy.RUNTIME表示会被jvm加载 @Documented java doc 会生成注解信息 @Inherited 是否会被继承 @SpringBootApplicationSpring Boot应用标注在某个类上说明这个类是SpringBoot的主配置类，SpringBoot 需要运行这个类的main方法来启动SpringBoot应用; @Configuration配置类上来标注这个注解。配置类 —– 配置文件;配置类也是容器中的一个组件;@Component。 其中有个属性proxyBeanMethods，该属性表示是否生成代理对象。默认值为true，生成代理对象后，通过对象的方法调用获取的bean就能保证多次调用都只生成一个。 @SpringBootConfigurationSpring Boot的配置类。标注在某个类上，表示这是一个Spring Boot的配置类；该注解其实就是@Configuration注解，只是名字换了而已 @EnableAutoConfiguration开启自动配置功能；以前我们需要配置的东西，Spring Boot帮我们自动配置;@EnableAutoConguration告诉SpringBoot开启自动配置，会帮我们自动去加载 自动配置类。在SpringBoot应用中，我们不需要显示的使用该注解 @ComponentScan扫描指定包下，有@Component注解的类，把这些类变成BeanDefinition。 @AutoConfigurationPackage将当前配置类所在包保存在BasePackages的Bean中。供Spring内部使用 @ImportSpring常用的扩展方式，通过该注解会引入一个类，来实现扩展 @EnableConfigurationProperties比如这样定义 1@EnableConfigurationProperties(&#123;ServerProperties.class&#125;) 表示将配置文件中对应的值和 ServerProperties绑定起来；并把 ServerProperties加入到 IOC 容器中。并注册ConfigurationPropertiesBindingPostProcessor用于将 @ConfigurationProperties的类和配置进行绑定 @ConfigurationProperties讲配置和类进行绑定，并将类注册的Spring的容器中 12345@ConfigurationProperties(prefix=&quot;xyz.demo&quot;)public class ConfigurationPropertiesDemo &#123; private String name; private Integer age;&#125; 通过这样设置后，只要配置中存在 1xyz.demo.name=xieyuezhi 等这样的配置，就会把对应的值设置到ConfigurationPropertiesDemo对象中。这样在项目中就能通过注入使用了。 @Conditional必须是@Conditional指定的条件成立，才给容器中添加组件，配置配里面的所有内容才生效; 派生注解 @Conditional扩展注解作用 (判断是否满足当前指定条件) @ConditionalOnJava 系统的java版本是否符合要求 @ConditionalOnBean 容器中存在指定Bean; @ConditionalOnMissingBean 容器中不存在指定Bean; @ConditionalOnExpression 满足SpEL表达式指定 @ConditionalOnClass 系统中有指定的类 @ConditionalOnMissingClass 系系统中没有指定的类 @ConditionalOnSingleCandidate 容器中只有一个指定的Bean，或者这个Bean是首选Bean @ConditionalOnProperty 系统中指定的属性是否有指定的值 @ConditionalOnResource 类路径下是否存在指定资源文件 @ConditionalOnWebApplication 当前是web环境 @ConditionalOnNotWebApplication 当前不是web环境 @ConditionalOnJndi JNDI存在指定 比如： @Configuration:表示这是一个配置类，以前编写的配置文件一样，也可以给容器中添加组件。 @ConditionalOnWebApplication:Spring底层@Conditional注解(Spring注解版)，根据不同的条件，如果满足指定的条 件，整个配置类里面的配置就会生效; 判断当前应用是否是web应用，如果是，当前配置类生效。 @ConditionalOnClass:判断当前项目有没有这个类CharacterEncodingFilter；SpringMVC中进行乱码解决的过滤器。 @ConditionalOnProperty:判断配置文件中是否存在某个配置 spring.http.encoding.enabled;如果不存在，其中matchIfMissing &#x3D; true表示，没有指定配置时也能生效 @EnableConfigurationProperties({ServerProperties.class}):将配置文件中对应的值和 ServerProperties绑定起来; 并把 ServerProperties加入到 IOC 容器中。并注册ConfigurationPropertiesBindingPostProcessor用于将 @ConfigurationProperties的类和配置进行绑定","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://sv.pointcut.cc/tags/springboot/"}]},{"title":"00-嵌入式Tomcat","slug":"springboot/00-嵌入式Tomcat","date":"2021-11-26T12:00:02.000Z","updated":"2022-03-23T09:03:55.953Z","comments":true,"path":"blog/springboot/00-嵌入式Tomcat/","link":"","permalink":"http://sv.pointcut.cc/blog/springboot/00-%E5%B5%8C%E5%85%A5%E5%BC%8FTomcat/","excerpt":"","text":"Maven: 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-core&lt;/artifactId&gt; &lt;version&gt;8.5.34&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617181920//自定义的一个Servlet(专门处理http请求)HttpServlet httpServlet = new HttpServlet() &#123; @Override public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123; res.getWriter().write(&quot;hello world!&quot;); &#125;&#125;;//引入嵌入式TomcatTomcat tomcat = new Tomcat();//部署应用的contextContext context = tomcat.addContext(&quot;/demo&quot;,null);//相当于往应用中添加Servlettomcat.addServlet(context,&quot;hello&quot;,httpServlet);//相当于添加了servletMapping 映射信息context.addServletMappingDecoded(&quot;/hello&quot;,&quot;hello&quot;);//启动Tomcat ---生命周期tomcat.init();tomcat.start();tomcat.getServer().await();//用于阻塞Tomcat,等待请求过来//http://localhost:8080/demo/hello 123456Tomcat tomcat = new Tomcat();tomcat.addWebapp(&quot;/ref&quot;,&quot;D:\\\\work_tomcat\\\\ref-comet&quot;);tomcat.getConnector().setPort(80);tomcat.init();tomcat.start();tomcat.getServer().await();","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://sv.pointcut.cc/tags/springboot/"}]},{"title":"00-如何调试jar包","slug":"springboot/00-如何调试jar包","date":"2021-11-26T12:00:01.000Z","updated":"2022-03-23T09:03:55.953Z","comments":true,"path":"blog/springboot/00-如何调试jar包/","link":"","permalink":"http://sv.pointcut.cc/blog/springboot/00-%E5%A6%82%E4%BD%95%E8%B0%83%E8%AF%95jar%E5%8C%85/","excerpt":"","text":"在IDEA中添加一个 配置：","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://sv.pointcut.cc/tags/springboot/"}]},{"title":"从一个带范型的接口中发现了一个问题","slug":"spring/从一个带范型的接口中发现了一个问题","date":"2021-11-25T12:00:50.000Z","updated":"2022-03-23T09:03:55.943Z","comments":true,"path":"blog/spring/从一个带范型的接口中发现了一个问题/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/%E4%BB%8E%E4%B8%80%E4%B8%AA%E5%B8%A6%E8%8C%83%E5%9E%8B%E7%9A%84%E6%8E%A5%E5%8F%A3%E4%B8%AD%E5%8F%91%E7%8E%B0%E4%BA%86%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98/","excerpt":"","text":"发现一个有意思的，以前没想过这个问题。 定了一个接口 123public interface TestBean&lt;T&gt; &#123; void print(T t);&#125; 实现类是 12345public class SubTestBean implements TestBean&lt;String&gt; &#123; @Override public void print(String s) &#123; &#125;&#125; 这是一个测试： 12345@Testpublic void test6() &#123; Method[] methods = ReflectionUtils.getDeclaredMethods(SubTestBean.class); Arrays.stream(methods).forEach(System.out::println);&#125; 打印的结果： 多了个print(Object)方法。 分析jvm的编译器对于范型是使用类型擦除的，也就是会把T变成Object，这时接口就变成了 123public interface TestBean&lt;Object&gt; &#123; void print(Object t);&#125; 那么实现类中的这个方法 1public void print(String s) &#123;&#125; 只是一个重载的方法，但由于接口的方法是必须在非抽象类中实现的，那么jvm就帮我们写了一个桥接方法，这个方法是这样定义的: 123public void print(Object s) &#123; this.print((String)s);&#125; 所以对于方法 1public void print(String s) &#123;&#125; 这个只是一个重载的方法，重载了桥接方法。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"spel表达式2","slug":"spring/spel表达式2","date":"2021-11-25T12:00:49.000Z","updated":"2022-03-23T09:03:55.941Z","comments":true,"path":"blog/spring/spel表达式2/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/spel%E8%A1%A8%E8%BE%BE%E5%BC%8F2/","excerpt":"","text":"SpEL简介​ 在Spring3中就已经支持EL表达式了, Spring Expression Language(SpEL)是类似于OGNL和JSF EL的表达式语言, 能够在运行时构建复杂表达式, 存取对象属性、调用对象方法等, 而且所有的SpEL都支持XML和Annotation两种方式, 使用的格式均为:#{SpEL expression}. 比如在XML中使用的例子: 12345&lt;bean id=&quot;helloBean&quot; class=&quot;com.***.Hello&quot;&gt; &lt;property name=&quot;item&quot; value=&quot;#&#123;otherBean&#125;&quot; /&gt; &lt;!-- 把otherBean注入到helloBean的item属性中 --&gt; &lt;property name=&quot;itemName&quot; value=&quot;#&#123;otherBean.name&#125;&quot; /&gt; &lt;!-- 把otherBean的name注入到helloBean的itemName属性中 --&gt;&lt;/bean&gt;1234 在Annotation中使用的例子: 12345@Componentpublic class Test &#123; @Value(&quot;#&#123;&#x27;Tom&#x27;.toUpperCase()&#125;&quot;) private String name;&#125; 上面的例子可以看出, 在使用Spring时, 我们已经在不知不觉中使用了SpEL表达式了. 另外我们自己使用SpEL表达式时, 大体可分三个步骤: 123456// 1. 构建解析器ExpressionParser parser = new SpelExpressionParser();// 2. 解析表达式Expression exp = parser.parseExpression(SpEl);// 3. 获取结果exp.getValue(); 这就是使用SpEL的基本方式, 还有许多功能, 下文将举例列举 一、文本表达式1234567 文本表达式支持: 字符串(需要用单引号声明)、日期、数字、布尔类型及null,对数字支持负数、指数及小数, 默认情况下实数使用Double.parseDouble()进行表达式类型转换.1parser.parseExpression(&quot;&#x27;hello&#x27;&quot;).getValue(String.class); // hello , 注意单引号parser.parseExpression(&quot;1.024E+3&quot;).getValue(Long.class); // 1024 , 指数形式parser.parseExpression(&quot;0xFFFF&quot;).getValue(Integer.class); // 65535 , 十六进制parser.parseExpression(&quot;true&quot;).getValue(Boolean.class); // trueparser.parseExpression(&quot;null&quot;).getValue(); // null 二、变量 变量可以通过StandardEvaluationContext的setVariable方法设置到上下文中, 表达式中可以通过#变量名使用变量;另外, 还可以直接使用构造方法创建对象. 123456789// 定义变量String name = &quot;Tom&quot;;EvaluationContext context = new StandardEvaluationContext(); // 表达式的上下文,context.setVariable(&quot;myName&quot;, name); // 为了让表达式可以访问该对象, 先把对象放到上下文中ExpressionParser parser = new SpelExpressionParser();// 访问变量parser.parseExpression(&quot;#myName&quot;).getValue(context, String.class); // Tom , 使用变量// 直接使用构造方法创建对象parser.parseExpression(&quot;new String(&#x27;aaa&#x27;)&quot;).getValue(String.class); // aaa 三、属性和方法调用属性可直接使用属性名,属性名首字母大小写均可(只有首字母可不区分大小写)。 数组、列表可直接通过下表形式(list[index])访问; map可以直接把key当成索引来访问(map[key]); 方法可以直接访问; 1234567891011121314151617181920// 准备工作Person person = new Person(&quot;Tom&quot;, 18); // 一个普通的POJOList&lt;String&gt; list = Lists.newArrayList(&quot;a&quot;, &quot;b&quot;);Map&lt;String, String&gt; map = Maps.newHashMap();map.put(&quot;A&quot;, &quot;1&quot;);map.put(&quot;B&quot;, &quot;2&quot;);EvaluationContext context = new StandardEvaluationContext(); // 表达式的上下文,context.setVariable(&quot;person&quot;, person); // 为了让表达式可以访问该对象, 先把对象放到上下文中context.setVariable(&quot;map&quot;, map);context.setVariable(&quot;list&quot;, list);ExpressionParser parser = new SpelExpressionParser();// 属性parser.parseExpression(&quot;#person.name&quot;).getValue(context, String.class); // Tom , 属性访问parser.parseExpression(&quot;#person.Name&quot;).getValue(context, String.class); // Tom , 属性访问, 但是首字母大写了// 列表parser.parseExpression(&quot;#list[0]&quot;).getValue(context, String.class) // a , 下标// mapparser.parseExpression(&quot;#map[A]&quot;).getValue(context, String.class); // 1 , key// 方法parser.parseExpression(&quot;#person.getAge()&quot;).getValue(context, Integer.class); // 18 , 方法访问 另外列表可以直接写在表达式中, {}表示一个空列表, 比如:parser.parseExpression(&quot;&#123;&#39;A&#39;, &#39;B&#39;, &#39;C&#39;&#125;[0]&quot;).getValue(String.class)跟上面效果一样, 同样会访问列表的第一个元素, 得到&quot;A&quot; 四、类型T操作符可以获取类型, 可以调用对象的静态方法 123456// 获取类型parser.parseExpression(&quot;T(java.util.Date)&quot;).getValue(Class.class); // class java.util.Date// 访问静态成员(方法或属性)parser.parseExpression(&quot;T(Math).abs(-1)&quot;).getValue(Integer.class); // 1// 判断类型parser.parseExpression(&quot;&#x27;asdf&#x27; instanceof T(String)&quot;).getValue(Boolean.class); // true; 五、操作符Spring EL 支持大多数的数学操作符、逻辑操作符、关系操作符. 关系操作符, 包括: eq(&#x3D;&#x3D;), ne(!&#x3D;), lt()&lt;, le(&lt;&#x3D;), gt(&gt;), ge(&gt;&#x3D;) 逻辑运算符, 包括: and(&amp;&amp;), or(||), not(!) 数学操作符, 包括: 加(+), 减(-), 乘(*), 除(&#x2F;), 取模(%), 幂指数(^) 其他操作符, 如: 三元操作符, instanceof, 赋值(&#x3D;), 正则匹配 另外三元操作符有个特殊的用法, 一般用于赋默认值, 比如: parseExpression(“#name?:’defaultName’”), 如果变量name为空时设置默认值. 123456789101112parser.parseExpression(&quot;1 &gt; -1&quot;).getValue(Boolean.class); // trueparser.parseExpression(&quot;1 gt -1&quot;).getValue(Boolean.class); // trueparser.parseExpression(&quot;true or true&quot;).getValue(Boolean.class); // trueparser.parseExpression(&quot;true || true&quot;).getValue(Boolean.class); // trueparser.parseExpression(&quot;2 ^ 3&quot;).getValue(Integer.class); // 8parser.parseExpression(&quot;true ? true : false&quot;).getValue(Boolean.class); // trueparser.parseExpression(&quot;#name ?: &#x27;default&#x27;&quot;).getValue(context, String.class); // defaultparser.parseExpression(&quot;1 instanceof T(Integer)&quot;).getValue(Boolean.class); // trueparser.parseExpression(&quot;&#x27;5.00&#x27; matches &#x27;^-?\\\\d+(\\\\.\\\\d&#123;2&#125;)?$&#x27;&quot;).getValue(Boolean.class); // trueparser.parseExpression(&quot;#person.name&quot;).getValue(context, String.class); // Tom , 原来的值parser.parseExpression(&quot;#person.name = &#x27;Jim&#x27;&quot;).getValue(context, String.class); // Jim , 赋值之后parser.parseExpression(&quot;#person.name&quot;).getValue(context, String.class); // Jim, 赋值起了作用 六、避免空指针 当访问一个对象的属性或方法时, 若该对象为null, 就会出现空指针异常. 安全导航会判断对象是否为null,如果是的话, 就返回null而不是抛出空指针异常. 使用方式就是在对象后面加个?. 如下: 12// 使用这种表达式可以避免抛出空指针异常parser.parseExpression(&quot;#name?.toUpperCase()&quot;).getValue(context, String.class); // null 七、#this变量有个特殊的变量#this来表示当前的对象. 常用于集合的过滤 12// this 使用示例parser.parseExpression(&quot;&#123;1, 3, 5, 7&#125;.?[#this &gt; 3]&quot;).getValue(); // [5, 7] 八、集合选择可以使用选择表达式对集合进行过滤或一些操作，从而生成一个新的符合选择条件的集合, 有如下一些形式: ?[expression]: 选择符合条件的元素 ^[expression]: 选择符合条件的第一个元素 $[expression]: 选择符合条件的最后一个元素 ![expression]: 可对集合中的元素挨个进行处理 对于集合可以配合#this变量进行过滤, 对于map, 可分别对keySet及valueSet分别使用key和value关键字; 12345678910111213141516// 集合parser.parseExpression(&quot;&#123;1, 3, 5, 7&#125;.?[#this &gt; 3]&quot;).getValue(); // [5, 7] , 选择元素parser.parseExpression(&quot;&#123;1, 3, 5, 7&#125;.^[#this &gt; 3]&quot;).getValue(); // 5 , 第一个parser.parseExpression(&quot;&#123;1, 3, 5, 7&#125;.$[#this &gt; 3]&quot;).getValue(); // 7 , 最后一个parser.parseExpression(&quot;&#123;1, 3, 5, 7&#125;.![#this + 1]&quot;).getValue(); // [2, 4, 6, 8] ,每个元素都加1// mapMap&lt;Integer, String&gt; map = Maps.newHashMap();map.put(1, &quot;A&quot;);map.put(2, &quot;B&quot;);map.put(3, &quot;C&quot;);map.put(4, &quot;D&quot;);EvaluationContext context = new StandardEvaluationContext();context.setVariable(&quot;map&quot;, map);parser.parseExpression(&quot;#map.?[key &gt; 3]&quot;).getValue(context); // &#123;4=D&#125;parser.parseExpression(&quot;#map.?[value == &#x27;A&#x27;]&quot;).getValue(context); // &#123;1=A&#125;parser.parseExpression(&quot;#map.?[key &gt; 2 and key &lt; 4]&quot;).getValue(context); // &#123;3=C&#125; 九、模板表达式模板表达式允许文字和表达式混合使用, 一般选择使用#{}作为一个定界符: 1parser.parseExpression(&quot;他的名字为#&#123;#person.name&#125;&quot;, new TemplateParserContext()).getValue(context); // 他的名字为Tom1","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"spel表达式","slug":"spring/spel表达式","date":"2021-11-25T12:00:48.000Z","updated":"2022-03-23T09:03:55.940Z","comments":true,"path":"blog/spring/spel表达式/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/spel%E8%A1%A8%E8%BE%BE%E5%BC%8F/","excerpt":"","text":"前言 SpEL是一种表达式语言，可以与spring应用程序运行时的对象交互，可简化开发，减少一些逻辑、配置的编写。 既能在配置文件中使用，也能在注解中使用（常用于@Value等）。 一、用法SpEL有三种用法，一种是在注解@Value中；一种是XML配置；最后一种是在代码块中使用Expression。 @Value1234//@Value能修饰成员变量和方法形参 //#&#123;&#125;内就是表达式的内容 @Value(&quot;#&#123;表达式&#125;&quot;) public String arg; 如果修饰成员变量，是从Spring容器中按照SpEL表达式筛选修改数据后，赋值给所修饰的变量；如果修饰方法形参，则是过滤传进来的参数值。 &lt;bean&gt;配置1234&lt;bean id=&quot;xxx&quot; class=&quot;com.java.XXXXX.xx&quot;&gt; &lt;!-- 同@Value,#&#123;&#125;内是表达式的值，可放在property或constructor-arg内 --&gt; &lt;property name=&quot;arg&quot; value=&quot;#&#123;表达式&#125;&quot;&gt;&lt;/bean&gt; 用法跟注解@ Value修饰形参类似 Expression 在使用下面代码时，表达式只能以#name这种模式 123456789101112131415161718192021222324252627282930313233343536import org.springframework.expression.Expression;import org.springframework.expression.ExpressionParser;import org.springframework.expression.spel.standard.SpelExpressionParser;import org.springframework.expression.spel.support.StandardEvaluationContext; public class SpELTest &#123; public static void main(String[] args) &#123; String spelExpression = &quot;#diffDead/#average * #score&quot;; //创建ExpressionParser解析表达式 ExpressionParser parser = new SpelExpressionParser(); //表达式放置 Expression exp = parser.parseExpression(spelExpression); //执行表达式，默认容器是spring本身的容器：ApplicationContext Object value = exp.getValue(); /**如果使用其他的容器，则用下面的方法*/ //创建一个虚拟的容器EvaluationContext StandardEvaluationContext ctx = new StandardEvaluationContext(); //向容器内添加bean BeanA beanA = new BeanA(); ctx.setVariable(&quot;bean_id&quot;, beanA); Map&lt;String, Object&gt; variables = new HashMap&lt;&gt;(); variables.put(&quot;diffDead&quot;, 25); variables.put(&quot;average&quot;, 21.0); variables.put(&quot;score&quot;, 100); context.setVariables(variables); //setRootObject并非必须；一个EvaluationContext只能有一个RootObject，引用它的属性时，可以不加前缀 //ctx.setRootObject(XXX); //getValue有参数ctx，从新的容器中根据SpEL表达式获取所需的值 Object value = exp.getValue(ctx); &#125;&#125; 用法比较灵活，可以在代码中使用SpEL进行数据的过滤和修改 以上概念还可以参考：SpEL详解 二、表达式语法在Spring项目中通过@Value等模式使用时，SpEL表达式都是#{}的格式。但是通过ExpressionParser类使用时，SpEL表达式都是以#name格式 表达式语法可以参考：SPEL语法 直接量表达式1234&quot;#&#123;&#x27;Hello World&#x27;&#125;&quot; //字符串&quot;#&#123;5&#125;&quot; //整数#&#123;true&#125; //boolean值等 引用Bean并使用其属性与方法12345678910111213141516通过ID引用bean#&#123;a&#125; //a为bean的id使用bean的属性#&#123;a.b&#125; //a为bean的id使用bean的方法#&#123;a.c()&#125;可链式调用#&#123;a.c().toUpperCase()&#125;通过?(类型安全的运算符)避免空指针(NullPointerException)#&#123;a.c()?.toUpperCase()&#125;即：a.c()存在时才使用toUpperCase() 集合定义使用”{表达式，……}”定义List，如”{1,2,3}” 对于字面量表达式列表，SpEL会使用java.util.Collections.unmodifiableList 方法将列表设置为不可修改。 1List&lt;Integer&gt; result1 = parser.parseExpression(&quot;&#123;1,2,3&#125;&quot;).getValue(List.class); properties文件中的内容如下： 123my.set=foo,barmy.list=foo,barmy.map=&#123;&quot;foo&quot;: &quot;bar&quot;&#125; 分别是我们要注入的Set，List，Map中的内容。注入方式如下： 123456789@Value(&quot;#&#123;$&#123;my.map&#125;&#125;&quot;)private Map&lt;String, String&gt; map;@Value(&quot;#&#123;&#x27;$&#123;my.set&#125;&#x27;&#125;&quot;)private Set&lt;String&gt; set;@Value(&quot;#&#123;&#x27;$&#123;my.list&#125;&#x27;&#125;&quot;)private List&lt;String&gt; list; 集合运算符集合过滤.?[](查询运算符)对集合过滤 #{jukebox.songs.?[artist eq ‘Aerosmith’]} 检查jukebox的歌曲songs的artist属性是不是等于Aerosmith，是的话放入新的集合。 集合访问SpEL目前支持所有集合类型和字典类型的元素访问 语法：”集合[索引]”、”map[key]” .^[]”和“.$[]”，它们分别用来在集合中查询第一个匹配项和最后一个匹配项 12345678910111213141516171819EvaluationContext context = new StandardEvaluationContext(); //即list.get(0)int result1 = parser.parseExpression(&quot;&#123;1,2,3&#125;[0]&quot;).getValue(int.class); //list获取某一项Collection&lt;Integer&gt; collection = new HashSet&lt;Integer&gt;();collection.add(1);collection.add(2); context.setVariable(&quot;collection&quot;, collection);int result2 = parser.parseExpression(&quot;#collection[1]&quot;).getValue(context, int.class); //map获取Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;();map.put(&quot;a&quot;, 1); context.setVariable(&quot;map&quot;, map);int result3 = parser.parseExpression(&quot;#map[&#x27;a&#x27;]&quot;).getValue(context, int.class); @Value 123my.set=foo,barmy.list=foo,barmy.map=&#123;&quot;foo&quot;: &quot;bar&quot;&#125; 代码： 12345@Value(&quot;#&#123;$&#123;my.map&#125;&#125;[0]&quot;)private String value1@Value(&quot;#&#123;&#x27;$&#123;my.list&#125;&#x27;&#125;[foo]&quot;)private String value2 投影运算符投影运算符（ .![]），它会从集合的每个成员中选择特定的属性放到另外一个集合中 根据集合中的元素中通过选择来构造另一个集合，该集合和原集合具有相同数量的元素 语法：”SpEL使用”（list|map）.![投影表达式]” 12345678public class Book &#123; public String name; //书名 public String author; //作者 public String publisher; //出版社 public double price; //售价 public boolean favorite; //是否喜欢&#125; 1234567public class BookList &#123; @Autowired protected ArrayList&lt;Book&gt; list = new ArrayList&lt;Book&gt;() ; protected int num = 0;&#125; 将BookList的实例映射为bean&#x3D;readList，在另一个bean中注入时，进行投影 123//从readList的list下筛选出favorite为true的子集合，再将他们的name字段投为新的list@Value(&quot;#&#123;readList?.list.?[favorite eq true].![name]&#125;&quot;)private ArrayList&lt;String&gt; favoriteBookName; 集合修改可以使用赋值表达式或Expression接口的setValue方法修改； 123456//赋值语句int result = parser.parseExpression(&quot;#array[1] = 3&quot;).getValue(context, int.class); //赋值语句//serValue方法parser.parseExpression(&quot;#array[2]&quot;).setValue(context, 4); 直接使用java代码new&#x2F;instance of​ 此方法只能是java.lang 下的类才可以省略包名 1Expression exp = parser.parseExpression(&quot;new Spring(&#x27;Hello World&#x27;)&quot;); 使用T(Type)​ 使用”T(Type)”来表示java.lang.Class实例，同样，只有java.lang 下的类才可以省略包名。此方法一般用来引用常量或静态方法 1234parser.parseExpression(&quot;T(Integer).MAX_VALUE&quot;);通过T()获取方法#&#123;T(java.lang.Math).random()&#125; 变量获取容器内的变量，可以使用”#bean_id”来获取。有两个特殊的变量，可以直接使用。 #this 使用当前正在计算的上下文 #root 引用容器的root对象 123456String result2 = parser.parseExpression(&quot;#root&quot;).getValue(ctx, String.class); String s = new String(&quot;abcdef&quot;);ctx.setVariable(&quot;abc&quot;,s);//取id为abc的bean，然后调用其中的substring方法parser.parseExpression(&quot;#abc.substring(0,1)&quot;).getValue(ctx, String.class); 方法调用 与Java代码没有什么区别 可以自定义方法，如下： 1234Method parseInt = Integer.class.getDeclaredMethod(&quot;parseInt&quot;, String.class); ctx.registerFunction(&quot;parseInt&quot;, parseInt); ctx.setVariable(&quot;parseInt2&quot;, parseInt); String expression1 = &quot;#parseInt(&#x27;3&#x27;) == #parseInt2(&#x27;3&#x27;)&quot;; “registerFunction”和”setVariable”都可以注册自定义函数，但是两个方法的含义不一样，推荐使用”registerFunction”方法注册自定义函数。 运算符表达式1234567算数表达式（&quot;1+2-3*4/2&quot;）比较表达式（&quot;1&gt;2”）逻辑表达式（&quot;2&gt;1 and (!true or !false)”）赋值表达式（&quot;#variableName=value”）三目表达式（&quot;表达式1?表达式2:表达式3”）正则表达式（&quot;123′ matches ‘\\\\d&#123;3&#125;”）等运算符，都可以直接放在SpEL中 安全保证为了避免操作对象本身可能为null，取属性时报错，定义语法 语法： “对象?” 1list? 语法： “对象?.变量|方法” 1list?.length 当对象为null时，直接返回”null”，不会抛出NullPointerException Elvis运算符是三目运算符的特殊写法，可以避免null报错的情况 1234name != null? name : &quot;other&quot;//简写为：name?:&quot;other&quot;","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"Spring的#和$的区别","slug":"spring/Spring的#和$的区别","date":"2021-11-25T12:00:47.000Z","updated":"2022-03-23T09:03:55.916Z","comments":true,"path":"blog/spring/Spring的#和$的区别/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/Spring%E7%9A%84#%E5%92%8C$%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"在Spring中有#的使用也有$的使用，那么这两个分别是做什么的呢？ ${key名称}： 12341、用户获取外部文件中指定key的值2、可以出现在xml配置文件中，也可以出现在注解@Value中3、获取properties中环境变量 #{表达式}： 123SpEL表达式的格式，详情点击[Spring的EL表达式](http://blog.csdn.net/u012834750/article/details/79388294)可以出现在xml配置文件中，也可以出现在注解@Value中可以任意表达式，支持运算符等 在使用的时候也允许#{‘${key}’}这样使用，比如： 12@Value(&quot;#&#123;&#x27;$&#123;jdbc.url&#125;&#x27;&#125;&quot;)private String jdbcUrl;","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"Spring中的@Qualifier注解","slug":"spring/Spring中的@Qualifier注解","date":"2021-11-25T12:00:46.000Z","updated":"2022-03-23T09:03:55.915Z","comments":true,"path":"blog/spring/Spring中的@Qualifier注解/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/Spring%E4%B8%AD%E7%9A%84@Qualifier%E6%B3%A8%E8%A7%A3/","excerpt":"","text":"该注解的作用就是限定注入的类 比如 123@Qualifier(&quot;beanName&quot;)@Autowiredprivate QualifierDemo demo 这样就是限定注入的对象的beanName 如果我自己定义了一个注解 1234567@Target(&#123; ElementType.FIELD, ElementType.PARAMETER, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@Qualifier // 限定符public @interface MyLoadBalanced &#123;&#125; 使用 123@MyLoadBalanced@Autowired(required = false) // 限定注入到list的RestTemplateprivate List&lt;RestTemplate&gt; restTemplates = Collections.emptyList(); 这样的话就限定了注入的RestTemplate，这就要求RestTemplate在创建时也需要加上@MyLoadBalanced注解。比如： 123456789101112131415161718@Configurationpublic class RestConfig &#123; @Bean @LoadBalanced @Primary public RestTemplate restTemplate1() &#123; return new RestTemplate(); &#125; @Bean @MyLoadBalanced public RestTemplate restTemplate2() &#123; return new RestTemplate(); &#125;&#125; 这项定义了RestTemplate后 123@MyLoadBalanced@Autowired(required = false) // 限定注入到list的RestTemplateprivate List&lt;RestTemplate&gt; restTemplates = Collections.emptyList(); 这里注入的RestTemplate对象只有一个，这个bean的Name为restTemplate2。也就是有 @MyLoadBalanced注解的那个。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"Spring中有用的注解或接口或扩展","slug":"spring/Spring中有用的注解或接口或扩展","date":"2021-11-25T12:00:45.000Z","updated":"2022-03-23T09:03:55.907Z","comments":true,"path":"blog/spring/Spring中有用的注解或接口或扩展/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/Spring%E4%B8%AD%E6%9C%89%E7%94%A8%E7%9A%84%E6%B3%A8%E8%A7%A3%E6%88%96%E6%8E%A5%E5%8F%A3%E6%88%96%E6%89%A9%E5%B1%95/","excerpt":"","text":"spel表达式.md [BeanPostProcessor 接口理解.md](BeanPostProcessor 接口理解.md) 00-Spring中重要的接口 Spring中反射的工具类使用 [00 Spring源码中经常出现的工具类](.&#x2F;00 Spring源码中经常出现的工具类) Conditional注解使用.md [Spring Event事件通知机制.md](Spring Event事件通知机制.md) [Sping AOP在开发中的问题.md](Sping AOP在开发中的问题.md) Spring-SPI.md [Spring 全局（切面）的拦截器.md](Spring 全局（切面）的拦截器.md) 11-Spring-用接口实现AOP.md 06-Spring自定义Scope.md 11-Spring中的代理的TargetSource接口的使用.md 12-Spring的事物.md 13-Spring的事务传播属性 14-Spring中使用事物编程 15-Spring缓存注解的使用 18-springMVC使用和扩展 24-SpringMVC在项目有用的代码 25-SpringMVC中的异步请求","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"Spring中反射的工具类使用","slug":"spring/Spring中反射的工具类使用","date":"2021-11-25T12:00:44.000Z","updated":"2022-03-23T09:03:55.906Z","comments":true,"path":"blog/spring/Spring中反射的工具类使用/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/Spring%E4%B8%AD%E5%8F%8D%E5%B0%84%E7%9A%84%E5%B7%A5%E5%85%B7%E7%B1%BB%E4%BD%BF%E7%94%A8/","excerpt":"","text":"","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"Spring中事物传播属性","slug":"spring/Spring中事物传播属性","date":"2021-11-25T12:00:43.000Z","updated":"2022-03-23T09:03:55.906Z","comments":true,"path":"blog/spring/Spring中事物传播属性/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/Spring%E4%B8%AD%E4%BA%8B%E7%89%A9%E4%BC%A0%E6%92%AD%E5%B1%9E%E6%80%A7/","excerpt":"","text":"1234567891011121314151617181920PROPAGATION_REQUIRED如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。PROPAGATION_SUPPORTS支持当前事务，如果当前没有事务，就以非事务方式执行。PROPAGATION_MANDATORY使用当前的事务，如果当前没有事务，就抛出异常。PROPAGATION_REQUIRES_NEW新建事务，如果当前存在事务，把当前事务挂起。PROPAGATION_NOT_SUPPORTED以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。PROPAGATION_NEVER以非事务方式执行，如果当前存在事务，则抛出异常。PROPAGATION_NESTED如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 重点看 123PROPAGATION_REQUIREDPROPAGATION_REQUIRES_NEWPROPAGATION_NESTED 事务传播属性的作用就是控制事务的流转的，比如： 1234567891011@Transactionalfun1()@Transactionalfun2();@Transactionalfun4() &#123; fun()1 fun()2&#125; 在fun4中，比如我希望fun1使用一个事务，fun2使用另一个事务。也比如我希望fun1和fun2用同一个事务。这种涉及到事物的传播的都是通过这个属性来控制的。 所以事物传播属性起作用的前提是一定要有方法中调用了另一个方法，而且这些方法都是有@Transactional注解的。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"Spring-SPI","slug":"spring/Spring-SPI","date":"2021-11-25T12:00:42.000Z","updated":"2022-03-23T09:03:55.905Z","comments":true,"path":"blog/spring/Spring-SPI/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/Spring-SPI/","excerpt":"","text":"在resources/META-INF/目录中添加一个文件spring.factories spring.factories文件的内容： 123456cn.enjoy.handler.InvokeHandler=cn.enjoy.handler.InvokeAllHandlerImpl,cn.enjoy.handler.InvokeAssignHandlerImpl换行cn.enjoy.handler.InvokeHandler=\\cn.enjoy.handler.InvokeAllHandlerImpl,\\cn.enjoy.handler.InvokeAssignHandlerImpl 接口&#x3D;实现类,实现类2 我们只需要通过SpringFactoriesLoader就能获取了 1234// 返回的值是=好后面的内容List&lt;String&gt; invokeHandlerclassNames = SpringFactoriesLoader.loadFactoryNames(InvokeHandler.class, ClassUtils.getDefaultClassLoader());List&lt;InvokeHandler&gt; invokeHandlers = SpringFactoriesLoader.loadFactories(InvokeHandler.class, ClassUtils.getDefaultClassLoader());","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"Spring 全局（切面）的拦截器","slug":"spring/Spring 全局（切面）的拦截器","date":"2021-11-25T12:00:41.000Z","updated":"2022-03-23T09:03:55.905Z","comments":true,"path":"blog/spring/Spring 全局（切面）的拦截器/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/Spring%20%E5%85%A8%E5%B1%80%EF%BC%88%E5%88%87%E9%9D%A2%EF%BC%89%E7%9A%84%E6%8B%A6%E6%88%AA%E5%99%A8/","excerpt":"","text":"12345678@Componentpublic class GirlAdvice implements MethodInterceptor &#123; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; System.out.println(&quot;=======GirlAdvice.invoke&quot;); return invocation.proceed(); &#125;&#125; 12345678910111213141516@Componentpublic class SetGlobleAdvice implements BeanPostProcessor, PriorityOrdered &#123; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if(bean instanceof AnnotationAwareAspectJAutoProxyCreator) &#123; AnnotationAwareAspectJAutoProxyCreator annotationAwareAspectJAutoProxyCreator = (AnnotationAwareAspectJAutoProxyCreator)bean; annotationAwareAspectJAutoProxyCreator.setInterceptorNames(&quot;girlAdvice&quot;); &#125; return bean; &#125; @Override public int getOrder() &#123; return 45; &#125;&#125; 通过这样的操作后，所有的有切面的代理类都会首先调用GirlAdvice这个增强。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"Spring Event事件通知机制","slug":"spring/Spring Event事件通知机制","date":"2021-11-25T12:00:40.000Z","updated":"2022-03-23T09:03:55.904Z","comments":true,"path":"blog/spring/Spring Event事件通知机制/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/Spring%20Event%E4%BA%8B%E4%BB%B6%E9%80%9A%E7%9F%A5%E6%9C%BA%E5%88%B6/","excerpt":"","text":"使用事件订阅实现ApplicationListener接口 123456789101112131415161718@Componentpublic class ContextStoppedListener implements ApplicationListener&lt;EnjoyEvent&gt; &#123; @Override public void onApplicationEvent(EnjoyEvent event) &#123; System.out.println(&quot;========EnjoyEvent&quot;); &#125;&#125;@Datapublic class EnjoyEvent extends ApplicationEvent &#123; private String name; public EnjoyEvent(Object source,String name) &#123; super(source); this.name = name; &#125;&#125; 或者使用@EventListener 123456789101112131415161718192021222324252627282930313233343536@Componentpublic class AnoListener &#123; /** * 监听所有ApplicationEvent类型及其子类型的事件（spring） */ @EventListener public void processApplicationEvent(ApplicationEvent event) &#123; System.out.println(&quot;process common event, class:&quot; + event.getClass().getSimpleName()); &#125; /** * 监听 HelloEvent类型 事件 */ @EventListener public void processHelloEvent(EnjoyEvent event) &#123; System.out.println(&quot;process helloEvent, name:&quot; + event.getName()); &#125; /** * 监听 CustomerEvent 类型事件，但是需要满足condition条件,即name=&quot;xieyuezhi&quot; */ @EventListener(condition = &quot;#event.getName().equals(&#x27;miaomiao&#x27;)&quot;) public void processMiaoMiaoEvent(EnjoyEvent event) &#123; System.out.println(&quot;process miaomiao&#x27;s CustomerEvent, name:&quot; + event.getName()); &#125; /** * 支持异步处理事件 */ @Async @EventListener public void processAsyncCustomerEvent(EnjoyEvent event) &#123; System.out.println(&quot;Async process CustomerEvent, name:&quot; + event.getName()); &#125;&#125; 事件发布实现ApplicationContextAware 1234567891011121314151617@Componentpublic class ApplicationContextText implements ApplicationContextAware &#123; private ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; //可以通过这种模式来天际一个任务执行器，默认情况下调用publishEvent，监听的方法会被同步执行 SimpleApplicationEventMulticaster bean = applicationContext.getBean(SimpleApplicationEventMulticaster.class); //该值默认为空，也就是同步执行，但默认就是为null的 bean.setTaskExecutor(null); &#125; public void publishEvent(ApplicationEvent event) &#123; applicationContext.publishEvent(event); &#125; &#125; ApplicationEventMulticaster的创建ApplicationEventMulticaster是Spring evnet事件通知机制的接口定义，就是用完成事件的通知和订阅的。他的初始化是在AbstractApplicationContext.refresh方法中调用initApplicationEventMulticaster方法来完成初始化的。 1234567891011121314151617//AbstractApplicationContext#initApplicationEventMulticasterpublic static final String APPLICATION_EVENT_MULTICASTER_BEAN_NAME = &quot;applicationEventMulticaster&quot;;protected void initApplicationEventMulticaster() &#123; // DefaultListableBeanFactory ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (beanFactory.containsLocalBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME)) &#123; this.applicationEventMulticaster = beanFactory.getBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, ApplicationEventMulticaster.class); //忽略打印日志 &#125; else &#123; this.applicationEventMulticaster = new SimpleApplicationEventMulticaster(beanFactory); beanFactory.registerSingleton(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, this.applicationEventMulticaster); //忽略打印日志 &#125;&#125; 方法也很简单，初始化有两种方式 自己实现ApplicationEventMulticaster接口，然后通过@Component注解，并制定beanName为applicationEventMulticaster。这样就可以实现自己的时间通知机制。 就是什么都不做，使用默认模式。也就是Spring自己创建SimpleApplicationEventMulticaster来完成事件的通知机制 事件订阅的初始化通过使用可知，事件订阅有两种创建方式。第一种就是通过ApplicationListener接口，第二种就是通过@EventListener注解。这两种模式的初始化分别由ApplicationListenerDetector和EventListenerMethodProcessor来完成的。 ApplicationListenerDetector这个类的初始化有两个地方 在AbstractApplicationContext.refresh方法中调用了prepareBeanFactory方法，在这个方法中有这一行代码 1beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); 这个方法会把ApplicationListenerDetector加入到beanFactory的beanPostProcessor集合中 在AbstractApplicationContext.refresh方法中调用了registerBeanPostProcessors方法，在这个方法中： 123protected void registerBeanPostProcessors(ConfigurableListableBeanFactory beanFactory) &#123; PostProcessorRegistrationDelegate.registerBeanPostProcessors(beanFactory, this);&#125; 在方法最后会调用 1beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext)); 把ApplicationListenerDetector加入到beanFactory的beanPostProcessor集合中 这两个方法在Spring的初始化时都会执行，虽然添加了两次，beanFactory.addBeanPostProcessor方法会对先remove掉旧的BeanPostProcessor，然后添加新的 创建完后，在Spring bean的初始化阶段中，当创建完对象并完成初始化后（依赖注入后）,会遍历beanFactory的beanPostProcessor集合然后执行beanPostProcessor的postProcessAfterInitialization方法，所以看该类的方法 12345678910111213@Overridepublic Object postProcessAfterInitialization(Object bean, String beanName) &#123; if (bean instanceof ApplicationListener) &#123; // potentially not detected as a listener by getBeanNamesForType retrieval Boolean flag = this.singletonNames.get(beanName); if (Boolean.TRUE.equals(flag)) &#123; // singleton bean (top-level or inner): register on the fly this.applicationContext.addApplicationListener((ApplicationListener&lt;?&gt;) bean); &#125; 。。。。。 &#125; return bean;&#125; 这个方法就是调用applicationContext.addApplicationListener也就是添加事件订阅者的。 这个方法能添加成功有个前提，就是singletonNames.get(beanName);要返回true，而这个singletonNames的设置是通过 123456@Overridepublic void postProcessMergedBeanDefinition(RootBeanDefinition beanDefinition, Class&lt;?&gt; beanType, String beanName) &#123; if (ApplicationListener.class.isAssignableFrom(beanType)) &#123; this.singletonNames.put(beanName, beanDefinition.isSingleton()); &#125;&#125; 来完成的，这个方法的调用时机就是在Spring bean对象创建后，对注解的收集阶段调用的。详情可以看04-Spring的Bean实例化 EventListenerMethodProcessor 这个类的创建分两步，第一步就是BeanDefinition的创建，第二部就是在AbstractApplicationContext.refresh方法中调用了registerBeanPostProcessors方法，该方法会按照BeanPostProcessor的BeanDefinition的定义，创建对应的BeanPostProcessor对象。现在看第一步EventListenerMethodProcessor的BeanDefinition是在什么时候创建的。 无论是XML文件还是基于注解，在ApplicationContext初始化时，扫描完XML文件和注解，创建完BeanDefinition后，都会调用AnnotationConfigUtils#registerAnnotationConfigProcessors方法（不一定是这个方法，有些ApplicationContext会自己写，而不是掉这个方法）来添加一些必须的BeanPostProcessor。其中就有EventListenerMethodProcessor 12345if (!registry.containsBeanDefinition(EVENT_LISTENER_PROCESSOR_BEAN_NAME)) &#123; RootBeanDefinition def = new RootBeanDefinition(EventListenerMethodProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, EVENT_LISTENER_PROCESSOR_BEAN_NAME));&#125; 所以在经过AbstractApplicationContext#registerBeanPostProcessors方法后，该类就创建成功了。 然后在Spring bean的初始化。 由于该类实现了接口SmartInitializingSingleton，该接口会在Spring所有的bean都完成了初始化后，都会调用该接口的方法。所以看该类的该方法就好了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public void afterSingletonsInstantiated() &#123; ConfigurableListableBeanFactory beanFactory = this.beanFactory; String[] beanNames = beanFactory.getBeanNamesForType(Object.class); for (String beanName : beanNames) &#123; ..... processBean(beanName, type); ..... &#125;&#125;private void processBean(final String beanName, final Class&lt;?&gt; targetType) &#123; if (!this.nonAnnotatedClasses.contains(targetType) &amp;&amp; AnnotationUtils.isCandidateClass(targetType, EventListener.class) &amp;&amp; !isSpringContainerClass(targetType)) &#123; //收集有EventListener注解的方法 Map&lt;Method, EventListener&gt; annotatedMethods = null; try &#123; annotatedMethods = MethodIntrospector.selectMethods(targetType, (MethodIntrospector.MetadataLookup&lt;EventListener&gt;) method -&gt; AnnotatedElementUtils.findMergedAnnotation(method, EventListener.class)); &#125; catch (Throwable ex) &#123; ...... &#125; if (CollectionUtils.isEmpty(annotatedMethods)) &#123; this.nonAnnotatedClasses.add(targetType); ..... &#125; else &#123; // Non-empty set of methods ConfigurableApplicationContext context = this.applicationContext; Assert.state(context != null, &quot;No ApplicationContext set&quot;); List&lt;EventListenerFactory&gt; factories = this.eventListenerFactories; Assert.state(factories != null, &quot;EventListenerFactory List not initialized&quot;); for (Method method : annotatedMethods.keySet()) &#123; for (EventListenerFactory factory : factories) &#123; if (factory.supportsMethod(method)) &#123; Method methodToUse = AopUtils.selectInvocableMethod(method, context.getType(beanName)); ApplicationListener&lt;?&gt; applicationListener = factory.createApplicationListener(beanName, targetType, methodToUse); if (applicationListener instanceof ApplicationListenerMethodAdapter) &#123; ((ApplicationListenerMethodAdapter) applicationListener).init(context, this.evaluator); &#125; context.addApplicationListener(applicationListener); break; &#125; &#125; &#125; ...... &#125; &#125;&#125; 在processBean中，先收集该类中有@EventListener注解的方法，通过EventListenerFactory列表，创建成ApplicationListenor对象，最后调用了context.addApplicationListener(applicationListener)方法。 下面讲EventListenerFactory。 EventListenerFactoryEventListenerFactory列表的初始化是在方法 123456789@Overridepublic void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; this.beanFactory = beanFactory; Map&lt;String, EventListenerFactory&gt; beans = beanFactory.getBeansOfType(EventListenerFactory.class, false, false); List&lt;EventListenerFactory&gt; factories = new ArrayList&lt;&gt;(beans.values()); AnnotationAwareOrderComparator.sort(factories); this.eventListenerFactories = factories;&#125; 完成的，由于该类实现了BeanFactoryPostProcessor接口，所以postProcessBeanFactory方法会在afterSingletonsInstantiated之前被执行。 由于Spring的事件订阅和通知的逻辑都是定义在ApplicationEventMulticaster接口上的，通过接口约定，订阅只接收ApplicationListener的实现类。所以对于有@EventListener注解的方法，需要一个实现了ApplicationListener接口的类来承载对象和方法，最终完成监听者的添加和事件的触发。而EventListenerFactory接口就是用来完成这个ApplicationListener类的创建的。 看这个接口的实现 Spring就提供了两个 DefaultEventListenerFactory TransactionalEventListenerFactory 其中TransactionalEventListenerFactory是用来处理TransactionalEventListener注解的。它是通过注解@EnableTransactionManagement引入的。TransactionalEventListener是用来监听事务的生命周期的（创建、commitBefore，commitAfter等事件的，如果要使用这个事件坚听，事件管理器一定不能添加线程池）。这个在事务章节展开讲。这里重点看DefaultEventListenerFactory。 DefaultEventListenerFactory和EventListenerMethodProcessor的加入时机一样，都是在AnnotationConfigUtils#registerAnnotationConfigProcessors方法中加入到Spring容器中的。 12345if (!registry.containsBeanDefinition(EVENT_LISTENER_FACTORY_BEAN_NAME)) &#123; RootBeanDefinition def = new RootBeanDefinition(DefaultEventListenerFactory.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, EVENT_LISTENER_FACTORY_BEAN_NAME));&#125; 对于该类，我们看createApplicationListener方法就好了。 1234public ApplicationListener&lt;?&gt; createApplicationListener(String beanName, Class&lt;?&gt; type, Method method) &#123; return new ApplicationListenerMethodAdapter(beanName, type, method);&#125; 所以对于每个@EventListener注解的方法，都会生成一个ApplicationListenerMethodAdapter对象来完成事件通知。由于ApplicationListenerMethodAdapter的onApplicationEvent方法的参数为ApplicationEvent，所以所有事件都会进来，但由于在注解的方法中有指定对应的ApplicationEvent类型（参数只能有一个，不然会报错）和注解有condition属性，所以在ApplicationListenerMethodAdapter内部会通过这两个条件进一步的滤掉事件。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"Sping AOP在开发中的问题","slug":"spring/Sping AOP在开发中的问题","date":"2021-11-25T12:00:39.000Z","updated":"2022-03-23T09:03:55.900Z","comments":true,"path":"blog/spring/Sping AOP在开发中的问题/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/Sping%20AOP%E5%9C%A8%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"123456789101112131415161718192021222324252627282930@Servicepublic class StudentServiceImpl implements StudentService, ApplicationContextAware &#123; private ApplicationContext applicationContext;// @Autowired// private StudentService studentService; @Override public void eat(String a) &#123; System.out.println(&quot;=====StudentServiceImpl.eat&quot;);// StudentServiceImpl o = (StudentServiceImpl)AopContext.currentProxy();// StudentService bean = applicationContext.getBean(StudentService.class);// bean.sleep(new ArrayList&lt;&gt;()) sleep(new ArrayList&lt;&gt;()) &#125; @Override @Transactional public String sleep(List b) &#123; System.out.println(&quot;=====StudentServiceImpl.sleep&quot;); if(true)throw new RuntimeException(&quot;xx&quot;); return &quot;=====StudentServiceImpl.sleep&quot;; &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125;&#125; 如果在代码中这样调用 1studentServiceImpl.eat(&quot;&quot;) 是不会为方法sleep生成一个事务的。因为上边的调用虽然是代理对象调用一个方法，但这个方法没有找到合适的切面，所以最终会有target，也就是目标对象通过反射调用方法。这时在eat方法中，对sleep的调用就相当于this.sleep，而这个this就是目标对象。 所以这样这样 设置了exposeProxy&#x3D;true后可以通过这样调用 123456@Overridepublic void eat(String a) &#123; System.out.println(&quot;=====StudentServiceImpl.eat&quot;); StudentService o = (StudentService)AopContext.currentProxy(); o.sleep(new ArrayList&lt;&gt;())&#125; 或者通过把自己注入到自己里面，然后通过注入的调用方法； 123456789101112131415161718192021222324252627@Servicepublic class StudentServiceImpl implements StudentService, ApplicationContextAware &#123; private ApplicationContext applicationContext; @Autowired private StudentService studentService; @Override public void eat(String a) &#123; System.out.println(&quot;=====StudentServiceImpl.eat&quot;); studentService.sleep(new ArrayList&lt;&gt;()) &#125; @Override @Transactional public String sleep(List b) &#123; System.out.println(&quot;=====StudentServiceImpl.sleep&quot;); if(true)throw new RuntimeException(&quot;xx&quot;); return &quot;=====StudentServiceImpl.sleep&quot;; &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125;&#125; 还可以使用使用ApplicationContextAware，获取到上下文对象后获取代理对现象再调用、或者自己调用自己 12345678910111213141516171819202122232425@Servicepublic class StudentServiceImpl implements StudentService, ApplicationContextAware &#123; private ApplicationContext applicationContext; @Override public void eat(String a) &#123; System.out.println(&quot;=====StudentServiceImpl.eat&quot;); StudentService bean = applicationContext.getBean(StudentService.class); bean.sleep(new ArrayList&lt;&gt;()) &#125; @Override @Transactional public String sleep(List b) &#123; System.out.println(&quot;=====StudentServiceImpl.sleep&quot;); if(true)throw new RuntimeException(&quot;xx&quot;); return &quot;=====StudentServiceImpl.sleep&quot;; &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125;&#125;","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"Conditional注解使用","slug":"spring/Conditional注解使用","date":"2021-11-25T12:00:38.000Z","updated":"2022-03-23T09:03:55.898Z","comments":true,"path":"blog/spring/Conditional注解使用/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/Conditional%E6%B3%A8%E8%A7%A3%E4%BD%BF%E7%94%A8/","excerpt":"","text":"使用1234567891011121314@Component@Conditional(value = &#123;CustomCondition.class,CustomCondition1.class&#125;)//@ConditionOnClass(name = &quot;com.enjoy.jack.bean.circular.CircularRefConB&quot;)@ConditionOnProperty(name = &quot;cn.enjoy.flag&quot;)public class ConditionalBean &#123;&#125;public class CustomCondition implements Condition &#123; @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; return true; &#125;&#125; 上边的定义中，只要@Conditional中的中，有一个类的matches返回true，那这个类才会被spring管理。 自定义条件注解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(value = OnBeanCondition.class)public @interface ConditionOnBean &#123; Class&lt;?&gt;[] value() default &#123;&#125;; String[] name() default &#123;&#125;;&#125;@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(value = OnPropertyCondition.class)public @interface ConditionOnProperty &#123; Class&lt;?&gt;[] value() default &#123;&#125;; String[] name() default &#123;&#125;;&#125;public class OnClassCondition implements Condition &#123; @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; if(metadata.isAnnotated(ConditionOnClass.class.getName())) &#123; Map&lt;String, Object&gt; annotationAttributes = metadata.getAnnotationAttributes(ConditionOnClass.class.getName()); try &#123; ClassUtils.forName(annotationAttributes.get(&quot;name&quot;).toString(),ClassUtils.getDefaultClassLoader()); return true; &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); return false; &#125; &#125; return false; &#125;&#125;public class OnPropertyCondition implements Condition &#123; @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; if(metadata.isAnnotated(ConditionOnProperty.class.getName()))&#123; //首先获取到这个类里面的所有的注解信息 AnnotationAttributes annotationAttributes = AnnotationAttributes.fromMap(metadata.getAnnotationAttributes(ConditionOnProperty.class.getName(), false)); String[] names = annotationAttributes.getStringArray(&quot;name&quot;); try &#123; Properties properties = PropertiesLoaderUtils.loadAllProperties(&quot;application.properties&quot;, ClassUtils.getDefaultClassLoader()); for (String name : names) &#123; String property = properties.getProperty(name); if(property.equalsIgnoreCase(&quot;true&quot;))&#123; return true; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); return false; &#125; &#125; return false; &#125;&#125;@Component@Conditional(value = &#123;CustomCondition.class,CustomCondition1.class&#125;)//@ConditionOnClass(name = &quot;com.enjoy.jack.bean.circular.CircularRefConB&quot;)@ConditionOnProperty(name = &quot;cn.enjoy.flag&quot;)public class ConditionalBean &#123;&#125;","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"CGLIB","slug":"spring/CGLIB","date":"2021-11-25T12:00:37.000Z","updated":"2022-03-23T09:03:55.898Z","comments":true,"path":"blog/spring/CGLIB/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/CGLIB/","excerpt":"","text":"cglib动态代理其实很简单，就是生成一个子类来继承被代理的类，并且重写它的方法。 12345678910111213141516171819202122232425public class UserServiceImpl implements UserService &#123; @Override public String doSomething0(String param) &#123; System.out.println(&quot;==============doSomething0&quot;); return &quot;doSomething0&quot;; &#125; @Override public String doSomething1(String param) &#123; System.out.println(&quot;==============doSomething1&quot;); return &quot;doSomething1&quot;; &#125; @Override public String doSomething2(String param) &#123; System.out.println(&quot;==============doSomething2&quot;); return &quot;doSomething2&quot;; &#125; @Override public String myMethod(String param) &#123; System.out.println(&quot;==============myMethod&quot;); return &quot;myMethod&quot;; &#125;&#125; 1234567891011121314151617181920public class CglibBeanFactory &#123; public static Object getInstance() &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(UserServiceImpl.class); CallbackFilter callbackFilter = new CglibCallbackFilter(); enhancer.setCallbackFilter(callbackFilter); Callback callback1 = new DosomethingIntercepter1(); Callback callback2 = new DosomethingIntercepter2(); Callback callback3 = new DosomethingIntercepter3(); //这个NoOp表示no operator，即什么操作也不做，代理类直接调用被代理的方法不进行拦截。 Callback noop = NoOp.INSTANCE; Callback fixdValueCallback = new FixedValueIntercepter(); Callback[] callbacks = &#123;callback1, callback2, callback3, noop, fixdValueCallback&#125;; enhancer.setCallbacks(callbacks); return enhancer.create(); &#125;&#125; 12345678910111213141516public class CglibCallbackFilter implements CallbackFilter &#123; @Override public int accept(Method method) &#123; if (&quot;doSomething0&quot;.equalsIgnoreCase(method.getName())) &#123; return 3; &#125; else if (&quot;doSomething1&quot;.equalsIgnoreCase(method.getName())) &#123; return 1; &#125; else if (&quot;doSomething2&quot;.equalsIgnoreCase(method.getName())) &#123; return 2; &#125; else if (&quot;com.jack.controller.xx&quot;.equalsIgnoreCase(method.getName()))&#123; return 3; &#125; else &#123; return 4; &#125; &#125;&#125; 12345678910public class DosomethingIntercepter1 implements MethodInterceptor &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println(method.getName() + &quot;执行前...&quot;); //被代理方法 Object object = methodProxy.invokeSuper(o, objects); System.out.println(method.getName() + &quot;执行后...&quot;); return object; &#125;&#125; 1234567public class Test &#123; public static void main(String[] args) &#123; UserService userService = (UserService)CglibBeanFactory.getInstance(); System.out.println(userService.doSomething1(&quot;Jack&quot;)); &#125;&#125; 上面的代码逻辑就是， UserService userService &#x3D; (UserService)CglibBeanFactory.getInstance();生成了一个代理对象了，当调用doSomething1时，会有一个拦截，触发了CglibCallbackFilter的accept方法，更具给方法的返回值，从Callback[]找一个Callback对象执行对应的方法。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"BeanPostProcessor 接口理解","slug":"spring/BeanPostProcessor 接口理解","date":"2021-11-25T12:00:36.000Z","updated":"2022-03-23T09:03:55.897Z","comments":true,"path":"blog/spring/BeanPostProcessor 接口理解/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/BeanPostProcessor%20%E6%8E%A5%E5%8F%A3%E7%90%86%E8%A7%A3/","excerpt":"","text":"BeanPostProcessor这个接口的原本功能就是在Spring bean初始化阶段，在AbstractAutowireCapableBeanFactory中的doCreateBean方法定义了bean的初始化整个流程。 bean对象的创建 对bean对象对应的类的@Autowire、@Resource、@PostConstruct、@PreDestroy、@Value等注解的收集 添加三级缓存，让其他类能提前发现该对象 populateBean，依赖注入 BeanPostProcessor#postProcessBeforeInitialization invokeInitMethods方法的执行，对InitializingBean接口，afterPropertiesSet，init-method属性调用 BeanPostProcessor#postProcessAfterInitialization 所以，从这个调用时机看，就是在对Bean对象的初始话做了前置和后置处理 在Spring中，扩展都是通过BeanPostProcessor来完成的，Spring通过在BeanPostProcessor的基础做了进一步的扩展，定义了多个接口 SmartInstantiationAwareBeanPostProcessor MergedBeanDefinitionPostProcessor InstantiationAwareBeanPostProcessor 这里主要是两个方法，对象实例化前调用和实例化后调用 DestructionAwareBeanPostProcessor 这些BeanPostProcessor，让这些接口在Spring的初始化阶段的不同实际触发调用，来完成功能的扩展。 获取有@Autowired 注解的构造函数过滤的接口类型是:SmartInstantiationAwareBeanPostProcessor 调用的方法是:determineCandidateConstructors AbstractAutowireCapableBeanFactory#createBeanInstance 收集@Resource、@Autowired、@Value、@PostConstruct和@PreDestroy注解的方法和属性过滤的接口类型是:MergedBeanDefinitionPostProcessor 调用的方法是:postProcessMergedBeanDefinition AbstractAutowireCapobleBeanFactory#applyMergedBeanDefinitionPostProcessors 循环依赖解决中 bean 的提前暴露埋点过滤的接口类型是:SmartInstantiationAwareBeanPostProcessor 调用的方法是:getEarlyBeanReference AbstractAutowireCapobleBeanFactory#getEarlyBeanReference这个方法的掉用实际是当循环依赖发生时才会调用 阻止依赖注入埋点过滤的接口类型是:InstantiationAwareBeanPostProcessor 调用的方法是:postProcessAfterInstantiation AbstractAutowireCapobleBeanFactory#populateBean 依赖注入(DI)埋点过滤的接口类型是:InstantiationAwareBeanPostProcessor 调用的方法是:postProcessProperties(新版本) AbstractAutowireCapobleBeanFactory#populateBean DI 依赖注入后对接口的调用过滤的接口类型是:BeanPostProcessor 调用的方法是:postProcessBeforeInitialization AbstractAutowireCapobleBeanFactory#initializeBean IOC&#x2F;DI 后对接口的调用完成后（动态代理的入口）过滤的接口类型是:BeanPostProcessor 调用的方法是:postProcessAfterInitialization AbstractAutowireCapobleBeanFactory#initializeBean 代理对象的创建过滤的接口类型是:SmartInstantiationAwareBeanPostProcessor 调用的方法是:getEarlyBeanReference AbstractAutowireCapobleBeanFactory#getEarlyBeanReference这个方法的掉用实际是当循环依赖发生时才会调用","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"26-SpringMVC的消息转换器","slug":"spring/26-SpringMVC的消息转换器","date":"2021-11-25T12:00:35.000Z","updated":"2022-03-23T09:03:55.867Z","comments":true,"path":"blog/spring/26-SpringMVC的消息转换器/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/26-SpringMVC%E7%9A%84%E6%B6%88%E6%81%AF%E8%BD%AC%E6%8D%A2%E5%99%A8/","excerpt":"","text":"实现自己的消息转换器通过继承AbstractGenericHttpMessageConverter类来实现自己的消息转换器。 当然可以实现HttpMessageConverter接口，不过这个工作量就大了。 重写boolean supports(Class&lt;?&gt; clazz)方法来指明这个转换器支持的的请求参数类型和返回类型的类是什么。 重写boolean canRead(@Nullable MediaType mediaType)方法来限定这个请求头的Content-Type类型 重写boolean canWrite(@Nullable MediaType mediaType)方法来限定这个响应头的Content-Type类型 重写writeInternal(T t, HttpOutputMessage outputMessage)方法来实现请求内容的转换 重写void writeInternal(T t, @Nullable Type type, HttpOutputMessage outputMessage)方法来实现返回数据的转黄 不一定只能重写上边的方法。可以冲洗很多方法的，根据具体的需求重写方法。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"25-SpringMVC中的异步请求","slug":"spring/25-SpringMVC中的异步请求","date":"2021-11-25T12:00:34.000Z","updated":"2022-03-23T09:03:55.866Z","comments":true,"path":"blog/spring/25-SpringMVC中的异步请求/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/25-SpringMVC%E4%B8%AD%E7%9A%84%E5%BC%82%E6%AD%A5%E8%AF%B7%E6%B1%82/","excerpt":"","text":"异步请求能提高请求数，但响应时间会更长（相对） 浏览器同步浏览器发起一个request然后会一直待一个响应response，在这期间里面它是阻塞的。比如早期我们在我们在逛电商平台的时候买东西我们打开一个商品的页面，大致流程是不是可能是这样，每次打开一个页面都是由一个线程从头到尾来处理，这个请求需要进行数据库的访问需要把商品价格库存啥的返回页面，还需要去调用第三方接口，比如优惠券接口等我们只有等到这些都处理完成后这个线程才会把结果响应给浏览器，在这等结果期间这个线程只能一直在干等着啥事情也不能干。这样的话是不是会有有一定的性能问题。大致的流程如下： 浏览器异步为了解决上面同步阻塞的问题，再Servlet3.0发布后，提供了一个新特性：异步处理请求。比如我们还是进入商品详情页面，这时候这个前端发起一个请求，然后会有一个线程来执行这个请求，这个请求需要去数据库查询库存、调用第三方接口查询优惠券等。这时候这个线程就不用干等着呢。它的任务到这就完成了，又可以处理下一个请求了。等查询数据库和第三方接口查询优惠券有结果了，这时候会有一个新的线程来把处理结果返回给前端。这样的话线程的工作量是不超级饱和，需要不停的干活，连休息的机会都不给了。 这个异步是纯后端的异步，对前端是无感的，异步也并不会带来响应时间上的优化，原来该执行多久照样还是需要执行多久。但是我们的请求线程（Tomcat 线程）为异步servlet之后，我们可以立即返回，依赖于业务的任务用业务线程来执行，也就是说，Tomcat的线程可以立即回收，默认情况下，Tomcat的核心线程是10，最大线程数是200,我们能及时回收线程，也就意味着我们能处理更多的请求，能够增加我们的吞吐量，这也是异步Servlet的主要作用。 Servlet规范 传统Servlet处理 Web容器会为每个请求分配一个线程，默认情况下，响应完成前，该线程占用的资源都不会被释放。若有些请求需要长时间(例如长处理时间运算、等待某个资源)，就会长时间占用线程所需资源，若这类请求很多，许多线程资源都被长时间占用，会对系统的性能造成负担。 Servlet 3.0新增了异步处理，可以先释放容器分配给请求的线程与相关资源，减轻系统负担，其响应将被延后，但此时客户端仍然堵塞。业务处理交给了业务线程来完成，可以在处理完成(例如长时间运算完成、所需资源已获得)时再对客户端进行响应（包含了HttpRequest和HttpResponse）。 Servlet 3.0异步接收请求后步骤： Servlet 接收到请求之后，可能首先需要对请求携带的数据进行一些预处理；Servlet 线程将请求转交给一个异步线程来执行业务处理，线程本身返回至容器，客户端仍然堵塞。Servlet 还没有生成响应数据，异步线程处理完业务以后，可以直接生成响应数据（异步线程拥有 ServletRequest 和 ServletResponse 对象的引用），或者将请求继续转发给其它 Servlet。 SpringMVC支持请求异步处理的返回值解析器 StreamingResponseBodyReturnValueHandler 看supportsReturnType方法 12345678910111213@Overridepublic boolean supportsReturnType(MethodParameter returnType) &#123; if (StreamingResponseBody.class.isAssignableFrom(returnType.getParameterType())) &#123; return true; &#125; // 如果是ResponseEntity，而且该返回对象的范型为StreamingResponseBody，也就是这样定义 // ResponseEntity&lt;StreamingResponseBody&gt; else if (ResponseEntity.class.isAssignableFrom(returnType.getParameterType())) &#123; Class&lt;?&gt; bodyType = ResolvableType.forMethodParameter(returnType).getGeneric().resolve(); return (bodyType != null &amp;&amp; StreamingResponseBody.class.isAssignableFrom(bodyType)); &#125; return false;&#125; 这个处理器是用来处理StreamingResponseBody返回值的。 其中StreamingResponseBody接口是在Controller里处理输出流的 我们在java中创建I&#x2F;O输入输出流时，一般用完流后都要关闭流，但是在Controller里面，处理Http request是异步的，这个时候如果往request里写入流的时候，我们无法确定什么时候关闭流，例如在完成下载的功能的时候，需要下载比较大的File Stream，例如Video File Stream ,Excel File Stream。这个时候如果不关闭流，会造成比较大开销，并且File的线程会一直开着。StreamingResponseBody可以很有效的解决这个问题。 也就是说一个Controller在处理异步请求的时候，StreamingResponseBody会直接把流写入到response的输出流中，并且不会占用Servlet容器线程。 CallableMethodReturnValueHandler 看supportsReturnType方法 1234@Overridepublic boolean supportsReturnType(MethodParameter returnType) &#123; return Callable.class.isAssignableFrom(returnType.getParameterType());&#125; 这个就是用来出个Callable返回值的 DeferredResultMethodReturnValueHandler 看supportsReturnType方法 1234567@Overridepublic boolean supportsReturnType(MethodParameter returnType) &#123; Class&lt;?&gt; type = returnType.getParameterType(); return (DeferredResult.class.isAssignableFrom(type) || ListenableFuture.class.isAssignableFrom(type) || CompletionStage.class.isAssignableFrom(type));&#125; CompletionStage的一个实现类为CompletableFuture AsyncTaskMethodReturnValueHandler 看supportsReturnType方法 1234@Overridepublic boolean supportsReturnType(MethodParameter returnType) &#123; return WebAsyncTask.class.isAssignableFrom(returnType.getParameterType());&#125; 在SpringMVC中使用异步请求配置业务线程12345678910111213141516171819@Configurationpublic class WebMvcConfigurerConfig implements WebMvcConfigurer &#123; @Bean public ThreadPoolTaskExecutor mvcTaskExecutor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(10); executor.setQueueCapacity(100); executor.setMaxPoolSize(25); return executor; &#125; // 这不是必须的，不设置的话会使用默认的SimpleAsyncTaskExecutor // 可以看RequestMappingHandlerAdapter @Override public void configureAsyncSupport(AsyncSupportConfigurer configurer) &#123; configurer.setTaskExecutor(mvcTaskExecutor()); &#125;&#125; 使用StreamingResponseBody12345678 @RequestMapping(&quot;/test&quot;)public StreamingResponseBody test()&#123; return o-&gt;&#123; try(ObjectOutputStream oos = new ObjectOutputStream (o))&#123; oos.writeBytes (&quot;abcdefg&quot;); &#125; &#125;;&#125; Callable123456789101112@RequestMapping(value=&quot;/test2&quot;,produces = &#123;&quot;text/html;charset=utf-8&quot;&#125;)public Callable&lt;String&gt; test2()&#123; log.info (&quot;请求开始,线程为&#123;&#125;&quot;,Thread.currentThread ()); Callable&lt;String&gt; callable = () -&gt; &#123; log.info (&quot;异步请求开始,线程为&#123;&#125;&quot;,Thread.currentThread ()); Thread.sleep (10000); log.info (&quot;异步请求结束,线程为&#123;&#125;&quot;,Thread.currentThread ()); return &quot;abcdefg&quot;; &#125;; log.info (&quot;请求结束,线程为&#123;&#125;&quot;,Thread.currentThread ()); return callable;&#125; DeferredResult1234567891011121314151617181920212223@GetMapping(&quot;deferredResult&quot;)public DeferredResult&lt;String&gt; deferredResult() &#123; System.out.println(LocalDateTime.now().toString() + &quot;---&gt;主线程(&quot;+Thread.currentThread().getName()+&quot;)开始&quot;); DeferredResult&lt;String&gt; deferredResult = new DeferredResult&lt;&gt;(); // 这里使用依赖注入注入执行线程池，不要用默认的ForkJoin线程池 CompletableFuture.supplyAsync(()-&gt; doBusiness(), Executors.newFixedThreadPool(5)).whenCompleteAsync((result, throwable)-&gt;&#123; if (throwable!=null) &#123; deferredResult.setErrorResult(throwable.getMessage()); &#125;else &#123; deferredResult.setResult(result); &#125; &#125;); // 异步请求超时时调用 deferredResult.onTimeout(()-&gt;&#123; System.out.println(LocalDateTime.now().toString() + &quot;---&gt;onTimeout&quot;); &#125;); // 异步请求完成后调用 deferredResult.onCompletion(()-&gt;&#123; System.out.println(LocalDateTime.now().toString() + &quot;---&gt;onCompletion&quot;); &#125;); System.out.println(LocalDateTime.now().toString() + &quot;---&gt;主线程(&quot;+Thread.currentThread().getName()+&quot;)结束&quot;); return deferredResult;&#125; 这种方式记得不要使用内置的不要使用内置的 ForkJoinPool线程池，需要自己创建线程池否则会有性能问题 ListenableFuture12345678910@GetMapping(&quot;listenableFuture&quot;)public ListenableFuture&lt;String&gt; listenableFuture() &#123; // 线程池一般不会放在这里，会使用static声明，这只是演示 ExecutorService executor = Executors.newCachedThreadPool(); System.out.println(LocalDateTime.now().toString() + &quot;---&gt;主线程开始&quot;); ListenableFutureTask&lt;String&gt; listenableFuture = new ListenableFutureTask&lt;&gt;(()-&gt; doBusiness()); executor.execute(listenableFuture); System.out.println(LocalDateTime.now().toString() + &quot;---&gt;主线程结束&quot;); return listenableFuture;&#125; 这种方式记得不要使用内置的不要使用内置的 ForkJoinPool线程池，需要自己创建线程池否则会有性能问题 CompletableFuture123456789@GetMapping(&quot;completableFuture&quot;)public CompletableFuture&lt;String&gt; completableFuture() &#123; // 线程池一般不会放在这里，会使用static声明，这只是演示 ExecutorService executor = Executors.newCachedThreadPool(); System.out.println(LocalDateTime.now().toString() + &quot;---&gt;主线程开始&quot;); CompletableFuture&lt;String&gt; completableFuture = CompletableFuture.supplyAsync(IndexController::doBusiness, executor); System.out.println(LocalDateTime.now().toString() + &quot;---&gt;主线程结束&quot;); return completableFuture;&#125; 这种方式记得不要使用内置的 ForkJoinPool线程池，需要自己创建线程池否则会有性能问题 WebAsyncTask12345678910111213141516@GetMapping(&quot;asynctask&quot;)public WebAsyncTask asyncTask() &#123; // 线程池一般不会放在这里，会使用static声明，这只是演示 SimpleAsyncTaskExecutor executor = new SimpleAsyncTaskExecutor(); System.out.println(LocalDateTime.now().toString() + &quot;---&gt;主线程开始&quot;); WebAsyncTask&lt;String&gt; task = new WebAsyncTask(1000L, executor, ()-&gt; doBusiness()); task.onCompletion(()-&gt;&#123; System.out.println(LocalDateTime.now().toString() + &quot;---&gt;调用完成&quot;); &#125;); task.onTimeout(()-&gt;&#123; System.out.println(&quot;onTimeout&quot;); return &quot;onTimeout&quot;; &#125;); System.out.println(LocalDateTime.now().toString() + &quot;---&gt;主线程结束&quot;); return task;&#125; 这种方式记得不要使用内置的不要使用内置的 ForkJoinPool线程池，需要自己创建线程池否则会有性能问题 返回void这种就是最基本的情况，直接使用Servlet3.0的API 123456789101112131415161718192021222324ScheduledExecutorService executorService = Executors.newScheduledThreadPool(10);@RequestMapping(&quot;/async1&quot;)public void async1(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; // 释放http连接，转为异步 AsyncContext context = request.startAsync(); // 直接超时了 context.setTimeout(0L); // 异步处理，等待3秒后执行 executorService.schedule(()-&gt;&#123; PrintWriter writer = null; try &#123; writer = context.getResponse().getWriter(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; writer.print(&quot;111111111111&quot;); // TODO 异步完成，返回客户端信息 context.complete(); &#125;,3, TimeUnit.SECONDS);&#125; 总结上面这几种异步方式都是会等到业务doBusiness执行完之后才会把response给到前端，执行请求的主线程会立即结束，响应结果会交给另外的线程来返回给前端。 这种异步跟下面的这个所谓的假异步是不同的，这种情况是由主线程执行完成之后立马返回值（主线程）给前端，不会等个5s在返回给前端。 1234567891011121314@GetMapping(&quot;call&quot;)public String call() &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); return &quot;这是个假异步&quot;;&#125; 这几种异步方式都跟返回Callable 差不多，都有对应的HandlerMethodReturnValueHandler 实现类，无非就是丰富了自己一些特殊的api、比如超时设置啥的，以及线程池的创建是谁来创建，执行流程基本都是一样的。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"24-SpringMVC在项目有用的代码","slug":"spring/24-SpringMVC在项目有用的代码","date":"2021-11-25T12:00:33.000Z","updated":"2022-03-23T09:03:55.862Z","comments":true,"path":"blog/spring/24-SpringMVC在项目有用的代码/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/24-SpringMVC%E5%9C%A8%E9%A1%B9%E7%9B%AE%E6%9C%89%E7%94%A8%E7%9A%84%E4%BB%A3%E7%A0%81/","excerpt":"","text":"添加自定义的Servlet、Filter、ListenerSpring版本使用自定义的ServletContainerInitializer项目中提供这个文件： 实现类为： 12345678910111213141516171819202122232425public interface LoadServlet &#123; void loadOnstarp(ServletContext servletContext);&#125;@HandlesTypes(LoadServlet.class)public class MyServletContainerInitializer implements ServletContainerInitializer &#123; @Override public void onStartup(Set&lt;Class&lt;?&gt;&gt; set, ServletContext servletContext) throws ServletException &#123; Iterator&lt;Class&lt;?&gt;&gt; iterator; if (set != null) &#123; iterator = set.iterator(); while (iterator.hasNext()) &#123; Class&lt;?&gt; clazz = iterator.next(); if (!clazz.isInterface() &amp;&amp; !Modifier.isAbstract(clazz.getModifiers()) &amp;&amp; LoadServlet.class.isAssignableFrom(clazz)) &#123; try &#123; ((LoadServlet) clazz.newInstance()).loadOnstarp(servletContext); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125;&#125; 意思就是加载实现了LoadServlet这个接口的类，然后把这些类实例化后执行方法。 这里我提供一个实现了LoadServlet的类 12345678910111213141516171819202122232425262728293031323334353637383940414243public class LoadServletImpl implements LoadServlet &#123; @Override public void loadOnstarp(ServletContext servletContext) &#123; ServletRegistration.Dynamic initServlet = servletContext.addServlet(&quot;initServlet&quot;, InitServlet.class); initServlet.setLoadOnStartup(1); initServlet.addMapping(&quot;/init&quot;); ServletRegistration aDefault = servletContext.getServletRegistration(&quot;default&quot;); aDefault.addMapping(&quot;*.css&quot;,&quot;*.gif&quot;,&quot;*.jpg&quot;,&quot;*.js&quot;,&quot;*.JPG&quot;);// ServletRegistration.Dynamic defaults = servletContext.addServlet(&quot;default&quot;, DefaultServlet.class);// defaults.setLoadOnStartup(1);// defaults.addMapping(&quot;*.css&quot;,&quot;*.gif&quot;,&quot;*.jpg&quot;,&quot;*.js&quot;,&quot;*.JPG&quot;); &#125;&#125;public class InitServlet extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println(&quot;=====doget===&quot;); PrintWriter writer = resp.getWriter(); writer.print(&quot;&lt;h1&gt;Jack&lt;/h1&gt;&quot;); RequestDispatcher requestDispatcher = req.getRequestDispatcher(&quot;/jsp/ok.jsp&quot;); requestDispatcher.forward(req,resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; super.doPost(req, resp); &#125; @Override public void init(ServletConfig config) throws ServletException &#123; super.init(config); &#125; @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; super.service(req, resp); &#125;&#125; tomcat启动后，会加入一个名为initServlet的Servlet。 tomcat启动时答应如下内容 使用默认的ServletContainerInitializer也就意味着使用WebApplicationInitializer接口。大部分情况下只需要继承SpringMVC提供的AbstractAnnotationConfigDispatcherServletInitializer类就可以了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// 有很多可以重写的方法public class WebAppInitializer extends AbstractAnnotationConfigDispatcherServletInitializer &#123; //父容器 @Override protected Class&lt;?&gt;[] getRootConfigClasses() &#123; System.out.println(&quot;-------调用了getRootConfigClasses---------&quot;); return new Class&lt;?&gt;[]&#123;SpringContainer.class&#125;; &#125; //SpringMVC配置子容器 @Override protected Class&lt;?&gt;[] getServletConfigClasses() &#123; System.out.println(&quot;-------调用了getServletConfigClasses---------&quot;); return new Class&lt;?&gt;[]&#123;MvcContainer.class&#125;; &#125; //获取DispatcherServlet的映射信息 @Override protected String[] getServletMappings() &#123; System.out.println(&quot;-------调用了getServletMappings---------&quot;); return new String[]&#123;&quot;/xyz&quot;&#125;; &#125; /** * 添加Filter * @return */ @Override protected Filter[] getServletFilters() &#123; System.out.println(&quot;-------调用了getServletFilters---------&quot;); MyFilter myFilter = new MyFilter(); CorsFilter corsFilter = new CorsFilter(); CharacterEncodingFilter characterEncodingFilter = new CharacterEncodingFilter(); characterEncodingFilter.setEncoding(&quot;UTF-8&quot;); characterEncodingFilter.setForceEncoding(true); return new Filter[]&#123;myFilter, corsFilter, characterEncodingFilter&#125;; &#125; @Override protected void registerContextLoaderListener(ServletContext servletContext) &#123; // 添加Servlet// servletContext.addServlet() // 添加Filter// servletContext.addFilter() //添加Listener// servletContext.addListener(); // 这里必须 调用父类的registerContextLoaderListener方法 super.registerContextLoaderListener(servletContext); &#125; @Override protected ApplicationContextInitializer&lt;?&gt;[] getRootApplicationContextInitializers() &#123; return new ApplicationContextInitializer[]&#123;new ApplicationContextInitializerDemo()&#125;; &#125; @Override protected ApplicationContextInitializer&lt;?&gt;[] getServletApplicationContextInitializers() &#123; return super.getServletApplicationContextInitializers(); &#125;&#125; Spring Boot版本只需要加上 1@ServletComponentScan(basePackages = &#123;&quot;com.xyz.demo&quot;&#125;) 这个注解就好了 然后在 Servlet、Filter、Listener 上面加入响应注解即可。如: 1234567891011121314151617181920212223242526272829@WebServlet(name=&quot;HelloServlet&quot;,urlPatterns = &quot;/HelloServlet&quot;,loadOnStartup = 1)public class HelloServlet extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; PrintWriter writer = resp.getWriter(); writer.println(&quot;hello servlet!&quot;); &#125;&#125;@WebFilter(urlPatterns = &quot;/*&quot;,filterName = &quot;myFilter&quot;)public class MyFilter implements Filter &#123; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;--------MyFilter----------&quot;); chain.doFilter(request,response); &#125;&#125;@WebListenerpublic class MyListener implements ServletContextListener &#123; @Override public void contextDestroyed(ServletContextEvent contextEvent) &#123; System.out.println(&quot;contextDestroyed&quot;); &#125; @Override public void contextInitialized(ServletContextEvent contextEvent) &#123; System.out.println(&quot;contextInitialized&quot;); &#125;&#125; ServletRequestAttributes1234ServletRequestAttributes servletRequestAttributes = (ServletRequestAttributes)RequestContextHolder.getRequestAttributes();HttpServletRequest request = servletRequestAttributes.getRequest();HttpServletResponse response = servletRequestAttributes.getResponse(); 通过Request对象获取SpringMVC上下文在项目中就能通过 1request.getAttribute(DispatcherServlet.WEB_APPLICATION_CONTEXT_ATTRIBUTE) 这段代码获取到SpringMVC的上下，而该对象的的类型为AnnotationConfigWebApplicationContext。也就是说可以通过getParentBeanFactory方法到父BeanFactory，也就是Spring的上下文，该上下的类型也为AnnotationConfigWebApplicationContext。而且也能通过getServletContext()方法，获取到ServletContext，这样也就能做一些添加Servlet等操作，也可以通过ServletContext.getAttribute方法获取很多内置的属性或对象。 设置跨域配置使用注解模式1234567891011121314151617181920212223242526272829303132333435363738@ComponentScanpublic class WebMvcConfigurerConfig implements WebMvcConfigurer &#123; /** * listen 80; * server_name test.enjoy.com; * * #是否允许请求带有验证信息 * add_header Access-Control-Allow-Credentials true; * #允许跨域访问的域名,可以是一个域的列表，也可以是通配符*，多个用空格分开 * add_header Access-Control-Allow-Origin http://static.enjoy.com; * #允许脚本访问的返回头 * add_header Access-Control-Allow-Headers &#x27;x-requested-with,content-type,Cache-Control,Pragma,Date,x-timestamp&#x27;; * #允许使用的请求方法，以逗号隔开 * add_header Access-Control-Allow-Methods &#x27;POST,GET,OPTIONS,PUT,DELETE&#x27;; * #允许自定义的头部，以逗号隔开,大小写不敏感 * add_header Access-Control-Expose-Headers &#x27;WWW-Authenticate,Server-Authorization&#x27;; * #P3P支持跨域cookie操作 * add_header P3P &#x27;policyref=&quot;/w3c/p3p.xml&quot;, CP=&quot;NOI DSP PSAa OUR BUS IND ONL UNI COM NAV INT LOC&quot;&#x27;; * if ($request_method = &#x27;OPTIONS&#x27;) &#123;##OPTIONS类的请求，是跨域先验请求 * return 204;##204代表ok * &#125; * * @param registry */ @Override public void addCorsMappings(CorsRegistry registry) &#123; // 路径 CorsRegistration registration = registry.addMapping(&quot;/*&quot;); registration.allowCredentials(true); registration.allowedOrigins(&quot;http://static.enjoy.com&quot;); registration.allowedHeaders(&quot;x-requested-with&quot;, &quot;content-type&quot;, &quot;Cache-Control&quot;, &quot;Pragma&quot;, &quot;Date&quot;, &quot;x-timestamp&quot;); registration.allowedMethods(&quot;POST&quot;, &quot;GET&quot;, &quot;OPTIONS&quot;, &quot;DELETE&quot;); registration.exposedHeaders(&quot;WWW-Authenticate&quot;, &quot;Server-Authorization&quot;); registration.maxAge(3600L); &#125;&#125; 使用HttpRequestHandler接口使用CorsConfigurationSource接口123456789101112131415161718@Componentpublic class AreaController implements HttpRequestHandler, CorsConfigurationSource &#123; @Override public void handleRequest(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; System.out.println(&quot;======AreaController&quot;); PrintWriter writer = response.getWriter(); writer.println(&quot;&lt;h1&gt;==========Jack&lt;/h1&gt;&quot;); writer.flush(); writer.close(); &#125; // 定义跨域配置 @Override public CorsConfiguration getCorsConfiguration(HttpServletRequest request) &#123; return null; &#125;&#125; 使用SimpleUrlHandlerMapping在配置定义的HttpRequestHandler是，会新建一个SimpleUrlHandlerMapping对像，这时可以通过下面的方式设置 其中setCorsConfigurationSource可以设置全局的CorsConfiguration。 UriComponentsBuilder和UriComponentsspring mvc提供了一种机制，可以构造和编码URI，即使用UriComponentsBuilder和UriComponents。比如：对url进行编码（比如请求中含有空格时编码为%20），同时支持变量替换。 构建URI 1234UriComponents uriComponents=UriComponentsBuilder .fromHttpUrl(&quot;http://localhost:8080//hello&quot;) .queryParams(params).build() //params是个MapString uri=uriComponents.toUriString(); 替换参数和编码（默认使用utf-8） 123UriComponents uriComponents = UriComponentsBuilder.fromUriString(&quot;http://example.com/hotels/&#123;hotel&#125;/bookings/&#123;booking&#125;&quot;).build();URI uri = uriComponents.expand(&quot;42&quot;, &quot;21&quot;).encode().toUri(); 注意：UriComponents是不可变的，expand()和encode()返回新的实例。 或者 1234UriComponents uriComponents = UriComponentsBuilder.newInstance() .scheme(&quot;http&quot;).host(&quot;example.com&quot;).path(&quot;/hotels/&#123;hotel&#125;/bookings/&#123;booking&#125;&quot;).build() .expand(&quot;42&quot;, &quot;21&quot;) .encode();","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"23-SpringMVC拦截器、跨域和异常处理","slug":"spring/23-SpringMVC拦截器、跨域和异常处理","date":"2021-11-25T12:00:32.000Z","updated":"2022-03-23T09:03:55.854Z","comments":true,"path":"blog/spring/23-SpringMVC拦截器、跨域和异常处理/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/23-SpringMVC%E6%8B%A6%E6%88%AA%E5%99%A8%E3%80%81%E8%B7%A8%E5%9F%9F%E5%92%8C%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/","excerpt":"","text":"拦截器使用1234567891011121314151617181920212223242526272829303132333435363738394041424344@Componentpublic class UserInterceptor extends HandlerInterceptorAdapter &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;======UserInterceptor用户权限校验=========&quot;); return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; System.out.println(&quot;========UserInterceptor修改modelAndView======&quot;); int status = response.getStatus(); System.out.println(&quot;========UserInterceptor修改modelAndView======&quot; + status);// HttpSession session = request.getSession();// if(modelAndView != null &amp;&amp; session != null) &#123;// String modifyViewName = modelAndView.getViewName() + &quot;_&quot; + session.getAttribute(&quot;language&quot;);// modelAndView.setViewName(modifyViewName);// &#125; &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; System.out.println(&quot;========UserInterceptor资源释放======&quot;); &#125;&#125;@Configurationpublic class AppConfig implements WebMvcConfigurer &#123; @Autowired private UserInterceptor userInterceptor; @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(userInterceptor) .addPathPatterns(&quot;/user/**&quot;).excludePathPatterns(&quot;/user/query/**&quot;); registry.addInterceptor(new UserInterceptor1()) .addPathPatterns(&quot;/user/**&quot;).excludePathPatterns(&quot;&quot;); &#125; &#125; 源码——添加在springMVC的启动中，通过使用@EnableWebMvc注解引入了WebMvcConfigurationSupport来完成mvc bean的创建，其中最重要的是RequestMappingHandlerMapping这个从之前的分析中可知，这是用来处理@RuestingMapping注解的handlerMapping。 在创建RequestMappingHandlerMapping的过程过程中，通过getInterceptors来添加拦截器： 而从getInterceptors方法的源码中可以看到，spring MVC通过addInterceptors方法来添加自定义拦截器，其中扩展的源码和原理在前面《springMVC启动讲解》已经讲过了，这里关注拦截器的初始化。从源码可以看到，所有的拦截器都需要经过InterceptorRegistry，所以看InterceptorRegistry的定义： 123456789101112131415161718192021222324252627282930313233public class InterceptorRegistry &#123; private final List&lt;InterceptorRegistration&gt; registrations = new ArrayList&lt;&gt;(); public InterceptorRegistration addInterceptor(HandlerInterceptor interceptor) &#123; InterceptorRegistration registration = new InterceptorRegistration(interceptor); this.registrations.add(registration); return registration; &#125; public InterceptorRegistration addWebRequestInterceptor(WebRequestInterceptor interceptor) &#123; WebRequestHandlerInterceptorAdapter adapted = new WebRequestHandlerInterceptorAdapter(interceptor); InterceptorRegistration registration = new InterceptorRegistration(adapted); this.registrations.add(registration); return registration; &#125; protected List&lt;Object&gt; getInterceptors() &#123; return this.registrations.stream() .sorted(INTERCEPTOR_ORDER_COMPARATOR) .map(InterceptorRegistration::getInterceptor) .collect(Collectors.toList()); &#125; private static final Comparator&lt;Object&gt; INTERCEPTOR_ORDER_COMPARATOR = OrderComparator.INSTANCE.withSourceProvider(object -&gt; &#123; if (object instanceof InterceptorRegistration) &#123; return (Ordered) ((InterceptorRegistration) object)::getOrder; &#125; return null; &#125;);&#125; 从源码中可知，有两种类型的拦截器，HandlerInterceptor和WebRequestInterceptor，这两种拦截器的添加基本一样，而且作用都一样，区别是WebRequestInterceptor需要生成一个代理对象，这个代理对象类型就是适配器WebRequestHandlerInterceptorAdapter，最终被包装成InterceptorRegistration而已。WebRequestHandlerInterceptorAdapter就是做了适配，使得WebRequestInterceptor能够使用HandlerInterceptorde 的初始化和调用逻辑。 接着看回getInterceptors方法的最后，执行了registry.getInterceptors()方法。 123456protected List&lt;Object&gt; getInterceptors() &#123; return this.registrations.stream() .sorted(INTERCEPTOR_ORDER_COMPARATOR) .map(InterceptorRegistration::getInterceptor) .collect(Collectors.toList());&#125; 从上边的InterceptorRegistry类定义看getInterceptors方法。可以看到InterceptorRegistration#getInterceptor这段代码，而这段代码的源码如下： 123456789101112protected Object getInterceptor() &#123; if (this.includePatterns.isEmpty() &amp;&amp; this.excludePatterns.isEmpty()) &#123; return this.interceptor; &#125; String[] include = StringUtils.toStringArray(this.includePatterns); String[] exclude = StringUtils.toStringArray(this.excludePatterns); MappedInterceptor mappedInterceptor = new MappedInterceptor(include, exclude, this.interceptor); if (this.pathMatcher != null) &#123; mappedInterceptor.setPathMatcher(this.pathMatcher); &#125; return mappedInterceptor;&#125; 可以看到，这只是把匹配的路径、排除的路径和具体的HandlerInterceptor封装成MappedInterceptor。最后把MappedInterceptor列表通过HandlerMapping的setInterceptors方法把MappedInterceptor列表添加到了AbstractHandlerMapping的interceptors属性中 拦截器的添加源码就这么简单，只是做了封装。 把HandlerInterceptor和WebRequestInterceptor封装成InterceptorRegistration。 通过InterceptorRegistration生成MappedInterceptor 把MappedInterceptor列表添加到AbstractHandlerMapping的interceptors属性中 其中InterceptorRegistration的作用只是为了收集匹配的路径、排除的路径。从设计模式看，这个类是HandlerInterceptor的工厂，而MappedInterceptor包含了路径的匹配逻辑和HandlerInterceptor的调用逻辑。也就是说MappedInterceptor就是Spring调用的拦截器，而HandlerInterceptor只是为了完成拦截器的初始化和扩展。InterceptorRegistration为了包装数据和生成MappedInterceptor。 源码——初始化看RequestMappingHandlerMapping类的类图： 其中他的某一个父类实现了ApplicationContextAware接口，也就是setApplicationContext会被Spring执行。 重点看红框的代码，根据类图跟踪子类： 好了，现在跟踪子类的initApplicationContext()方法，一直到AbstractHandlerMapping的initApplicationContext()方法： 123456// AbstractHandlerMappingprotected void initApplicationContext() throws BeansException &#123; extendInterceptors(this.interceptors); detectMappedInterceptors(this.adaptedInterceptors); initInterceptors();&#125; 现在一个一个方法看。 extendInterceptors：该方法是一个钩子方法，是给子类扩展的，在AbstractHandlerMapping上，这个方法是一个空方法，但RequestMappingHandlerMapping并没有重写这个方法，所以这个方法没用 detectMappedInterceptors(this.adaptedInterceptors)： 入参adaptedInterceptors是AbstractHandlerMapping的一个内部属性，定义如下： 1private final List&lt;HandlerInterceptor&gt; adaptedInterceptors = new ArrayList&lt;&gt;() 看detectMappedInterceptors的源码： 12345protected void detectMappedInterceptors(List&lt;HandlerInterceptor&gt; mappedInterceptors) &#123; mappedInterceptors.addAll( BeanFactoryUtils.beansOfTypeIncludingAncestors( obtainApplicationContext(), MappedInterceptor.class, true, false).values());&#125; 从源码可以看到，该方法会在spring的上下文容器中获取MappedInterceptor的子类，并把这类类添加到AbstractHandlerMapping的内部属性adaptedInterceptors中 initInterceptors：看源码： 1234567891011protected void initInterceptors() &#123; if (!this.interceptors.isEmpty()) &#123; for (int i = 0; i &lt; this.interceptors.size(); i++) &#123; Object interceptor = this.interceptors.get(i); if (interceptor == null) &#123; throw new IllegalArgumentException(&quot;Entry number &quot; + i + &quot; in interceptors array is null&quot;); &#125; this.adaptedInterceptors.add(adaptInterceptor(interceptor)); &#125; &#125;&#125; 这代码就很简单了，interceptors在上边讲过，里面包含的就是拦截器，有自定义的和spring定义的，类型是MappedInterceptor。这个方法就是的作用就是把这些拦截器添加到内部属性adaptedInterceptors中。 源码——调用看DispatcherServlet的doDispatch方法，这方法在&lt;20-SpringMVC中请求是怎么到达Controller的&gt;中就已经讲过了，现在继续跟踪其中的getHandler方法，一直到AbstractHandlerMapping的getHandler(HttpServletRequest request)方法，在这个方法调用了方法getHandlerExecutionChain，这个方法在20中也讲过，这里简单看下源码： 从源码可以看到，这里按路径匹配出合适的HandlerInterceptor然后添加到HandlerExecutionChain中的interceptorList中，之后的流程在&lt;20-SpringMVC中请求是怎么到达Controller的&gt;这里已经说明过了。 跨域跨域问题由来：浏览器拒绝执行其它域名下的ajax运作。 如果浏览器在static.enjoy.com对应的html页面内，发起ajax请求偷盗www.enjoy.com域名下的内容来填充自己的页面,整个互联网秩序将混乱. 为了防止这种混乱,W3C组织制定了浏览器安全规范，即html页面发起的ajax请求仅限于同域名后端范围，跨越域名的ajax请求不得执行，此谓跨域问题。 cors方案的解决之道W3C制定跨域限制的本意，是防止页面领域安全混乱，即防止A公司不经B公司同意，使用ajax盗取B公司的服务内容。 出于这个本意，W3C改进了跨域的方案，即：如果B公司是同意将自己的内容分享给A公司的，跨域限制可放开，此方案即CORS方案，如下图： nginx配置跨域操作1234567891011121314151617181920server &#123; listen 80; server_name test.enjoy.com; #是否允许请求带有验证信息 add_header Access-Control-Allow-Credentials true; #允许跨域访问的域名,可以是一个域的列表，也可以是通配符*，多个用空格分开 add_header Access-Control-Allow-Origin http://static.enjoy.com; #允许脚本访问的返回头 add_header Access-Control-Allow-Headers &#x27;x-requested-with,content-type,Cache-Control,Pragma,Date,x-timestamp&#x27;; #允许使用的请求方法，以逗号隔开 add_header Access-Control-Allow-Methods &#x27;POST,GET,OPTIONS,PUT,DELETE&#x27;; #允许自定义的头部，以逗号隔开,大小写不敏感 add_header Access-Control-Expose-Headers &#x27;WWW-Authenticate,Server-Authorization&#x27;; #P3P支持跨域cookie操作 add_header P3P &#x27;policyref=&quot;/w3c/p3p.xml&quot;, CP=&quot;NOI DSP PSAa OUR BUS IND ONL UNI COM NAV INT LOC&quot;&#x27;; if ($request_method = &#x27;OPTIONS&#x27;) &#123;##OPTIONS类的请求，是跨域先验请求 return 204;##204代表ok &#125;&#125; spring中解决跨越从nginx中解决跨域配置中可以看到，就是在response中添加一些信息。比如这样: 12345HttpServletResponse response = (HttpServletResponse) res;response.setHeader(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;);response.setHeader(&quot;Access-Control-Allow-Methods&quot;, &quot;POST, GET, OPTIONS, DELETE&quot;);response.setHeader(&quot;Access-Control-Max-Age&quot;, &quot;3600&quot;);response.setHeader(&quot;Access-Control-Allow-Headers&quot;, &quot;x-requested-with&quot;); 而在spring中有很多方法可以在response中添加这些信息，可以在spring的拦截器、servlet的Filter等，下面介绍一下spring中基于注解的形式: 123456789101112@CrossOrigin(origins = &quot;*&quot; ,allowedHeaders = &quot;x-requested-with&quot; ,allowCredentials = &quot;true&quot; ,maxAge = 3600 ,methods = &#123;RequestMethod.GET,RequestMethod.POST,RequestMethod.OPTIONS,RequestMethod.DELETE&#125;)@RequestMapping(&quot;/queryArea&quot;)public @ResponseBodyList&lt;ConsultConfigArea&gt; queryArea(@RequestParam(required = false) String areaCode) &#123; Map map = new HashMap&lt;&gt;(); map.put(&quot;areaCode&quot;,areaCode); return areaService.queryAreaFromDB(map);&#125; 这个注解的解析也很简单，就是在AbstractHandlerMethodMapping#detectHandlerMethods方法中完成。而这个方法在20-SpringMVC中请求是怎么到达Controller的也已经说过了，就是将@RequestMapping解析成RequestMappingInfo，然后建立&lt;url,List&lt;RequestMapping&gt;&gt;和&lt;RequestMapping,HandlerMethod&gt;的对应关系，在建立这种对应关系后会做一件事： 就是建立&lt;HandlerMethod, CorsConfiguration&gt;的对应关系，有了这个对应关系后，继续看下调用时是怎么处理的。 回到DispatcherServlet，其中在doDispatch中会执行getHandler获取HandlerExecutionChain，跟踪getHandler，到这里： 上边的代码就是获取跨域配置的，其中有两种获取方式， 一种是通过AbstractHandlerMapping的属性CorsConfigurationSource， 一种是通过getCorsConfiguration方法。 第一种使用CorsConfigurationSource属性，该属性的初始化是需要调用AbstractHandlerMapping的setCorsConfigurations方法或者setCorsConfigurationSource方法设置的。对于使用注解的模式，可以直接通过WebMvcConfigurer接口的addCorsMappings方法添加。对于使用HttpRequestHandler接口的，需要使用SimpleUrlHandlerMapping对象设置。下面看第二种方式。 看getCorsConfiguration方法，这个方法更具不同的HandlerMapping，有不同的获取逻辑 如果不是RequestMappingHandlerMapping，那该方法就看AbstractHandlerMapping#getCorsConfiguration方法 12345678910protected CorsConfiguration getCorsConfiguration(Object handler, HttpServletRequest request) &#123; Object resolvedHandler = handler; if (handler instanceof HandlerExecutionChain) &#123; resolvedHandler = ((HandlerExecutionChain) handler).getHandler(); &#125; if (resolvedHandler instanceof CorsConfigurationSource) &#123; return ((CorsConfigurationSource) resolvedHandler).getCorsConfiguration(request); &#125; return null;&#125; 逻辑很简单，就是检查handler是否实现了CorsConfigurationSource接口，如果实现了，调用对应的方法获取就行了。 如果是RequestMappingHandlerMapping，该类的父类重写了getCorsConfiguration方法，看 AbstractHandlerMethodMapping#getCorsConfiguration方法 12345678910111213141516@Overrideprotected CorsConfiguration getCorsConfiguration(Object handler, HttpServletRequest request) &#123; CorsConfiguration corsConfig = super.getCorsConfiguration(handler, request); if (handler instanceof HandlerMethod) &#123; HandlerMethod handlerMethod = (HandlerMethod) handler; if (handlerMethod.equals(PREFLIGHT_AMBIGUOUS_MATCH)) &#123; return AbstractHandlerMethodMapping.ALLOW_CORS_CONFIG; &#125; else &#123; // MappingRegistry.crosLookup 缓存中取，建立映射发生在AbstractHandlerMethodMapping的register方法中建立 CorsConfiguration corsConfigFromMethod = this.mappingRegistry.getCorsConfiguration(handlerMethod); corsConfig = (corsConfig != null ? corsConfig.combine(corsConfigFromMethod) : corsConfigFromMethod); &#125; &#125; return corsConfig;&#125; 可以看到，会调用super.getCorsConfiguration(handler, request);，这个就是第一种模式的逻辑，这里看下面的代码。 这代码会在从MappingRegistry.crosLookup 缓存中获取，这个缓存就是解析@RequestMapping注解，建立uri与HandlerMethod映射前，前检查对应方法上是否有@CrossOrigin注解，有就在建立完立uri与HandlerMethod映射后，就建立HandlerMethod与CorsConfiguration的映射，这额映射就是保存在MappingRegister.corsLookup缓存中。 通过上边的代码，走上边的逻辑后，可能会获取到两个CorsConfiguration，这时需要把这个两个CorsConfiguration合并，然后执行getCorsHandlerExecutionChain方法，把一个拦截器添加到头部。看getCorsHandlerExecutionChain源码： 可以看到，该方法会添加一个CorsInterceptor拦截器，而且这个拦截器放在了第一个。这个拦截器的作用就是在response的handler中添加必要的参数的。 异常处理异常处理流程，看回DispatcherServlet中的doDispatch方法，在最后： 如果出现异常，异常被赋值给dispatchException引用，而该引用会作为参数传入processDispatchResult方法中。 看else代码块的，handler就是MethodHandle，看processHandlerException： 这里就是异常处理的核心。handlerExceptionResolvers的值是在SpringMVC启动前添加进去的，现在先看异常处理器的初始化 异常处理器的初始化先看源码WebMvcConfigurationSupport的handlerExceptionResolver： 钩子方法就不看了，if中的代码addDefaultHandlerExceptionResolvers，该方法会添加一些默认的异常处理器。 这里添加了3个异常处理器 ExceptionHandlerExceptionResolver ResponseStatusExceptionResolver DefaultHandlerExceptionResolver 接着会把上面的3个异常处理器放到HandlerExceptionResolverComposite这个对象中，并把该对象注入到Spring容器中。 接着回到DispatcherServlet中的onRefresh，这个方法会在SpringMVC容器启动完成后，发起一个ContextRefreshedEvent事件后触发调用的。而在onRefresh方法中有一个方法initHandlerExceptionResolvers(context)，该方法就是用来初始化handlerExceptionResolvers： 从源码看到，这里初始化后的结果就是handlerExceptionResolvers中只有一个值，这个值就是HandlerExceptionResolverComposite类型对象。 回到DispatcherServlet#processHandlerException 这里可以看成HandlerExceptionResolverComposite#resolveException： 从之前可以知道，resolvers中有3个值，分别是： ExceptionHandlerExceptionResolver ResponseStatusExceptionResolver DefaultHandlerExceptionResolver 顺序就是上边的顺序，所以这里可以就按上边的顺序解析 ExceptionHandlerExceptionResolver跟踪代码 resolveException—&gt;doResolveException—&gt;doResolveHandlerMethodException 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455protected ModelAndView doResolveHandlerMethodException(HttpServletRequest request, HttpServletResponse response, @Nullable HandlerMethod handlerMethod, Exception exception) &#123; ServletInvocableHandlerMethod exceptionHandlerMethod = getExceptionHandlerMethod(handlerMethod, exception); if (exceptionHandlerMethod == null) &#123; return null; &#125; if (this.argumentResolvers != null) &#123; exceptionHandlerMethod.setHandlerMethodArgumentResolvers(this.argumentResolvers); &#125; if (this.returnValueHandlers != null) &#123; exceptionHandlerMethod.setHandlerMethodReturnValueHandlers(this.returnValueHandlers); &#125; ServletWebRequest webRequest = new ServletWebRequest(request, response); ModelAndViewContainer mavContainer = new ModelAndViewContainer(); try &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Using @ExceptionHandler &quot; + exceptionHandlerMethod); &#125; Throwable cause = exception.getCause(); if (cause != null) &#123; // Expose cause as provided argument as well exceptionHandlerMethod.invokeAndHandle(webRequest, mavContainer, exception, cause, handlerMethod); &#125; else &#123; // Otherwise, just the given exception as-is exceptionHandlerMethod.invokeAndHandle(webRequest, mavContainer, exception, handlerMethod); &#125; &#125; catch (Throwable invocationEx) &#123; 。。。。。。 return null; &#125; if (mavContainer.isRequestHandled()) &#123; return new ModelAndView(); &#125; else &#123; ModelMap model = mavContainer.getModel(); HttpStatus status = mavContainer.getStatus(); ModelAndView mav = new ModelAndView(mavContainer.getViewName(), model, status); mav.setViewName(mavContainer.getViewName()); if (!mavContainer.isViewReference()) &#123; mav.setView((View) mavContainer.getView()); &#125; if (model instanceof RedirectAttributes) &#123; Map&lt;String, ?&gt; flashAttributes = ((RedirectAttributes) model).getFlashAttributes(); RequestContextUtils.getOutputFlashMap(request).putAll(flashAttributes); &#125; return mav; &#125;&#125; 先看getExceptionHandlerMethod方法，这个方法是异常处理的核心逻辑。 123456789101112131415161718192021222324252627282930313233343536373839protected ServletInvocableHandlerMethod getExceptionHandlerMethod( @Nullable HandlerMethod handlerMethod, Exception exception) &#123; Class&lt;?&gt; handlerType = null; if (handlerMethod != null) &#123; // Local exception handler methods on the controller class itself. // To be invoked through the proxy, even in case of an interface-based proxy. handlerType = handlerMethod.getBeanType(); ExceptionHandlerMethodResolver resolver = this.exceptionHandlerCache.get(handlerType); if (resolver == null) &#123; //在这里建立异常和method的映射关系 resolver = new ExceptionHandlerMethodResolver(handlerType); this.exceptionHandlerCache.put(handlerType, resolver); &#125; Method method = resolver.resolveMethod(exception); if (method != null) &#123; return new ServletInvocableHandlerMethod(handlerMethod.getBean(), method); &#125; // For advice applicability check below (involving base packages, assignable types // and annotation presence), use target class instead of interface-based proxy. if (Proxy.isProxyClass(handlerType)) &#123; handlerType = AopUtils.getTargetClass(handlerMethod.getBean()); &#125; &#125; for (Map.Entry&lt;ControllerAdviceBean, ExceptionHandlerMethodResolver&gt; entry : this.exceptionHandlerAdviceCache.entrySet()) &#123; ControllerAdviceBean advice = entry.getKey(); if (advice.isApplicableToBeanType(handlerType)) &#123; ExceptionHandlerMethodResolver resolver = entry.getValue(); Method method = resolver.resolveMethod(exception); if (method != null) &#123; return new ServletInvocableHandlerMethod(advice.resolveBean(), method); &#125; &#125; &#125; return null;&#125; 而这段代码的核心在这里： 可以看到先从缓存中拿ExceptionHandlerMethodResolver，没有就new一个，现在看该类的构造方法。 123456789101112131415161718public static final MethodFilter EXCEPTION_HANDLER_METHODS = method -&gt; AnnotatedElementUtils.hasAnnotation(method, ExceptionHandler.class);public ExceptionHandlerMethodResolver(Class&lt;?&gt; handlerType) &#123; for (Method method : MethodIntrospector.selectMethods(handlerType, EXCEPTION_HANDLER_METHODS)) &#123; for (Class&lt;? extends Throwable&gt; exceptionType : detectExceptionMappings(method)) &#123; addExceptionMapping(exceptionType, method); &#125; &#125;&#125;private void addExceptionMapping(Class&lt;? extends Throwable&gt; exceptionType, Method method) &#123; Method oldMethod = this.mappedMethods.put(exceptionType, method); if (oldMethod != null &amp;&amp; !oldMethod.equals(method)) &#123; throw new IllegalStateException(&quot;Ambiguous @ExceptionHandler method mapped for [&quot; + exceptionType + &quot;]: &#123;&quot; + oldMethod + &quot;, &quot; + method + &quot;&#125;&quot;); &#125;&#125; 这个段代码的意思就是，获取handlerType的方法中有ExceptionHandler注解的方法，然后解析方法上的ExceptionHandler，得到Throwable类型也就是异常class对象的列表后，建立&lt;异常class对象, 方法的映射关系&gt;，这个映射关系保存在ExceptionHandlerMethodResolver的mappedMethods&#96;属性中。new完之后，这时有两个逻辑选择： 一种就是@ExceptionHandler中定义了错误 这里的意思就是错误类型在之前获取到的ExceptionHandlerMethodResolver中找到方法。那么这时就返回new一个ServletInvocableHandlerMethod返回，添加获取参数解析起和返回值处理器然后就只执行invokeAndHandle方法，也就是对应方法的调用了，使用就是在controller中这样定义后， 123456789101112131415161718@Controller@RequestMapping(&quot;/user&quot;)public class Controller &#123; @RequestMapping(&quot;/exceptionTest&quot;) public @ResponseBody String exceptionTest(String param) &#123; if(!param.equalsIgnoreCase(&quot;ok&quot;)) &#123; throw new RuntimeException(&quot;xs&quot;); &#125; return &quot;ok&quot;; &#125; @ExceptionHandler(Exception.class) public String exceptionHandler() &#123; return &quot;error&quot;; &#125;&#125; 当/exceptionTest?param=111调用后，后抛出一个错误RuntimeExceptionc错误，然后会创建一个ExceptionHandlerMethodResolver对象，在该对象中，建立了&lt;Exception.class, exceptionHandler()&gt;这样的映射关系后，由于RuntimeException能匹配到exceptionHandler()方法，所以在SpringMVC中就会调用到Controller中的exceptionHandler()方法。 另外一种逻辑@ControllerAdvice定义了错误处理这个逻辑涉及到的代码如下： 在这段代码中，exceptionHandlerAdviceCache的值是在异常处理器的初始化这步完成初始化的，具体逻辑就在这： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Overridepublic void afterPropertiesSet() &#123; // Do this first, it may add ResponseBodyAdvice beans initExceptionHandlerAdviceCache(); if (this.argumentResolvers == null) &#123; List&lt;HandlerMethodArgumentResolver&gt; resolvers = getDefaultArgumentResolvers(); this.argumentResolvers = new HandlerMethodArgumentResolverComposite().addResolvers(resolvers); &#125; if (this.returnValueHandlers == null) &#123; List&lt;HandlerMethodReturnValueHandler&gt; handlers = getDefaultReturnValueHandlers(); this.returnValueHandlers = new HandlerMethodReturnValueHandlerComposite().addHandlers(handlers); &#125;&#125;private void initExceptionHandlerAdviceCache() &#123; if (getApplicationContext() == null) &#123; return; &#125; // 找有@ControllerAdvice注解的Bean List&lt;ControllerAdviceBean&gt; adviceBeans = ControllerAdviceBean.findAnnotatedBeans(getApplicationContext()); for (ControllerAdviceBean adviceBean : adviceBeans) &#123; Class&lt;?&gt; beanType = adviceBean.getBeanType(); if (beanType == null) &#123; throw new IllegalStateException(&quot;Unresolvable type for ControllerAdviceBean: &quot; + adviceBean); &#125; ExceptionHandlerMethodResolver resolver = new ExceptionHandlerMethodResolver(beanType); if (resolver.hasExceptionMappings()) &#123; this.exceptionHandlerAdviceCache.put(adviceBean, resolver); &#125; if (ResponseBodyAdvice.class.isAssignableFrom(beanType)) &#123; this.responseBodyAdvice.add(adviceBean); &#125; &#125; if (logger.isDebugEnabled()) &#123; int handlerSize = this.exceptionHandlerAdviceCache.size(); int adviceSize = this.responseBodyAdvice.size(); if (handlerSize == 0 &amp;&amp; adviceSize == 0) &#123; logger.debug(&quot;ControllerAdvice beans: none&quot;); &#125; else &#123; logger.debug(&quot;ControllerAdvice beans: &quot; + handlerSize + &quot; @ExceptionHandler, &quot; + adviceSize + &quot; ResponseBodyAdvice&quot;); &#125; &#125;&#125; 上面代码的意思就是先通过 1234567891011121314151617List&lt;ControllerAdviceBean&gt; adviceBeans = ControllerAdviceBean.findAnnotatedBeans(getApplicationContext());public static List&lt;ControllerAdviceBean&gt; findAnnotatedBeans(ApplicationContext context) &#123; List&lt;ControllerAdviceBean&gt; adviceBeans = new ArrayList&lt;&gt;(); for (String name : BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context, Object.class)) &#123; if (!ScopedProxyUtils.isScopedTarget(name)) &#123; ControllerAdvice controllerAdvice = context.findAnnotationOnBean(name, ControllerAdvice.class); if (controllerAdvice != null) &#123; // Use the @ControllerAdvice annotation found by findAnnotationOnBean() // in order to avoid a subsequent lookup of the same annotation. adviceBeans.add(new ControllerAdviceBean(name, context, controllerAdvice)); &#125; &#125; &#125; OrderComparator.sort(adviceBeans); return adviceBeans;&#125; 获取到有@ControllerAdvice注解的类，然后遍历new ExceptionHandlerMethodResolver(beanType)，而ExceptionHandlerMethodResolver的构造方法在上边已经讲过，就是扫描注解@ExceptionHandler，然后建立&lt;异常class对象，Method&gt;这样的映射。 回到异常处理逻辑，还是更具异常class选择对应的方法，并把方法所在的对象和方法封装成ServletInvocableHandlerMethod，之后的流程还是一样。 ResponseStatusExceptionResolver这个就不看源码了，直接说怎么使用 123456789101112131415161718192021@ResponseStatus(code = HttpStatus.FORBIDDEN,reason = &quot;xyz error!&quot;)public class ResponseStatusEx extends RuntimeException &#123; public ResponseStatusEx(String x) &#123; super(x); &#125;&#125;@Controller@RequestMapping(&quot;/user&quot;)public class Controller &#123; @RequestMapping(&quot;/exceptionTest&quot;) public @ResponseBody String exceptionTest(String param) &#123; if(!param.equalsIgnoreCase(&quot;ok&quot;)) &#123; throw new ResponseStatusEx(&quot;xs&quot;); &#125; return &quot;ok&quot;; &#125;&#125; 当/exceptionTest?param=111调用后，就会把匹配到ResponseStatusEx，然后把注解的信息输出：","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"22-SpringMVC的视图解析","slug":"spring/22-SpringMVC的视图解析","date":"2021-11-25T12:00:31.000Z","updated":"2022-03-23T09:03:55.759Z","comments":true,"path":"blog/spring/22-SpringMVC的视图解析/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/22-SpringMVC%E7%9A%84%E8%A7%86%E5%9B%BE%E8%A7%A3%E6%9E%90/","excerpt":"","text":"视图解析已经时边缘技术了，满足不了现在的应用了，这里只是简单的过下 12345678910111213141516public void invokeAndHandle(ServletWebRequest webRequest, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; //具体调用逻辑，重点看 Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs); setResponseStatus(webRequest); ..... try &#123; //返回值处理 this.returnValueHandlers.handleReturnValue( returnValue, getReturnValueType(returnValue), mavContainer, webRequest); &#125; catch (Exception ex) &#123; ..... &#125;&#125; 还是看 12345678910111213this.returnValueHandlers.handleReturnValue( returnValue, getReturnValueType(returnValue), mavContainer, webRequest);@Overridepublic void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception &#123; HandlerMethodReturnValueHandler handler = selectHandler(returnValue, returnType); if (handler == null) &#123; throw new IllegalArgumentException(&quot;Unknown return value type: &quot; + returnType.getParameterType().getName()); &#125; handler.handleReturnValue(returnValue, returnType, mavContainer, webRequest);&#125; 对于视图解析，这里的HandlerMethodReturnValueHandler是ViewNameMethodReturnValueHandler，看这个类只需要看两个方法 supportsReturnType 12345@Overridepublic boolean supportsReturnType(MethodParameter returnType) &#123; Class&lt;?&gt; paramType = returnType.getParameterType(); return (void.class == paramType || CharSequence.class.isAssignableFrom(paramType));&#125; handleReturnValue 12345678910111213141516@Overridepublic void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception &#123; if (returnValue instanceof CharSequence) &#123; String viewName = returnValue.toString(); mavContainer.setViewName(viewName); if (isRedirectViewName(viewName)) &#123; mavContainer.setRedirectModelScenario(true); &#125; &#125; else if (returnValue != null) &#123; //抛错 。。。。 &#125;&#125; 可以看到，这里还没有涉及到视图解析，只是拿到了viewName，然后放入到ModelAndViewContainer中了 这时invokeAndHandle方法返回，栈顶的栈帧变成了RequestMappingHandlerAdapter.invokeHandlerMethod方法的栈帧，所以看回RequestMappingHandlerAdapter方法： 看getModelAndView方法 12345678910111213141516171819202122private ModelAndView getModelAndView(ModelAndViewContainer mavContainer, ModelFactory modelFactory, NativeWebRequest webRequest) throws Exception &#123; modelFactory.updateModel(webRequest, mavContainer); //如果不需要响应视图，这里为true if (mavContainer.isRequestHandled()) &#123; return null; &#125; ModelMap model = mavContainer.getModel(); ModelAndView mav = new ModelAndView(mavContainer.getViewName(), model, mavContainer.getStatus()); if (!mavContainer.isViewReference()) &#123; mav.setView((View) mavContainer.getView()); &#125; if (model instanceof RedirectAttributes) &#123; Map&lt;String, ?&gt; flashAttributes = ((RedirectAttributes) model).getFlashAttributes(); HttpServletRequest request = webRequest.getNativeRequest(HttpServletRequest.class); if (request != null) &#123; RequestContextUtils.getOutputFlashMap(request).putAll(flashAttributes); &#125; &#125; return mav;&#125; 这个方法只是把上边的viewName封装成了ModelAndView。方法继续返回，回到了DispatcherServlet.doDispatch: 方法processDispatchResult就是真正的进行视图解析的了。 Servlet的视图解析： 这里还是选择匹配的视图解析器，这个些视图解析器和参数解析器一样，都是在WebMvcConfigurationSupport中通过@Bean创建的。 可以看到，ViewResolver返回的是ViewResolverComposite。之后的解析只是把上边返回的ModelAndView进一步解析，把viewName解析成视图文件的目录，然后最后通过Servlet规范，也就是: 12RequestDispatcher requestDispatcher = req.getRequestDispatcher(&quot;/jsp/ok.jsp&quot;);requestDispatcher.forward(req,resp); 把视图的加载和编译交给Servlet容器（tomcat）来完成","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"21-SpringMVC中Controller调用","slug":"spring/21-SpringMVC中Controller调用","date":"2021-11-25T12:00:30.000Z","updated":"2022-03-23T09:03:55.737Z","comments":true,"path":"blog/spring/21-SpringMVC中Controller调用/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/21-SpringMVC%E4%B8%ADController%E8%B0%83%E7%94%A8/","excerpt":"","text":"以注解为例，也就是RequestMappingHandlerAdapter 上面讲了一个请求的处理过程，这里讲请求时怎么调用到对应的controller的。以 在DispatcherServlet的doDispatch方法中，请求匹配到一个HandlerMethod（以注解的为例），然后封装成HandlerExecutionChain，然后根据HandlerMethod匹配到一个HandlerAdapter，接着执行 12// DispatcherServlet.doDispatchModelAndView mv = ha.handle(processedRequest, response, mappedHandler.getHandler()) 这里的返回值是RequestMappingHandlerAdapter，下面看HandlerAdapter的handle方法是怎么调用Controller的方法的。 这个方法在RequestMappingHandlerAdapter的父类AbstractHandlerMethodAdapter中实现的，看源码： 123456// AbstractHandlerMethodAdapterpublic final ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; return handleInternal(request, response, (HandlerMethod) handler);&#125; handleInternal是一个钩子方法，它在RequestMappingHandlerAdapter中实现，跟踪源码后，看invokeHandlerMethod： RequestMappingHandlerAdapter.invokeHandlerMethod——请求处理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// RequestMappingHandlerAdapterprotected ModelAndView invokeHandlerMethod(HttpServletRequest request, HttpServletResponse response, HandlerMethod handlerMethod) throws Exception &#123; ServletWebRequest webRequest = new ServletWebRequest(request, response); try &#123; //获取数据绑定工厂 @InitBinder注解支持，没太多用 WebDataBinderFactory binderFactory = getDataBinderFactory(handlerMethod); //Model工厂,收集了@ModelAttribute注解的方法 ModelFactory modelFactory = getModelFactory(handlerMethod, binderFactory); //可调用的方法对象 ServletInvocableHandlerMethod invocableMethod = createInvocableHandlerMethod(handlerMethod); if (this.argumentResolvers != null) &#123; //设置参数解析器 invocableMethod.setHandlerMethodArgumentResolvers(this.argumentResolvers); &#125; if (this.returnValueHandlers != null) &#123; //设置返回值解析器 invocableMethod.setHandlerMethodReturnValueHandlers(this.returnValueHandlers); &#125; //设置参数绑定工厂 invocableMethod.setDataBinderFactory(binderFactory); //设置参数名称解析类 invocableMethod.setParameterNameDiscoverer(this.parameterNameDiscoverer); ModelAndViewContainer mavContainer = new ModelAndViewContainer(); mavContainer.addAllAttributes(RequestContextUtils.getInputFlashMap(request)); //调用有@ModelAttribute注解的方法。每次请求都会调用有@ModelAttribute注解的方法 //把@ModelAttribute注解的方法的返回值存储到 ModelAndViewContainer对象的map中了 modelFactory.initModel(webRequest, mavContainer, invocableMethod); mavContainer.setIgnoreDefaultModelOnRedirect(this.ignoreDefaultModelOnRedirect); AsyncWebRequest asyncWebRequest = WebAsyncUtils.createAsyncWebRequest(request, response); asyncWebRequest.setTimeout(this.asyncRequestTimeout); //异步处理 WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); asyncManager.setTaskExecutor(this.taskExecutor); asyncManager.setAsyncWebRequest(asyncWebRequest); asyncManager.registerCallableInterceptors(this.callableInterceptors); asyncManager.registerDeferredResultInterceptors(this.deferredResultInterceptors); if (asyncManager.hasConcurrentResult()) &#123; Object result = asyncManager.getConcurrentResult(); mavContainer = (ModelAndViewContainer) asyncManager.getConcurrentResultContext()[0]; asyncManager.clearConcurrentResult(); LogFormatUtils.traceDebug(logger, traceOn -&gt; &#123; String formatted = LogFormatUtils.formatValue(result, !traceOn); return &quot;Resume with async result [&quot; + formatted + &quot;]&quot;; &#125;); invocableMethod = invocableMethod.wrapConcurrentResult(result); &#125; //Controller方法调用，重点看看 invocableMethod.invokeAndHandle(webRequest, mavContainer); if (asyncManager.isConcurrentHandlingStarted()) &#123; return null; &#125; return getModelAndView(mavContainer, modelFactory, webRequest); &#125; finally &#123; webRequest.requestCompleted(); &#125;&#125; 上面的方法有很多细枝的逻辑，我们这里只关注核心逻辑，把细枝的逻辑去掉后，代码变成这样 1234567891011121314151617181920212223242526272829private ParameterNameDiscoverer parameterNameDiscoverer = new DefaultParameterNameDiscoverer();protected ModelAndView invokeHandlerMethod(HttpServletRequest request, HttpServletResponse response, HandlerMethod handlerMethod) throws Exception &#123; ServletWebRequest webRequest = new ServletWebRequest(request, response); try &#123; // 可调用的方法对象 // new ServletInvocableHandlerMethod(handlerMethod); ServletInvocableHandlerMethod invocableMethod = createInvocableHandlerMethod(handlerMethod); if (this.argumentResolvers != null) &#123; // 设置参数解析器 invocableMethod.setHandlerMethodArgumentResolvers(this.argumentResolvers); &#125; if (this.returnValueHandlers != null) &#123; // 设置返回值解析器 invocableMethod.setHandlerMethodReturnValueHandlers(this.returnValueHandlers); &#125; //设置参数绑定工厂 invocableMethod.setDataBinderFactory(binderFactory); // 设置参数名称解析类 invocableMethod.setParameterNameDiscoverer(this.parameterNameDiscoverer); ModelAndViewContainer mavContainer = new ModelAndViewContainer(); // Controller方法调用，重点看看 invocableMethod.invokeAndHandle(webRequest, mavContainer); return getModelAndView(mavContainer, null, webRequest); &#125; finally &#123; webRequest.requestCompleted(); &#125;&#125; 简化后就简单多了。 参数解析器和参数解析器初始化在讲方法调用前先看这段代码 12345678if (this.argumentResolvers != null) &#123; //设置参数解析器 invocableMethod.setHandlerMethodArgumentResolvers(this.argumentResolvers);&#125;if (this.returnValueHandlers != null) &#123; //设置返回值解析器 invocableMethod.setHandlerMethodReturnValueHandlers(this.returnValueHandlers);&#125; 这段代码就是用来设置参数解析器和返回值解析器的。argumentResolvers和returnValueHandlers的值的初始化是在afterPropertiesSet方法内完成的 12345678910111213141516171819202122// RequestMappingHandlerAdapterpublic void afterPropertiesSet() &#123; // Do this first, it may add ResponseBody advice beans initControllerAdviceCache(); // 初始化参数解析器 if (this.argumentResolvers == null) &#123; // getDefaultArgumentResolvers这里会返回很多默认的，就是直接new出来的 List&lt;HandlerMethodArgumentResolver&gt; resolvers = getDefaultArgumentResolvers(); this.argumentResolvers = new HandlerMethodArgumentResolverComposite().addResolvers(resolvers); &#125; if (this.initBinderArgumentResolvers == null) &#123; List&lt;HandlerMethodArgumentResolver&gt; resolvers = getDefaultInitBinderArgumentResolvers(); this.initBinderArgumentResolvers = new HandlerMethodArgumentResolverComposite().addResolvers(resolvers); &#125; // 初始化返回值解析器 if (this.returnValueHandlers == null) &#123; List&lt;HandlerMethodReturnValueHandler&gt; handlers = getDefaultReturnValueHandlers(); this.returnValueHandlers = new HandlerMethodReturnValueHandlerComposite().addHandlers(handlers); &#125;&#125; getDefaultArgumentResolvers 12345678910111213141516171819202122232425262728293031323334353637383940414243// RequestMappingHandlerAdapterprivate List&lt;HandlerMethodArgumentResolver&gt; getDefaultArgumentResolvers() &#123; List&lt;HandlerMethodArgumentResolver&gt; resolvers = new ArrayList&lt;&gt;(30); // Annotation-based argument resolution resolvers.add(new RequestParamMethodArgumentResolver(getBeanFactory(), false)); resolvers.add(new RequestParamMapMethodArgumentResolver()); resolvers.add(new PathVariableMethodArgumentResolver()); resolvers.add(new PathVariableMapMethodArgumentResolver()); resolvers.add(new MatrixVariableMethodArgumentResolver()); resolvers.add(new MatrixVariableMapMethodArgumentResolver()); resolvers.add(new ServletModelAttributeMethodProcessor(false)); resolvers.add(new RequestResponseBodyMethodProcessor(getMessageConverters(), this.requestResponseBodyAdvice)); resolvers.add(new RequestPartMethodArgumentResolver(getMessageConverters(), this.requestResponseBodyAdvice)); resolvers.add(new RequestHeaderMethodArgumentResolver(getBeanFactory())); resolvers.add(new RequestHeaderMapMethodArgumentResolver()); resolvers.add(new ServletCookieValueMethodArgumentResolver(getBeanFactory())); resolvers.add(new ExpressionValueMethodArgumentResolver(getBeanFactory())); resolvers.add(new SessionAttributeMethodArgumentResolver()); resolvers.add(new RequestAttributeMethodArgumentResolver()); // Type-based argument resolution resolvers.add(new ServletRequestMethodArgumentResolver()); resolvers.add(new ServletResponseMethodArgumentResolver()); resolvers.add(new HttpEntityMethodProcessor(getMessageConverters(), this.requestResponseBodyAdvice)); resolvers.add(new RedirectAttributesMethodArgumentResolver()); resolvers.add(new ModelMethodProcessor()); resolvers.add(new MapMethodProcessor()); resolvers.add(new ErrorsMethodArgumentResolver()); resolvers.add(new SessionStatusMethodArgumentResolver()); resolvers.add(new UriComponentsBuilderMethodArgumentResolver()); // Custom arguments if (getCustomArgumentResolvers() != null) &#123; resolvers.addAll(getCustomArgumentResolvers()); &#125; // Catch-all resolvers.add(new RequestParamMethodArgumentResolver(getBeanFactory(), true)); resolvers.add(new ServletModelAttributeMethodProcessor(true)); return resolvers;&#125; getDefaultReturnValueHandlers 123456789101112131415161718192021222324252627282930313233343536373839404142// RequestMappingHandlerAdapterprivate List&lt;HandlerMethodReturnValueHandler&gt; getDefaultReturnValueHandlers() &#123; List&lt;HandlerMethodReturnValueHandler&gt; handlers = new ArrayList&lt;&gt;(20); // Single-purpose return value types handlers.add(new ModelAndViewMethodReturnValueHandler()); handlers.add(new ModelMethodProcessor()); handlers.add(new ViewMethodReturnValueHandler()); handlers.add(new ResponseBodyEmitterReturnValueHandler(getMessageConverters(), this.reactiveAdapterRegistry, this.taskExecutor, this.contentNegotiationManager)); handlers.add(new StreamingResponseBodyReturnValueHandler()); handlers.add(new HttpEntityMethodProcessor(getMessageConverters(), this.contentNegotiationManager, this.requestResponseBodyAdvice)); handlers.add(new HttpHeadersReturnValueHandler()); handlers.add(new CallableMethodReturnValueHandler()); handlers.add(new DeferredResultMethodReturnValueHandler()); handlers.add(new AsyncTaskMethodReturnValueHandler(this.beanFactory)); // Annotation-based return value types handlers.add(new ModelAttributeMethodProcessor(false)); handlers.add(new RequestResponseBodyMethodProcessor(getMessageConverters(), this.contentNegotiationManager, this.requestResponseBodyAdvice)); // Multi-purpose return value types handlers.add(new ViewNameMethodReturnValueHandler()); handlers.add(new MapMethodProcessor()); // Custom return value types if (getCustomReturnValueHandlers() != null) &#123; handlers.addAll(getCustomReturnValueHandlers()); &#125; // Catch-all if (!CollectionUtils.isEmpty(getModelAndViewResolvers())) &#123; handlers.add(new ModelAndViewResolverMethodReturnValueHandler(getModelAndViewResolvers())); &#125; else &#123; handlers.add(new ModelAttributeMethodProcessor(true)); &#125; return handlers;&#125; 可以看到，Spring提供了很多的默认的解析器，这些解析器都有一个共同的特征，就是都用一个boolean supportsReturnType方法。这里明显就是用了策略模式，根据情况选择某一个策略。 参数解析器简单介绍注解类型 注解的都是用在参数上的，除了ModelAttribute RequestResponseBodyMethodProcessor 1用来处理@RequestBody注解 RequestParamMethodArgumentResolver 1用来处理@RequestParam注解 RequestParamMapMethodArgumentResolver 1用来处理@RequestParam注解，并把结果以Map的形式返回 RequestPartMethodArgumentResolver 123456789101112用来处理@RequestPart注解。@RequestPart这个注解用在multipart/form-data表单提交请求的方法上。 而且支持同时接收文件和json参数，比如：@PostMapping(&quot;/updateGoods&quot;)@ResponseBodypublic void updateGoods( @RequestPart(&quot;goods&quot;) Goods good, @RequestPart(&quot;file&quot;) MultipartFile file) &#123; System.out.println(&quot;good&quot;+good); System.out.println(&quot;file&quot;+file);&#125;@RequestParam和@RequestPart的区别是：@RequestParam适用于name-valueString类型的请求域，@RequestPart适用于复杂的请求域（像JSON，XML） PathVariableMethodArgumentResolver 123456用于解析@PathVariable注解，获取 URI 中的单个路径变量@PostMapping(&quot;/test/&#123;userId&#125;&quot;)@ResponseBodypublic void updateGoods(@PathVariable(&quot;userId&quot;) Long userId) &#123;&#125; PathVariableMapMethodArgumentResolver 12345678用于解析@PathVariable注解，获取 URI 中的单个路径变量，并把结果保存到Map&lt;String, String&gt;中@PostMapping(&quot;/test/&#123;userId&#125;/&#123;taskId&#125;&quot;)@ResponseBodypublic void updateGoods(@PathVariable Map&lt;String, String&gt; map) &#123;&#125;注意，该只能是Map&lt;String, String&gt; MatrixVariableMethodArgumentResolver 123456789101112131415161718192021用来处理处理@MatrixVariable注解，也就是解析矩阵数据的根据 URI 规范 RFC 3986 中 URL 的定义，路径片段中可以可以包含键值对。规范中没对对应的术语。一般 “URL 路径参数” 可以被应用，尽管更加独特的 “矩阵 URI” 也经常被使用并且相当有名。在 Spring MVC 它被成为矩阵变量矩阵变量可以出现在任何路径片段中，每一个矩阵变量都用分号（;）隔开。比如 “/cars;color=red;year=2012”。多个值可以用逗号隔开，比如 “color=red,green,blue”，或者分开写 “color=red;color=green;color=blue”。如果你希望一个 URL 包含矩阵变量，那么请求映射模式必须用 URI 模板来表示这些矩阵变量。这样的话，不管矩阵变量顺序如何，都能够保证请求可以正确的匹配。开启@Configurationpublic class WebConfig implements WebMvcConfigurer &#123; @Override public void configurePathMatch(PathMatchConfigurer configurer) &#123; UrlPathHelper urlPathHelper=new UrlPathHelper(); urlPathHelper.setRemoveSemicolonContent(false); configurer.setUrlPathHelper(urlPathHelper); &#125;&#125;使用看这https://blog.csdn.net/weixin_43808717/article/details/118771500 MatrixVariableMapMethodArgumentResolver 同上，但参数类型为Map ServletModelAttributeMethodProcessor 1处理@ModelAttribute注解，没啥用 RequestHeaderMethodArgumentResolver 12345678处理@RequestHeader注解，获取request对象的header， 比如 @RequestMapping(&quot;/requestHeaderTest&quot;)public void requestHeaderTest(@RequestHeader(&quot;User-Agent&quot;) String userAgent, @RequestHeader(value=&quot;Accept&quot;) String []accepts) &#123; &#125; RequestHeaderMapMethodArgumentResolver 12345678作用同上，只是参数的类型必须为Map的实现类，比如@RequestMapping(&quot;/requestHeaderTest&quot;)public void requestHeaderTest(@RequestHeadeString HttpHeaders headers, @RequestHeader Map&lt;String, String&gt; headerMap) &#123; &#125;HttpHeaders是MultiValueMap&lt;String, String&gt;的子类 ServletCookieValueMethodArgumentResolver 1234处理@CookieValue注解，获取cookie的值，比如public void requestCookieTest(@CookieValue(&quot;JSESSIONID&quot;) String cookie) &#123;&#125; SessionAttributeMethodArgumentResolver 12345处理@SessionAttribute注解，获取session的值，比如public void requestCookieTest(@SessionAttribute(&quot;userId&quot;) String userId) &#123;&#125;等价于request.getAttribute(name, RequestAttributes.SCOPE_SESSION); RequestAttributeMethodArgumentResolver 1234处理@RequestAttribute注解，现在没啥用了， 等价于下面于request.getAttributerequest.setAttribute(&quot;name&quot;, &quot;baixue&quot;);request.getAttribute(&quot;name&quot;, &quot;baixue&quot;); ExpressionValueMethodArgumentResolver 1处理参数上的@Value注解，@Value的值需要符合spel表达式 spel表达式 基于类型 ServletRequestMethodArgumentResolver 看下该解析器的supportParameter方法 12345678910111213141516@Overridepublic boolean supportsParameter(MethodParameter parameter) &#123; Class&lt;?&gt; paramType = parameter.getParameterType(); return (WebRequest.class.isAssignableFrom(paramType) || ServletRequest.class.isAssignableFrom(paramType) || MultipartRequest.class.isAssignableFrom(paramType) || HttpSession.class.isAssignableFrom(paramType) || (pushBuilder != null &amp;&amp; pushBuilder.isAssignableFrom(paramType)) || Principal.class.isAssignableFrom(paramType) || InputStream.class.isAssignableFrom(paramType) || Reader.class.isAssignableFrom(paramType) || HttpMethod.class == paramType || Locale.class == paramType || TimeZone.class == paramType || ZoneId.class == paramType);&#125; 这个解析器就是解析这些参数的 WebRequest，实际返回的是NativeWebRequest类型的对象 ServletResponseMethodArgumentResolver看下该解析器的supportParameter方法 1234567@Overridepublic boolean supportsParameter(MethodParameter parameter) &#123; Class&lt;?&gt; paramType = parameter.getParameterType(); return (ServletResponse.class.isAssignableFrom(paramType) || OutputStream.class.isAssignableFrom(paramType) || Writer.class.isAssignableFrom(paramType));&#125; HttpEntityMethodProcessor 看下该解析器的supportParameter方法 12345@Overridepublic boolean supportsParameter(MethodParameter parameter) &#123; return (HttpEntity.class == parameter.getParameterType() || RequestEntity.class == parameter.getParameterType());&#125; RequestEntiry和HttpEntity为请求实体对象，内部封装了 请求行，请求头，请求体 。该对象可以用作 Controller 控制器中 处理方法的入参，SpringMVC会将请求解析成对象 12345@Override@GetMapping(&quot;/test&quot;)public String supportsParameter(RequestEntiry&lt;String&gt; entiry) &#123; return &quot;success&quot;&#125; RedirectAttributesMethodArgumentResolver 1234@Overridepublic boolean supportsParameter(MethodParameter parameter) &#123; return RedirectAttributes.class.isAssignableFrom(parameter.getParameterType());&#125; RedirectAttributes是用来接收重定向的属性的，所以RedirectAttributes只会用在重新的情况中 ModelMethodProcessor 123public boolean supportsParameter(MethodParameter parameter) &#123; return Model.class.isAssignableFrom(parameter.getParameterType());&#125; 使用Model意味着就是后端渲染页面了，这已经过时了。 MapMethodProcessor 12345@Overridepublic boolean supportsParameter(MethodParameter parameter) &#123; return Map.class.isAssignableFrom(parameter.getParameterType()) &amp;&amp; parameter.getParameterAnnotations().length == 0;&#125; 用Map接收 UriComponentsBuilderMethodArgumentResolver 12345@Overridepublic boolean supportsParameter(MethodParameter parameter) &#123; Class&lt;?&gt; type = parameter.getParameterType(); return (UriComponentsBuilder.class == type || ServletUriComponentsBuilder.class == type);&#125; spring mvc提供了一种机制，可以构造和编码URI，这这些功能就是由UriComponentsBuilder或ServletUriComponentsBuilder完成，其中UriComponentsBuilder是ServletUriComponentsBuilder的父类。 这两个类可以作为工具类使用，也可以作为一个请求方法的入参 第一种情况的话可以看这UriComponentsBuilder和UriComponents 而作为入参的话，会返回一个UriComponentsBuilder或ServletUriComponentsBuilder对象，里面包含了这个请求的url信息 自定义还自持自定义的参数解析器 项目中可以这样定义 1234567891011121314151617181920public class HandlerMethodArgumentResolverDemo implements HandlerMethodArgumentResolver &#123; @Override public boolean supportsParameter(MethodParameter parameter) &#123; return false; &#125; @Override public Object resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) throws Exception &#123; return null; &#125;&#125;@Configurationpublic class WebMvcConfigurerConfig implements WebMvcConfigurer &#123; @Override public void addArgumentResolvers(List&lt;HandlerMethodArgumentResolver&gt; resolvers) &#123; resolvers.add(new HandlerMethodArgumentResolverDemo()); &#125;&#125; 重要的结果解析器基于注解 RequestResponseBodyMethodProcessor 处理@ResponseBody 单一用途 ResponseBodyEmitterReturnValueHandler 看supportsReturnType方法 123456789@Overridepublic boolean supportsReturnType(MethodParameter returnType) &#123; Class&lt;?&gt; bodyType = ResponseEntity.class.isAssignableFrom(returnType.getParameterType()) ? ResolvableType.forMethodParameter(returnType).getGeneric().resolve() : returnType.getParameterType(); return (bodyType != null &amp;&amp; (ResponseBodyEmitter.class.isAssignableFrom(bodyType) || this.reactiveHandler.isReactiveType(bodyType)));&#125; 就是处理ResponseEntity类型返回值 ResponseEntity标识整个http相应：状态码、头部信息以及相应体内容。因此我们可以使用其对http响应实现完整配置。 支持请求异步处理的返回值解析器支持请求异步处理的返回值解析器 自定义返回值解析器123456789101112131415161718192021public class HandlerMethodReturnValueHandlerDemo implements HandlerMethodReturnValueHandler &#123; @Override public boolean supportsReturnType(MethodParameter returnType) &#123; return false; &#125; @Override public void handleReturnValue(Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception &#123; &#125;&#125;@Configurationpublic class WebMvcConfigurerConfig implements WebMvcConfigurer &#123; @Override public void addReturnValueHandlers(List&lt;HandlerMethodReturnValueHandler&gt; handlers) &#123; handlers.add(new HandlerMethodReturnValueHandlerDemo()); &#125;&#125; 处理请求源码回到源码RequestMappingHandlerAdapter.invokeHandlerMethod 12345678910111213141516171819202122// handlerMethod 为HandlerMethodServletInvocableHandlerMethod invocableMethod = new ServletInvocableHandlerMethod(handlerMethod);ModelAndViewContainer mavContainer = new ModelAndViewContainer();invocableMethod.invokeAndHandle(webRequest, mavContainer);// ServletInvocableHandlerMethodpublic void invokeAndHandle(ServletWebRequest webRequest, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; //具体调用逻辑，重点看 Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs); setResponseStatus(webRequest); ..... try &#123; //返回值处理 this.returnValueHandlers.handleReturnValue( returnValue, getReturnValueType(returnValue), mavContainer, webRequest); &#125; catch (Exception ex) &#123; ..... &#125;&#125; 上面代码就是请求的Controller的调用过程，先看invokeForRequest方法 1234567891011// ServletInvocableHandlerMethodpublic Object invokeForRequest(NativeWebRequest request, @Nullable ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; //获取参数数组,重点看 Object[] args = getMethodArgumentValues(request, mavContainer, providedArgs); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Arguments: &quot; + Arrays.toString(args)); &#125; return doInvoke(args);&#125; 这个方法有两个逻辑 通过参数解析器获取方法的参数值 方法调用 方法调用很简单，就是一个反射调用，现在重点看第一点，获取方法的参数值getMethodArgumentValues 123456789101112131415161718192021222324252627282930313233343536373839protected Object[] getMethodArgumentValues(NativeWebRequest request, @Nullable ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; //入参的包装类，里面包装了参数类型，参数名称，参数注解等等信息 MethodParameter[] parameters = getMethodParameters(); if (ObjectUtils.isEmpty(parameters)) &#123; return EMPTY_ARGS; &#125; Object[] args = new Object[parameters.length]; for (int i = 0; i &lt; parameters.length; i++) &#123; MethodParameter parameter = parameters[i]; //设置参数名称解析器 parameter.initParameterNameDiscovery(this.parameterNameDiscoverer); args[i] = findProvidedArgument(parameter, providedArgs); if (args[i] != null) &#123; continue; &#125; //典型的策略模式，根据parameter能否找到对应参数的处理类，能找到就返回true if (!this.resolvers.supportsParameter(parameter)) &#123; throw new IllegalStateException(formatArgumentError(parameter, &quot;No suitable resolver&quot;)); &#125; try &#123; //具体参数值解析过程,重点看看 args[i] = this.resolvers.resolveArgument(parameter, mavContainer, request, this.dataBinderFactory); &#125; catch (Exception ex) &#123; // Leave stack trace for later, exception may actually be resolved and handled... if (logger.isDebugEnabled()) &#123; String exMsg = ex.getMessage(); if (exMsg != null &amp;&amp; !exMsg.contains(parameter.getExecutable().toGenericString())) &#123; logger.debug(formatArgumentError(parameter, exMsg)); &#125; &#125; throw ex; &#125; &#125; return args;&#125; 这个方法的都有注释了，这里的难点就是参数的处理类和参数的解析过程，因为这些过程涉及到很多类，这些类在上边也截图了，在这里就看最常用的RequestResponseBodyMethodProcessor 解析JSON参数在Controller中这样定义了一个方法 12345@RequestMapping(&quot;/converter&quot;)@ResponseBodypublic ConsultConfigArea converter(@RequestBody ConsultConfigArea area) &#123; return area;&#125; 这样形式是最常用到的，在局部变量上加上@RequestBody就是把Request body中的json数据解析成对象ConsultConfigArea。完成这一工作的解析类是RequestResponseBodyMethodProcessor 先看supportsParameter方法： 1234@Overridepublic boolean supportsParameter(MethodParameter parameter) &#123; return parameter.hasParameterAnnotation(RequestBody.class);&#125; 就是判断参数上是否有@RequestBody注解 最后看resolveArgument方法 1234567891011121314151617181920212223@Overridepublic Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception &#123; parameter = parameter.nestedIfOptional(); Object arg = readWithMessageConverters(webRequest, parameter, parameter.getNestedGenericParameterType()); String name = Conventions.getVariableNameForParameter(parameter); if (binderFactory != null) &#123; WebDataBinder binder = binderFactory.createBinder(webRequest, arg, name); if (arg != null) &#123; validateIfApplicable(binder, parameter); if (binder.getBindingResult().hasErrors() &amp;&amp; isBindExceptionRequired(binder, parameter)) &#123; throw new MethodArgumentNotValidException(parameter, binder.getBindingResult()); &#125; &#125; if (mavContainer != null) &#123; mavContainer.addAttribute(BindingResult.MODEL_KEY_PREFIX + name, binder.getBindingResult()); &#125; &#125; return adaptArgumentIfNecessary(arg, parameter);&#125; 这个方法的核心方法是 1Object arg = readWithMessageConverters(webRequest, parameter, parameter.getNestedGenericParameterType()); 从这个方法从名字上看就用消息转换器（MessageConverters）这些转换器都实现了HttpMessageConverter接口，这个消息转换器就是在RequestMappingHandlerAdapter初始化参数解析器和结果解析器时，某些解析器通过getMessageConverters()获取到MessageConverters。 消息转换器——HttpMessageConverters 而对于参数解析器来说，用到MessageConverters的不多，就3个参数解析器用到，其中最常用的参数解析就时这节讲的RequestResponseBodyMethodProcessor解析器。而这个解析器是在启动过程设置进去的，回到SpringMVC的启动注解@EnableWebMvc。 123456789101112//WebMvcConfigurationSupportprotected final List&lt;HttpMessageConverter&lt;?&gt;&gt; getMessageConverters() &#123; if (this.messageConverters == null) &#123; this.messageConverters = new ArrayList&lt;&gt;(); configureMessageConverters(this.messageConverters); if (this.messageConverters.isEmpty()) &#123; addDefaultHttpMessageConverters(this.messageConverters); &#125; extendMessageConverters(this.messageConverters); &#125; return this.messageConverters;&#125; configureMessageConverters是一个钩子方法，他的实现类DelegatingWebMvcConfiguration，之前已经讲过了，这个类是通过 123456789101112131415private final WebMvcConfigurerComposite configurers = new WebMvcConfigurerComposite();@Autowired(required = false)public void setConfigurers(List&lt;WebMvcConfigurer&gt; configurers) &#123; if (!CollectionUtils.isEmpty(configurers)) &#123; this.configurers.addWebMvcConfigurers(configurers); &#125;&#125;@Overrideprotected void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; this.configurers.configureMessageConverters(converters);&#125; 这样的方式，提供了灵活的扩展。 addDefaultHttpMessageConverters这个方法是在我们没有添加自己的HttpMessageConverters时，Spring帮我们添加一些默认的HttpMessageConverters。(Spring Boot不同了，因为有个WebMvcAutoConfiguration的存在) extendMessageConverters也是一个扩展方法，它和configureMessageConverters一样，但不同的是这两个方法假如都实现了，执行完后HttpMessageConverters的顺序会不同，configureMessageConverters会在前面，extendMessageConverters的会在后面，因为messageConverters是一个ArrayList，有顺序的，而顺序是很重要的，下面在看RequestResponseBodyMethodProcessor的readWithMessageConverters中会看到。而且还有一点，假如重写了configureMessageConverters这个方法，那么就不会有默认的HttpMessageConverters了，而重写extendMessageConverters不会有这个问题。所以一般的情况下都是重写extendMessageConverters这个方法的。 回到源码，RequestResponseBodyMethodProcessor的readWithMessageConverters方法最重要的就是这个地方。可以看到，就是对messageConverters进行遍历，而且只要有一个符合条件并执行完了，那么for就结束了。 返回JSON结果上边看了参数解析，结果解析也很简单 1234567891011121314151617// ServletInvocableHandlerMethodpublic void invokeAndHandle(ServletWebRequest webRequest, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; //具体调用逻辑，重点看 Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs); setResponseStatus(webRequest); ..... try &#123; //返回值处理 this.returnValueHandlers.handleReturnValue( returnValue, getReturnValueType(returnValue), mavContainer, webRequest); &#125; catch (Exception ex) &#123; ..... &#125;&#125; 上边看了Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs); 这里看 12this.returnValueHandlers.handleReturnValue( returnValue, getReturnValueType(returnValue), mavContainer, webRequest); 跟踪代码，最后： 可以看到和参数解析的逻辑基本是一样的。 这里最好debug跟踪下代码，入口类和方法通过上边的已经知道了，debug下代码可以时得印象更加深刻","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"20-SpringMVC中请求是怎么到达Controller的","slug":"spring/20-SpringMVC中请求是怎么到达Controller的","date":"2021-11-25T12:00:29.000Z","updated":"2022-03-23T09:03:55.708Z","comments":true,"path":"blog/spring/20-SpringMVC中请求是怎么到达Controller的/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/20-SpringMVC%E4%B8%AD%E8%AF%B7%E6%B1%82%E6%98%AF%E6%80%8E%E4%B9%88%E5%88%B0%E8%BE%BEController%E7%9A%84/","excerpt":"","text":"注意：主要讲注解的形式 上节中讲了SpringMVC的启动流程，这节讲的就是主干流程，请求是怎么到达SpringMVC的Controller的。 SpringMVC中的Servlet是DispatcherServlet，SpringMVC中所有请求都会由DispatcherServlet来处理，而根据Servlet的规范，请求最先到达的方法是service(HttpServletRequest req, HttpServletResponse resp)。所以看DispatcherServlet的这个方法： 123456789101112// FrameworkServlet DispatcherServletprotected void service(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; HttpMethod httpMethod = HttpMethod.resolve(request.getMethod()); if (httpMethod == HttpMethod.PATCH || httpMethod == null) &#123; processRequest(request, response); &#125; else &#123; super.service(request, response); &#125;&#125; super.service(request, response)这里是Servlet规范的方法，根据请求的HttpMethod来选择对应的do方法，比如doGet、doPost等，比如现在有一个Get请求，那看doGet方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// FrameworkServletprotected final void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; processRequest(request, response);&#125;// FrameworkServletprotected final void processRequest(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; long startTime = System.currentTimeMillis(); Throwable failureCause = null; LocaleContext previousLocaleContext = LocaleContextHolder.getLocaleContext(); LocaleContext localeContext = buildLocaleContext(request); RequestAttributes previousAttributes = RequestContextHolder.getRequestAttributes(); ServletRequestAttributes requestAttributes = buildRequestAttributes(request, response, previousAttributes); WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); asyncManager.registerCallableInterceptor(FrameworkServlet.class.getName(), new RequestBindingInterceptor()); initContextHolders(request, localeContext, requestAttributes); try &#123; doService(request, response); &#125; catch (ServletException | IOException ex) &#123; failureCause = ex; throw ex; &#125; catch (Throwable ex) &#123; failureCause = ex; throw new NestedServletException(&quot;Request processing failed&quot;, ex); &#125; finally &#123; resetContextHolders(request, previousLocaleContext, previousAttributes); if (requestAttributes != null) &#123; requestAttributes.requestCompleted(); &#125; logResult(request, response, failureCause, asyncManager); publishRequestHandledEvent(request, response, startTime, failureCause); &#125;&#125; 这个方法最重要的就是调用了doService(request, response)，这是个钩子方法，在看这个方法前，先看调用之前做了什么： 看箭头的3行代码，这里3行的逻辑很简单，就是通过HttpServletRequest和HttpServletResponse创建ServletRequestAttributes对象，然后把对象放入到RequestContextHolder里面的一个ThreadLocal中。也就是说，在业务代码中可以通过 1234ServletRequestAttributes servletRequestAttributes = (ServletRequestAttributes)RequestContextHolder.getRequestAttributes();HttpServletRequest request = servletRequestAttributes.getRequest();HttpServletResponse response = servletRequestAttributes.getResponse(); 这样的方式来获取到Reqeust和Respon对象，回到源码，现在看doService(request, response)，该方法是由DispatcherServlet重写： 123456789101112131415161718192021222324252627282930313233343536373839404142434445//DispatcherServletprotected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; logRequest(request); // Keep a snapshot of the request attributes in case of an include, // to be able to restore the original attributes after the include. Map&lt;String, Object&gt; attributesSnapshot = null; if (WebUtils.isIncludeRequest(request)) &#123; attributesSnapshot = new HashMap&lt;&gt;(); Enumeration&lt;?&gt; attrNames = request.getAttributeNames(); while (attrNames.hasMoreElements()) &#123; String attrName = (String) attrNames.nextElement(); if (this.cleanupAfterInclude || attrName.startsWith(DEFAULT_STRATEGIES_PREFIX)) &#123; attributesSnapshot.put(attrName, request.getAttribute(attrName)); &#125; &#125; &#125; // Make framework objects available to handlers and view objects. request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext()); request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver); request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver); request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource()); if (this.flashMapManager != null) &#123; FlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response); if (inputFlashMap != null) &#123; request.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap)); &#125; request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap()); request.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager); &#125; try &#123; doDispatch(request, response); &#125; finally &#123; if (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; // Restore the original attribute snapshot, in case of an include. if (attributesSnapshot != null) &#123; restoreAttributesAfterInclude(request, attributesSnapshot); &#125; &#125; &#125;&#125; 这里就是往request中设置属性，我们看最核心的方法doDispatch(request, response) 有一个request属性很有意思，就是 1234567// DispatcherServlet// String WEB_APPLICATION_CONTEXT_ATTRIBUTE = DispatcherServlet.class.getName() + &quot;.CONTEXT&quot;;request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext());public final WebApplicationContext getWebApplicationContext() &#123; return this.webApplicationContext;&#125; 通过这样设置后，在项目中就能通过request.getAttribute(DispatcherServlet.WEB_APPLICATION_CONTEXT_ATTRIBUTE)这段代码获取到SpringMVC的上下，而该对象的的类型为AnnotationConfigWebApplicationContext。也就是说可以通过getParentBeanFactory方法到父BeanFactory，也就是Spring的上下文，该上下的类型也为AnnotationConfigWebApplicationContext。而且也能通过getServletContext()方法，获取到ServletContext，这样也就能做一些添加Servlet等操作，也可以通过ServletContext.getAttribute方法获取很多内置的对象。 DispatcherServlet.doDispatch这个是SpringMVC最核心的方法！这里包含了一个请求在SpringMVC中的完整的处理过程。看源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990// DispatcherServletprotected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; //异步管理器 WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; //文件上传 processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); //这个方法很重要，重点看 // Determine handler for the current request. mappedHandler = getHandler(processedRequest); if (mappedHandler == null) &#123; noHandlerFound(processedRequest, response); return; &#125; //获取跟HandlerMethod匹配的HandlerAdapter对象 // Determine handler adapter for the current request. HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. String method = request.getMethod(); boolean isGet = &quot;GET&quot;.equals(method); if (isGet || &quot;HEAD&quot;.equals(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; //前置过滤器，如果为false则直接返回 if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; //调用到Controller具体方法，核心方法调用，重点看看 // Actually invoke the handler. mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; applyDefaultViewName(processedRequest, mv); //中置过滤器 mappedHandler.applyPostHandle(processedRequest, response, mv); &#125; catch (Exception ex) &#123; dispatchException = ex; &#125; catch (Throwable err) &#123; // As of 4.3, we&#x27;re processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException(&quot;Handler dispatch failed&quot;, err); &#125; //视图渲染 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125; catch (Exception ex) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125; catch (Throwable err) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(&quot;Handler processing failed&quot;, err)); &#125; finally &#123; if (asyncManager.isConcurrentHandlingStarted()) &#123; // Instead of postHandle and afterCompletion if (mappedHandler != null) &#123; mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); &#125; &#125; else &#123; // Clean up any resources used by a multipart request. if (multipartRequestParsed) &#123; cleanupMultipart(processedRequest); &#125; &#125; &#125;&#125; 从上面贴出的注释可以看到，这个方法很重要，直接包含了一个请求的整个处理过程。 DispatcherServlet#getHandler——确定当前请求的处理程序（HandlerExecutionChain） 这里以注解形式形式的controller讲解。还有BeanNameUrlHandlerMapping和SimpleUrlHandlerMapping。下面的SpringMVC中定义Controller就讲了这些HandlerMapping对于的controller定义方式，顺序就按这里的顺序来，BeanNameUrlHandlerMapping和SimpleUrlHandlerMapping对应两种使用场景，具体看SpringMVC中定义Controller 前置知识：SpringMVC中定义Controller 所以先看这块代码： 12345678910// DispatcherServletHandlerExecutionChain mappedHandler = null;.....// 这个方法很重要，重点看// Determine handler for the current request.mappedHandler = getHandler(processedRequest);if (mappedHandler == null) &#123; noHandlerFound(processedRequest, response); return;&#125; 通过HandlerMapping获取Handler。看这块代码块的核心方法getHandler: 1234567891011121314// DispatcherServletprotected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123; //handlerMappering实例 if (this.handlerMappings != null) &#123; for (HandlerMapping mapping : this.handlerMappings) &#123; //获取HandlerMethod和过滤器链的包装类 HandlerExecutionChain handler = mapping.getHandler(request); if (handler != null) &#123; return handler; &#125; &#125; &#125; return null;&#125; 代码中handlerMappings在上一节已经说过了，它是在DispatcherServlet中的initHandlerMapper()方法中初始化的，触发过程就是当SpringMVC上下文初始化完毕后，会发布一个ContextRefreshEvent事件，而这个事件的监听器是ContextRefreshListener。 而initHandlerMappings方法只是在Spring的两个上下文中（父上下文和SpringMVC上下文）收集实现了HandlerMapping接口的类 而这些类是通过@EnableWebMVC注解@Import引入的DelegatingWebMvcConfiguration类@Bean进去到上下文的。@Bean的类有 RequestMappingHandlerMapping SimpleUrlHandlerMapping BeanNameUrlHandlerMapping RouterFunctionMapping beanName&#x3D;viewControllerHandlerMapping，类型为SimpleUrlHandlerMapping handlerMappings在SpringMVC的初始化完成后已经有值了，默认情况下有这些值： 上面这些类的作用就是根据request对象中的uri，去所有的HandlerMapping中的映射关系中查找对应的handler对象。而不同的HandlerMapping对象的作用都是一样的，就是建立uri和handler映射，然后通过uri获取handler。 看回源码看mapping.getHandler(request)。该方法会返回一个HandlerExecutionChain。HandlerExecutionChain就是一个Handler链，其中包含了拦截器和HandlerMethod。 看源码： 而这个AbstractHandlerMapping都是Spring中所有HandlerMapping的父类，这其实就是用了模版设计模式，其中整个请求getHandler也就是获取HandlerMethod的逻辑都定义在了AbstractHandlerMapping中，而其子类的作用就只是在这个这个基础了重写一些方法，用不同的逻辑进行URI和Handler的绑定。 AbstractHandlerMapping#getHandler——获取Handler的骨干方法1234567891011121314151617181920212223242526272829303132// AbstractHandlerMappingpublic final HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123; //根据请求的uri拿到对应的HandlerMethod对象 Object handler = getHandlerInternal(request); if (handler == null) &#123; handler = getDefaultHandler(); &#125; if (handler == null) &#123; return null; &#125; // Bean name or resolved handler? if (handler instanceof String) &#123; String handlerName = (String) handler; handler = obtainApplicationContext().getBean(handlerName); &#125; // 对拦截器链和HandlerMethod进行封装 HandlerExecutionChain executionChain = getHandlerExecutionChain(handler, request); //是否是跨域请求,就是查看request请求头中是否有Origin属性 if (hasCorsConfigurationSource(handler) || CorsUtils.isPreFlightRequest(request)) &#123; //自定义的钩子方法获取跨域配置 CorsConfiguration config = (this.corsConfigurationSource != null ? this.corsConfigurationSource.getCorsConfiguration(request) : null); //注解获取跨域配置 CorsConfiguration handlerConfig = getCorsConfiguration(handler, request); config = (config != null ? config.combine(handlerConfig) : handlerConfig); //这里添加了跨域的过滤器CorsInterceptor executionChain = getCorsHandlerExecutionChain(request, executionChain, config); &#125; return executionChain;&#125; 这个方法中，只有getHandlerInternal是钩子方法，其他的方法都没有被重写。 而这个方法的逻辑也很简答，就是先通过getHandlerInternal获取到handler，然后把handler和项目定义的HandlerInterceptor通过getHandlerExecutionChain方法，包装成&#96;&#96;HandlerExecutionChain&#96;对象，最后添加跨域拦截器。 下面看不同的HandlerMapping是如何处理请求的。 RequestMappingHandlerMapping——处理@Controller注解或者@RequestMapping注解 getHandlerInternal该类没有重写该方，所以看父类。 跟踪代码： 123456789101112131415161718//AbstractHandlerMethodMapping@Overrideprotected HandlerMethod getHandlerInternal(HttpServletRequest request) throws Exception &#123; //从request对象中获取uri，/common/query2 String lookupPath = getUrlPathHelper().getLookupPathForRequest(request); request.setAttribute(LOOKUP_PATH, lookupPath); // 获取读锁 this.mappingRegistry.acquireReadLock(); try &#123; //根据uri从映射关系中找到对应的HandlerMethod对象 HandlerMethod handlerMethod = lookupHandlerMethod(lookupPath, request); //把Controller类实例化 return (handlerMethod != null ? handlerMethod.createWithResolvedBean() : null); &#125; finally &#123; this.mappingRegistry.releaseReadLock(); &#125;&#125; 这里看第一行代码： 1String lookupPath = getUrlPathHelper().getLookupPathForRequest(request); 我通过浏览器访问了这个地址： 可以看到，这行代码的作用就是获取请求的uri，这个方法可以作为工具类在项目中使用，下面看下这个getUrlPathHelper： 1234public UrlPathHelper getUrlPathHelper() &#123; return this.urlPathHelper;&#125;UrlPathHelper urlPathHelper = new UrlPathHelper(); lookupHandlerMethod代码继续中，获取了读锁后，执行了这行代码： 1HandlerMethod handlerMethod = lookupHandlerMethod(lookupPath, request); 这行代码的作用就是根据uri从映射关系中找到对应的HandlerMethod对象，看源码： 1234567891011121314151617181920212223242526272829303132333435363738394041// AbstractHandlerMethodMappingprotected HandlerMethod lookupHandlerMethod(String lookupPath, HttpServletRequest request) throws Exception &#123; List&lt;Match&gt; matches = new ArrayList&lt;&gt;(); List&lt;T&gt; directPathMatches = this.mappingRegistry.getMappingsByUrl(lookupPath); if (directPathMatches != null) &#123; addMatchingMappings(directPathMatches, matches, request); &#125; if (matches.isEmpty()) &#123; // No choice but to go through all mappings... addMatchingMappings(this.mappingRegistry.getMappings().keySet(), matches, request); &#125; if (!matches.isEmpty()) &#123; Match bestMatch = matches.get(0); if (matches.size() &gt; 1) &#123; Comparator&lt;Match&gt; comparator = new MatchComparator(getMappingComparator(request)); matches.sort(comparator); bestMatch = matches.get(0); if (logger.isTraceEnabled()) &#123; logger.trace(matches.size() + &quot; matching mappings: &quot; + matches); &#125; if (CorsUtils.isPreFlightRequest(request)) &#123; return PREFLIGHT_AMBIGUOUS_MATCH; &#125; Match secondBestMatch = matches.get(1); if (comparator.compare(bestMatch, secondBestMatch) == 0) &#123; Method m1 = bestMatch.handlerMethod.getMethod(); Method m2 = secondBestMatch.handlerMethod.getMethod(); String uri = request.getRequestURI(); throw new IllegalStateException( &quot;Ambiguous handler methods mapped for &#x27;&quot; + uri + &quot;&#x27;: &#123;&quot; + m1 + &quot;, &quot; + m2 + &quot;&#125;&quot;); &#125; &#125; request.setAttribute(BEST_MATCHING_HANDLER_ATTRIBUTE, bestMatch.handlerMethod); handleMatch(bestMatch.mapping, lookupPath, request); return bestMatch.handlerMethod; &#125; else &#123; return handleNoMatch(this.mappingRegistry.getMappings().keySet(), lookupPath, request); &#125;&#125; 看开头的代码。 12345678910// AbstractHandlerMethodMappingList&lt;T&gt; directPathMatches = this.mappingRegistry.getMappingsByUrl(lookupPath);// AbstractHandlerMethodMappingprivate final MappingRegistry mappingRegistry = new MappingRegistry();// MappingRegistrypublic List&lt;T&gt; getMappingsByUrl(String urlPath) &#123; return this.urlLookup.get(urlPath);&#125; 这行代码就是根据请求的uri从urlLookup中取值，那问题来了，mappingRegistry.urlLookup这个值是什么时候初始化的？ url和方法的映射关系建立——RequestMappingHandlerMapping先看下urlLookup的定义: 12// MappingRegistryprivate final MultiValueMap&lt;String, T&gt; urlLookup = new LinkedMultiValueMap&lt;&gt;(); 可以看到，他就是一个map，不过这个map的值是一个List。现在的问题就是这个映射关系是什么时候建立的。或者这样说，在我们的项目中，我们都会写很多的Controller，比如： 而Controller中，在类上和方法上都有uri，这些uri肯定要拼凑起来的，接着通过拼凑起来的uri建立与方法的映射。上边建立的映射是： &#x2F;common&#x2F;index——commonController.index() &#x2F;common&#x2F;query1——commonController.query1() 好了，问题来了，就是这些映射是什么时候建立的。 现在看下RequestMappingHandlerMapping的类图： 可以发现，他实现了InitializingBean接口，也就是会执行afterPropertiesSet方法。 1234567891011121314151617// RequestMappingHandlerMappingpublic void afterPropertiesSet() &#123; this.config = new RequestMappingInfo.BuilderConfiguration(); this.config.setUrlPathHelper(getUrlPathHelper()); this.config.setPathMatcher(getPathMatcher()); this.config.setSuffixPatternMatch(useSuffixPatternMatch()); this.config.setTrailingSlashMatch(useTrailingSlashMatch()); this.config.setRegisteredSuffixPatternMatch(useRegisteredSuffixPatternMatch()); this.config.setContentNegotiationManager(getContentNegotiationManager()); super.afterPropertiesSet();&#125;// AbstractHandlerMethodMappingpublic void afterPropertiesSet() &#123; initHandlerMethods();&#125; 重点看initHandlerMethods方法。 1234567891011// AbstractHandlerMethodMappingprotected void initHandlerMethods() &#123; for (String beanName : getCandidateBeanNames()) &#123; // 这里把以targetSource.开去的beanName去掉，因为这开头的对象，是不参与依赖注入的，而且这种对象在Spring中没有实例的作用，有用的是该对象的代理对象 // 具体看 11-Spring中多例TargetSource if (!beanName.startsWith(SCOPED_TARGET_NAME_PREFIX)) &#123; processCandidateBean(beanName); &#125; &#125; handlerMethodsInitialized(getHandlerMethods());&#125; 第一行getCandidateBeanNames()就是获取上下文中所有的beanNames。 现在看processCandidateBean方法: 12345678910111213141516// AbstractHandleMethodMappingprotected void processCandidateBean(String beanName) &#123; Class&lt;?&gt; beanType = null; try &#123; beanType = obtainApplicationContext().getType(beanName); &#125; catch (Throwable ex) &#123; // An unresolvable bean type, probably from a lazy bean - let&#x27;s ignore it. ... &#125; //如果类上面有@Controller注解或者@RequestMapping注解 if (beanType != null &amp;&amp; isHandler(beanType)) &#123; //建立uri和method的映射关系 detectHandlerMethods(beanName); &#125;&#125; 这段代码的重点就是isHandler(beanType)方法，这个方法是钩子方法，在RequestMappingHandlerMapping中的实现为： 12345// RequestMappingHandlerMapping protected boolean isHandler(Class&lt;?&gt; beanType) &#123; return (AnnotatedElementUtils.hasAnnotation(beanType, Controller.class) || AnnotatedElementUtils.hasAnnotation(beanType, RequestMapping.class));&#125; 这个判断很简单，就是判断类上面是否有@Controller注解或者@RequestMapping注解，如果有，这个类就是要处理的类，就执行detectHandlerMethods方法。现在看detectHandlerMethods的源码： 123456789101112131415161718192021222324252627// AbstractHandlerMethodMappingprotected void detectHandlerMethods(Object handler) &#123; Class&lt;?&gt; handlerType = (handler instanceof String ? obtainApplicationContext().getType((String) handler) : handler.getClass()); if (handlerType != null) &#123; Class&lt;?&gt; userType = ClassUtils.getUserClass(handlerType); // 这个在Spring初始化的收集注解阶段就看过类似的 //获取方法对象和方法上面的@RequestMapping注解属性封装对象的映射关系 Map&lt;Method, T&gt; methods = MethodIntrospector.selectMethods(userType, (MethodIntrospector.MetadataLookup&lt;T&gt;) method -&gt; &#123; try &#123; return getMappingForMethod(method, userType); &#125; catch (Throwable ex) &#123; throw new IllegalStateException(&quot;Invalid mapping on handler class [&quot; + userType.getName() + &quot;]: &quot; + method, ex); &#125; &#125;); methods.forEach((method, mapping) -&gt; &#123; Method invocableMethod = AopUtils.selectInvocableMethod(method, userType); //建立uri和方法的各种映射关系，反正一条，根据uri要能够找到method对象 registerHandlerMethod(handler, invocableMethod, mapping); &#125;); &#125;&#125; 这段代码在Bean初始化时，在@Autowire等注解的收集上讲过了，作用就是遍历类上的方法。所以这段代码只需要看getMappingForMethod方法就好了。这个方法时钩子方法，在RequestMappingHandlerMapping中实现了，RequestMappingHandlerMapping上的实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344// RequestMappingHandlerMappingprotected RequestMappingInfo getMappingForMethod(Method method, Class&lt;?&gt; handlerType) &#123; //寻找有@RequestMapping注解的方法，然后注解里面的内容封装成对象 RequestMappingInfo info = createRequestMappingInfo(method); if (info != null) &#123; //类上面的@RequestMapping注解也封装成对象 RequestMappingInfo typeInfo = createRequestMappingInfo(handlerType); if (typeInfo != null) &#123; //把方法上面的注解属性结合到类上面的RequestMappingInfo对象中 info = typeInfo.combine(info); &#125; String prefix = getPathPrefix(handlerType); if (prefix != null) &#123; info = RequestMappingInfo.paths(prefix).options(this.config).build().combine(info); &#125; &#125; return info;&#125;// RequestMappingHandlerMappingprivate RequestMappingInfo createRequestMappingInfo(AnnotatedElement element) &#123; RequestMapping requestMapping = AnnotatedElementUtils.findMergedAnnotation(element, RequestMapping.class); RequestCondition&lt;?&gt; condition = (element instanceof Class ? getCustomTypeCondition((Class&lt;?&gt;) element) : getCustomMethodCondition((Method) element)); return (requestMapping != null ? createRequestMappingInfo(requestMapping, condition) : null);&#125;// RequestMappingHandlerMappingprotected RequestMappingInfo createRequestMappingInfo( RequestMapping requestMapping, @Nullable RequestCondition&lt;?&gt; customCondition) &#123; RequestMappingInfo.Builder builder = RequestMappingInfo .paths(resolveEmbeddedValuesInPatterns(requestMapping.path())) .methods(requestMapping.method()) .params(requestMapping.params()) .headers(requestMapping.headers()) .consumes(requestMapping.consumes()) .produces(requestMapping.produces()) .mappingName(requestMapping.name()); if (customCondition != null) &#123; builder.customCondition(customCondition); &#125; return builder.options(this.config).build();&#125; RequestMappingInfo的创建使用了建造者模式 这段代码的意思就是： 将方法上的这些信息封装成RequestMappingInfo对象，然后将： 类上的@RequestMapping注解封装成RequestMappingInfo对象，然后把方法上面的注解属性结合到类上面的RequestMappingInfo对象中，最后返回。 回到detectHandlerMethods的源码，把类上的方法都遍历完后，就建立了这样的映射关系&lt;Method, RequestMappingInfo&gt;。看运行时的情况： 继续看detectHandlerMethods这块代码： 现在有了这个对应关系&lt;Method, RequestMappingInfo&gt;，比如&lt;CommonController.index(), RequestMappingInfo(&#x2F;common&#x2F;index)&gt;，现在回到最开始的问题，就是MappingRegistry.urlLookup这个属性的值是怎么初始化话的？答案就在，上边代码块的registerHandlerMethod方法中，看这个方法的源码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// RequestMappingHandlerMapping@Overrideprotected void registerHandlerMethod(Object handler, Method method, RequestMappingInfo mapping) &#123; super.registerHandlerMethod(handler, method, mapping); updateConsumesCondition(mapping, method);&#125;// super.registerHandlerMethod(handler, method, mapping);// AbstractHandlerMethodMappingprotected void registerHandlerMethod(Object handler, Method method, T mapping) &#123; this.mappingRegistry.register(mapping, handler, method);&#125;// MappingRegistrypublic void register(T mapping, Object handler, Method method) &#123; // Assert that the handler method is not a suspending one. if (KotlinDetector.isKotlinType(method.getDeclaringClass())) &#123; Class&lt;?&gt;[] parameterTypes = method.getParameterTypes(); if ((parameterTypes.length &gt; 0) &amp;&amp; &quot;kotlin.coroutines.Continuation&quot;.equals(parameterTypes[parameterTypes.length - 1].getName())) &#123; throw new IllegalStateException(&quot;Unsupported suspending handler method detected: &quot; + method); &#125; &#125; // 获取写锁 this.readWriteLock.writeLock().lock(); try &#123; //创建HandlerMethod对象，其实 HandlerMethod handlerMethod = createHandlerMethod(handler, method); //检验是否唯一 validateMethodMapping(handlerMethod, mapping); //建立uri对象和handlerMethod的映射关系 this.mappingLookup.put(mapping, handlerMethod); List&lt;String&gt; directUrls = getDirectUrls(mapping); for (String url : directUrls) &#123; //建立url和RequestMappingInfo映射关系 this.urlLookup.add(url, mapping); &#125; String name = null; if (getNamingStrategy() != null) &#123; name = getNamingStrategy().getName(handlerMethod, mapping); addMappingName(name, handlerMethod); &#125; //判断method上是否有CrossOrigin注解，把注解里面的属性封装成CorsConfiguration，这个是做跨域访问控制的 CorsConfiguration corsConfig = initCorsConfiguration(handler, method, mapping); if (corsConfig != null) &#123; //建立映射关系 this.corsLookup.put(handlerMethod, corsConfig); &#125; this.registry.put(mapping, new MappingRegistration&lt;&gt;(mapping, handlerMethod, directUrls, name)); &#125; finally &#123; this.readWriteLock.writeLock().unlock(); &#125;&#125; 上边的MappingRegistry#register入参为 mapping：就是RequestMappingInfo handler：beanName，在运行时从上下文中获取对应的bean method：要执行的方法 好了，已经到建立映射的核心源码了，就是MappingRegistry的register！下面按步骤讲解 注意，MappingRegistry是AbstractHandlerMethodMapping的内部类 创建HandlerMethod对象 12345678910HandlerMethod handlerMethod = createHandlerMethod(handler, method);// AbstractHandlerMethodMappingprotected HandlerMethod createHandlerMethod(Object handler, Method method) &#123; if (handler instanceof String) &#123; return new HandlerMethod((String) handler, obtainApplicationContext().getAutowireCapableBeanFactory(), method); &#125; return new HandlerMethod(handler, method);&#125; HandlerMethod就是用来封装方法、方法的参数和执行方法的对象的，但在这里有可能会一种情况，就是传入的不是目标对象，而是目标对象的beanName的时候： 这时，这个HandlerMethod，就是用来封装beanName，beanFactory、方法和方法的参数的了。 通过validateMethodMapping(handlerMethod, mapping)方法来检查uri是否唯一 this.mappingLookup.put(mapping, handlerMethod);建立uri对象（RequestMappingInfo）和HandlerMethod的映射关系 这时已经有了&lt;RequestMappingInfo, HandlerMethod&gt;的对应关系了，但在上面的请求调用中，是通过uri从urlLookup中找某个对象的的（这里其实可以猜到，找的对象是RequestMappingInfo） 建立uri和HandlerMethod的映射 通过List&lt;String&gt; directUrls = getDirectUrls(mapping);获取到RequestMappingInfo上的uri，然后 1234for (String url : directUrls) &#123; //建立url和RequestMappingInfo映射关系 this.urlLookup.add(url, mapping);&#125; 通过this.urlLookup.add(url, mapping);建立了映射关系，而这个映射关系为 1&lt;uri, List&lt;RequestMappingInfo&gt; 这也就解答了开头的问题。 建立@CrossOrigin注解与HandlerMethod的映射 123456//判断method上是否有CrossOrigin注解，把注解里面的属性封装成CorsConfiguration，这个是做跨域访问控制的CorsConfiguration corsConfig = initCorsConfiguration(handler, method, mapping);if (corsConfig != null) &#123; //建立映射关系 this.corsLookup.put(handlerMethod, corsConfig);&#125; 上边的initCorsConfiguration在RequestMappingHandlerMapping重写，逻辑是检查类上和方法上的CrossOrigin注解，然后封装成CorsConfiguration对象，最后建立 1&lt;HandlerMethod, CorsConfiguration&gt; 这种映射，最后把这个映射保存到corsLookup中。 回到lookupHandlerMethodAbstractHandlerMethodMapping.lookupHandlerMethod List&lt;T&gt; directPathMatches = this.mappingRegistry.getMappingsByUrl(lookupPath);通过这行代码根据url就能找到匹配的RequestMappingInfo对象列表了，而之所以是一个List，是因为Controller中可以定义同一个url但是Method不同的，比如GET、POST。 继续看代码： 1234567891011121314// AbstractHandlerMethodMapping.lookupHandlerMethodif (directPathMatches != null) &#123; addMatchingMappings(directPathMatches, matches, request);&#125;private void addMatchingMappings(Collection&lt;T&gt; mappings, List&lt;Match&gt; matches, HttpServletRequest request) &#123; for (T mapping : mappings) &#123; //根据request对象来创建RequestMappingInfo对象 T match = getMatchingMapping(mapping, request); if (match != null) &#123; matches.add(new Match(match, this.mappingRegistry.getMappings().get(mapping))); &#125; &#125;&#125; getMatchingMapping方法被RequestMappingHandlerMapping重写了，源码为 1234// RequestMappingHandlerMappingprotected RequestMappingInfo getMatchingMapping(RequestMappingInfo info, HttpServletRequest request) &#123; return info.getMatchingCondition(request);&#125; 这段代码的意思就是根据请求去匹配RequestMappingInfo的，看getMatchingMapping的方法就一目了然了 1234567891011121314151617181920212223242526272829303132333435363738//RequestMappingInfoHandlerMappingprotected RequestMappingInfo getMatchingMapping(RequestMappingInfo info, HttpServletRequest request) &#123; return info.getMatchingCondition(request);&#125;public RequestMappingInfo getMatchingCondition(HttpServletRequest request) &#123; RequestMethodsRequestCondition methods = this.methodsCondition.getMatchingCondition(request); if (methods == null) &#123; return null; &#125; ParamsRequestCondition params = this.paramsCondition.getMatchingCondition(request); if (params == null) &#123; return null; &#125; HeadersRequestCondition headers = this.headersCondition.getMatchingCondition(request); if (headers == null) &#123; return null; &#125; ConsumesRequestCondition consumes = this.consumesCondition.getMatchingCondition(request); if (consumes == null) &#123; return null; &#125; ProducesRequestCondition produces = this.producesCondition.getMatchingCondition(request); if (produces == null) &#123; return null; &#125; PatternsRequestCondition patterns = this.patternsCondition.getMatchingCondition(request); if (patterns == null) &#123; return null; &#125; RequestConditionHolder custom = this.customConditionHolder.getMatchingCondition(request); if (custom == null) &#123; return null; &#125; return new RequestMappingInfo(this.name, patterns, methods, params, headers, consumes, produces, custom.getCondition());&#125; 可以看到，就算通过url匹配到了RequestMappingInfo，但这个请需要满足RequestMappingInfo设置的条件才能完成匹配。看下面的例子。 例如：我在Controller中定义了这个请求： 1234567891011121314@Controller@RequestMapping(&quot;/common&quot;)public class CommonController &#123; @RequestMapping(value = &quot;/getUser&quot;, method = RequestMethod.GET, params = &quot;username=jack&quot;, consumes = &quot;application/json&quot;, //输入参数的格式 produces = &quot;application/json&quot;, //返回参数的格式 headers = &quot;Referer=http://www.com.jack.controller.xx.com/&quot;) public @ResponseBody String getUser(HttpSession session, OutputStream outputStream) &#123; return &quot;com.jack.controller.xx&quot;; &#125;&#125; 也就是说只有在请求都符合上边的条件，这个方法才会被调用成功。不符合条件是： 好了，回到匹配代码，比如匹配成功了，执行这样代码 1matches.add(new Match(match, this.mappingRegistry.getMappings().get(mapping))); mappingRegistry.getMappings()这返回的是mappingLookup，经过上边的分析，这个属性保存的是&lt;RequestMappingInfo, HandlerMethod&gt;这个映射关系的，所以get返回的就是HandlerMethod，而match就是通过getMatchingMapping匹配成功后创建的新的RequestMappingInfo。所以matches里放的就是新的RequestMappingInfo和HandlerMethod，代码继续走（这里都考虑匹配到的情况）： 如果没有匹配到，就只能从所有的RequestMappingInfo中查找了 1234567891011121314151617181920212223242526// AbstractHandlerMethodMapping.lookupHandlerMethodif (!matches.isEmpty()) &#123; Match bestMatch = matches.get(0); if (matches.size() &gt; 1) &#123; Comparator&lt;Match&gt; comparator = new MatchComparator(getMappingComparator(request)); matches.sort(comparator); bestMatch = matches.get(0); if (logger.isTraceEnabled()) &#123; logger.trace(matches.size() + &quot; matching mappings: &quot; + matches); &#125; if (CorsUtils.isPreFlightRequest(request)) &#123; return PREFLIGHT_AMBIGUOUS_MATCH; &#125; Match secondBestMatch = matches.get(1); if (comparator.compare(bestMatch, secondBestMatch) == 0) &#123; Method m1 = bestMatch.handlerMethod.getMethod(); Method m2 = secondBestMatch.handlerMethod.getMethod(); String uri = request.getRequestURI(); throw new IllegalStateException( &quot;Ambiguous handler methods mapped for &#x27;&quot; + uri + &quot;&#x27;: &#123;&quot; + m1 + &quot;, &quot; + m2 + &quot;&#125;&quot;); &#125; &#125; request.setAttribute(BEST_MATCHING_HANDLER_ATTRIBUTE, bestMatch.handlerMethod); handleMatch(bestMatch.mapping, lookupPath, request); return bestMatch.handlerMethod;&#125; 这里代码就是如果匹配到多个，会重新排序后，取第一个为最佳匹配，然后用第一个和第二个比较，如果设置的条件都完全匹配，那么就报错了。获取到了最佳的匹配后往request中设置一些值然后就把匹配的HandlerMethod返回了。 找到最佳的匹配后，会把当前的HandlerMethod暴露到Request对象中。 12// HandlerMapping.BEST_MATCHING_HANDLER_ATTRIBUTErequest.setAttribute(BEST_MATCHING_HANDLER_ATTRIBUTE, bestMatch.handlerMethod); 这样设置后，就能在项目中获取到HandlerMethod了。 handleMatch方法在RequestMappingHandlerMapping重写了，该方法也是去暴露一些属性而已，其中我觉得有用的是 12// HandlerMapping.PATH_WITHIN_HANDLER_MAPPING_ATTRIBUTErequest.setAttribute(HandlerMapping.PATH_WITHIN_HANDLER_MAPPING_ATTRIBUTE, lookupPath); 这个暴露的是请求的地址。 lookupHandlerMethod方法最后就返回了匹配到的HandlerMethod（对象、方法）。 lookupHandlerMethod方法已经讲完了。 回到getHandlerInternal——通过BeanName获取Bean1234//根据uri从映射关系中找到对应的HandlerMethod对象HandlerMethod handlerMethod = lookupHandlerMethod(lookupPath, request);//把Controller类实例化return (handlerMethod != null ? handlerMethod.createWithResolvedBean() : null); 现在返回了HandlerMethod后就执行了handlerMethod.createWithResolvedBean方法。看源码: 12345678910public HandlerMethod createWithResolvedBean() &#123; Object handler = this.bean; if (this.bean instanceof String) &#123; Assert.state(this.beanFactory != null, &quot;Cannot resolve bean name without BeanFactory&quot;); String beanName = (String) this.bean; handler = this.beanFactory.getBean(beanName); &#125; // 这里之每次调用都需要从beanFactory中获取对象，是因为在SpringMVC中有新增了一些Scope（比如Session） return new HandlerMethod(this, handler);&#125; 这个方法其实挺简单的，最终结果都会返回一个新的HandlerMethod，而如果旧的HandlerMethod中的bean是字符串，就会从BeanFactory中获取到Bean。而我们使用注解，也就是使用RequestMappingHandlerMapping去匹配请求时，这个HandlerMethod的bean就是beanName。 SimpleUrlHandlerMapping——处理HttpRequestHandler接口的实现在SpringMVC中定义Controller中讲了HttpRequestHandler接口是如何定义Controller的，这里看下在Spring中是如何实现的 getHandlerInternal 看AbstractUrlHandlerMapping.getHandlerInternal 1234567891011121314151617181920212223242526272829// AbstractUrlHandlerMapping@Override@Nullableprotected Object getHandlerInternal(HttpServletRequest request) throws Exception &#123; String lookupPath = getUrlPathHelper().getLookupPathForRequest(request); request.setAttribute(LOOKUP_PATH, lookupPath); Object handler = lookupHandler(lookupPath, request); if (handler == null) &#123; // We need to care for the default handler directly, since we need to // expose the PATH_WITHIN_HANDLER_MAPPING_ATTRIBUTE for it as well. Object rawHandler = null; if (&quot;/&quot;.equals(lookupPath)) &#123; rawHandler = getRootHandler(); &#125; if (rawHandler == null) &#123; rawHandler = getDefaultHandler(); &#125; if (rawHandler != null) &#123; // Bean name or resolved handler? if (rawHandler instanceof String) &#123; String handlerName = (String) rawHandler; rawHandler = obtainApplicationContext().getBean(handlerName); &#125; validateHandler(rawHandler, request); handler = buildPathExposingHandler(rawHandler, lookupPath, lookupPath, null); &#125; &#125; return handler;&#125; 看lookupHandler 可以看到，这个方法其实就是从handleMap中拿handler，如果handler是String，也就是是beanName的话，就从beanFactory中拿。整个流程很简答， 这里有个问题，就是url什么时候和handler建立联系的 url与handler建立映射对项目中自定义的SimpleUrlHandlerMapping，比如： 123456789101112@Configurationpublic class SimpleUrlConfig &#123; @Bean public SimpleUrlHandlerMapping simpleUrlHandlerMapping() &#123; SimpleUrlHandlerMapping suhm = new SimpleUrlHandlerMapping(); Properties properties = new Properties(); properties.put(&quot;area/index&quot;,&quot;areaController&quot;); suhm.setMappings(properties); return suhm; &#125;&#125; 看setMappings 1234// SimpleUrlHandlerMappingpublic void setMappings(Properties mappings) &#123; CollectionUtils.mergePropertiesIntoMap(mappings, this.urlMap);&#125; 这个方法只是把路径和baenName保存到SimpleUrlHandlerMapping的urlMap集合中。现在看下SimpleUrlHandlerMapping的类图，该类实现了一个接口ApplicationContextAware接口，所以看setApplicationContext方法，该方法在ApplicationObjectSupport中实现了 这里调用initApplicationContext方法，跟踪代码： 1234567ApplicationObjectSupport.initApplicationContext// ApplicationObjectSupportprotected void initApplicationContext(ApplicationContext context) throws BeansException &#123; initApplicationContext();&#125; 方法initApplicationContext()被SimpleUrlHandlerMapping重写了 123456// SimpleUrlHandlerMapping@Overridepublic void initApplicationContext() throws BeansException &#123; super.initApplicationContext(); registerHandlers(this.urlMap);&#125; 方法registerHandlers就是把SimpleUrlHandlerMapping的urlMap的添加到AbstractUrlHandlerMapping的handlerMap中。 看回AbstractUrlHandlerMapping#lookupHandler方法 获取了handler后，应为handler有可能只是一个beanName，所以这里会有从获取bean的逻辑。最后会调用buildPathExposingHandler方法来创建一个HandlerExecutionChain对象。 12345678910protected Object buildPathExposingHandler(Object rawHandler, String bestMatchingPattern, String pathWithinMapping, @Nullable Map&lt;String, String&gt; uriTemplateVariables) &#123; HandlerExecutionChain chain = new HandlerExecutionChain(rawHandler); chain.addInterceptor(new PathExposingHandlerInterceptor(bestMatchingPattern, pathWithinMapping)); if (!CollectionUtils.isEmpty(uriTemplateVariables)) &#123; chain.addInterceptor(new UriTemplateVariablesHandlerInterceptor(uriTemplateVariables)); &#125; return chain;&#125; 回到AbstractHandlerMapping.getHandler通过返回的handler构建HandlerExecutionChain现在通过 1Object handler = getHandlerInternal(request); 已经获取到HandlerMethod了，代码继续走，走到 12345678910111213141516171819202122//获取HandlerMethod和过滤器链的包装类HandlerExecutionChain executionChain = getHandlerExecutionChain(handler, request);//AbstractHandlerMappingprotected HandlerExecutionChain getHandlerExecutionChain(Object handler, HttpServletRequest request) &#123; HandlerExecutionChain chain = (handler instanceof HandlerExecutionChain ? (HandlerExecutionChain) handler : new HandlerExecutionChain(handler)); String lookupPath = this.urlPathHelper.getLookupPathForRequest(request, LOOKUP_PATH); for (HandlerInterceptor interceptor : this.adaptedInterceptors) &#123; if (interceptor instanceof MappedInterceptor) &#123; MappedInterceptor mappedInterceptor = (MappedInterceptor) interceptor; if (mappedInterceptor.matches(lookupPath, this.pathMatcher)) &#123; chain.addInterceptor(mappedInterceptor.getInterceptor()); &#125; &#125; else &#123; chain.addInterceptor(interceptor); &#125; &#125; return chain;&#125; 这里就是把拦截器和HandlerMethod封装成一个HandlerExecutionChain对象，这个对象的作用就是使得方法调用变成了这样 1Interceptor.preHandle——&gt; HandlerMethod——&gt; Interceptor.postHandle 这段代码在SpringMVC拦截器 回到DispatcherServlet.doDispatch方法获取跟HandlerMethod匹配的HandlerAdapter对象通过 12345678// DispatcherServlet.doDispatch// 这个方法很重要，重点看// Determine handler for the current request.HandlerExecutionChain mappedHandler = getHandler(processedRequest);if (mappedHandler == null) &#123; noHandlerFound(processedRequest, response); return;&#125; 获取到了HandlerExecutionChain后，接着执行了： 123456789101112131415161718// DispatcherServlet.doDispatch// 获取跟HandlerMethod匹配的HandlerAdapter对象// Determine handler adapter for the current request.HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler());// DispatcherServletprotected HandlerAdapter getHandlerAdapter(Object handler) throws ServletException &#123; // 根据handlerMethod对象，找到合适的HandlerAdapter对象，这里用到了策略模式 if (this.handlerAdapters != null) &#123; for (HandlerAdapter adapter : this.handlerAdapters) &#123; if (adapter.supports(handler)) &#123; return adapter; &#125; &#125; &#125; // 抛出错误 ......&#125; 去获取HandlerAdapter，而HandlerAdapter是用来做参数解析、方法调用的和结果解析的。这里的handlerAdapters就是在SpringMvc的上下文初始化后，调用的 initHandlerAdapters(context);方法初始化的，而这个方法只是去BeanFactory拿对象而已，HandlerAdapter类型的对象的添加是在@EnableWebMvc注解引入的DelegatingWebMvcConfiguration对象通过@Bean添加的。这里添加的对象有 HandlerFunctionAdapter SimpleServletHandlerAdapter SimpleControllerHandlerAdapter RequestMappingHandlerAdapter HandlerFunctionAdapter 这里是RequestMappingHandlerAdapter 前置拦截器执行获取完HandlerAdapter后（这里是RequestMappingHandlerAdapter），就调用过滤器链的前置方法preHandle 1234567891011121314151617181920212223// DispatcherServlet.doDispatch// 前置过滤器，如果为false则直接返回if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return;&#125;// HandlerExecutionChainboolean applyPreHandle(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HandlerInterceptor[] interceptors = getInterceptors(); if (!ObjectUtils.isEmpty(interceptors)) &#123; for (int i = 0; i &lt; interceptors.length; i++) &#123; HandlerInterceptor interceptor = interceptors[i]; // 这里的handler是HandlerMethod if (!interceptor.preHandle(request, response, this.handler)) &#123; // 最后都会执行拦截器的afterCompletion方法 triggerAfterCompletion(request, response, null); return false; &#125; this.interceptorIndex = i; &#125; &#125; return true;&#125; 可以看到，只要其中一个前置过滤器返回false了就会执行拦截器的afterCompletion方法，而且调用结束了。 调用到具体Controller的具体方法1234// DispatcherServlet.doDispatch// 调用到Controller具体方法，核心方法调用，重点看看// Actually invoke the handler.mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); 这个是SpringMVC中一个核心的方法，在SpringMVC中Controller调用，现在只关注一个请求的处理过程 后置拦截器调用 注意，这个方法只有在目标方法没有报错时才会执行，所以如果要做一些资源释放的操作，不要在HandlerInterceptor#postHandle中做，而是要在 HandlerInterceptor#afterCompletion中完成 123456789101112131415// DispatcherServlet.doDispatch// 后置拦截器mappedHandler.applyPostHandle(processedRequest, response, mv);void applyPostHandle(HttpServletRequest request, HttpServletResponse response, @Nullable ModelAndView mv) throws Exception &#123; HandlerInterceptor[] interceptors = getInterceptors(); if (!ObjectUtils.isEmpty(interceptors)) &#123; for (int i = interceptors.length - 1; i &gt;= 0; i--) &#123; HandlerInterceptor interceptor = interceptors[i]; interceptor.postHandle(request, response, this.handler, mv); &#125; &#125;&#125; 最后就是结果的处理12// DispatcherServlet.doDispatchprocessDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException) 在这个方法中主要有3个作用 异常处理 视图渲染，响应视图 过滤器afterCompletion方法执行 12345//后置过滤器if (mappedHandler != null) &#123; // Exception (if any) is already handled.. mappedHandler.triggerAfterCompletion(request, response, null);&#125; 总结在Servlet启动时，Spring上文初始化完成，接着触发了DispatcherServlet的init方法来完成SpringMVC上下文的初始化，当SpringMVC上下文初始化时，通过@EnableWebMVC注解@Import的DelegatingWebMvcConfiguration对象@Bean进了HandlerMapping和HandlerAdapth；接着HandlerMapping对象初始化完成后，会触发afterPropertiesSet方法调用，完成MappingRegistry对象的初始化，也就是建立如下映射关系： 123&lt;uri, List&lt;RequestMappingInfo&gt;&gt;&lt;RequestMappingInfo, HandlerMethod&gt;..... 最后，SpringMVC上下文refresh方法完成后，会通过一个ContextRefreshedEvent事件来触发DispatcherServlet的onRefresh的方法来完成对HandlerMapping、HandlerAdapth实现类的收集； 当发起一个请求时，请求会进入到DispatcherServlet（默认情况下），最后通过doDispatch方法来处理这个请求： 根据不同的HandlerMapping对象，调用HandlerMapping的getHandler方法获取HandlerExecutionChain对象 根据Request对象获取uri 从MappingRegister的uriMapping映射中，通过uri获取到RequestMappingInfo列表。（根据注解匹配） 然后匹配最合适的RequestMappingInfo对象后，获取对应的HandlerMethod 根据匹配到的HandlerMethod和定义的拦截器，生成HandlerExecutionChain对象。 更具匹配到的HandlerMethod获取HandlerAdapter 拦截器链的前置方法执行——preHandle 通过HandlerAdapter来调用目标方法（SpringMVC中Controller调用） 拦截器链的后置方法执行——postHandle 如果第4步的处理出错了，这一步是不会执行的，所以一些资源释放的操作要放到拦截器的afterCompletion方法中 错误处理（可能需要） 响应视图（如果需要） 拦截器链的afterCompletion方法执行","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"20-SpringMVC中定义Controller","slug":"spring/20-SpringMVC中定义Controller","date":"2021-11-25T12:00:28.000Z","updated":"2022-03-23T09:03:55.622Z","comments":true,"path":"blog/spring/20-SpringMVC中定义Controller/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/20-SpringMVC%E4%B8%AD%E5%AE%9A%E4%B9%89Controller/","excerpt":"","text":"主流的定义Controller12345678910//@RestController@Controller@ResponseBody@RequestMapping(&quot;/common&quot;)public class CommonController &#123; @RequestMapping(&quot;/index&quot;) public void index() &#123; System.out.println(applicationContext.getBean(&quot;requestSessionBean&quot;)); &#125;&#125; 实现Controller接口1234567891011// 这种模式是需要相应视图的@Component(&quot;/order/index&quot;)public class BeanNameController extends AbstractController &#123; @Override protected ModelAndView handleRequestInternal(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; System.out.println(&quot;====/order/index&quot;); ModelAndView modelAndView = new ModelAndView(); modelAndView.setViewName(&quot;ok&quot;); return modelAndView; &#125;&#125; 实现HttpRequestHandler接口123456789101112131415161718192021222324252627282930313233// 不需要相应视图的用这种模式@Componentpublic class AreaController implements HttpRequestHandler, CorsConfigurationSource &#123; @Override public void handleRequest(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; System.out.println(&quot;======AreaController&quot;); PrintWriter writer = response.getWriter(); writer.println(&quot;&lt;h1&gt;==========Jack&lt;/h1&gt;&quot;); writer.flush(); writer.close(); &#125; // 定义跨域配置 @Override public CorsConfiguration getCorsConfiguration(HttpServletRequest request) &#123; return null; &#125;&#125;@Configurationpublic class SimpleUrlConfig &#123; @Bean public SimpleUrlHandlerMapping simpleUrlHandlerMapping() &#123; SimpleUrlHandlerMapping suhm = new SimpleUrlHandlerMapping(); Properties properties = new Properties(); properties.put(&quot;area/index&quot;,&quot;areaController&quot;); suhm.setMappings(properties); return suhm; &#125;&#125;","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"19-springMVC启动讲解","slug":"spring/19-springMVC启动讲解","date":"2021-11-25T12:00:27.000Z","updated":"2022-03-23T09:03:55.620Z","comments":true,"path":"blog/spring/19-springMVC启动讲解/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/19-springMVC%E5%90%AF%E5%8A%A8%E8%AE%B2%E8%A7%A3/","excerpt":"","text":"在上一节中讲了Spring是怎么和tomcat集合的和怎么利用servlet规范进行扩展，而且也讲了SpringMVC的入口方法这里讲tomcat在启动时，spring做了什么。这里作下总结。 在spring web中，是tomcat先启动，创建了ServletContext后通过java SPI加载并实例化了org.springframework.web.SpringServletContainerInitializer类型额对象， 接着执行了该对象的onStartup方法，而通过这个对象又引入了实现了WebApplicationInitializer接口的类，接着实例化后遍历执行了该接口的onStartup方法。通过这种方式可以很简单的实现功能的扩展。 在上一节中讲了SpringMVC的入口方法为WebAppInitializer的onStartup方法。跟踪下代码后入口方法在AbstractDispatcherServletInitializer类中 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class WebAppInitializer extends AbstractAnnotationConfigDispatcherServletInitializer &#123; //父容器 @Override protected Class&lt;?&gt;[] getRootConfigClasses() &#123; return new Class&lt;?&gt;[]&#123;SpringContainer.class&#125;; &#125; //SpringMVC配置子容器 @Override protected Class&lt;?&gt;[] getServletConfigClasses() &#123; return new Class&lt;?&gt;[]&#123;MvcContainer.class&#125;; &#125; //获取DispatcherServlet的映射信息 @Override protected String[] getServletMappings() &#123; return new String[]&#123;&quot;/&quot;&#125;; &#125; @Override protected Filter[] getServletFilters() &#123; MyFilter myFilter = new MyFilter(); CorsFilter corsFilter = new CorsFilter(); CharacterEncodingFilter characterEncodingFilter = new CharacterEncodingFilter(); characterEncodingFilter.setEncoding(&quot;UTF-8&quot;); characterEncodingFilter.setForceEncoding(true); return new Filter[]&#123;myFilter/*,corsFilter*/, characterEncodingFilter&#125;; &#125; @Override protected FilterRegistration.Dynamic registerServletFilter(ServletContext servletContext, Filter filter) &#123; return super.registerServletFilter(servletContext, filter); &#125;&#125;// AbstractDispatcherServletInitializer@Overridepublic void onStartup(ServletContext servletContext) throws ServletException &#123; //创建根上下文，创建servletListener super.onStartup(servletContext); //创建mvc上下文，注册DispatcherServlet registerDispatcherServlet(servletContext);&#125; 这个类会被SpringServletContainerInitializer实例化并执行，onStartup方法在AbstractDispatcherServletInitializer中，看该类的onStartup方法： 12345678910111213141516//AbstractDispatcherServletInitializer.onStartup@Overridepublic void onStartup(ServletContext servletContext) throws ServletException &#123; //创建根上下文，创建servletListener super.onStartup(servletContext); //创建mvc上下文，注册DispatcherServlet registerDispatcherServlet(servletContext);&#125;//super.onStartup(servletContext);//AbstractContextLoaderInitializer.onStartup//AbstractContextLoaderInitializer是AbstractDispatcherServletInitializer父类@Overridepublic void onStartup(ServletContext servletContext) throws ServletException &#123; registerContextLoaderListener(servletContext);&#125; 可以看到，onStratup方法有两个核心的方法方法，分别是： AbstractContextLoaderInitializer#registerContextLoaderListener(servletContext) AbstractDispatcherServletInitializer#registerDispatcherServlet(servletContext) 按顺序看，先看方法AbstractContextLoaderInitializer.registerContextLoaderListener(servletContext) registerContextLoaderListener(servletContext)——Spring容器的创建12345678910111213141516171819202122232425//AbstractContextLoaderInitializerprotected void registerContextLoaderListener(ServletContext servletContext) &#123; //创建spring上下文，注册了SpringContainer WebApplicationContext rootAppContext = createRootApplicationContext(); if (rootAppContext != null) &#123; //创建监听器 /* 形如这种配置 * &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;!--&lt;listener-class&gt;org.springframework.web.context.request.RequestContextListener&lt;/listener-class&gt;--&gt; &lt;/listener&gt; * * */ ContextLoaderListener listener = new ContextLoaderListener(rootAppContext); listener.setContextInitializers(getRootApplicationContextInitializers()); servletContext.addListener(listener); &#125; else &#123; logger.debug(&quot;No ContextLoaderListener registered, as &quot; + &quot;createRootApplicationContext() did not return an application context&quot;); &#125;&#125; Spring容器的创建可以看到，第一步就是创建一个上下文对象WebApplicationContext，而这个创建WebApplicationContext的方法源码如下： 12345678910111213//AbstractAnnotationConfigDispatcherServletInitializerprotected WebApplicationContext createRootApplicationContext() &#123; //钩子方法，调用子类的方法 Class&lt;?&gt;[] configClasses = getRootConfigClasses(); if (!ObjectUtils.isEmpty(configClasses)) &#123; AnnotationConfigWebApplicationContext context = new AnnotationConfigWebApplicationContext(); context.register(configClasses); return context; &#125; else &#123; return null; &#125;&#125; 可以看到，该方法中有一个钩子方法getRootConfigClasses()，这是我们自定义类重写的方法： 通过这个方法，引入配置类，然后创建了AnnotationConfigWebApplicationContext这个上下文对象，接着执行context.register(configClasses)，这个方法只是注册一个类而已，这个类在Spring容器启动时会处理。 注意，这时这个上下对象还没有启动的，也就是还没执行ApplicationContext.refresh()方法 ServletContextListener的创建和添加创建完Spring容器的创建后会，把刚刚创建的AnnotationConfigWebApplicationContext作为入参创建ContextLoaderListener对象。 可以看到该类实现ServletContextListener接口。而且这里也有一个扩展方法getRootApplicationContextInitializers，这个是扩展方法，如果有需要可以重写，作用在后面讲。最后后执行servletContext.addListener(listener)。 1234// AbstractContextLoaderInitializerContextLoaderListener listener = new ContextLoaderListener(rootAppContext);listener.setContextInitializers(getRootApplicationContextInitializers());servletContext.addListener(listener); Spring容器的启动现在看回ContextLoaderListener类，它实现了ServletContextListener接口，在servlet的规范中，在servlet的启动时会调用ServletContextListener接口的contextInitialized方法，Web应用关闭时会调用contextDestroyed方法。所以下现在看ContextLoaderListener的contextInitialized方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// ContextLoaderListenerpublic void contextInitialized(ServletContextEvent event) &#123; //在这里初始化了spring容器 initWebApplicationContext(event.getServletContext());&#125;// ContextLoaderpublic WebApplicationContext initWebApplicationContext(ServletContext servletContext) &#123; ..... long startTime = System.currentTimeMillis(); try &#123; .... if (this.context instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) this.context; if (!cwac.isActive()) &#123; if (cwac.getParent() == null) &#123; ApplicationContext parent = loadParentContext(servletContext); cwac.setParent(parent); &#125; //启动spring上下文 configureAndRefreshWebApplicationContext(cwac, servletContext); &#125; &#125; //把spring的上下文对象设置到servlet上下文对象中去了 servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); ClassLoader ccl = Thread.currentThread().getContextClassLoader(); if (ccl == ContextLoader.class.getClassLoader()) &#123; currentContext = this.context; &#125; else if (ccl != null) &#123; currentContextPerThread.put(ccl, this.context); &#125; .... return this.context; &#125; catch (RuntimeException | Error ex) &#123; ...... &#125;&#125;// ContextLoaderprotected void configureAndRefreshWebApplicationContext(ConfigurableWebApplicationContext wac, ServletContext sc) &#123; ..... wac.setServletContext(sc); String configLocationParam = sc.getInitParameter(CONFIG_LOCATION_PARAM); if (configLocationParam != null) &#123; wac.setConfigLocation(configLocationParam); &#125; ..... ConfigurableEnvironment env = wac.getEnvironment(); if (env instanceof ConfigurableWebEnvironment) &#123; ((ConfigurableWebEnvironment) env).initPropertySources(sc, null); &#125; customizeContext(sc, wac); wac.refresh();&#125; 从上边的源码可以看到，它最重要的只完成两件事 执行Spring上下文对象的refresh()方法来启动上下文。 把spring的上下文对象设置到servlet属性中 123String ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE = WebApplicationContext.class.getName() + &quot;.ROOT&quot;;// context =AnnotationConfigWebApplicationContextservletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); 好了，这个上下文对象只是用来启动了Spring容器（初始化看这），SpringMVC容器还需要看下一个方法。 Spring的上下文启动前提提供了扩展——ApplicationContextInitializer看回启动spring容器启动的方法： 12345678910111213141516171819protected void customizeContext(ServletContext sc, ConfigurableWebApplicationContext wac) &#123; // 通过Servlet启动参数引入`ApplicationContextInitializer，这个我们不会用，我们使用最下边的 List&lt;Class&lt;ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt;&gt;&gt; initializerClasses = determineContextInitializerClasses(sc); for (Class&lt;ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt;&gt; initializerClass : initializerClasses) &#123; Class&lt;?&gt; initializerContextClass = GenericTypeResolver.resolveTypeArgument(initializerClass, ApplicationContextInitializer.class); if (initializerContextClass != null &amp;&amp; !initializerContextClass.isInstance(wac)) &#123; throw new ApplicationContextException(。。。。。。); &#125; this.contextInitializers.add(BeanUtils.instantiateClass(initializerClass)); &#125; AnnotationAwareOrderComparator.sort(this.contextInitializers); for (ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt; initializer : this.contextInitializers) &#123; initializer.initialize(wac); &#125;&#125; 最核心的就看最下面的代码 这里的contextInitializers参数就是ApplicationContextInitializer列表。而这个列表的值在Spring中为我们提供了一个扩展方法，在ContextLoaderListener的创建时会调用AbstractContextLoaderInitializer#getRootApplicationContextInitializers()的方法，这个方法默认情况下返回null，但这个方法能被子类重写。 而这个接口的作用其实也很明显，就是对ApplicationContext做一些定制处理，比如这样 12345678910111213141516171819public class WebAppInitializer extends AbstractAnnotationConfigDispatcherServletInitializer &#123; .... @Override protected ApplicationContextInitializer&lt;?&gt;[] getRootApplicationContextInitializers() &#123; return new ApplicationContextInitializer[]&#123;new ApplicationContextInitializerDemo()&#125;; &#125;&#125;public class ApplicationContextInitializerDemo implements ApplicationContextInitializer &#123; @Override public void initialize(ConfigurableApplicationContext applicationContext) &#123;// applicationContext.addApplicationListener();// applicationContext.registerShutdownHook();// applicationContext.addProtocolResolver(); &#125;&#125; 但有一点需要注意，这个接口的initialize方法由于是在AbstractApplicationContext.refresh方法执行前执行的，所以该ApplicationContext有些属性还没有初始化，所以在initialize方法中，applicationContext的有些方法不是能调用的。除非在initialize中调用refresh()， 多次调用refresh()是允许的，Spring对这做了保证。 registerDispatcherServlet(servletContext)——SpringMVC容器的创建12345678910111213141516171819202122232425262728293031323334353637// AbstractDispatcherServletInitializerprotected void registerDispatcherServlet(ServletContext servletContext) &#123; String servletName = getServletName(); Assert.hasLength(servletName, &quot;getServletName() must not return null or empty&quot;); //创建mvc的上下文 WebApplicationContext servletAppContext = createServletApplicationContext(); Assert.notNull(servletAppContext, &quot;createServletApplicationContext() must not return null&quot;); //创建DispatcherServlet对象，把springmvc上下文设置到DispatcherServlet中 FrameworkServlet dispatcherServlet = createDispatcherServlet(servletAppContext); Assert.notNull(dispatcherServlet, &quot;createDispatcherServlet(WebApplicationContext) must not return null&quot;); dispatcherServlet.setContextInitializers(getServletApplicationContextInitializers()); //把DispatcherServlet丢到servlet上下文中 ServletRegistration.Dynamic registration = servletContext.addServlet(servletName, dispatcherServlet); if (registration == null) &#123; throw new IllegalStateException(&quot;Failed to register servlet with name &#x27;&quot; + servletName + &quot;&#x27;. &quot; + &quot;Check if there is another servlet registered under the same name.&quot;); &#125; registration.setLoadOnStartup(1); //钩子方法 registration.addMapping(getServletMappings()); registration.setAsyncSupported(isAsyncSupported()); //定义拦截器 Filter[] filters = getServletFilters(); if (!ObjectUtils.isEmpty(filters)) &#123; for (Filter filter : filters) &#123; registerServletFilter(servletContext, filter); &#125; &#125; customizeRegistration(registration);&#125; SpringMVC的上下文对象创建还是先通过createServletApplicationContext创建了Spring的上下文对象——AnnotationConfigWebApplicationContext. 1234567891011WebApplicationContext servletAppContext = createServletApplicationContext();// AbstractAnnotationConfigDispatcherServletInitializerprotected WebApplicationContext createServletApplicationContext() &#123; AnnotationConfigWebApplicationContext context = new AnnotationConfigWebApplicationContext(); Class&lt;?&gt;[] configClasses = getServletConfigClasses(); if (!ObjectUtils.isEmpty(configClasses)) &#123; context.register(configClasses); &#125; return context;&#125; getServletConfigClasses这个方法是钩子方法，是交给子类实现的，这是我们自定义的类实现的： 注意，这里只是创建了AnnotationConfigWebApplicationContext还没有执行refresh方法。 DispatcherServlet类的创建接着执行，去创建了一个DispatcherServlet类，这个类是真正的符合Servlet规范的Servlet类，这也是SpringMVC请求过程中的核心类 123456FrameworkServlet dispatcherServlet = createDispatcherServlet(servletAppContext);// AbstractDispatcherServletInitializerprotected FrameworkServlet createDispatcherServlet(WebApplicationContext servletAppContext) &#123; return new DispatcherServlet(servletAppContext);&#125; 注意，DispatcherServlet包含了SpringMVC的Spring上下文对象。 这里的createDispatcherServlet方法也可以被子类重写，这样可以扩展下DispatcherServlet。 ApplicationContextInitializer类添加到DispatcherServlet中然后添加ApplicationContextInitializer类 1dispatcherServlet.setContextInitializers(getServletApplicationContextInitializers()); 这个方法只是往DispatchServlet的contextInitializers列表添加值。这个列表的是用了Spring的事件监听器的。 把DispatcherServlet添加到servlet上下文中接着把DispatcherServlet丢到servlet上下文中 12345678910String servletName = getServletName();//AbstractDispatcherServletInitializerprotected String getServletName() &#123; return DEFAULT_SERVLET_NAME;&#125;public static final String DEFAULT_SERVLET_NAME = &quot;dispatcher&quot;;ServletRegistration.Dynamic registration = servletContext.addServlet(servletName, dispatcherServlet); 为DispatcherServle指定路径接着为DispatcherServlet添加拦截的url 12// registration = DispatchServletregistration.addMapping(getServletMappings()); 这个getServletMappings()方法是钩子方法，是子类实现的，这是我自定义的类实现的： 也就是后面的所有路径都会进入到该DispatchServlet中。比如这样写后： 路径为&#x2F;xyz&#x2F;**这些路径会走到该DispatchServlet中。 接着设置DispatchServlet是否支持异步12// registration = DispatchServletregistration.setAsyncSupported(isAsyncSupported()); isAsyncSupported方法默认会返回true 1234// AbstractDispatcherServletInitializerprotected boolean isAsyncSupported() &#123; return true;&#125; 添加自定义过滤器最后是添加自定义的过滤器 12345678// AbstractDispatcherServletInitializer// registration = DispatchServletFilter[] filters = getServletFilters();if (!ObjectUtils.isEmpty(filters)) &#123; for (Filter filter : filters) &#123; registerServletFilter(servletContext, filter); &#125;&#125; getServletFilters()也是一个钩子方法，这是自定义类重写的方法： 1234567891011121314151617181920212223242526272829@Overrideprotected Filter[] getServletFilters() &#123; MyFilter myFilter = new MyFilter(); CorsFilter corsFilter = new CorsFilter(); CharacterEncodingFilter characterEncodingFilter = new CharacterEncodingFilter(); characterEncodingFilter.setEncoding(&quot;UTF-8&quot;); characterEncodingFilter.setForceEncoding(true); return new Filter[]&#123;myFilter/*,corsFilter*/, characterEncodingFilter&#125;;&#125;public class MyFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(&quot;======&quot;); &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;======doFilter&quot;); //放行 chain.doFilter(request, response); &#125; @Override public void destroy() &#123; &#125;&#125; 在方法的最后，spring为我们提供了一个扩展方法customizeRegistration(ServletRegistration.Dynamic)，这个方法是用来扩展使用的。 最后对DIspatchServlet做定制Spring为我们提供了一个扩展发方法customizeRegistration，它的默认实现为空方法。这个我们可以通过重写该方法做一些定制操作。 12// registration = DispatchServletcustomizeRegistration(registration); SpringMVC上下文启动（SpringMVC上下问启动）DispatcherServlet的初始化已经看了，但上边创建的SpringMVC上下文是在什么时候启动的？ 在Servlet的规范中Servlet的启动都会执行init()方法，而DispatcherServlet就是一个Servlet，而且在创建的时候会传入一个SpringMVC的上下对象。 看DispatcherServlet的init方法前，先看下类图： DispatcherServlet的init方法是在HttpServletBean中的： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// HttpServletBeanpublic final void init() throws ServletException &#123; ..... //钩子方法 initServletBean();&#125;// FrameworkServletprotected final void initServletBean() throws ServletException &#123; ..... try &#123; //初始化springmvc上下文 this.webApplicationContext = initWebApplicationContext(); initFrameworkServlet(); &#125; ......&#125;// FrameworkServletprotected WebApplicationContext initWebApplicationContext() &#123; //从servlet下午文中获取spring的上下文对象 WebApplicationContext rootContext = WebApplicationContextUtils.getWebApplicationContext(getServletContext()); WebApplicationContext wac = null; if (this.webApplicationContext != null) &#123; // A context instance was injected at construction time -&gt; use it wac = this.webApplicationContext; if (wac instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) wac; if (!cwac.isActive()) &#123; if (cwac.getParent() == null) &#123; //把spring的上下文设置成springmvc上下文的父 cwac.setParent(rootContext); &#125; //启动容器 configureAndRefreshWebApplicationContext(cwac); &#125; &#125; &#125; if (wac == null) &#123; wac = findWebApplicationContext(); &#125; if (wac == null) &#123; // No context instance is defined for this servlet -&gt; create a local one wac = createWebApplicationContext(rootContext); &#125; if (!this.refreshEventReceived) &#123; synchronized (this.onRefreshMonitor) &#123; onRefresh(wac); &#125; &#125; if (this.publishContext) &#123; // Publish the context as a servlet context attribute. String attrName = getServletContextAttributeName(); getServletContext().setAttribute(attrName, wac); &#125; return wac;&#125; 好了，看FrameworkServlet#initWebApplicationContext的第一行代码 123456789WebApplicationContext rootContext = WebApplicationContextUtils.getWebApplicationContext(getServletContext());....ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) wac;cwac.setParent(rootContext);public static WebApplicationContext getWebApplicationContext(ServletContext sc) &#123; return getWebApplicationContext(sc, WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE);&#125; 在第一个Spring上下文创建后，会把该Spring上下文作为值，以WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE为key，添加到Servlet的上下问的属性中。所以在这段代码中，最后会从Servlet的上下中获取第一次创建的Spring上下，然后作为SpringMVC上下文的父上下文。 而SpringMVC上下文启动也很简单，就是调用SpringMVC上下文的refresh()方法而已。看源码： refresh()前有个applyInitializers方法，这个方法和父Spring上下文启动前执行的customizeContext一样，都是用来执行加入到上下文的 ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt; ，而这里是使用getServletApplicationContextInitializers方法来初始化的，这也是能被子类重写的。 这里除了refresh()外，还有一行代码需要关注，就是： 12// 添加ApplicationListener接口wac.addApplicationListener(new SourceFilteringListener(wac, new ContextRefreshListener())); 往上下文中添加了一个事件SourceFilteringListener，注意SourceFilteringListener从设计上看只是一个代理对象，被代理对象为ContextRefreshListener。 123456789101112131415161718192021222324252627// SourceFilteringListenerpublic SourceFilteringListener(Object source, ApplicationListener&lt;?&gt; delegate) &#123; this.source = source; this.delegate = (delegate instanceof GenericApplicationListener ? (GenericApplicationListener) delegate : new GenericApplicationListenerAdapter(delegate));&#125;@Overridepublic void onApplicationEvent(ApplicationEvent event) &#123; if (event.getSource() == this.source) &#123; if (this.delegate == null) &#123; throw new IllegalStateException( &quot;Must specify a delegate object or override the onApplicationEventInternal method&quot;); &#125; this.delegate.onApplicationEvent(event); &#125;&#125;// ContextRefreshListenerprivate class ContextRefreshListener implements ApplicationListener&lt;ContextRefreshedEvent&gt; &#123; @Override public void onApplicationEvent(ContextRefreshedEvent event) &#123; //容器启动完成后触发 FrameworkServlet.this.onApplicationEvent(event); &#125;&#125; 事件监听的详情看这里[registerListeners()](.&#x2F;Spring Event事件通知机制) 注意，这个类实现了ApplicationListener，也就是是一个事件监听器，而监听的事件类型为ContextRefreshedEvent，这个事件是在Spring上下文启动完毕后发布的事件： FrameworkServlet.this.onApplicationEvent(event)方法当SpringMVC的上下问refresh方法执行完成后，就会发布一个ContextRefreshedEvent事件，那么此时就会触发该方法。 跟踪代码： 12345678910111213141516171819202122232425// FrameworkServletpublic void onApplicationEvent(ContextRefreshedEvent event) &#123; this.refreshEventReceived = true; synchronized (this.onRefreshMonitor) &#123; onRefresh(event.getApplicationContext()); &#125;&#125;// DispatcherServletprotected void onRefresh(ApplicationContext context) &#123; initStrategies(context);&#125;// DispatcherServletprotected void initStrategies(ApplicationContext context) &#123; initMultipartResolver(context); initLocaleResolver(context); initThemeResolver(context); initHandlerMappings(context); initHandlerAdapters(context); initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); initViewResolvers(context); initFlashMapManager(context);&#125; 这里有很多初始化方法，重点的只有4个： initHandlerMappings(context); HandlerMapping，是做controller的映射的 initHandlerAdapters(context); HandlerAdapter是负责去调用具体的controller以及完成参数解析和返回值参数解析 initViewResolvers(context); ViewResolver是负责视图解析、响应视图的 initRequestToViewNameTranslator(context) 把请求转化为视图名 initHandlerMappings——@EnableWebMvc讲解先看其中一个源码 123456789101112131415161718192021222324252627282930private void initHandlerMappings(ApplicationContext context) &#123; this.handlerMappings = null; if (this.detectAllHandlerMappings) &#123; // Find all HandlerMappings in the ApplicationContext, including ancestor contexts. Map&lt;String, HandlerMapping&gt; matchingBeans = BeanFactoryUtils.beansOfTypeIncludingAncestors(context, HandlerMapping.class, true, false); if (!matchingBeans.isEmpty()) &#123; this.handlerMappings = new ArrayList&lt;&gt;(matchingBeans.values()); // We keep HandlerMappings in sorted order. AnnotationAwareOrderComparator.sort(this.handlerMappings); &#125; &#125; else &#123; try &#123; HandlerMapping hm = context.getBean(HANDLER_MAPPING_BEAN_NAME, HandlerMapping.class); this.handlerMappings = Collections.singletonList(hm); &#125; catch (NoSuchBeanDefinitionException ex) &#123; // Ignore, we&#x27;ll add a default HandlerMapping later. &#125; &#125; // Ensure we have at least one HandlerMapping, by registering // a default HandlerMapping if no other mappings are found. if (this.handlerMappings == null) &#123; this.handlerMappings = getDefaultStrategies(context, HandlerMapping.class); .... &#125;&#125; 代码很简单，就是从上下文中拿实现了HandlerMapping接口的类，但问题是在springMVC使用和扩展中，并没有配置HandlerMapping类，那么这里的这行代码会获取为空吗？ 12Map&lt;String, HandlerMapping&gt; matchingBeans = BeanFactoryUtils.beansOfTypeIncludingAncestors(context, HandlerMapping.class, true, false); 结果是并不为空，而是有值的，那么这些是从哪里引入到Spring的容器中的？ 要使用SpringMVC就需要加上这个注解@EnableWebMvc，看下这个注解的定义： 它Import了DelegatingWebMvcConfiguration，先看类图就： DelegatingWebMvcConfiguration并不负责@Bean，这个工作它的父类WebMvcConfigurationSupport完成的。 不过这里有个问题，就是通过这样帮我们默认配置了，但我们怎么改默认的配置呢？也就是要怎么进行扩展呢？ 看上边红框的代码： 12345678910111213141516171819// WebMvcConfigurationSupportgetInterceptors(conversionService, resourceUrlProvider) protected final Object[] getInterceptors( FormattingConversionService mvcConversionService, ResourceUrlProvider mvcResourceUrlProvider) &#123; if (this.interceptors == null) &#123; InterceptorRegistry registry = new InterceptorRegistry(); // 钩子方法 addInterceptors(registry); registry.addInterceptor(new ConversionServiceExposingInterceptor(mvcConversionService)); registry.addInterceptor(new ResourceUrlProviderExposingInterceptor(mvcResourceUrlProvider)); this.interceptors = registry.getInterceptors(); &#125; return this.interceptors.toArray();&#125;protected void addInterceptors(InterceptorRegistry registry) &#123;&#125; 可以看到，在这里提供了一个钩子方法addInterceptors来对配置进行拓展 在SpringMVC中引入的的是DelegatingWebMvcConfiguration，而它的父类就是WebMvcConfigurationSupport看源码： 12345678910111213141516171819202122232425@Configuration(proxyBeanMethods = false)public class DelegatingWebMvcConfiguration extends WebMvcConfigurationSupport &#123; private final WebMvcConfigurerComposite configurers = new WebMvcConfigurerComposite(); @Autowired(required = false) public void setConfigurers(List&lt;WebMvcConfigurer&gt; configurers) &#123; if (!CollectionUtils.isEmpty(configurers)) &#123; this.configurers.addWebMvcConfigurers(configurers); &#125; &#125; ...... @Override protected void addInterceptors(InterceptorRegistry registry) &#123; this.configurers.addInterceptors(registry); &#125; @Override protected void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; this.configurers.configureMessageConverters(converters); &#125; ......&#125; 可以看到，在类重写了这个钩子方法，也就是说在@Bean时调用的钩子方法就是这个类中的方法。而这个addInterceptors方法很简单，只有一行代码，但却有很强的扩展性！在DelegatingWebMvcConfiguration中，通过注入实现了WebMvcConfigurer接口的类来实现功能的扩展，也就是说只要在项目中实现WebMvcConfigurer这个接口重写对应方法就能对SpringMVC中一些对应的配置进行修改了，下面看下我自定义的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@Configuration@EnableWebMvcpublic class AppConfig implements WebMvcConfigurer &#123; @Autowired private UserInterceptor userInterceptor; @Autowired private ApplicationContext applicationContext; @Bean public FreeMarkerConfigurer freeMarkerConfigurer() &#123; FreeMarkerConfigurer freeMarkerConfigurer = new FreeMarkerConfigurer(); freeMarkerConfigurer.setTemplateLoaderPath(&quot;classpath:/ftl/&quot;); freeMarkerConfigurer.setDefaultEncoding(&quot;UTF-8&quot;); freeMarkerConfigurer.setConfigLocation(applicationContext.getResource(&quot;classpath:ftl/ftl.properties&quot;)); return freeMarkerConfigurer; &#125; @Override public void configureViewResolvers(ViewResolverRegistry registry) &#123; registry.enableContentNegotiation(new MappingJackson2JsonView()); registry.freeMarker(); registry.jsp(&quot;/jsp/&quot;, &quot;.jsp&quot;); &#125; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(&quot;/view/ok&quot;).setViewName(&quot;ok&quot;); registry.addViewController(&quot;/view/index&quot;).setViewName(&quot;index&quot;); &#125; @Override public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) &#123;// configurer.enable(); &#125; @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(userInterceptor).addPathPatterns(&quot;/user/**&quot;).excludePathPatterns(&quot;/user/query/**&quot;); registry.addInterceptor(new UserInterceptor1()).addPathPatterns(&quot;/user/**&quot;).excludePathPatterns(&quot;&quot;); &#125; @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123;// registry.addResourceHandler(&quot;/image/**&quot;)// .addResourceLocations(&quot;classpath:/img/&quot;); &#125;/* @Override public void addCorsMappings(CorsRegistry registry) &#123; registry.addMapping(&quot;/user/**&quot;) .allowedOrigins(&quot;*&quot;) .allowCredentials(true) .allowedMethods(&quot;GET&quot;, &quot;POST&quot;, &quot;DELETE&quot;, &quot;PUT&quot;, &quot;PATCH&quot;) .maxAge(3600); &#125;*/ @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123;// converters.add(new FastJsonHttpMessageConverter()); &#125; @Override public void extendMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; MappingJackson2HttpMessageConverter mappingJackson2HttpMessageConverter = new MappingJackson2HttpMessageConverter(); //设置日期格式 ObjectMapper objectMapper = new ObjectMapper(); SimpleDateFormat smt = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); objectMapper.setDateFormat(smt); mappingJackson2HttpMessageConverter.setObjectMapper(objectMapper); //设置中文编码格式 List&lt;MediaType&gt; list = new ArrayList&lt;MediaType&gt;(); list.add(MediaType.APPLICATION_JSON); mappingJackson2HttpMessageConverter.setSupportedMediaTypes(list); converters.add(mappingJackson2HttpMessageConverter); &#125;&#125; 看回WebMvcConfigurationSupport类 在这个类中@Bean了这类 RequestMappingHandlerMapping UrlPathHelper的一个实例 SimpleUrlHandlerMapping BeanNameUrlHandlerMapping RouterFunctionMapping ResourceUrlProvider DefaultServletHandlerConfigurer RequestMappingHandlerAdapter HandlerFunctionAdapter HttpRequestHandlerAdapter SimpleControllerHandlerAdapter HandlerExceptionResolver 另外两个方法另外两个都一样，这里贴下源码： 这3个init方法都是收集对应的类，然后把收集到的类赋值给DispatcherServlet中的对应的属性。而这些对象可以是通过@Componetn标记的，或者使用@EnabelWebMvc默认引入的。 现在已经知道这些类是由@EnabelWebMvc 注解中 @Import的类所@Bean的。而这个类是DelegatingWebMvcConfiguration。好了SpringMVC的启动就完了，下面将看请求是怎么到SpringMVC的Controller中的","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"18-springMVC使用和扩展","slug":"spring/18-springMVC使用和扩展","date":"2021-11-25T12:00:26.000Z","updated":"2022-03-23T09:03:55.562Z","comments":true,"path":"blog/spring/18-springMVC使用和扩展/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/18-springMVC%E4%BD%BF%E7%94%A8%E5%92%8C%E6%89%A9%E5%B1%95/","excerpt":"","text":"传统XML配置：spring-dispatcher.xml spring中的配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@Configuration@EnableWebMvcpublic class AppConfig implements WebMvcConfigurer &#123; @Autowired private UserInterceptor userInterceptor; @Autowired private ApplicationContext applicationContext; @Bean public FreeMarkerConfigurer freeMarkerConfigurer() &#123; FreeMarkerConfigurer freeMarkerConfigurer = new FreeMarkerConfigurer(); freeMarkerConfigurer.setTemplateLoaderPath(&quot;classpath:/ftl/&quot;); freeMarkerConfigurer.setDefaultEncoding(&quot;UTF-8&quot;); freeMarkerConfigurer.setConfigLocation(applicationContext.getResource(&quot;classpath:ftl/ftl.properties&quot;)); return freeMarkerConfigurer; &#125; @Override public void configureViewResolvers(ViewResolverRegistry registry) &#123; registry.enableContentNegotiation(new MappingJackson2JsonView()); registry.freeMarker(); registry.jsp(&quot;/jsp/&quot;, &quot;.jsp&quot;); &#125; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(&quot;/view/ok&quot;).setViewName(&quot;ok&quot;); registry.addViewController(&quot;/view/index&quot;).setViewName(&quot;index&quot;); &#125; @Override public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) &#123;// configurer.enable(); &#125; @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(userInterceptor).addPathPatterns(&quot;/user/**&quot;).excludePathPatterns(&quot;/user/query/**&quot;); registry.addInterceptor(new UserInterceptor1()).addPathPatterns(&quot;/user/**&quot;).excludePathPatterns(&quot;&quot;); &#125; @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123;// registry.addResourceHandler(&quot;/image/**&quot;)// .addResourceLocations(&quot;classpath:/img/&quot;); &#125;/* @Override public void addCorsMappings(CorsRegistry registry) &#123; registry.addMapping(&quot;/user/**&quot;) .allowedOrigins(&quot;*&quot;) .allowCredentials(true) .allowedMethods(&quot;GET&quot;, &quot;POST&quot;, &quot;DELETE&quot;, &quot;PUT&quot;, &quot;PATCH&quot;) .maxAge(3600); &#125;*/ @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123;// converters.add(new FastJsonHttpMessageConverter()); &#125; @Override public void extendMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; MappingJackson2HttpMessageConverter mappingJackson2HttpMessageConverter = new MappingJackson2HttpMessageConverter(); //设置日期格式 ObjectMapper objectMapper = new ObjectMapper(); SimpleDateFormat smt = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); objectMapper.setDateFormat(smt); mappingJackson2HttpMessageConverter.setObjectMapper(objectMapper); //设置中文编码格式 List&lt;MediaType&gt; list = new ArrayList&lt;MediaType&gt;(); list.add(MediaType.APPLICATION_JSON); mappingJackson2HttpMessageConverter.setSupportedMediaTypes(list); converters.add(mappingJackson2HttpMessageConverter); &#125;&#125; spring与tomcat结合123456789101112131415161718192021222324252627282930313233343536373839public class WebAppInitializer extends AbstractAnnotationConfigDispatcherServletInitializer &#123; //父容器 @Override protected Class&lt;?&gt;[] getRootConfigClasses() &#123; System.out.println(&quot;-------调用了getRootConfigClasses---------&quot;); return new Class&lt;?&gt;[]&#123;SpringContainer.class&#125;; &#125; //SpringMVC配置子容器 @Override protected Class&lt;?&gt;[] getServletConfigClasses() &#123; System.out.println(&quot;-------调用了getServletConfigClasses---------&quot;); return new Class&lt;?&gt;[]&#123;MvcContainer.class&#125;; &#125; //获取DispatcherServlet的映射信息 @Override protected String[] getServletMappings() &#123; System.out.println(&quot;-------调用了getServletMappings---------&quot;); return new String[]&#123;&quot;/&quot;&#125;; &#125; @Override protected Filter[] getServletFilters() &#123; System.out.println(&quot;-------调用了getServletFilters---------&quot;); MyFilter myFilter = new MyFilter(); CorsFilter corsFilter = new CorsFilter(); CharacterEncodingFilter characterEncodingFilter = new CharacterEncodingFilter(); characterEncodingFilter.setEncoding(&quot;UTF-8&quot;); characterEncodingFilter.setForceEncoding(true); return new Filter[]&#123;myFilter/*,corsFilter*/, characterEncodingFilter&#125;; &#125; @Override protected FilterRegistration.Dynamic registerServletFilter(ServletContext servletContext, Filter filter) &#123; return super.registerServletFilter(servletContext, filter); &#125;&#125; 启动 下面的代码不是必须的，可以把项目打成war包后，放到tomcat中运行。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class SpringApplication &#123; public static void main(String[] args) &#123; run(SpringApplication.class, args); &#125; public static void run(Object source, String... args) &#123;// return run(new Object[] &#123; source &#125;, args); try &#123; // 创建Tomcat容器 Tomcat tomcatServer = new Tomcat(); // 端口号设置 tomcatServer.setPort(9090); // 读取项目路径 加载静态资源// StandardContext ctx = (StandardContext) tomcatServer.addWebapp(&quot;/&quot;, new File(&quot;spring-source/src/main&quot;).getAbsolutePath()); String basePath = System.getProperty(&quot;user.dir&quot;) + File.separator; tomcatServer.getHost().setAppBase(basePath);//// Connector connector = tomcatServer.getConnector();// connector.setURIEncoding(&quot;UTF-8&quot;); //改变文件读取路径，从resources目录下去取文件// StandardContext ctx = (StandardContext) tomcatServer.addWebapp(&quot;/&quot;, basePath + &quot;src&quot; + File.separator + &quot;main&quot; + File.separator + &quot;resources&quot;); StandardContext ctx = (StandardContext)tomcatServer.addWebapp(&quot;/&quot;, basePath + &quot;src&quot; + File.separator + &quot;main&quot; + File.separator + &quot;resources&quot;);// tomcatServer.addWebapp(&quot;/&quot;, basePath + &quot;src&quot; + File.separator + &quot;main&quot; + File.separator + &quot;resources&quot;);// ctx.addServletMappingDecoded(&quot;*.jpg&quot;, &quot;default&quot;); // 禁止重新载入 ctx.setReloadable(false); // class文件读取地址 File additionWebInfClasses = new File(&quot;target/classes&quot;); // 创建WebRoot WebResourceRoot resources = new StandardRoot(ctx); // tomcat内部读取Class执行 resources.addPreResources( new DirResourceSet(resources, &quot;/WEB-INF/classes&quot;, additionWebInfClasses.getAbsolutePath(), &quot;/&quot;)); ctx.setResources(resources); tomcatServer.start(); // 异步等待请求执行 tomcatServer.getServer().await(); &#125; catch (LifecycleException e) &#123; e.printStackTrace(); &#125; catch (ServletException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 在启动的时候打印了如下内容，也就意味着WebAppInitializer里面的方法被执行了！ 解析上边的WebAppInitializer类没有使用任何注解，也就是活，从类的定义上看，该类的对象创建是不会交给Spring管理的。但是在启动过时候，却调用了这个类的方法。所以到底怎么样创建WebAppInitializer对象的了？下面分析。 上面这种启动方式之所以是可行的，是因为tomcat提供了一个扩展，而这个扩展通过java的SPI，给使用者进行扩展。在spring-web模块中定义这样一个文件： tomcat在启动的时候会调用java的API实例化这个文件上定义的类，看下这个类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144/** * Servlet 3.0 &#123;@link ServletContainerInitializer&#125; designed to support code-based * configuration of the servlet container using Spring&#x27;s &#123;@link WebApplicationInitializer&#125; * SPI as opposed to (or possibly in combination with) the traditional * &#123;@code web.xml&#125;-based approach. * * &lt;h2&gt;Mechanism of Operation&lt;/h2&gt; * This class will be loaded and instantiated and have its &#123;@link #onStartup&#125; * method invoked by any Servlet 3.0-compliant container during container startup assuming * that the &#123;@code spring-web&#125; module JAR is present on the classpath. This occurs through * the JAR Services API &#123;@link ServiceLoader#load(Class)&#125; method detecting the * &#123;@code spring-web&#125; module&#x27;s &#123;@code META-INF/services/javax.servlet.ServletContainerInitializer&#125; * service provider configuration file. See the * &lt;a href=&quot;https://download.oracle.com/javase/6/docs/technotes/guides/jar/jar.html#Service%20Provider&quot;&gt; * JAR Services API documentation&lt;/a&gt; as well as section &lt;em&gt;8.2.4&lt;/em&gt; of the Servlet 3.0 * Final Draft specification for complete details. * * &lt;h3&gt;In combination with &#123;@code web.xml&#125;&lt;/h3&gt; * A web application can choose to limit the amount of classpath scanning the Servlet * container does at startup either through the &#123;@code metadata-complete&#125; attribute in * &#123;@code web.xml&#125;, which controls scanning for Servlet annotations or through an * &#123;@code &lt;absolute-ordering&gt;&#125; element also in &#123;@code web.xml&#125;, which controls which * web fragments (i.e. jars) are allowed to perform a &#123;@code ServletContainerInitializer&#125; * scan. When using this feature, the &#123;@link SpringServletContainerInitializer&#125; * can be enabled by adding &quot;spring_web&quot; to the list of named web fragments in * &#123;@code web.xml&#125; as follows: * * &lt;pre class=&quot;code&quot;&gt; * &amp;lt;absolute-ordering&amp;gt; * &amp;lt;name&gt;some_web_fragment&amp;lt;/name&amp;gt; * &amp;lt;name&gt;spring_web&amp;lt;/name&amp;gt; * &amp;lt;/absolute-ordering&amp;gt; * &lt;/pre&gt; * * &lt;h2&gt;Relationship to Spring&#x27;s &#123;@code WebApplicationInitializer&#125;&lt;/h2&gt; * Spring&#x27;s &#123;@code WebApplicationInitializer&#125; SPI consists of just one method: * &#123;@link WebApplicationInitializer#onStartup(ServletContext)&#125;. The signature is intentionally * quite similar to &#123;@link ServletContainerInitializer#onStartup(Set, ServletContext)&#125;: * simply put, &#123;@code SpringServletContainerInitializer&#125; is responsible for instantiating * and delegating the &#123;@code ServletContext&#125; to any user-defined * &#123;@code WebApplicationInitializer&#125; implementations. It is then the responsibility of * each &#123;@code WebApplicationInitializer&#125; to do the actual work of initializing the * &#123;@code ServletContext&#125;. The exact process of delegation is described in detail in the * &#123;@link #onStartup onStartup&#125; documentation below. * * &lt;h2&gt;General Notes&lt;/h2&gt; * In general, this class should be viewed as &lt;em&gt;supporting infrastructure&lt;/em&gt; for * the more important and user-facing &#123;@code WebApplicationInitializer&#125; SPI. Taking * advantage of this container initializer is also completely &lt;em&gt;optional&lt;/em&gt;: while * it is true that this initializer will be loaded and invoked under all Servlet 3.0+ * runtimes, it remains the user&#x27;s choice whether to make any * &#123;@code WebApplicationInitializer&#125; implementations available on the classpath. If no * &#123;@code WebApplicationInitializer&#125; types are detected, this container initializer will * have no effect. * * &lt;p&gt;Note that use of this container initializer and of &#123;@code WebApplicationInitializer&#125; * is not in any way &quot;tied&quot; to Spring MVC other than the fact that the types are shipped * in the &#123;@code spring-web&#125; module JAR. Rather, they can be considered general-purpose * in their ability to facilitate convenient code-based configuration of the * &#123;@code ServletContext&#125;. In other words, any servlet, listener, or filter may be * registered within a &#123;@code WebApplicationInitializer&#125;, not just Spring MVC-specific * components. * * &lt;p&gt;This class is neither designed for extension nor intended to be extended. * It should be considered an internal type, with &#123;@code WebApplicationInitializer&#125; * being the public-facing SPI. * * &lt;h2&gt;See Also&lt;/h2&gt; * See &#123;@link WebApplicationInitializer&#125; Javadoc for examples and detailed usage * recommendations.&lt;p&gt; * * @author Chris Beams * @author Juergen Hoeller * @author Rossen Stoyanchev * @since 3.1 * @see #onStartup(Set, ServletContext) * @see WebApplicationInitializer */@HandlesTypes(WebApplicationInitializer.class)public class SpringServletContainerInitializer implements ServletContainerInitializer &#123; /** * Delegate the &#123;@code ServletContext&#125; to any &#123;@link WebApplicationInitializer&#125; * implementations present on the application classpath. * &lt;p&gt;Because this class declares @&#123;@code HandlesTypes(WebApplicationInitializer.class)&#125;, * Servlet 3.0+ containers will automatically scan the classpath for implementations * of Spring&#x27;s &#123;@code WebApplicationInitializer&#125; interface and provide the set of all * such types to the &#123;@code webAppInitializerClasses&#125; parameter of this method. * &lt;p&gt;If no &#123;@code WebApplicationInitializer&#125; implementations are found on the classpath, * this method is effectively a no-op. An INFO-level log message will be issued notifying * the user that the &#123;@code ServletContainerInitializer&#125; has indeed been invoked but that * no &#123;@code WebApplicationInitializer&#125; implementations were found. * &lt;p&gt;Assuming that one or more &#123;@code WebApplicationInitializer&#125; types are detected, * they will be instantiated (and &lt;em&gt;sorted&lt;/em&gt; if the @&#123;@link * org.springframework.core.annotation.Order @Order&#125; annotation is present or * the &#123;@link org.springframework.core.Ordered Ordered&#125; interface has been * implemented). Then the &#123;@link WebApplicationInitializer#onStartup(ServletContext)&#125; * method will be invoked on each instance, delegating the &#123;@code ServletContext&#125; such * that each instance may register and configure servlets such as Spring&#x27;s * &#123;@code DispatcherServlet&#125;, listeners such as Spring&#x27;s &#123;@code ContextLoaderListener&#125;, * or any other Servlet API componentry such as filters. * @param webAppInitializerClasses all implementations of * &#123;@link WebApplicationInitializer&#125; found on the application classpath * @param servletContext the servlet context to be initialized * @see WebApplicationInitializer#onStartup(ServletContext) * @see AnnotationAwareOrderComparator */ @Override public void onStartup(@Nullable Set&lt;Class&lt;?&gt;&gt; webAppInitializerClasses, ServletContext servletContext) throws ServletException &#123; List&lt;WebApplicationInitializer&gt; initializers = new LinkedList&lt;&gt;(); if (webAppInitializerClasses != null) &#123; for (Class&lt;?&gt; waiClass : webAppInitializerClasses) &#123; // Be defensive: Some servlet containers provide us with invalid classes, // no matter what @HandlesTypes says... if (!waiClass.isInterface() &amp;&amp; !Modifier.isAbstract(waiClass.getModifiers()) &amp;&amp; WebApplicationInitializer.class.isAssignableFrom(waiClass)) &#123; try &#123; initializers.add((WebApplicationInitializer) ReflectionUtils.accessibleConstructor(waiClass).newInstance()); &#125; catch (Throwable ex) &#123; throw new ServletException(&quot;Failed to instantiate WebApplicationInitializer class&quot;, ex); &#125; &#125; &#125; &#125; if (initializers.isEmpty()) &#123; servletContext.log(&quot;No Spring WebApplicationInitializer types detected on classpath&quot;); return; &#125; servletContext.log(initializers.size() + &quot; Spring WebApplicationInitializers detected on classpath&quot;); AnnotationAwareOrderComparator.sort(initializers); for (WebApplicationInitializer initializer : initializers) &#123; initializer.onStartup(servletContext); &#125; &#125;&#125; 从上边的类中的注释可以了解到这个类的作用 ServletContainerInitializer接口是Servlet3.0提出的一个接口，spring通过这个接口和使用SPI来实现servlet容器基于代码的扩展。而该接口的方法onStartup方法从该注解可以了解到调用的时机。 Servlet容器在启动阶段会调用java的ServiceProvider（java SPI）去加载项目所有jar包中的 1META-INF/services/javax.servlet.ServletContainerInitializer 配置文件，然后去加载并实例化该配置文件中指定的类。然后在调用这些对象的onStartup方法。 而且在SpringMVC提供的这个配置文件锁定义的类上有这一个注解@HandlesTypes。这也是Servlet3.0的注解。看下该注解的作用 Servlet容器发现有@HandlesTypes注解后，会扫描应用中实现了WebApplicationInitializer接口的类，把这些类的Class对象封装成一个集合后，把该集合作为onStartup方法的入参。 所以这也就是解析了WebAppInitializer对象为什么会被实例化了，因为该类实现了WebApplicationInitializer接口 看SpringServletContainerInitializer的onStartup，逻辑很简单就是遍历项目中实现WebApplicationInitializer接口的类的class对象，实例化这些对象后，然后遍历执行这些对象的onStartup方法。所以现在可以确定了，SpringMVC的入口方法为WebAppInitializer的onStartup方法。跟踪下代码后入口方法在AbstractDispatcherServletInitializer类中 12345678// AbstractDispatcherServletInitializer@Overridepublic void onStartup(ServletContext servletContext) throws ServletException &#123; //创建根上下文，创建servletListener super.onStartup(servletContext); //创建mvc上下文，注册DispatcherServlet registerDispatcherServlet(servletContext);&#125; 在spring中使用Servlet规范进行扩展项目中提供这个文件： 实现类为： 12345678910111213141516171819202122232425public interface LoadServlet &#123; void loadOnstarp(ServletContext servletContext);&#125;@HandlesTypes(LoadServlet.class)public class MyServletContainerInitializer implements ServletContainerInitializer &#123; @Override public void onStartup(Set&lt;Class&lt;?&gt;&gt; set, ServletContext servletContext) throws ServletException &#123; Iterator&lt;Class&lt;?&gt;&gt; iterator; if (set != null) &#123; iterator = set.iterator(); while (iterator.hasNext()) &#123; Class&lt;?&gt; clazz = iterator.next(); if (!clazz.isInterface() &amp;&amp; !Modifier.isAbstract(clazz.getModifiers()) &amp;&amp; LoadServlet.class.isAssignableFrom(clazz)) &#123; try &#123; ((LoadServlet) clazz.newInstance()).loadOnstarp(servletContext); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125;&#125; 意思就是加载实现了LoadServlet这个接口的类，然后把这些类实例化后执行方法。 这里我提供一个实现了LoadServlet的类 12345678910111213141516171819202122232425262728293031323334353637383940414243public class LoadServletImpl implements LoadServlet &#123; @Override public void loadOnstarp(ServletContext servletContext) &#123; ServletRegistration.Dynamic initServlet = servletContext.addServlet(&quot;initServlet&quot;, InitServlet.class); initServlet.setLoadOnStartup(1); initServlet.addMapping(&quot;/init&quot;); ServletRegistration aDefault = servletContext.getServletRegistration(&quot;default&quot;); aDefault.addMapping(&quot;*.css&quot;,&quot;*.gif&quot;,&quot;*.jpg&quot;,&quot;*.js&quot;,&quot;*.JPG&quot;);// ServletRegistration.Dynamic defaults = servletContext.addServlet(&quot;default&quot;, DefaultServlet.class);// defaults.setLoadOnStartup(1);// defaults.addMapping(&quot;*.css&quot;,&quot;*.gif&quot;,&quot;*.jpg&quot;,&quot;*.js&quot;,&quot;*.JPG&quot;); &#125;&#125;public class InitServlet extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println(&quot;=====doget===&quot;); PrintWriter writer = resp.getWriter(); writer.print(&quot;&lt;h1&gt;Jack&lt;/h1&gt;&quot;); RequestDispatcher requestDispatcher = req.getRequestDispatcher(&quot;/jsp/ok.jsp&quot;); requestDispatcher.forward(req,resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; super.doPost(req, resp); &#125; @Override public void init(ServletConfig config) throws ServletException &#123; super.init(config); &#125; @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; super.service(req, resp); &#125;&#125; tomcat启动后，会加入一个名为initServlet的Servlet。 tomcat启动时答应如下内容","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"17-Spring异步注解","slug":"spring/17-Spring异步注解","date":"2021-11-25T12:00:25.000Z","updated":"2022-03-23T09:03:55.529Z","comments":true,"path":"blog/spring/17-Spring异步注解/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/17-Spring%E5%BC%82%E6%AD%A5%E6%B3%A8%E8%A7%A3/","excerpt":"","text":"@Async12345678910111213141516171819202122232425@Component//@EnableScheduling@EnableAsync(proxyTargetClass = true)public class EnableAsyncBean &#123; private int corePoolSize = 10; private int maxPoolSize = 200; private int queueCapacity = 10; private String ThreadNamePrefix = &quot;JackExecutor-&quot;; @Bean public Executor executor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(corePoolSize); executor.setMaxPoolSize(maxPoolSize); executor.setQueueCapacity(queueCapacity); executor.setThreadNamePrefix(ThreadNamePrefix); // rejection-policy：当pool已经达到max size的时候，如何处理新任务 // CALLER_RUNS：不在新线程中执行任务，而是有调用者所在的线程来执行 executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); executor.initialize(); return executor; &#125;&#125; 然后加上这个注解@Async就好了。看@EnableAsync的定义 123456789101112131415161718@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Import(AsyncConfigurationSelector.class)public @interface EnableAsync &#123; Class&lt;? extends Annotation&gt; annotation() default Annotation.class; boolean proxyTargetClass() default false; AdviceMode mode() default AdviceMode.PROXY; int order() default Ordered.LOWEST_PRECEDENCE;&#125; 作用就是要引入AsyncConfigurationSelector.class这个类。 12345678910public String[] selectImports(AdviceMode adviceMode) &#123; switch (adviceMode) &#123; case PROXY: return new String[] &#123;ProxyAsyncConfiguration.class.getName()&#125;; case ASPECTJ: return new String[] &#123;ASYNC_EXECUTION_ASPECT_CONFIGURATION_CLASS_NAME&#125;; default: return null; &#125;&#125; 看ProxyAsyncConfiguration 1234567891011121314151617181920@Configuration@Role(BeanDefinition.ROLE_INFRASTRUCTURE)public class ProxyAsyncConfiguration extends AbstractAsyncConfiguration &#123; @Bean(name = TaskManagementConfigUtils.ASYNC_ANNOTATION_PROCESSOR_BEAN_NAME) @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public AsyncAnnotationBeanPostProcessor asyncAdvisor() &#123; Assert.notNull(this.enableAsync, &quot;@EnableAsync annotation metadata was not injected&quot;); AsyncAnnotationBeanPostProcessor bpp = new AsyncAnnotationBeanPostProcessor(); bpp.configure(this.executor, this.exceptionHandler); Class&lt;? extends Annotation&gt; customAsyncAnnotation = this.enableAsync.getClass(&quot;annotation&quot;); if (customAsyncAnnotation != AnnotationUtils.getDefaultValue(EnableAsync.class, &quot;annotation&quot;)) &#123; bpp.setAsyncAnnotationType(customAsyncAnnotation); &#125; bpp.setProxyTargetClass(this.enableAsync.getBoolean(&quot;proxyTargetClass&quot;)); bpp.setOrder(this.enableAsync.&lt;Integer&gt;getNumber(&quot;order&quot;)); return bpp; &#125;&#125; 这里就是为了要实例化AsyncAnnotationBeanPostProcessor这个对象，看类图 我们都知道，既然异步了，那也要使用代理类来实现的，而代理类是在类初始化的最后里会调用BeanPostProcessor#postProcessAfterInitialization这个方法的，那我们看这个类的postProcessAfterInitialization方法。 123456789101112131415161718192021222324252627282930313233public Object postProcessAfterInitialization(Object bean, String beanName) &#123; if (this.advisor == null || bean instanceof AopInfrastructureBean) &#123; // Ignore AOP infrastructure such as scoped proxies. return bean; &#125; if (bean instanceof Advised) &#123; Advised advised = (Advised) bean; if (!advised.isFrozen() &amp;&amp; isEligible(AopUtils.getTargetClass(bean))) &#123; // Add our local Advisor to the existing proxy&#x27;s Advisor chain... if (this.beforeExistingAdvisors) &#123; advised.addAdvisor(0, this.advisor); &#125; else &#123; advised.addAdvisor(this.advisor); &#125; return bean; &#125; &#125; if (isEligible(bean, beanName)) &#123; ProxyFactory proxyFactory = prepareProxyFactory(bean, beanName); if (!proxyFactory.isProxyTargetClass()) &#123; evaluateProxyInterfaces(bean.getClass(), proxyFactory); &#125; proxyFactory.addAdvisor(this.advisor); customizeProxyFactory(proxyFactory); return proxyFactory.getProxy(getProxyClassLoader()); &#125; // No proxy needed. return bean;&#125; 上边代码中的isEligible方法最后会调用这块代码 12345678910111213public static boolean canApply(Advisor advisor, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; if (advisor instanceof IntroductionAdvisor) &#123; return ((IntroductionAdvisor) advisor).getClassFilter().matches(targetClass); &#125; else if (advisor instanceof PointcutAdvisor) &#123; PointcutAdvisor pca = (PointcutAdvisor) advisor; return canApply(pca.getPointcut(), targetClass, hasIntroductions); &#125; else &#123; // It doesn&#x27;t have a pointcut so we assume it applies. return true; &#125;&#125; 可以看到，就是通过切面来判断否匹配。而且，创建代理类的时候加入了一个切面。而这个切面时对象的一个属性，但是这个异步的切面是在哪里放到Spring 容器中的？可以看到，这里面有个advisor属性，看名字就知道，它是一个切面，但问题是它是怎么初始化的。我们看会哪个类图，发现它还实现了一个接口BeanFactoryAware，我们都知道，Spring Bean在IOC和DI完成后就会执行一些接口的方法，而这个接口就是在这时调用的，也就是在创建代理对象之前调用的。看这个接口的setBeanFactory方法 1234567891011@Overridepublic void setBeanFactory(BeanFactory beanFactory) &#123; super.setBeanFactory(beanFactory); AsyncAnnotationAdvisor advisor = new AsyncAnnotationAdvisor(this.executor, this.exceptionHandler); if (this.asyncAnnotationType != null) &#123; advisor.setAsyncAnnotationType(this.asyncAnnotationType); &#125; advisor.setBeanFactory(beanFactory); this.advisor = advisor;&#125; 可以看到，在这里创建了一个AsyncAnnotationAdvisor的切面，看下它的构造方法。 123456789101112131415public AsyncAnnotationAdvisor( @Nullable Supplier&lt;Executor&gt; executor, @Nullable Supplier&lt;AsyncUncaughtExceptionHandler&gt; exceptionHandler) &#123; Set&lt;Class&lt;? extends Annotation&gt;&gt; asyncAnnotationTypes = new LinkedHashSet&lt;&gt;(2); asyncAnnotationTypes.add(Async.class); try &#123; asyncAnnotationTypes.add((Class&lt;? extends Annotation&gt;) ClassUtils.forName(&quot;javax.ejb.Asynchronous&quot;, AsyncAnnotationAdvisor.class.getClassLoader())); &#125; catch (ClassNotFoundException ex) &#123; // If EJB 3.1 API not present, simply ignore. &#125; this.advice = buildAdvice(executor, exceptionHandler); this.pointcut = buildPointcut(asyncAnnotationTypes);&#125; Advice是AnnotationAsyncExecutionInterceptor，而pointcut比较特殊ComposablePointcut里面有两个 ComposablePointcut虽然看起来和之前的不同的，但是这个pointcut无非就是做两件事，进行类的匹配和方法匹配，匹配的依据就是@Async 这个注解。这里就不看了。重点看Advice吧 AnnotationAsyncExecutionInterceptor1234567891011121314151617181920212223242526272829public Object invoke(final MethodInvocation invocation) throws Throwable &#123; Class&lt;?&gt; targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); Method specificMethod = ClassUtils.getMostSpecificMethod(invocation.getMethod(), targetClass); final Method userDeclaredMethod = BridgeMethodResolver.findBridgedMethod(specificMethod); AsyncTaskExecutor executor = determineAsyncExecutor(userDeclaredMethod); if (executor == null) &#123; throw new IllegalStateException( &quot;No executor specified and no default executor set on AsyncExecutionInterceptor either&quot;); &#125; Callable&lt;Object&gt; task = () -&gt; &#123; try &#123; Object result = invocation.proceed(); if (result instanceof Future) &#123; return ((Future&lt;?&gt;) result).get(); &#125; &#125; catch (ExecutionException ex) &#123; handleError(ex.getCause(), userDeclaredMethod, invocation.getArguments()); &#125; catch (Throwable ex) &#123; handleError(ex, userDeclaredMethod, invocation.getArguments()); &#125; return null; &#125;; return doSubmit(task, executor, invocation.getMethod().getReturnType());&#125; 先看是怎么获取TaskExecutor的，获取的方法为determineAsyncExecutor() 1234567891011121314151617181920protected AsyncTaskExecutor determineAsyncExecutor(Method method) &#123; AsyncTaskExecutor executor = this.executors.get(method); if (executor == null) &#123; Executor targetExecutor; String qualifier = getExecutorQualifier(method); if (StringUtils.hasLength(qualifier)) &#123; targetExecutor = findQualifiedExecutor(this.beanFactory, qualifier); &#125; else &#123; targetExecutor = this.defaultExecutor.get(); &#125; if (targetExecutor == null) &#123; return null; &#125; executor = (targetExecutor instanceof AsyncListenableTaskExecutor ? (AsyncListenableTaskExecutor) targetExecutor : new TaskExecutorAdapter(targetExecutor)); this.executors.put(method, executor); &#125; return executor;&#125; 代码的意思就是去获取执行器的（可以理解为线程池）。 先通过getExecutorQualifier方法获取，该方法AnnotationAsyncExecutionInterceptor重写了，看源码 12345678910//AnnotationAsyncExecutionInterceptorprotected String getExecutorQualifier(Method method) &#123; // Maintainer&#x27;s note: changes made here should also be made in // AnnotationAsyncExecutionAspect#getExecutorQualifier Async async = AnnotatedElementUtils.findMergedAnnotation(method, Async.class); if (async == null) &#123; async = AnnotatedElementUtils.findMergedAnnotation(method.getDeclaringClass(), Async.class); &#125; return (async != null ? async.value() : null);&#125; 可以看到，就是通返回注解的value值 通过@Async注解的value从beanFactory中获取bean 如果第的value为空或获取不到对象，那就通过defaultExecutor去获取，该defaultExecutor类型为SingletonSupplier，在初始化时创建的 1234public AsyncExecutionAspectSupport(@Nullable Executor defaultExecutor) &#123; this.defaultExecutor = new SingletonSupplier&lt;&gt;(defaultExecutor, () -&gt; getDefaultExecutor(this.beanFactory)); this.exceptionHandler = SingletonSupplier.of(SimpleAsyncUncaughtExceptionHandler::new);&#125; 这里先说下SingletonSupplier的get方法逻辑为先通过defaultExecutor去获取，如果没有在通过() -&gt; getDefaultExecutor(this.beanFactory)获取。而第二步的获取其实就是在beanFactory中获取实现了Executor接口的bean。 这里的defaultExecutor是可以在项目中指定的，比如在代码中实现了接口AsyncConfigurer来执行的 123456789101112@Componentpublic class AsyncConfigurerDemo implements AsyncConfigurer &#123; // 指定@Async 注解使用的TaskExecutor public Executor getAsyncExecutor() &#123; return null; &#125; // 设置错误处理器 public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() &#123; return null; &#125;&#125; 这样写后，在使用@EnableCaching注解后，就会在ProxyAsyncConfiguration中的 123456789101112@Autowired(required = false)void setConfigurers(Collection&lt;AsyncConfigurer&gt; configurers) &#123; if (CollectionUtils.isEmpty(configurers)) &#123; return; &#125; if (configurers.size() &gt; 1) &#123; throw new IllegalStateException(&quot;Only one AsyncConfigurer may exist&quot;); &#125; AsyncConfigurer configurer = configurers.iterator().next(); this.executor = configurer::getAsyncExecutor; this.exceptionHandler = configurer::getAsyncUncaughtExceptionHandler;&#125; 这段代码设置对应的值，而这些值会在创建Advisor的时候传入。 获取到Executor后，就是执行逻辑代码了，而这里时进行火炬传递 invoke代码中接着会对执行方法包装成一个Callable对象。 12345678910111213141516Callable&lt;Object&gt; task = () -&gt; &#123; try &#123; // 火炬传递 Object result = invocation.proceed(); if (result instanceof Future) &#123; return ((Future&lt;?&gt;) result).get(); &#125; &#125; catch (ExecutionException ex) &#123; handleError(ex.getCause(), userDeclaredMethod, invocation.getArguments()); &#125; catch (Throwable ex) &#123; handleError(ex, userDeclaredMethod, invocation.getArguments()); &#125; return null;&#125;; 接着就执行了doSubmit，看代码： 1234567891011121314151617181920212223protected Object doSubmit(Callable&lt;Object&gt; task, AsyncTaskExecutor executor, Class&lt;?&gt; returnType) &#123; if (CompletableFuture.class.isAssignableFrom(returnType)) &#123; return CompletableFuture.supplyAsync(() -&gt; &#123; try &#123; return task.call(); &#125; catch (Throwable ex) &#123; throw new CompletionException(ex); &#125; &#125;, executor); &#125; else if (ListenableFuture.class.isAssignableFrom(returnType)) &#123; return ((AsyncListenableTaskExecutor) executor).submitListenable(task); &#125; else if (Future.class.isAssignableFrom(returnType)) &#123; return executor.submit(task); &#125; else &#123; //没返回值就走这里 executor.submit(task); return null; &#125;&#125; 这里就是根据放回值类型走不同的逻辑。 这个好像是响应式编程的，其他的都很好理解。 和@Transactional配合使用的一些问题 正常来说这样写是没问题的，不过这样会设计到一个执行顺序的问题。也就是如果先执行了 但如果在类中出现循环依赖了，那么这样写后启动会报错，比如这样 分析下这个原因 TransationServiceImpl在实例话完成后会在三级缓存中放入一个ObjectFactory。然后在依赖注入时，由于依赖了自己，所以会调用beanFactory(TransationServiceImpl.class)，这个调用会从三级缓存中拿到ObjectFactory后，接着会执行了这个方法 在这里面，在事务的启动类InfrastructureAdvisorAutoProxyCreator中重写了这个方法 所以在这里会生成一个代理对象。最后把这个代理对象升级为二级缓存，最后赋值给了 这个属性。TransationServiceImpl类的依赖注入完成后，他的初始化流程继续，走到 这里后他又会在InfrastructureAdvisorAutoProxyCreator这个类的postProcessAfterInitialization判断下是否有代理对象 因为依赖注入的时候就生成一个了 所以它不会再生成了，直接返回。但是这时又有个@Async这个注解，所以又会到 AsyncAnnotationBeanPostProcessor的postProcessAfterInitialization方法中，这里面不会考虑什么缓存，会又生成一个代理对象。所以这里会有两个bean 接着代码继续走到这里 也就是，包括在二级缓存中的bean，这时候这个类中，有3个不同的实例。所以最后 走到了else if里了。解决的办法也很简单，就是让allowRawInjectionDespiteWrapping这个属性为true。而这个属性是在beanFactory中，所以只要实现个BeanDefinitionRegistryPostProcessor这个接口，在postProcessBeanFactory方法中修改值就好了 但是并不推荐，还是以刚刚的例子，如果这样操作的话，就会导致 这里又一个对象了。对于一个单例的对象，却出现了两个对象，这是Spring不允许的 还有一种就是加个@Lazy，还有也可以用applicationContext对象获取，推荐这两种，上面那种不要使用。 @Scheduled使用这个的话要这样 1234567891011121314151617181920212223242526@Component@EnableScheduling//这个不是必须的@EnableAsync(proxyTargetClass = true)public class EnableAsyncBean &#123; private int corePoolSize = 10; private int maxPoolSize = 200; private int queueCapacity = 10; private String ThreadNamePrefix = &quot;JackExecutor-&quot;; @Bean public Executor executor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(corePoolSize); executor.setMaxPoolSize(maxPoolSize); executor.setQueueCapacity(queueCapacity); executor.setThreadNamePrefix(ThreadNamePrefix); // rejection-policy：当pool已经达到max size的时候，如何处理新任务 // CALLER_RUNS：不在新线程中执行任务，而是有调用者所在的线程来执行 executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); executor.initialize(); return executor; &#125;&#125; 老套路，看注解@EnableScheduling定义 1234567@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Import(SchedulingConfiguration.class)@Documentedpublic @interface EnableScheduling &#123;&#125; 看SchedulingConfiguration这个类 12345678910@Configuration@Role(BeanDefinition.ROLE_INFRASTRUCTURE)public class SchedulingConfiguration &#123; @Bean(name = TaskManagementConfigUtils.SCHEDULED_ANNOTATION_PROCESSOR_BEAN_NAME) @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public ScheduledAnnotationBeanPostProcessor scheduledAnnotationProcessor() &#123; return new ScheduledAnnotationBeanPostProcessor(); &#125;&#125; ScheduledAnnotationBeanPostProcessor这个类是真的负责（指实现的接口是真的多） 可以看到，这个不是用AOP了。看postProcessAfterInitialization这个方法， 首先收集收集对象上有@Scheduled的方法，如果有就遍历这个集合，然后执行processScheduled这个方法。这个方法很长，我就概括下吧，首先会把对象和方法包装成一个Runnable对象。然后对注解中的属性解析（cron、fixedDelay等），接着就是调用JDK的定时任务线程池的API。 注意，这个注解的在Spring项目中使用时，需要想项目中创建一个TaskScheduler，然后交给Spring管理，不然会报错。但在Spring boot中不需要","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"16-Spring缓存切面","slug":"spring/16-Spring缓存切面","date":"2021-11-25T12:00:24.000Z","updated":"2022-03-23T09:03:55.471Z","comments":true,"path":"blog/spring/16-Spring缓存切面/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/16-Spring%E7%BC%93%E5%AD%98%E5%88%87%E9%9D%A2/","excerpt":"","text":"缓存切面在有了AOP和事务的知识后就觉得很简单了。 先说使用。加上这个注解@EnableCaching，还是老套路，看注解的定义 12345678910111213@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Import(CachingConfigurationSelector.class)public @interface EnableCaching &#123; boolean proxyTargetClass() default false; AdviceMode mode() default AdviceMode.PROXY; int order() default Ordered.LOWEST_PRECEDENCE;&#125; 这个注解就是为了引入CachingConfigurationSelector.class这个启动类。老套路了，我就不细说了，注解看核心代码。 123456789private String[] getProxyImports() &#123; List&lt;String&gt; result = new ArrayList&lt;&gt;(3); result.add(AutoProxyRegistrar.class.getName()); result.add(ProxyCachingConfiguration.class.getName()); if (jsr107Present &amp;&amp; jcacheImplPresent) &#123; result.add(PROXY_JCACHE_CONFIGURATION_CLASS); &#125; return StringUtils.toStringArray(result);&#125; 还是一样的套路啊，通过AutoProxyRegistrar.class加上生成代理功能。ProxyCachingConfiguration.class这个不看都知道就是为了创建切面的。 1234567891011121314151617181920212223242526272829303132@Configuration@Role(BeanDefinition.ROLE_INFRASTRUCTURE)public class ProxyCachingConfiguration extends AbstractCachingConfiguration &#123; @Bean(name = CacheManagementConfigUtils.CACHE_ADVISOR_BEAN_NAME) @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public BeanFactoryCacheOperationSourceAdvisor cacheAdvisor() &#123; BeanFactoryCacheOperationSourceAdvisor advisor = new BeanFactoryCacheOperationSourceAdvisor(); advisor.setCacheOperationSource(cacheOperationSource()); advisor.setAdvice(cacheInterceptor()); if (this.enableCaching != null) &#123; advisor.setOrder(this.enableCaching.&lt;Integer&gt;getNumber(&quot;order&quot;)); &#125; return advisor; &#125; @Bean @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public CacheOperationSource cacheOperationSource() &#123; return new AnnotationCacheOperationSource(); &#125; @Bean @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public CacheInterceptor cacheInterceptor() &#123; CacheInterceptor interceptor = new CacheInterceptor(); interceptor.configure(this.errorHandler, this.keyGenerator, this.cacheResolver, this.cacheManager); interceptor.setCacheOperationSource(cacheOperationSource()); return interceptor; &#125;&#125; 切面是BeanFactoryCacheOperationSourceAdvisor，pointcut是CacheOperationSourcePointcut，Advice是CacheInterceptor，通知切面和Advice中包含AnnotationCacheOperationSource。这是真的和事务很像啊。功事务切面的经验猜测AnnotationCacheOperationSource的作用就是解析缓存注解的。 CacheOperationSourcePointcut12345@Overridepublic boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; CacheOperationSource cas = getCacheOperationSource(); return (cas != null &amp;&amp; !CollectionUtils.isEmpty(cas.getCacheOperations(method, targetClass)));&#125; 老实说，这个和事务是一个套路，我在这重点说下解析那部分，代码 123AbstractFallbackCacheOperationSource#getCacheOperations --&gt;AbstractFallbackCacheOperationSource#computeCacheOperations --&gt;AnnotationCacheOperationSource#findCacheOperations 123protected Collection&lt;CacheOperation&gt; findCacheOperations(Method method) &#123; return determineCacheOperations(parser -&gt; parser.parseCacheAnnotations(method));&#125; 12345678910111213141516171819@Nullableprotected Collection&lt;CacheOperation&gt; determineCacheOperations(CacheOperationProvider provider) &#123; Collection&lt;CacheOperation&gt; ops = null; for (CacheAnnotationParser parser : this.annotationParsers) &#123; Collection&lt;CacheOperation&gt; annOps = provider.getCacheOperations(parser); if (annOps != null) &#123; if (ops == null) &#123; ops = annOps; &#125; else &#123; Collection&lt;CacheOperation&gt; combined = new ArrayList&lt;&gt;(ops.size() + annOps.size()); combined.addAll(ops); combined.addAll(annOps); ops = combined; &#125; &#125; &#125; return ops;&#125; this.annotationParsers是在构造函数中传入的。 provider的入参是parser -&gt; parser.parseCacheAnnotations(method)，所以看 SpringCacheAnnotationParser#parseCacheAnnotations，跟踪代码到 这里会把不同的注解解析成不同的对象，下面汇总下对应关系： Cacheable——CacheableOperation CacheEvict——CacheEvictOperation CachePut——CachePutOperation CacheInterceptor12345678910111213141516171819public Object invoke(final MethodInvocation invocation) throws Throwable &#123; Method method = invocation.getMethod(); CacheOperationInvoker aopAllianceInvoker = () -&gt; &#123; try &#123; return invocation.proceed(); &#125; catch (Throwable ex) &#123; throw new CacheOperationInvoker.ThrowableWrapper(ex); &#125; &#125;; try &#123; return execute(aopAllianceInvoker, invocation.getThis(), method, invocation.getArguments()); &#125; catch (CacheOperationInvoker.ThrowableWrapper th) &#123; throw th.getOriginal(); &#125;&#125; 这里先记下CacheOperationInvoker的作用就是进行火炬传递的。 12345678910111213141516protected Object execute(CacheOperationInvoker invoker, Object target, Method method, Object[] args) &#123; // Check whether aspect is enabled (to cope with cases where the AJ is pulled in automatically) if (this.initialized) &#123; Class&lt;?&gt; targetClass = getTargetClass(target); CacheOperationSource cacheOperationSource = getCacheOperationSource(); if (cacheOperationSource != null) &#123; Collection&lt;CacheOperation&gt; operations = cacheOperationSource.getCacheOperations(method, targetClass); if (!CollectionUtils.isEmpty(operations)) &#123; return execute(invoker, method, new CacheOperationContexts(operations, method, args, target, targetClass)); &#125; &#125; &#125; return invoker.invoke();&#125; execute的入参是CacheOperationInvoker，被代理对象，执行的方法，方法参数 initialized这个参数在afterSingletonsInstantiated后就变成true了。 这行的作用就是通过方法和class对象生成一个MethodClassKey对象，然后用这个对象从缓存中获取到之前解析得到对象。 接着在执行下一个execute方法前会把必须信息包装成CacheOperationContexts对象。看第二个execute方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private Object execute(final CacheOperationInvoker invoker, Method method, CacheOperationContexts contexts) &#123; //非关键代码 ..... //先处理@CacheEvicts的逻辑,,其实就是掉clear方法 // Process any early evictions processCacheEvicts(contexts.get(CacheEvictOperation.class), true, CacheOperationExpressionEvaluator.NO_RESULT); //处理@Cacheable的逻辑,,其实就是掉get方法 // Check if we have a cached item matching the conditions Cache.ValueWrapper cacheHit = findCachedItem(contexts.get(CacheableOperation.class)); // Collect puts from any @Cacheable miss, if no cached item is found List&lt;CachePutRequest&gt; cachePutRequests = new LinkedList&lt;&gt;(); //如果缓存没命中或者不是使用的@Cacheable注解 if (cacheHit == null) &#123; //处理@Cacheable的逻辑，收集插入请求,插入缓存的值需要调用被代理方法 collectPutRequests(contexts.get(CacheableOperation.class), CacheOperationExpressionEvaluator.NO_RESULT, cachePutRequests); &#125; Object cacheValue; Object returnValue; //如果缓存命中了 if (cacheHit != null &amp;&amp; !hasCachePut(contexts)) &#123; // If there are no put requests, just use the cache hit cacheValue = cacheHit.get(); //直接返回缓存中的值 returnValue = wrapCacheValue(method, cacheValue); &#125; else &#123; //在这里调用被代理方法 // Invoke the method if we don&#x27;t have a cache hit returnValue = invokeOperation(invoker); cacheValue = unwrapReturnValue(returnValue); &#125; //处理@CachePut注解,收集put请求 // Collect any explicit @CachePuts collectPutRequests(contexts.get(CachePutOperation.class), cacheValue, cachePutRequests); //处理put请求，其实就是掉put方法 // Process any collected put requests, either from @CachePut or a @Cacheable miss for (CachePutRequest cachePutRequest : cachePutRequests) &#123; cachePutRequest.apply(cacheValue); &#125; // Process any late evictions processCacheEvicts(contexts.get(CacheEvictOperation.class), false, cacheValue); return returnValue;&#125; 我们重点看Cacheable注解的。 获取方法方法上的Cacheable对应的操作类CacheableOperation。 12345678910111213141516171819202122232425262728293031323334353637private Cache.ValueWrapper findCachedItem(Collection&lt;CacheOperationContext&gt; contexts) &#123; Object result = CacheOperationExpressionEvaluator.NO_RESULT; for (CacheOperationContext context : contexts) &#123; if (isConditionPassing(context, result)) &#123; Object key = generateKey(context, result); Cache.ValueWrapper cached = findInCaches(context, key); if (cached != null) &#123; return cached; &#125; else &#123; 。。。 &#125; &#125; &#125; return null;&#125;private Cache.ValueWrapper findInCaches(CacheOperationContext context, Object key) &#123; for (Cache cache : context.getCaches()) &#123; Cache.ValueWrapper wrapper = doGet(cache, key); if (wrapper != null) &#123; 。。。 return wrapper; &#125; &#125; return null;&#125;protected Cache.ValueWrapper doGet(Cache cache, Object key) &#123; try &#123; return cache.get(key); &#125; catch (RuntimeException ex) &#123; getErrorHandler().handleCacheGetError(ex, cache, key); return null; // If the exception is handled, return a cache miss &#125;&#125; 然后从Cache对象中遍历获取，找到就返回。如果缓存没命中呢？流程代码接着走， 如果缓存没命中，这时就需要执行业务代码，获取值了，而Spring是先收集下@Cacheable的需要设置值的缓存，并生成对应的CachePutRequest对象。接着会就执行这段代码 这个invokeOperation()就是进行火炬传递，执行目标方法的，有值了，Spring这时还没有去往缓存中放值，而是好要收集下@CachePut的CachePutRequest。 最后 这了就是真正的往缓存中设置值的代码了。apply方法只是调用了Cache.put方法而已。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"15-Spring缓存注解的使用","slug":"spring/15-Spring缓存注解的使用","date":"2021-11-25T12:00:23.000Z","updated":"2022-03-23T09:03:55.446Z","comments":true,"path":"blog/spring/15-Spring缓存注解的使用/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/15-Spring%E7%BC%93%E5%AD%98%E6%B3%A8%E8%A7%A3%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"配置123456789101112131415161718192021222324252627282930@Component@Configuration@EnableCachingpublic class CacheBean &#123; @Bean public RedisCache redisCache(RedisTemplate redisTemplate) &#123; RedisCache redisCache = new RedisCache(); redisCache.setName(&quot;redisCache&quot;); redisCache.setRedisTemplate(redisTemplate); return redisCache; &#125; @Bean public FactoryBean&lt;ConcurrentMapCache&gt; mapCache() &#123; ConcurrentMapCacheFactoryBean bean = new ConcurrentMapCacheFactoryBean(); bean.setName(&quot;mapCache&quot;); return bean; &#125; @Bean public CacheManager simpleCacheManager(Cache redisCache, Cache mapCache) &#123; SimpleCacheManager simpleCacheManager = new SimpleCacheManager(); List&lt;Cache&gt; list = new ArrayList&lt;&gt;(); list.add(redisCache); list.add(mapCache); simpleCacheManager.setCaches(list); return simpleCacheManager; &#125;&#125; RedisCache是实现了Spring的Cache接口的一个类。现在要配置下RedisTemplate RedisTemplate: 1234567891011121314151617181920212223242526272829303132333435@Component@PropertySource(&quot;classpath:core/redis.properties&quot;)@Configurationpublic class RedisConfig &#123; @Value(&quot;$&#123;redis.host&#125;&quot;) private String hostName; @Value(&quot;$&#123;redis.port&#125;&quot;) private int port; private JedisPoolConfig jedisPoolConfig() &#123; JedisPoolConfig jedisPoolConfig = new JedisPoolConfig();// jedisPoolConfig.setMaxIdle(maxIdle);// jedisPoolConfig.setMaxWaitMillis(maxWaitMillis);// jedisPoolConfig.setTestOnBorrow(testOnBorrow); return jedisPoolConfig; &#125; public JedisConnectionFactory jedisConnectionFactory()&#123; JedisConnectionFactory jedisConnectionFactory = new JedisConnectionFactory(); jedisConnectionFactory.setHostName(hostName); jedisConnectionFactory.setPort(port);// jedisConnectionFactory.setPassword(password); jedisConnectionFactory.setPoolConfig(jedisPoolConfig()); return jedisConnectionFactory; &#125; @Bean public RedisTemplate&lt;String, String&gt; redisTemplate() &#123; RedisTemplate&lt;String, String&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(jedisConnectionFactory()); return template; &#125;&#125; 使用养成习惯，先创建接口 12345678public interface CacheService &#123; String redisGetData(String id); String redisPutData(String id); String mapGetData(String id); String mapPutData(String id);&#125; 实现 12345678910111213141516171819202122232425262728@Service@Slf4jpublic class CacheServiceImpl implements CacheService &#123; @Cacheable(cacheNames = &quot;redisCache&quot;,key = &quot;&#x27;xyz&#x27; + #id&quot;) @Override public String redisGetData(String id) &#123; System.out.println(&quot;======CacheServiceImpl.queryData&quot;); return &quot;测试-redis-Cacheable&quot;; &#125; @CachePut(cacheNames = &quot;redisCache&quot;, key = &quot;&#x27;xyz&#x27; + #id&quot;) @Override public String redisPutData(String id) &#123; System.out.println(&quot;======CacheServiceImpl.queryData&quot;); return &quot;测试-redis-CachePut&quot;; &#125; @Override public String mapGetData(String id) &#123; return null; &#125; @Override public String mapPutData(String id) &#123; return null; &#125;&#125; 还有很多的，但是我就常用@Cacheable和@CachePut","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"14-Spring中使用事物编程","slug":"spring/14-Spring中使用事物编程","date":"2021-11-25T12:00:22.000Z","updated":"2022-03-23T09:03:55.444Z","comments":true,"path":"blog/spring/14-Spring中使用事物编程/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/14-Spring%E4%B8%AD%E4%BD%BF%E7%94%A8%E4%BA%8B%E7%89%A9%E7%BC%96%E7%A8%8B/","excerpt":"","text":"声明式事物的弊端 其实PROPAGATION_REQUIRES_NEW这个传播级别最明显，就是会获取多个链接对象。 声明试事物只能作用于方法； 而且如果在方法那调用还有限制，不然会使@Transaction失效。 比如： 12345678910class A &#123; @Transactional() void fun1() &#123; // something do &#125; void fun2() &#123; fun1(); &#125;&#125; 这样定义了一个类，在项目中如果发生这样的调用 12345678Class B &#123; @Autowire A a; void fun1() &#123; a.fun2(); &#125;&#125; 那么A的fun1的注解就会失效。因为A的fun2没有注解。就算B中的a对象是代理对象，由于A的fun2没有注解，所以不会走到事物切面，而是直接使用真实对象调用了fun1。 如果A改成 12345678910111213class A &#123; @Autowire A a; @Transactional() void fun1() &#123; // something do &#125; void fun2() &#123; a.fun1(); &#125;&#125; 这样事物注解就会生效。 如果在使用时不注意，会导致事务传播得很深，也就是会导致事务的流程过长了，而且在优化代码时，过深也会导致很难下手。 声明式事物所以我不喜欢用声明式事物，更喜欢用编程式事物 编程式事物我是喜欢用这中方式 123456789101112131415161718@Beanpublic TransactionTemplate transactionTemplate(PlatformTransactionManager platformTransactionManager) &#123; TransactionTemplate transactionTemplate = new TransactionTemplate(); // 可以设置只读 transactionTemplate.setTransactionManager(platformTransactionManager); return transactionTemplate;&#125;@Autowiredprivate TransactionTemplate transactionTemplate;public void txt() &#123; transactionTemplate.execute(status -&gt; &#123; addArea(&quot;xyz&quot;); addGoods(&quot;xyzGoods&quot;); return 1; &#125;);&#125; 这样使用。 看这个源码: 1234567891011121314151617181920212223242526public &lt;T&gt; T execute(TransactionCallback&lt;T&gt; action) throws TransactionException &#123; Assert.state(this.transactionManager != null, &quot;No PlatformTransactionManager set&quot;); if (this.transactionManager instanceof CallbackPreferringPlatformTransactionManager) &#123; return ((CallbackPreferringPlatformTransactionManager) this.transactionManager).execute(this, action); &#125; else &#123; TransactionStatus status = this.transactionManager.getTransaction(this); T result; try &#123; result = action.doInTransaction(status); &#125; catch (RuntimeException | Error ex) &#123; // Transactional code threw application exception -&gt; rollback rollbackOnException(status, ex); throw ex; &#125; catch (Throwable ex) &#123; // Transactional code threw unexpected exception -&gt; rollback rollbackOnException(status, ex); throw new UndeclaredThrowableException(ex, &quot;TransactionCallback threw undeclared checked exception&quot;); &#125; this.transactionManager.commit(status); return result; &#125;&#125; 代码就和注解的一样。 甚至可以使用PlatformTransactionManager 123456789101112131415161718@AutowiredPlatformTransactionManager platformTransactionManager;public void xxx() &#123; DefaultTransactionDefinition defaultTransactionDefinition = new DefaultTransactionDefinition(); defaultTransactionDefinition.setPropagationBehavior(0); TransactionStatus transaction = platformTransactionManager.getTransaction(defaultTransactionDefinition); try &#123; System.out.println(&quot;业务代码&quot;); &#125; catch (Exception e) &#123; platformTransactionManager.rollback(transaction); &#125; platformTransactionManager.commit(transaction);&#125; TransactionSynchronizationManager 只有使用声明式事物才有效 TransactionSynchronizationManager这个类在Spring的事物中处理处理链接资源的绑定和一些状态的记录，还有一个就是触发器。 在源码中，比如事物提交了，都有执行一些方法 还有很多，但它们这些方法都是在不同的阶段执行的，而且这些都是Spring提供出来的扩展点。我们看一个方法就知道了。 1234567891011121314protected final void triggerBeforeCommit(DefaultTransactionStatus status) &#123; if (status.isNewSynchronization()) &#123; if (status.isDebug()) &#123; logger.trace(&quot;Triggering beforeCommit synchronization&quot;); &#125; TransactionSynchronizationUtils.triggerBeforeCommit(status.isReadOnly()); &#125;&#125;public static void triggerBeforeCommit(boolean readOnly) &#123; for (TransactionSynchronization synchronization : TransactionSynchronizationManager.getSynchronizations()) &#123; synchronization.beforeCommit(readOnly); &#125;&#125; 也就是，只要我们在方法中调用TransactionSynchronizationManager.registerSynchronization()方法传入一个对应的对象，我们就能在对应的阶段做一些额外的业务操作。而这个对象的类Spring也为我们提供了一个适配器，我们只要继承它然后重写自己的关注的方法就好了，这个适配器是TransactionSynchronization 123456789101112131415161718192021222324252627282930313233343536public abstract class TransactionSynchronizationAdapter implements TransactionSynchronization, Ordered &#123; @Override public int getOrder() &#123; return Ordered.LOWEST_PRECEDENCE; &#125; @Override public void suspend() &#123; &#125; @Override public void resume() &#123; &#125; @Override public void flush() &#123; &#125; @Override public void beforeCommit(boolean readOnly) &#123; &#125; @Override public void beforeCompletion() &#123; &#125; @Override public void afterCommit() &#123; &#125; @Override public void afterCompletion(int status) &#123; &#125;&#125; 比如我定义这样一个类 1234567public class XyzTransactionCommitAfter extends TransactionSynchronizationAdapter &#123; @Override public void afterCommit() &#123; System.out.println(&quot;做一些其他事情&quot;); &#125;&#125; 然后在业务代码中 12345678public void transation() throws JamesException &#123; TransactionSynchronizationManager.registerSynchronization(new XyzTransactionCommitAfter()); try &#123; transationService.addArea(&quot;xyz&quot;); transationService.addGoods(&quot;xyzGoods&quot;); &#125; catch (Exception e) &#123; &#125;&#125; 那么在事物提交后就能执行到System.out.println(&quot;做一些其他事情&quot;);这行代码。 注意了，不要想用多线程来玩这个，这个是放在一个ThreadLocal中的。 还有 12ConnectionHolder resource = (ConnectionHolder) TransactionSynchronizationManager.getResource(dataSource); 可以获取到链接对象，然后做一些骚操作。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"13-Spring的事务传播属性","slug":"spring/13-Spring的事务传播属性","date":"2021-11-25T12:00:21.000Z","updated":"2022-03-23T09:03:55.434Z","comments":true,"path":"blog/spring/13-Spring的事务传播属性/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/13-Spring%E7%9A%84%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD%E5%B1%9E%E6%80%A7/","excerpt":"","text":"先看下这里简单介绍事务传播属性 重点讲这三个 PROPAGATION_REQUIRED PROPAGATION_REQUIRES_NEW PROPAGATION_NESTED 在这先定义下伪代码： 123456789101112131415class fake &#123; @Transactional fun1() @Transactional fun2(); @Transactional fun4() &#123; fun1() fun2() &#125;&#125;fake.fun4(); 在TransactionManager.setGlobalRollbackOnParticipationFailure(false);这个前提下，下面的伪代码才成立，原因看调试时发现了一个问题 fun1()和fun2()的执行过程分别用process1和process2表示，从上节的知识，可以得出fun4()的执行过程伪代码。 123456789101112131415161718192021222324252627282930313233343536373839404142info = createTransactionIfNecessary &#123; DefaultTransactionStatus status = transactionManager.getTransaction &#123; DataSourceTransactionObject txObject = transactionManager.doGetTransaction() &#123; DataSourceTransactionObject txObject = new DataSourceTransactionObject(); ConnectionHolder connectionHolder = threadLocal.get(); txObject.setConnectionHolder(connectionHolder) return txObject; &#125; DefaultTransactionStatus status = newTransactionStatus(txObject, newTransaction=true); // 该if会返回ture if (txObject.connectionHolder == null || txObject.connectionHolder.transactionActive == false) &#123; // 获取链接 transactionManager.doBegin() &#123; if (txObject.connectionHolder == null) &#123; txObject.connectionHolder = new ConnectionHolder(dataSource.getConnection); &#125; txObject.connectionHolder.connection.autoCommit = false; threadLocal.set(&lt;dataSource,txObject.connectionHolder&gt;); txObject.connectionHolder.transactionActive = true; &#125; &#125; return status; &#125; return new TransactionInfo(status);&#125;try &#123; fake.fun4() &#123; process1 process2 &#125;&#125; catch &#123; if info.status.newTransaction &#123; transactionManager.doRollback(); threadLocal.remove(dataSource) &#125; throw e&#125;if info.status.newTransaction &#123; transactionManager.doCommit(); threadLocal.remove(dataSource)&#125; 而process1，的大致执行流程如下： 12345678910111213141516171819202122232425262728info = createTransactionIfNecessary &#123; DefaultTransactionStatus status = transactionManager.getTransaction &#123; DataSourceTransactionObject txObject = transactionManager.doGetTransaction() &#123; DataSourceTransactionObject txObject = new DataSourceTransactionObject(); ConnectionHolder connectionHolder = threadLocal.get(); txObject.setConnectionHolder(connectionHolder) return txObject; &#125; // 该if会返回ture if (txObject.connectionHolder != null &amp;&amp; txObject.connectionHolder.transactionActive != false) &#123; return handleExistingTransaction(txObject) &#123; DefaultTransactionStatus status = newTransactionStatus(txObject, newTransaction=false); //do something; return status; &#125; &#125; &#125; return new TransactionInfo(status);&#125;try &#123; fake.fun1();&#125; catch &#123; if info.status.newTransaction transactionManager.doRollback(); throw e&#125;if info.status.newTransaction transactionManager.doCommit(); process2一样。 上边的process1中的handleExistingTransaction就是今天重点代码！ 下面分情况讲该代码 PROPAGATION_REQUIRED这个是@Transactional默认的传播级别。 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。也就是在有方法的嵌套时，使用的都是同一个链接。这是最常见的选择。 我有个service，里面有几个方法： 123456789101112131415161718192021222324@Transactional@Overridepublic void transation() throws JamesException &#123; ConnectionHolder resource = (ConnectionHolder) TransactionSynchronizationManager.getResource(dataSource); System.out.println(&quot;-----------------&quot; + resource.getConnection()); transationService.addArea(&quot;xyz&quot;); transationService.addGoods(&quot;xyzGoods&quot;);&#125;@Transactional()public int addArea(String name) &#123; ConnectionHolder resource = (ConnectionHolder) TransactionSynchronizationManager.getResource(dataSource); System.out.println(&quot;-----------------&quot; + resource.getConnection()); int i = areaService.insertOne(name); return i;&#125;@Transactionalpublic void addGoods(String name) &#123; ConnectionHolder resource = (ConnectionHolder) TransactionSynchronizationManager.getResource(dataSource); System.out.println(&quot;-----------------&quot; + resource.getConnection()); int i = goodsService.insertOne(name); if (true) throw new RuntimeException(&quot;ssf&quot;);&#125; TransactionSynchronizationManager.getResource(dataSource);这个在前面就已经讲过了，获取链接后会建立&lt;dataSource,ConnectionHolder&gt;这种关系，并保存到一个threadLocal中。这个方法就是从threadLocal中获取通过dataSource获取链接。 代码很简单，最主要还是打印了Connection对象。 看下答应结果： 可以看到，Connection对象是一样的。完全符合它的定义。现在分析下源码。回到createTransactionIfNecessary 跟踪代码： 12TransactionAspectSupport#createTransactionIfNecessary ---&gt;AbstractPlatformTransactionManager#getTransaction 先debug，看这个transaction打印的值。 第一次： 第二次： 第三次： 从上节就已经知道，在第一次进来这个方法的时候connectionHolder对象是空的，因为： 这个时候这行代码TransactionSynchronizationManager.getResource(obtainDataSource())中从ThreadLocal获取不到数据。所以返回为空。在第一次开启了事物后就创建了一个connectionHolder对象了，并开启事物的最后会把这个对象放到ThreadLocal中。而且，第一次调用的方法的对象是代理对象，而且被调用的方法有@Transactional注解，所以在第二次调用方法的时候就能匹配到事务切面，而且也会走到getTransactino方法，也就是说，在doGetTransaction的时候就可以从这个ThreadLocal中获取的第一次创建的connectionHolder对象，而且下面的isExistingTransaction方法会返回true。第三次也一样。接着执行到这里 因为 第二次和第三次的时候connectionHolder对象不为空，而且transactionActive&#x3D;&#x3D;true。所以重点看 handleExistingTransaction方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879private TransactionStatus handleExistingTransaction( TransactionDefinition definition, Object transaction, boolean debugEnabled) throws TransactionException &#123; //不允许有事务，直接异常 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NEVER) &#123; throw new IllegalTransactionStateException( &quot;Existing transaction found for transaction marked with propagation &#x27;never&#x27;&quot;); &#125; //以非事务方式执行操作，如果当前存在事务，就把当前事务挂起 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NOT_SUPPORTED) &#123; if (debugEnabled) &#123; logger.debug(&quot;Suspending current transaction&quot;); &#125; //挂起当前事务 Object suspendedResources = suspend(transaction); boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); //修改事务状态信息，把事务的一些信息存储到当前线程中，ThreadLocal中 return prepareTransactionStatus( definition, null, false, newSynchronization, debugEnabled, suspendedResources); &#125; if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW) &#123; if (debugEnabled) &#123; logger.debug(&quot;Suspending current transaction, creating new transaction with name [&quot; + definition.getName() + &quot;]&quot;); &#125; //挂起 SuspendedResourcesHolder suspendedResources = suspend(transaction); try &#123; return startTransaction(definition, transaction, debugEnabled, suspendedResources); &#125; catch (RuntimeException | Error beginEx) &#123; resumeAfterBeginException(transaction, suspendedResources, beginEx); throw beginEx; &#125; &#125; if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; if (!isNestedTransactionAllowed()) &#123; throw new NestedTransactionNotSupportedException( &quot;Transaction manager does not allow nested transactions by default - &quot; + &quot;specify &#x27;nestedTransactionAllowed&#x27; property with value &#x27;true&#x27;&quot;); &#125; if (debugEnabled) &#123; logger.debug(&quot;Creating nested transaction with name [&quot; + definition.getName() + &quot;]&quot;); &#125; //默认是可以嵌套事务的 if (useSavepointForNestedTransaction()) &#123; // Create savepoint within existing Spring-managed transaction, // through the SavepointManager API implemented by TransactionStatus. // Usually uses JDBC 3.0 savepoints. Never activates Spring synchronization. //这里的newTransaction 为 false DefaultTransactionStatus status = prepareTransactionStatus(definition, transaction, false, false, debugEnabled, null); //创建回滚点 status.createAndHoldSavepoint(); return status; &#125; else &#123; // Nested transaction through nested begin and commit/rollback calls. // Usually only for JTA: Spring synchronization might get activated here // in case of a pre-existing JTA transaction. return startTransaction(definition, transaction, debugEnabled, null); &#125; &#125; // Assumably PROPAGATION_SUPPORTS or PROPAGATION_REQUIRED. if (debugEnabled) &#123; logger.debug(&quot;Participating in existing transaction&quot;); &#125; if (isValidateExistingTransaction()) &#123; ..... &#125; boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); //如果第二次进来还是PROPAGATION_REQUIRED，走这里，newTransaction为false return prepareTransactionStatus(definition, transaction, false, newSynchronization, debugEnabled, null);&#125; 这个代码就是对事物传播级别做不同的处理的。从代码可知PROPAGATION_REQUIRED，这个级别会走到这里： 这代码块的重点就是在prepareTransactionStatus，这个方法的重点就是创建DefaultTransactionStatus对象的，而且这个对象的newTransaction设置为false。从上一节的知识可以知DefaultTransactionStatus.newTransaction是用来判断事务是否需要提交和回滚的。为false意思就是不提交事物不执行回滚。也就是说，对于伪代码process1，do something就是什么都不做，就变成了如下： 123456789101112131415161718192021222324252627info = createTransactionIfNecessary &#123; DefaultTransactionStatus status = transactionManager.getTransaction &#123; DataSourceTransactionObject txObject = transactionManager.doGetTransaction() &#123; DataSourceTransactionObject txObject = new DataSourceTransactionObject(); ConnectionHolder connectionHolder = threadLocal.get(); txObject.setConnectionHolder(connectionHolder) return txObject; &#125; // 该if会返回ture if (txObject.connectionHolder != null &amp;&amp; txObject.connectionHolder.transactionActive != false) &#123; return handleExistingTransaction(txObject) &#123; DefaultTransactionStatus status = newTransactionStatus(txObject, newTransaction=false); return status; &#125; &#125; &#125; return new TransactionInfo(status);&#125;try &#123; fake.fun1();&#125; catch &#123; if info.status.newTransaction transactionManager.doRollback(); throw e&#125;if info.status.newTransaction transactionManager.doCommit(); process2一样。整个fun4()的流程就变成了: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263info = createTransactionIfNecessary &#123; DefaultTransactionStatus status = transactionManager.getTransaction &#123; DataSourceTransactionObject txObject = transactionManager.doGetTransaction() &#123; DataSourceTransactionObject txObject = new DataSourceTransactionObject(); ConnectionHolder connectionHolder = threadLocal.get(); txObject.setConnectionHolder(connectionHolder) return txObject; &#125; DefaultTransactionStatus status = newTransactionStatus(txObject, newTransaction=true); // 该if会返回ture if (txObject.connectionHolder == null || txObject.connectionHolder.transactionActive == false) &#123; transactionManager.doBegin() &#123; if (txObject.connectionHolder == null) &#123; txObject.connectionHolder = new ConnectionHolder(dataSource.getConnection); &#125; txObject.connectionHolder.connection.autoCommit = false; threadLocal.set(&lt;dataSource,txObject.connectionHolder&gt;); txObject.connectionHolder.transactionActive = true; &#125; &#125; return status; &#125; return new TransactionInfo(status);&#125;try &#123; fake.fun4() &#123; process1: &#123; info = createTransactionIfNecessary &#123; DefaultTransactionStatus status = transactionManager.getTransaction &#123; DataSourceTransactionObject txObject = transactionManager.doGetTransaction() &#123; DataSourceTransactionObject txObject = new DataSourceTransactionObject(); ConnectionHolder connectionHolder = threadLocal.get(); txObject.setConnectionHolder(connectionHolder) return txObject; &#125; // 该if会返回ture if (txObject.connectionHolder != null &amp;&amp; txObject.connectionHolder.transactionActive != false) &#123; return handleExistingTransaction(txObject) &#123; DefaultTransactionStatus status = newTransactionStatus(txObject, newTransaction=false); return status; &#125; &#125; &#125; return new TransactionInfo(status); &#125; try &#123; fake.fun1(); // 因为status.newTransaction为false，所以提交和回顾代码去掉了 &#125; catch &#123; throw e &#125; &#125; &#123; process2 &#125; &#125;&#125; catch &#123; transactionManager.doRollback(); throw e&#125;transactionManager.doCommit(); 这样了。也就是，整个调用过程都是在一个事物中，因为整个调用过程都是使用同一个链接对象。从这个伪代码中可以看到，提交是在栈尾的栈帧代表的方法提交的，事务回滚也是，如果process1或process2其中一个抛出异常，都会被栈尾的栈帧代表的方法捕获，从而执行真正的事务回滚，然后向外层抛出异常。 调试时发现了一个问题在上边说的是fun4能抛出异常的情况，那如果fun4捕获了异常，但不抛出异常，那会回滚吗？下面研究这个问题。 代码这样定义： 12345678910111213141516171819202122232425@Transactional@Overridepublic void transation() throws JamesException &#123; try &#123; transationService.addArea(&quot;xyz&quot;); transationService.addGoods(&quot;xyzGoods&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125;@Transactional()public int addArea(String name) &#123; int i = areaService.insertOne(name); return i;&#125;@Transactionalpublic void addGoods(String name) &#123; int i = goodsService.insertOne(name); if (true) throw new RuntimeException(&quot;ssf&quot;);&#125;测试：transation(); 这代码有个特点，就是调用的方法会吞了异常，而被包含的方法不会吞异常。 如果按伪代码的流程就会走到doCommit()，也就是事物提交了。但是真的是这样吗？现在TransactionAspectSupport#invokeWithinTransaction方法的最后打个断点： 最后会走到这里 执行回滚了。现在分析原因。 看下源码就知道shouldCommitOnGlobalRollbackOnly()这个方法是默认情况是返回false的，所以看defStatus.isGlobalRollbackOnly()这个。这个defStatus是DefaultTransactionStatus。看这个源码 12345@Overridepublic boolean isGlobalRollbackOnly() &#123; return ((this.transaction instanceof SmartTransactionObject) &amp;&amp; ((SmartTransactionObject) this.transaction).isRollbackOnly());&#125; (this.transaction instanceof SmartTransactionObject)这个肯定为true的，因为 而且 问题就出现在(SmartTransactionObject) this.transaction).isRollbackOnly()这段代码了。看isRollbackOnly()源码 也就是说看ConnectionHolder就好了： 可以看到默认值为false的，只要调用了setRollbackOnly的时候才会为true。跟踪下，这个又会被DataSourceTransactionObject调用 再跟踪，会被这里调用 先记住方法doSetRollbackOnly。现在回想下流程。就是process2那里抛出异常了。我们可以看下catch的语句块，跟踪下这里代码。 由于TransactionStatus.newTransaction &#x3D; false，所以最终会走到else代码块： 走到回滚代码的else代码块了，也就是，现在造成这种原因有两个 DefaultTransactionStatus#isLocalRollbackOnly AbstractPlatformTransactionManager#isGlobalRollbackOnParticipationFailure() 这两个方法。 DefaultTransactionStatus#isLocalRollbackOnly先看第一个方法 这个rollbackOnly默认值是false的，而这个setRollbackOnly()方法，在源码中没有调用，这个方法是给给业务代码调用的（编程式事务）。所以排除这个看第二个原因 AbstractPlatformTransactionManager#isGlobalRollbackOnParticipationFailure() 这个是TransactionManager的一个成员属性，翻译为“在发生部分错误后进行全局回滚”，从名字就知道这个属性的作用了。那是不是我只要TransactionManager.setGlobalRollbackOnParticipationFailure(false)，那某个嵌套方法错误，主方法捕捉错误，这种情况下不就不会发生回滚了？ 继续跟踪代码 没有执行那段代码了！！！问题真的出现在这个属性上！！最后会执行到提交代码。不过并不建议设置修改这个值，因为这样如果开发人员代码有问题，那就容易导致数据库的数据异常了。因为这有部分代码提交代码了！ 回到源码，这时代码没有执行doSetRollbackOnly方法，报错然后被调用方捕捉到，接着调用方就执行了TransactionManager.commit方法。最后看源码 由于没有把ConnectionHolder的rollback属性设置为true，所以会走到processCommit方法。所以会提交事务。 总结出现这个问题的原因是因为，最外层的方法把异常吞了，而里面的方法会抛出异常，这样才会出现了上面出现的现象——Spring 帮我们回滚了。也就是说只要最外层方法的内部中，有方法调用了 这个方法，而且TransactionManager没有setGlobalRollbackOnParticipationFailure(false)执行这行代码，才会出现这样的现象。 老实说，这样的设计挺合理的，只要方法里面的方法抛出异常了，就算外层的方法不抛出异常，都会帮我们回滚。这样就可以减少人为代码的错误，而导致数据错误了。这样设置后，由于fun4捕捉了错误，那就会执行打TransactionManager.commit方法。 结论对于事物，只要一个方法的异常被Spring捕捉到，那在默认的情况下Spring都会帮我们回滚。除非我们通过在新建事务管理器TransactionManager的时候setGlobalRollbackOnParticipationFailure(false)来取消这一特性 经典问题12345678910111213141516171819202122@Transactional@Overridepublic void transation() throws JamesException &#123; transationService.addArea(&quot;xyz&quot;); transationService.addGoods(&quot;xyzGoods&quot;);&#125;@Transactional()public int addArea(String name) &#123; int i = areaService.insertOne(name); return i;&#125;@Transactionalpublic void addGoods(String name) &#123; try &#123; int i = goodsService.insertOne(name); if (true) throw new RuntimeException(&quot;ssf&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 如果这样设计的话，从之前的伪代码可以得出，这个事物是会提交的。因为没有走到 TransactionAspectSupport#invokeWithinTransaction的catch语句快。 PROPAGATION_REQUIRES_NEW新建事务，如果当前存在事务，把当前事务挂起。 其实从jdbc中的事物就可以猜到，这个传播级别的实现方式就是拿一个新的Condition对象。这样定义: 1234567891011121314151617181920212223@Transactional@Overridepublic void transation() throws JamesException &#123; ConnectionHolder resource = (ConnectionHolder) TransactionSynchronizationManager.getResource(dataSource); System.out.println(&quot;-----------------&quot; + resource.getConnection()); transationService.addArea(&quot;xyz&quot;); transationService.addGoods(&quot;xyzGoods&quot;);&#125;@Transactional(propagation = Propagation.REQUIRES_NEW)public int addArea(String name) &#123; ConnectionHolder resource = (ConnectionHolder) TransactionSynchronizationManager.getResource(dataSource); System.out.println(&quot;-----------------&quot; + resource.getConnection()); int i = areaService.insertOne(name); return i;&#125;@Transactional(propagation = Propagation.REQUIRES_NEW)public void addGoods(String name) &#123; ConnectionHolder resource = (ConnectionHolder) TransactionSynchronizationManager.getResource(dataSource); System.out.println(&quot;-----------------&quot; + resource.getConnection()); int i = goodsService.insertOne(name);&#125; 和猜侧的一样。还是回到AbstractPlatformTransactionManager#getTransaction方法 第一次过程和上面一样 第二次还是会获取到第一次创建的connection对象还会走到handleExistingTransaction方法，但在由于隔离级别是 PROPAGATION_REQUIRES_NEW没，所以在handleExistingTransaction方法中会走到这里： 先看 1SuspendedResourcesHolder suspendedResources = suspend(transaction); 1234567891011121314151617181920212223242526272829303132333435protected final SuspendedResourcesHolder suspend(@Nullable Object transaction) throws TransactionException &#123; if (TransactionSynchronizationManager.isSynchronizationActive()) &#123; List&lt;TransactionSynchronization&gt; suspendedSynchronizations = doSuspendSynchronization(); try &#123; Object suspendedResources = null; if (transaction != null) &#123; suspendedResources = doSuspend(transaction); &#125; String name = TransactionSynchronizationManager.getCurrentTransactionName(); TransactionSynchronizationManager.setCurrentTransactionName(null); boolean readOnly = TransactionSynchronizationManager.isCurrentTransactionReadOnly(); TransactionSynchronizationManager.setCurrentTransactionReadOnly(false); Integer isolationLevel = TransactionSynchronizationManager.getCurrentTransactionIsolationLevel(); TransactionSynchronizationManager.setCurrentTransactionIsolationLevel(null); boolean wasActive = TransactionSynchronizationManager.isActualTransactionActive(); TransactionSynchronizationManager.setActualTransactionActive(false); return new SuspendedResourcesHolder( suspendedResources, suspendedSynchronizations, name, readOnly, isolationLevel, wasActive); &#125; catch (RuntimeException | Error ex) &#123; // doSuspend failed - original transaction is still active... doResumeSynchronization(suspendedSynchronizations); throw ex; &#125; &#125; else if (transaction != null) &#123; // Transaction active but no synchronization active. Object suspendedResources = doSuspend(transaction); return new SuspendedResourcesHolder(suspendedResources); &#125; else &#123; // Neither transaction nor synchronization active. return null; &#125;&#125; 先看doSuspend，而这个是在DataSourceTransactionManager中实现的 DataSourceTransactionManager#suspend: 这段代码的的意思就是先txObject.setConnectionHolder(null);，然后执行了解除资源绑定方法（可以理解为threadLocal.remove(connectionHolder)），返回第一次的链接对象。 接着在suspend方法中，把这个第一次链接对象作为参数，传入新建的SuspendedResourcesHolder对象中，接着返回，然后执行了startTransaction方法，然后又开启了一次事物（因为DataSourceTransactionObject.connectionHolder属性为null了） 那么第二次执行到最后，由于DefaultTransactionStatus.newTransaction&#x3D;&#x3D;true，所以第二次的调用时肯定会提交事物的。现在看提交的方法AbstractPlatformTransactionManager#processCommit，在这个方法中有个finally，看里面的方法： 12345678910111213141516171819private void cleanupAfterCompletion(DefaultTransactionStatus status) &#123; status.setCompleted(); if (status.isNewSynchronization()) &#123; TransactionSynchronizationManager.clear(); &#125; //释放连接 if (status.isNewTransaction()) &#123; doCleanupAfterCompletion(status.getTransaction()); &#125; //如果存在挂起的事务 if (status.getSuspendedResources() != null) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Resuming suspended transaction after completion of inner transaction&quot;); &#125; Object transaction = (status.hasTransaction() ? status.getTransaction() : null); //恢复绑定关系 resume(transaction, (SuspendedResourcesHolder) status.getSuspendedResources()); &#125;&#125; 这里： 在第二次的时候这个SuspendedResources是不为null的，里面包含了第一次的链接对象。跟踪resume方法： 12AbstractPlatformTransactionManager#resume --&gt;DataSourceTransactionManager#doResume(第一次的链接对象) 又重新建立绑定关系了。回滚都一样，最后都会调用cleanupAfterCompletion方法，这里就不跟了。 也就是，对于伪代码process1，就变成了如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647info = createTransactionIfNecessary &#123; DefaultTransactionStatus status = transactionManager.getTransaction &#123; DataSourceTransactionObject txObject = transactionManager.doGetTransaction() &#123; DataSourceTransactionObject txObject = new DataSourceTransactionObject(); ConnectionHolder connectionHolder = threadLocal.get(); txObject.setConnectionHolder(connectionHolder) return txObject; &#125; // 该if会返回ture if (txObject.connectionHolder != null &amp;&amp; txObject.connectionHolder.transactionActive != false) &#123; return handleExistingTransaction(txObject) &#123; // 解除旧链接绑定 txObject.connectionHolder = null; oldConnectionHolder = threadLocal.remove(dataSource); SuspendedResourcesHolder suspendedResources = new SuspendedResourcesHolder(oldConnectionHolder) DefaultTransactionStatus status = newTransactionStatus(txObject, newTransaction=true, suspendedResources); // 获取新的链接，并重新绑定 transactionManager.doBegin() &#123; if (txObject.connectionHolder == null) &#123; txObject.connectionHolder = new ConnectionHolder(dataSource.getConnection); &#125; txObject.connectionHolder.connection.autoCommit = false; threadLocal.set(&lt;dataSource,txObject.connectionHolder&gt;); txObject.connectionHolder.transactionActive = true; &#125; return status; &#125; &#125; &#125; return new TransactionInfo(status);&#125;try &#123; fake.fun1();&#125; catch &#123; if info.status.newTransaction &#123; transactionManager.doRollback(); threadLocal.set&lt;dataSource,info.status.suspendedResources.oldConnectionHolder&gt; &#125; throw e&#125;if info.status.newTransaction &#123; transactionManager.doCommit(); threadLocal.set&lt;dataSource,info.status.suspendedResources.oldConnectionHolder&gt;&#125; 整个就变成了: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495info = createTransactionIfNecessary &#123; DefaultTransactionStatus status = transactionManager.getTransaction &#123; DataSourceTransactionObject txObject = transactionManager.doGetTransaction() &#123; DataSourceTransactionObject txObject = new DataSourceTransactionObject(); ConnectionHolder connectionHolder = threadLocal.get(); txObject.setConnectionHolder(connectionHolder) return txObject; &#125; DefaultTransactionStatus status = newTransactionStatus(txObject, newTransaction=true); // 该if会返回ture if (txObject.connectionHolder == null || txObject.connectionHolder.transactionActive == false) &#123; // 获取链接 transactionManager.doBegin() &#123; if (txObject.connectionHolder == null) &#123; txObject.connectionHolder = new ConnectionHolder(dataSource.getConnection); &#125; txObject.connectionHolder.connection.autoCommit = false; threadLocal.set(&lt;dataSource,txObject.connectionHolder&gt;); txObject.connectionHolder.transactionActive = true; &#125; &#125; return status; &#125; return new TransactionInfo(status);&#125;try &#123; fake.fun4() &#123; process1: &#123; info = createTransactionIfNecessary &#123; DefaultTransactionStatus status = transactionManager.getTransaction &#123; DataSourceTransactionObject txObject = transactionManager.doGetTransaction() &#123; DataSourceTransactionObject txObject = new DataSourceTransactionObject(); ConnectionHolder connectionHolder = threadLocal.get(); txObject.setConnectionHolder(connectionHolder) return txObject; &#125; // 该if会返回ture if (txObject.connectionHolder != null &amp;&amp; txObject.connectionHolder.transactionActive != false) &#123; return handleExistingTransaction(txObject) &#123; // 解除旧链接绑定 txObject.connectionHolder = null; oldConnectionHolder = threadLocal.remove(dataSource); SuspendedResourcesHolder suspendedResources = new SuspendedResourcesHolder(oldConnectionHolder) DefaultTransactionStatus status = newTransactionStatus(txObject, newTransaction=true, suspendedResources); // 获取新的链接，并重新绑定 transactionManager.doBegin() &#123; if (txObject.connectionHolder == null) &#123; txObject.connectionHolder = new ConnectionHolder(dataSource.getConnection); &#125; txObject.connectionHolder.connection.autoCommit = false; threadLocal.set(&lt;dataSource,txObject.connectionHolder&gt;); txObject.connectionHolder.transactionActive = true; &#125; return status; &#125; &#125; &#125; return new TransactionInfo(status); &#125; try &#123; fake.fun2(); &#123; 套娃 &#125; &#125; catch &#123; if info.status.newTransaction &#123; transactionManager.doRollback(); threadLocal.set&lt;dataSource, info.status.suspendedResources.oldConnectionHolder&gt; &#125; throw e &#125; if info.status.newTransaction &#123; transactionManager.doCommit(); threadLocal.set&lt;dataSource, info.status.suspendedResources.oldConnectionHolder&gt; &#125; &#125; &#123; process2 &#125; &#125;&#125; catch &#123; if info.status.newTransaction &#123; transactionManager.doRollback(); threadLocal.remove(dataSource) &#125; throw e&#125;if info.status.newTransaction &#123; transactionManager.doCommit(); threadLocal.remove(dataSource)&#125; 简化的伪代码: 1234567891011begin a; //a do something // 解除a的绑定 suspend a; begin b; //b do something commit b; // 恢复a的绑定 resume a; //a do somethingcommit a; PROPAGATION_NESTED如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 在说这个的时候先看下jdbc的回滚点。 先看下这段伪代码 123456Connection connection = ...;insert1connection.setSavepoint(&quot;a1&quot;);insert2connection.setSavepoint(&quot;a2&quot;);connection.commit(); 如果代码变成了这样 1234567Connection connection = ...;insert1connection.setSavepoint(&quot;a1&quot;);insert2connection.setSavepoint(&quot;a2&quot;);connection.rollback(&quot;a1&quot;);connection.commit(); 也就是说执行了connection.rollback(“a1”);这行后，就把insert2给回滚了。 现在回到Spring的事物传播级别，老实说，如果是正常不报错的情况下，它的表现和PROPAGATION_REQUIRED一样的。所以就看下抛出错误的情况。 我定义了下面的代码。调用了transation()，还是看答应的链接对象。 1234567891011121314151617181920@Transactional@Overridepublic void transation() throws JamesException &#123; try &#123; transationService.addArea(&quot;xyz&quot;); transationService.addGoods(&quot;xyzGoods&quot;); &#125; catch (Exception e) &#123; &#125;&#125;public int addArea(String name) &#123; int i = areaService.insertOne(name); return i;&#125;@Transactional(propagation = Propagation.NESTED)public void addGoods(String name) &#123; int i = goodsService.insertOne(name); if (true) throw new RuntimeException(&quot;ssf&quot;);&#125; 默认情况下： 如果是PROPAGATION_REQUIRED这个传播级别，在默认情况下最后两张表还是没数据(前面已经说过原因了)。现在先看下PROPAGATION_NESTED这个隔离级别的效果。也就是如下代码： 123456789101112131415161718192021@Transactional(propagation = Propagation.NESTED)@Overridepublic void transation() throws JamesException &#123; try &#123; transationService.addArea(&quot;xyz&quot;); transationService.addGoods(&quot;xyzGoods&quot;); &#125; catch (Exception e) &#123; &#125;&#125;@Transactional()public int addArea(String name) &#123; int i = areaService.insertOne(name); return i;&#125;@Transactional()public void addGoods(String name) &#123; int i = goodsService.insertOne(name); if (true) throw new RuntimeException(&quot;ssf&quot;);&#125; 现在有数据了。为什么会这样，我们看下代码。 还是看AbstractPlatformTransactionManager#getTransaction方法。第一次和之前的一样，第二次还会获取到第一次的链接对象，而且会执行到handleExistingTransaction方法中的这段代码块： 12345678910111213141516//去掉一些打印日志的和抛错的if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; ... //默认是可以嵌套事务的 if (useSavepointForNestedTransaction()) &#123; //这里的newTransaction 为 false DefaultTransactionStatus status = prepareTransactionStatus(definition, transaction, false, false, debugEnabled, null); //创建回滚点 status.createAndHoldSavepoint(); return status; &#125; else &#123; return startTransaction(definition, transaction, debugEnabled, null); &#125;&#125; 先看useSavepointForNestedTransaction()这个方法。 默认为true的，也就说默认是允许嵌套事务的，所以看看if的代码块就好了。在这代码块中先做了 1DefaultTransactionStatus status = newTransactionStatus(newTransaction=false) 然后status.createAndHoldSavepoint(); 12345678910111213141516171819202122// AbstractTransactionStatuspublic void createAndHoldSavepoint() throws TransactionException &#123; setSavepoint(getSavepointManager().createSavepoint());&#125;// JdbcTransactionObjectSupport@Overridepublic Object createSavepoint() throws TransactionException &#123; ConnectionHolder conHolder = getConnectionHolderForSavepoint(); try &#123; return conHolder.createSavepoint(); &#125; catch (SQLException ex) &#123; ...... &#125;&#125;// ConnectionHolderpublic Savepoint createSavepoint() throws SQLException &#123; this.savepointCounter++; return getConnection().setSavepoint(SAVEPOINT_NAME_PREFIX + this.savepointCounter);&#125; 也就是说，现在这个DefaultTransactionStatus持有了一个Savepoint对象，而且是在调用目标方法之前创建的Savepoint，也就是说只要执行rollback(savepoint)就能把这次的操作回滚掉 接着执行完目标方法后由于DefaultTransactionStatus.newTransaction&#x3D;&#x3D;false所以在此方法调用中不会真正的提交事物。 我们看下提交代码。TransactionAspectSupport#commitTransactionAfterReturning跟踪代码到AbstractPlatformTransactionManager#processCommit，在这代码中有这段代码： 重点看status.releaseHeldSavepoint(): 也就是说，执行第二次执行到这里后，由于这个if符合条件就执行了releaseHeldSavepoint方法。也就是去释放savepoint方法. 注意，这种释放是不会提交代码的，只是删除了具体的一个savepoint。 也就是说这行代码的意思就是释放连接点。第二次的调用完毕了，看第三次的。 第三次还是一样，不同的就 第三次抛错了。所以看回滚代码:TransactionAspectSupport#completeTransactionAfterThrowing，跟踪代码到AbstractPlatformTransactionManager#processRollback，执行到这里了： 这块代码这提交那里的很像，只是多了getSavepointManager().rollbackToSavepoint(savepoint);，跟踪代码到 第一行就是回滚到savepoint的位置，第二行就是： 这一步很关键，会使得事物得以提交，而不是整个回滚掉，原因可以看上边 也就是说，process1流程的伪代码在这种传播级别下变成了这样: 12345678910DefaultTransactionStatus status = newTransactionStatus(newTransaction=false);savepoint = transactionManager.savePoint(&quot;savepoint_&quot; + ++i)status.setSavepoint(savepoint);try &#123; fun1();&#125; catch &#123; transactionManager.rollback(status.savepoint); status.rollbackOnly = false; throw ex;&#125; Fun4()的伪代码变成了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647status = createTransactionIfNecessary &#123; DataSourceTransactionObject txObject = transactionManager.doGetTransaction() &#123; connectionHolder = threadLocal.get(); &#125; DefaultTransactionStatus status = null; if (txObject.connectionHolder == null || txObject.connectionHolder.transactionActive == false) &#123; status = newTransactionStatus(newTransaction=true); transactionManager.doBegin() &#123; if (txObject.connectionHolder == null) &#123; txObject.connectionHolder = new ConnectionHolder(dataSource.getConnection); &#125; txObject.connectionHolder.connection.autoCommit = false; threadLocal.set(&lt;dataSource,txObject.connectionHolder&gt;); txObject.connectionHolder.transactionActive = true; &#125; &#125; return status;&#125;try &#123; fake.fun4() &#123; // 这里在方法内捕捉了错误，就不会回滚 &#123; DefaultTransactionStatus status = newTransactionStatus(newTransaction=false); savepoint = transactionManager.savePoint(&quot;savepoint_&quot; + ++i) status.setSavepoint(savepoint); try &#123; fun1(); &#125; catch &#123; transactionManager.rollback(status.savepoint); status.rollbackOnly = false; throw ex; &#125; &#125; &#123;....&#125; &#125;&#125; catch &#123; if status.newTransaction transactionManager.doRollback(); throw e&#125;if status.rollbackOnly &#123; transactionManager.doRollback();&#125; else &#123; if status.newTransaction transactionManager.doCommit();&#125; 对于这段代码 @Transactional public void transation() throws JamesException &#123; try &#123; transationService.addArea(&quot;xyz&quot;); transationService.addGoods(&quot;xyzGoods&quot;); &#125; catch (Exception e) &#123; &#125; &#125; 这段代码根据上边的伪代码，变成了这样 12345678910111213141516171819begin; try&#123; transation() &#123; //transationService.addArea(&quot;xyz&quot;); savepoint1 = con.savePoint(..) transationService.addArea(&quot;xyz&quot;); con.releaseSavePoing(savepoint1); savepoint2 = con.savePoint(..) try &#123; transationService.addGoods(&quot;xyzGoods&quot;); throw ex; &#125; catch &#123; con.rollback(savepoint2); throw ex; &#125; &#125; &#125; catch &#123; &#125;commit; 所以最后transationService.addArea(&quot;xyz&quot;);的数据提交了。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"12-Spring的事物","slug":"spring/12-Spring的事物","date":"2021-11-25T12:00:20.000Z","updated":"2022-03-23T09:03:55.312Z","comments":true,"path":"blog/spring/12-Spring的事物/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/12-Spring%E7%9A%84%E4%BA%8B%E7%89%A9/","excerpt":"","text":"先复习下jdbc的事物管理123456789101112131415161718192021222324252627282930313233public static void main(String[] args) &#123; Connection connection = null; try &#123; Class.forName(&quot;com.mysql.jdbc.Driver&quot;); connection = DriverManager.getConnection(DB_URL, USER, PASS); //开启事务 connection.setAutoCommit(false); insertTest(connection); insertTest1(connection); connection.commit(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (SQLException e) &#123; e.printStackTrace(); try &#123; connection.rollback(); System.out.println(&quot;JDBC Transaction rolled back successfully&quot;); &#125; catch (SQLException e1) &#123; System.out.println(&quot;JDBC Transaction rolled back fail&quot; + e1.getMessage()); &#125; &#125; finally &#123; if (connection != null) &#123; try &#123; selectAll(connection); connection.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 上面就是jdbc事务开启和提交（回滚）的代码。要使两次insert都在一个事务中，那必须要把自动提交关闭。而判断两次insert是否在一个同一个事务的依据就是是否为同一个Connection对象，而且这个Connection对象必须关闭自动提交！ Spring中的式事务管理声明式事务就是使用@Transactional定义的方法。 在这里说明下@Transactional这里面的超时属性，这个超时是指sql执行的超时。 开启的方法也很简单 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155@Configuration@EnableTransactionManagement// &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt;@MapperScan(basePackages = &#123;&quot;com.xyz.dao&quot;&#125;)public class EnableTransactionManagementBean &#123; /** &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; */ //这是mybatis @Bean public SqlSessionFactoryBean sqlSessionFactoryBean(DataSource dataSource) &#123; SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(dataSource); return sqlSessionFactoryBean; &#125; // Spring的事务管理器 @Bean public PlatformTransactionManager annotationDrivenTransactionManager(DataSource dataSource) &#123; DataSourceTransactionManager dtm = new DataSourceTransactionManager(); dtm.setDataSource(dataSource); return dtm; &#125; //编程式事务使用 @Bean public TransactionTemplate transactionTemplate(PlatformTransactionManager platformTransactionManager) &#123; TransactionTemplate transactionTemplate = new TransactionTemplate(); transactionTemplate.setTransactionManager(platformTransactionManager); return transactionTemplate; &#125;&#125;@Configuration@PropertySource(&quot;classpath:core/core.properties&quot;)public class DataSourceConfiguration &#123; @Value(&quot;$&#123;jdbc.driverClassName&#125;&quot;) private String driverClass; @Value(&quot;$&#123;jdbc.url&#125;&quot;) private String jdbcUrl; @Value(&quot;$&#123;jdbc.username&#125;&quot;) private String user; @Value(&quot;$&#123;jdbc.password&#125;&quot;) private String password; @Value(&quot;$&#123;jdbc.url&#125;&quot;) private String jdbcUrl1; @Resource Environment environment; @Bean public DataSource comboPooledDataSource() &#123; ComboPooledDataSource comboPooledDataSource = new ComboPooledDataSource(); try &#123; comboPooledDataSource.setDriverClass(driverClass); comboPooledDataSource.setJdbcUrl(jdbcUrl); comboPooledDataSource.setUser(user); comboPooledDataSource.setPassword(password); comboPooledDataSource.setMinPoolSize(10); comboPooledDataSource.setMaxPoolSize(100); comboPooledDataSource.setMaxIdleTime(1800); comboPooledDataSource.setAcquireIncrement(3); comboPooledDataSource.setMaxStatements(1000); comboPooledDataSource.setInitialPoolSize(10); comboPooledDataSource.setIdleConnectionTestPeriod(60); comboPooledDataSource.setAcquireRetryAttempts(30); comboPooledDataSource.setBreakAfterAcquireFailure(false); comboPooledDataSource.setTestConnectionOnCheckout(false); comboPooledDataSource.setAcquireRetryDelay(100); &#125; catch (PropertyVetoException e) &#123; e.printStackTrace(); &#125; return comboPooledDataSource; &#125;&#125;//使用也能简单，用@Transactional注解就好了@Service(&quot;transationServiceImpl&quot;)public class TransationServiceImpl implements TransationService &#123; @Autowired private AreaNameMapper areaService; @Autowired private GoodsNameMapper goodsService; @Autowired private UserNameMapper userMapper; @Autowired @Qualifier(&quot;transationServiceImpl&quot;) private TransationService transationService; @Autowired private DataSource dataSource; @Autowired private TransactionTemplate transactionTemplate; public void txt() &#123; transactionTemplate.execute(status -&gt; &#123; addArea(&quot;xyz&quot;); addGoods(&quot;xyzGoods&quot;); return 1; &#125;); &#125; @Transactional// @Async @Override public void transation() throws JamesException &#123; TransactionSynchronizationManager.registerSynchronization(new XyzTransactionCommitAfter()); ConnectionHolder connectionHolder = (ConnectionHolder)TransactionSynchronizationManager.getResource(dataSource); try &#123; transationService.addArea(&quot;xyz&quot;); transationService.addGoods(&quot;xyzGoods&quot;); &#125; catch (Exception e) &#123; &#125; &#125; @Transactional() public int addArea(String name) &#123; int i = areaService.insertOne(name); return i; &#125; @Transactional(propagation = Propagation.NESTED) public void addGoods(String name) &#123; int i = goodsService.insertOne(name); if (true) throw new RuntimeException(&quot;ssf&quot;); &#125; @Autowired private PlatformTransactionManager platformTransactionManager; public void xxx() &#123; DefaultTransactionDefinition defaultTransactionDefinition = new DefaultTransactionDefinition(); defaultTransactionDefinition.setPropagationBehavior(0); TransactionStatus transaction = platformTransactionManager.getTransaction(defaultTransactionDefinition); try &#123; System.out.println(&quot;业务代码&quot;); &#125; catch (Exception e) &#123; platformTransactionManager.rollback(transaction); &#125; platformTransactionManager.commit(transaction); &#125;&#125; 入口回想下Spring要使用AOP需要什么？需要一个入口类，这个入口类是通过注解@EnableAspectJAutoProxy()引入的。在Spring中要使用事物注解的话就要加上@EnableTransactionManagement注解。看下这个注解的定义： 1234567891011@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Import(&#123;TransactionManagementConfigurationSelector.class&#125;)public @interface EnableTransactionManagement &#123; boolean proxyTargetClass() default false; AdviceMode mode() default AdviceMode.PROXY; int order() default 2147483647;&#125; 所以事物的入口类是TransactionManagementConfigurationSelector，看下类图 它实现了ImportSelector接口，也就是说，在BeanDefinition的创建阶段，会执行selectImports方法。看源码： 12345678910111213141516171819202122232425262728293031//AdviceModeImportSelectorpublic final String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; Class&lt;?&gt; annType = GenericTypeResolver.resolveTypeArgument(getClass(), AdviceModeImportSelector.class); Assert.state(annType != null, &quot;Unresolvable type argument for AdviceModeImportSelector&quot;); AnnotationAttributes attributes = AnnotationConfigUtils.attributesFor(importingClassMetadata, annType); if (attributes == null) &#123; throw new 。。。。。 &#125; AdviceMode adviceMode = attributes.getEnum(getAdviceModeAttributeName()); String[] imports = selectImports(adviceMode); if (imports == null) &#123; throw new IllegalArgumentException(&quot;Unknown AdviceMode: &quot; + adviceMode); &#125; return imports;&#125;//TransactionManagementConfigurationSelector@Overrideprotected String[] selectImports(AdviceMode adviceMode) &#123; switch (adviceMode) &#123; case PROXY: return new String[] &#123;AutoProxyRegistrar.class.getName(), ProxyTransactionManagementConfiguration.class.getName()&#125;; case ASPECTJ: return new String[] &#123;determineTransactionAspectClass()&#125;; default: return null; &#125;&#125; 默认值是：AdviceMode mode() default AdviceMode.PROXY; 所以向Spring中注入了两个类，AutoProxyRegistrar和ProxyTransactionManagementConfiguration AutoProxyRegistrarAutoProxyRegistrar实现了ImportBeanDefinitionRegistrar，所以registerBeanDefinitions方法一定会被执行的，看源码： 12345678910111213141516171819202122232425262728//AutoProxyRegistrar@Overridepublic void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; boolean candidateFound = false; Set&lt;String&gt; annTypes = importingClassMetadata.getAnnotationTypes(); for (String annType : annTypes) &#123; AnnotationAttributes candidate = AnnotationConfigUtils.attributesFor(importingClassMetadata, annType); if (candidate == null) &#123; continue; &#125; Object mode = candidate.get(&quot;mode&quot;); // 设置是否使用Cglib，默认为false，表示使用jdk的动态代理，但如果目标类没有实现接口，那只能使用cglib了 Object proxyTargetClass = candidate.get(&quot;proxyTargetClass&quot;); if (mode != null &amp;&amp; proxyTargetClass != null &amp;&amp; AdviceMode.class == mode.getClass() &amp;&amp; Boolean.class == proxyTargetClass.getClass()) &#123; candidateFound = true; if (mode == AdviceMode.PROXY) &#123; //注册aop的入口类InfrastructureAdvisorAutoProxyCreator AopConfigUtils.registerAutoProxyCreatorIfNecessary(registry); if ((Boolean) proxyTargetClass) &#123; //把属性设置到入口类中，最终会copy到proxyFactory中 AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry); return; &#125; &#125; &#125; &#125;&#125; 最重要的代码为 1AopConfigUtils.registerAutoProxyCreatorIfNecessary(registry); 跟踪代码 12345@Nullablepublic static BeanDefinition registerAutoProxyCreatorIfNecessary( BeanDefinitionRegistry registry, @Nullable Object source) &#123; return registerOrEscalateApcAsRequired(InfrastructureAdvisorAutoProxyCreator.class, registry, source);&#125; 也就是注册了InfrastructureAdvisorAutoProxyCreator，看下它的类图： 可以看到，它的父类为AbstractAdvisorAutoProxyCreater，这个类在AOP中讲过。它的作用就是收集切面、切面匹配和代理类的生成。其实@Transactional就是需要为对象生成一个切面，这是AOP的一个运用。ProxyTransactionManagementConfiguration就是引入事务切面的。如果项目中有使用AOP，也就是引入了AnnotationAwareAspectJAutoProxyCreater，那也是能收集到这个事务切面的。而在@EnableTransactionManagement注解引入这个类的目的就是在没用使用AOP的时候，能收集到事务注解。但收集的切面范围不同，AOP的AnnotationAwareAspectJAutoProxyCreator这个类能收集全部切面，而InfrastructureAdvisorAutoProxyCreator只能收集到事务切面。这个限制的代码为： 123456// InfrastructureAdvisorAutoProxyCreator@Overrideprotected boolean isEligibleAdvisorBean(String beanName) &#123; return (this.beanFactory != null &amp;&amp; this.beanFactory.containsBeanDefinition(beanName) &amp;&amp; this.beanFactory.getBeanDefinition(beanName).getRole() == BeanDefinition.ROLE_INFRASTRUCTURE);&#125; 这块代码是在收集切面的代码中调用的，也就是BeanFactoryAdvisorRetrievalHelper#findAdvisorBeans方法，调用的地方就是 这个isEligibleBean最终会调用isEligibleAdvisorBean。这个方法的默认实现为 123protected boolean isEligibleBean(String beanName) &#123; return true;&#125; 回到isEligibleAdvisorBean,方法中的 1this.beanFactory.getBeanDefinition(beanName).getRole() == BeanDefinition.ROLE_INFRASTRUCTURE) 这个就是用来判断是否系统的事务切面的。这至于为什么这么说，往下看。 ProxyTransactionManagementConfiguration1234567891011121314151617181920212223242526272829303132333435363738394041424344@Configuration(proxyBeanMethods = false)@Role(BeanDefinition.ROLE_INFRASTRUCTURE)public class ProxyTransactionManagementConfiguration extends AbstractTransactionManagementConfiguration &#123; @Bean(name = TransactionManagementConfigUtils.TRANSACTION_ADVISOR_BEAN_NAME) // 设置这个BeanDefinition的Role属性为BeanDefinition.ROLE_INFRASTRUCTURE @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public BeanFactoryTransactionAttributeSourceAdvisor transactionAdvisor( TransactionAttributeSource transactionAttributeSource, TransactionInterceptor transactionInterceptor) &#123; //创建事务切面 BeanFactoryTransactionAttributeSourceAdvisor advisor = new BeanFactoryTransactionAttributeSourceAdvisor(); //切面里面设置处理事务属性对象 advisor.setTransactionAttributeSource(transactionAttributeSource); //设置切面的advice advisor.setAdvice(transactionInterceptor); //设置切面排序 if (this.enableTx != null) &#123; advisor.setOrder(this.enableTx.&lt;Integer&gt;getNumber(&quot;order&quot;)); &#125; return advisor; &#125; @Bean @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public TransactionAttributeSource transactionAttributeSource() &#123; //创建事务属性处理器 return new AnnotationTransactionAttributeSource(); &#125; @Bean @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public TransactionInterceptor transactionInterceptor(TransactionAttributeSource transactionAttributeSource) &#123; //创建事务切面，MethodInterceptor的实现 TransactionInterceptor interceptor = new TransactionInterceptor(); //事务属性处理器设置到advice中 interceptor.setTransactionAttributeSource(transactionAttributeSource); //把事务管理器设置到advice中 if (this.txManager != null) &#123; interceptor.setTransactionManager(this.txManager); &#125; return interceptor; &#125;&#125; 可以看到，这个类的作用就简单明了，就是引入一个Bean而已，而这些Bean就是一个事物切面。 看transactionInterceptor，有个txManager，看下这个txManager是怎么注入的，跟踪代码，在它的父类中： 1234567891011@Autowired(required = false)void setConfigurers(Collection&lt;TransactionManagementConfigurer&gt; configurers) &#123; if (CollectionUtils.isEmpty(configurers)) &#123; return; &#125; if (configurers.size() &gt; 1) &#123; throw new IllegalStateException(&quot;Only one TransactionManagementConfigurer may exist&quot;); &#125; TransactionManagementConfigurer configurer = configurers.iterator().next(); this.txManager = configurer.annotationDrivenTransactionManager();&#125; 这里的代码意思就是，比如我定义一个类： 12345678910111213@Componentpublic class TransactionManagementConfigurerBean implements TransactionManagementConfigurer &#123; @Autowired private DataSource dataSource; @Override public PlatformTransactionManager annotationDrivenTransactionManager() &#123; DataSourceTransactionManager dtm = new DataSourceTransactionManager(); dtm.setDataSource(dataSource); return dtm; &#125;&#125; 它实现了TransactionManagementConfigurer接口，那么在上一段代码中就能通过我们自定义的TransactionManagementConfigurer拿到事务管理器。不过这种方式不是必须的，就算在txManager&#x3D;&#x3D;null，在调用过程中也能拿到事务管理器。 总之，ProxyTransactionManagementConfiguration这个类的主要作用就是注册一个BeanFactoryTransactionAttributeSourceAdvisor切面。这个切面的Pointcut需要看BeanFactoryTransactionAttributeSourceAdvisor。从源码可值，pointcut的创建如下： 12345678//定义事务切面的pointCutprivate final TransactionAttributeSourcePointcut pointcut = new TransactionAttributeSourcePointcut() &#123; @Override @Nullable protected TransactionAttributeSource getTransactionAttributeSource() &#123; return transactionAttributeSource; &#125;&#125;; Advice是：TransactionInterceptor。而且还包含一个AnnotationTransactionAttributeSource。而且这些@Bean进来的bean有加了一个Role注解，而且值为BeanDefinition.ROLE_INFRASTRUCTURE。所以在上节中，最后的问题的答案就在这了。 在Spring Bean的初始化最后都会去检查这个类是否匹配到切面的，判断是否匹配的依据就是pointcut。而切面的逻辑又封装在Advice中。 所以只要看BeanFactoryTransactionAttributeSourceAdvisor的TransactionAttributeSourcePointcut和TransactionInterceptor就好了。 TransactionAttributeSourcePointcut这里先回忆下Pointcut的作用，有3个 用ClassFilter接口进行类匹配 用MethodMatcher接口进行方法匹配 当MethodMatcher的isRunntime方法返回true时在增强方法调用时进行参数匹配 现在看这个类的类图： 从类图可知，这个类既是Pointcut也是MethodMatcher。看这个类的构造方法 ClassFilter是TransactionAttributeSourceClassFilter，而这个类是内部类，看定义： 12345678910111213private class TransactionAttributeSourceClassFilter implements ClassFilter &#123; @Override public boolean matches(Class&lt;?&gt; clazz) &#123; if (TransactionalProxy.class.isAssignableFrom(clazz) || TransactionManager.class.isAssignableFrom(clazz) || PersistenceExceptionTranslator.class.isAssignableFrom(clazz)) &#123; return false; &#125; TransactionAttributeSource tas = getTransactionAttributeSource(); return (tas == null || tas.isCandidateClass(clazz)); &#125;&#125; 在这里说下这ClassFilter的作用吧，其实跟踪源码就可以知道，这个ClassFilter其实没什么卵用，它会进行一些不痛不痒的判断，判断下class对象是否是java.开头的的。所以判断是否匹配的还是看MethodMatcher。看matches方法。 1234567// TransactionAttributeSourcePointcut@Overridepublic boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; TransactionAttributeSource tas = getTransactionAttributeSource(); //只要方法上面能拿到事务属性就返回true，就生成代理 return (tas == null || tas.getTransactionAttribute(method, targetClass) != null);&#125; 从上面的分析中getTransactionAttributeSource()方法会返回AnnotationTransactionAttributeSource对象。其实这个不用分析都可以知道，只要某个方法上有@Transactional这个注解就放回true了。跟踪源码，由于AnnotationTransactionAttributeSource没有重写getTransactionAttribute这个方法，所以看他的父类 AbstractFallbackTransactionAttributeSource 1234567891011121314151617181920212223242526272829303132333435363738394041public TransactionAttribute getTransactionAttribute(Method method, @Nullable Class&lt;?&gt; targetClass) &#123; if (method.getDeclaringClass() == Object.class) &#123; return null; &#125; // First, see if we have a cached value. Object cacheKey = getCacheKey(method, targetClass); TransactionAttribute cached = this.attributeCache.get(cacheKey); if (cached != null) &#123; // Value will either be canonical value indicating there is no transaction attribute, // or an actual transaction attribute. if (cached == NULL_TRANSACTION_ATTRIBUTE) &#123; return null; &#125; else &#123; return cached; &#125; &#125; else &#123; // We need to work it out. TransactionAttribute txAttr = computeTransactionAttribute(method, targetClass); // Put it in the cache. if (txAttr == null) &#123; this.attributeCache.put(cacheKey, NULL_TRANSACTION_ATTRIBUTE); &#125; else &#123; String methodIdentification = ClassUtils.getQualifiedMethodName(method, targetClass); if (txAttr instanceof DefaultTransactionAttribute) &#123; ((DefaultTransactionAttribute) txAttr).setDescriptor(methodIdentification); &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Adding transactional method &#x27;&quot; + methodIdentification + &quot;&#x27; with attribute: &quot; + txAttr); &#125; this.attributeCache.put(cacheKey, txAttr); &#125; return txAttr; &#125;&#125;protected Object getCacheKey(Method method, @Nullable Class&lt;?&gt; targetClass) &#123; return new MethodClassKey(method, targetClass);&#125; 这段代码，先生成cacheKey，然后从缓存中，缓存没有就调用computeTransactionAttribute方法，看这个方法。 123456789101112131415161718192021222324252627282930313233343536373839404142protected TransactionAttribute computeTransactionAttribute(Method method, @Nullable Class&lt;?&gt; targetClass) &#123; // Don&#x27;t allow no-public methods as required. // 如果是非public方法，则返回null不会生成代理，allowPublicMethodsOnly()在AnnotationTransactionAttributeSource中重写了 // 默认传了个true if (allowPublicMethodsOnly() &amp;&amp; !Modifier.isPublic(method.getModifiers())) &#123; return null; &#125; // The method may be on an interface, but we need attributes from the target class. // If the target class is null, the method will be unchanged. // 获取原始方法，因为这个method对象可能是从接口的class对象上获取的，接口上的method没有注解 Method specificMethod = AopUtils.getMostSpecificMethod(method, targetClass); // First try is the method in the target class. //获取方法上面的@Transactional注解的属性 TransactionAttribute txAttr = findTransactionAttribute(specificMethod); if (txAttr != null) &#123; return txAttr; &#125; //如果方法上面没有@Transactional注解，则去找类上面是否有@Transactional注解 // Second try is the transaction attribute on the target class. txAttr = findTransactionAttribute(specificMethod.getDeclaringClass()); if (txAttr != null &amp;&amp; ClassUtils.isUserLevelMethod(method)) &#123; return txAttr; &#125; if (specificMethod != method) &#123; // Fallback is to look at the original method. txAttr = findTransactionAttribute(method); if (txAttr != null) &#123; return txAttr; &#125; // Last fallback is the class of the original method. txAttr = findTransactionAttribute(method.getDeclaringClass()); if (txAttr != null &amp;&amp; ClassUtils.isUserLevelMethod(method)) &#123; return txAttr; &#125; &#125; return null;&#125; 看看这行代码 1234// 只允许publicif (allowPublicMethodsOnly() &amp;&amp; !Modifier.isPublic(method.getModifiers())) &#123; return null;&#125; 这行代码直接限制了@Transactional只能加载public修饰的方法上。接着执行了这样代码: 12//获取原始方法，因为这个method对象可能是从接口的class对象上获取的，接口上的method没有注解Method specificMethod = AopUtils.getMostSpecificMethod(method, targetClass); 在上一节已经介绍了这行代码的作用了为什么这样做。接着是下面代码 1TransactionAttribute txAttr = findTransactionAttribute(specificMethod); 这行就是获取方法上@Transaction注解，并把注解的属性填充到RuleBasedTransactionAttribute对象中。 上面这些都都是方法上的，但有时方法上没注解，而类上有@Transactional注解，所以这时还会有一个检查类上的注解的逻辑。这段代码即完成这逻辑的。 1234txAttr = findTransactionAttribute(specificMethod.getDeclaringClass());if (txAttr != null &amp;&amp; ClassUtils.isUserLevelMethod(method)) &#123; return txAttr;&#125; 当TransactionAttribute创建完成后，computeTransactionAttribute返回后，在getTransactionAttribute方法中会把TransactionAttribute对象保存到缓存attributeCache中。所以，知道getTransactionAttribute中找到@Transaction注解，那就意味着某个类的某个方法匹配到事物切面。 简单看下findTransactionAttribute(Method method)方法 12AnnotationTransactionAttributeSource#findTransactionAttribute ——&gt; AnnotationTransactionAttributeSource#determineTransactionAttribute 12345678910//AnnotationTransactionAttributeSource#determineTransactionAttributeprotected TransactionAttribute determineTransactionAttribute(AnnotatedElement element) &#123; for (TransactionAnnotationParser parser : this.annotationParsers) &#123; TransactionAttribute attr = parser.parseTransactionAnnotation(element); if (attr != null) &#123; return attr; &#125; &#125; return null;&#125; annotationParsers这个属性的初始化是在AnnotationTransactionAttributeSource创建时初始化的，默认情况下 只会走到红款的代码。继续跟踪代码 1SpringTransactionAnnotationParser.parseTransactionAnnotation 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//SpringTransactionAnnotationParser@Override@Nullable// 把@Transactional转换成TransactionAttribute对象public TransactionAttribute parseTransactionAnnotation(AnnotatedElement element) &#123; AnnotationAttributes attributes = AnnotatedElementUtils.findMergedAnnotationAttributes( element, Transactional.class, false, false); if (attributes != null) &#123; //解析Transactional注解中的属性，并封装成对象 return parseTransactionAnnotation(attributes); &#125; else &#123; return null; &#125;&#125;protected TransactionAttribute parseTransactionAnnotation(AnnotationAttributes attributes) &#123; RuleBasedTransactionAttribute rbta = new RuleBasedTransactionAttribute(); Propagation propagation = attributes.getEnum(&quot;propagation&quot;); rbta.setPropagationBehavior(propagation.value()); Isolation isolation = attributes.getEnum(&quot;isolation&quot;); rbta.setIsolationLevel(isolation.value()); rbta.setTimeout(attributes.getNumber(&quot;timeout&quot;).intValue()); rbta.setReadOnly(attributes.getBoolean(&quot;readOnly&quot;)); // 这里把@Transactional中的value设置到TransactionAttribute的qualifier属性中 // @Transactional中的value对应的是事务管理器的beanName rbta.setQualifier(attributes.getString(&quot;value&quot;)); List&lt;RollbackRuleAttribute&gt; rollbackRules = new ArrayList&lt;&gt;(); for (Class&lt;?&gt; rbRule : attributes.getClassArray(&quot;rollbackFor&quot;)) &#123; rollbackRules.add(new RollbackRuleAttribute(rbRule)); &#125; for (String rbRule : attributes.getStringArray(&quot;rollbackForClassName&quot;)) &#123; rollbackRules.add(new RollbackRuleAttribute(rbRule)); &#125; for (Class&lt;?&gt; rbRule : attributes.getClassArray(&quot;noRollbackFor&quot;)) &#123; rollbackRules.add(new NoRollbackRuleAttribute(rbRule)); &#125; for (String rbRule : attributes.getStringArray(&quot;noRollbackForClassName&quot;)) &#123; rollbackRules.add(new NoRollbackRuleAttribute(rbRule)); &#125; rbta.setRollbackRules(rollbackRules); return rbta;&#125; 从上边的源码看到TransactionAttribute就是@Transaction注解的抽象，包含了注解的全部属性。 TransactionAttribute有一个属性需要关注下 1rbta.setQualifier(attributes.getString(&quot;value&quot;)); 这里把@Transactional中的value设置到TransactionAttribute的qualifier属性中 而@Transactional中的value对应的是事务管理器的beanName TransactionInterceptor看adivce只要看invoke方法就好了 1234567public Object invoke(MethodInvocation invocation) throws Throwable &#123; Class&lt;?&gt; targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); // Adapt to TransactionAspectSupport&#x27;s invokeWithinTransaction... return invokeWithinTransaction(invocation.getMethod(), targetClass, invocation::proceed);&#125; 看invokeWithinTransaction方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// TransactionAspectSupportprotected Object invokeWithinTransaction(Method method, @Nullable Class&lt;?&gt; targetClass, final InvocationCallback invocation) throws Throwable &#123; // If the transaction attribute is null, the method is non-transactional. //获取事务属性类 这里返回的是AnnotationTransactionAttributeSource TransactionAttributeSource tas = getTransactionAttributeSource(); //获取事务属性，这里前面就解析过了，在这里会从缓存中拿到该对象。 final TransactionAttribute txAttr = (tas != null ? tas.getTransactionAttribute(method, targetClass) : null); //获取事务管理器， // 1. 这里先看@Transactional的value值是有有值，有的话就从BeanFactory中拿到对应的事物管理器 // 2. 没有值就获取一个默认的，会通过beanFactory.getBean(TransactionManager.class)这样获取事务管理 // 1和2获取到的事务管理都会放到一个ConcurrentMap&lt;Object, TransactionManager&gt;中，之后就从缓存中拿 // 如果这一步没有获取到TransactionManager，这里会抛出错误 final TransactionManager tm = determineTransactionManager(txAttr); if (this.reactiveAdapterRegistry != null &amp;&amp; tm instanceof ReactiveTransactionManager) &#123; //这里好像是响应式编程的代码 ... &#125; // 这里校验事务管理器的对象是否PlatformTransactionManager，否则会抛出错误 PlatformTransactionManager ptm = asPlatformTransactionManager(tm); //获取到有注解的方法名称 final String joinpointIdentification = methodIdentification(method, targetClass, txAttr); //注解事务会走这里 if (txAttr == null || !(ptm instanceof CallbackPreferringPlatformTransactionManager)) &#123; // Standard transaction demarcation with getTransaction and commit/rollback calls. //开启事务 TransactionInfo txInfo = createTransactionIfNecessary(ptm, txAttr, joinpointIdentification); Object retVal; try &#123; // This is an around advice: Invoke the next interceptor in the chain. // This will normally result in a target object being invoked. //火炬传递 retVal = invocation.proceedWithInvocation(); &#125; catch (Throwable ex) &#123; // target invocation exception //事务回滚 completeTransactionAfterThrowing(txInfo, ex); throw ex; &#125; finally &#123; cleanupTransactionInfo(txInfo); &#125; if (retVal != null &amp;&amp; vavrPresent &amp;&amp; VavrDelegate.isVavrTry(retVal)) &#123; // Set rollback-only in case of Vavr failure matching our rollback rules... TransactionStatus status = txInfo.getTransactionStatus(); if (status != null &amp;&amp; txAttr != null) &#123; retVal = VavrDelegate.evaluateTryFailure(retVal, txAttr, status); &#125; &#125; //事务提交 commitTransactionAfterReturning(txInfo); return retVal; &#125; else &#123; //不是核心逻辑，省略 ... &#125;&#125; 从上面可知InvocationCallback invocation = invocation::proceed，所以invocation对象的作用就是进行火炬传递的。 上面的代码一行行看。先看这个 根据前面，这里就是从缓存中获取TransactionAttribute，也就是事物注解的属性对象。 这代码就是获取事物管理器的，看事物管理怎么获取的。 123456789101112131415161718protected TransactionManager determineTransactionManager(@Nullable TransactionAttribute txAttr) &#123; //非关键代码 .... else &#123; //从advice中获取事务管理器 TransactionManager defaultTransactionManager = getTransactionManager(); if (defaultTransactionManager == null) &#123; defaultTransactionManager = this.transactionManagerCache.get(DEFAULT_TRANSACTION_MANAGER_KEY); if (defaultTransactionManager == null) &#123; //从spring容器中获取事务管理器 defaultTransactionManager = this.beanFactory.getBean(TransactionManager.class); this.transactionManagerCache.putIfAbsent( DEFAULT_TRANSACTION_MANAGER_KEY, defaultTransactionManager); &#125; &#125; return defaultTransactionManager; &#125;&#125; 整段代码只看else就好了。这段代码的意思就是。我这样创建一个事物管理器后，可以从上面的代码中获取到。 123456@Beanpublic PlatformTransactionManager annotationDrivenTransactionManager(DataSource dataSource) &#123; DataSourceTransactionManager dtm = new DataSourceTransactionManager(); dtm.setDataSource(dataSource); return dtm;&#125; 现在开始看核心代码，这个代码的就是整个事物切面的核心！ 123456789101112131415161718192021222324252627282930313233343536final String joinpointIdentification = methodIdentification(method, targetClass, txAttr);//注解事务会走这里if (txAttr == null || !(ptm instanceof CallbackPreferringPlatformTransactionManager)) &#123; // Standard transaction demarcation with getTransaction and commit/rollback calls. //开启事务 TransactionInfo txInfo = createTransactionIfNecessary(ptm, txAttr, joinpointIdentification); Object retVal; try &#123; // This is an around advice: Invoke the next interceptor in the chain. // This will normally result in a target object being invoked. //火炬传递 retVal = invocation.proceedWithInvocation(); &#125; catch (Throwable ex) &#123; // target invocation exception //事务回滚 completeTransactionAfterThrowing(txInfo, ex); throw ex; &#125; finally &#123; cleanupTransactionInfo(txInfo); &#125; if (retVal != null &amp;&amp; vavrPresent &amp;&amp; VavrDelegate.isVavrTry(retVal)) &#123; // Set rollback-only in case of Vavr failure matching our rollback rules... TransactionStatus status = txInfo.getTransactionStatus(); if (status != null &amp;&amp; txAttr != null) &#123; retVal = VavrDelegate.evaluateTryFailure(retVal, txAttr, status); &#125; &#125; //事务提交 commitTransactionAfterReturning(txInfo); return retVal;&#125; 先回忆下，在数据库中开启事物是怎么样的 12345678910111213begin;//do somethingcommit或者rollback//下面是jdbc的规范中，开启事物的伪代码Connection connection = DriverManager.getConnection(DB_URL, USER, PASS);connection.setAutoCommit(false);//do somethingconnection.commit(); 或者 connection.rollback();connection.setAutoCommit(false); 这样代码相当于beginconnection.commit(); 或者 connection.rollback();就是commit或者rollback了 而Spring的事物管理器也是遵循JDBC规范的，所以Spring的事物处理代码和伪代码的逻辑都是一样的。 下面就看这个逻辑来讲解代码 开启事物下面就是开启事物的代码： 12345678910111213141516171819// TransactionAspectSupportprotected TransactionInfo createTransactionIfNecessary(@Nullable PlatformTransactionManager tm, @Nullable TransactionAttribute txAttr, final String joinpointIdentification) &#123; .... TransactionStatus status = null; if (txAttr != null) &#123; if (tm != null) &#123; //开启事务，这里重点看 status = tm.getTransaction(txAttr); &#125; else &#123; .... &#125; &#125; //创建事务信息对象，记录新老事务信息对象 return prepareTransactionInfo(tm, txAttr, joinpointIdentification, status);&#125; 这个事务管理器，我们重点看DataSourceTransactionManager，因为这是常用的。好了，现在看getTransaction方法: getTransaction12345678910111213141516171819202122232425262728293031323334353637383940414243444546// AbstractPlatformTransactionManagerpublic final TransactionStatus getTransaction(@Nullable TransactionDefinition definition) throws TransactionException &#123; // Use defaults if no transaction definition given. TransactionDefinition def = (definition != null ? definition : TransactionDefinition.withDefaults()); //这里重点看，拿到DataSourceTransactionObject对象 Object transaction = doGetTransaction(); boolean debugEnabled = logger.isDebugEnabled(); //第一次进来connectionHolder为空的，所以不存在事务 if (isExistingTransaction(transaction)) &#123; // Existing transaction found -&gt; check propagation behavior to find out how to behave. //如果不是第一次进来，则会走这个逻辑，重点看 return handleExistingTransaction(def, transaction, debugEnabled); &#125; //一些校验 .... //第一次进来大部分会走这里 else if (def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRED || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; //先挂起 SuspendedResourcesHolder suspendedResources = suspend(null); if (debugEnabled) &#123; logger.debug(&quot;Creating new transaction with name [&quot; + def.getName() + &quot;]: &quot; + def); &#125; try &#123; return startTransaction(def, transaction, debugEnabled, suspendedResources); &#125; catch (RuntimeException | Error ex) &#123; resume(null, suspendedResources); throw ex; &#125; &#125; else &#123; // Create &quot;empty&quot; transaction: no actual transaction, but potentially synchronization. if (def.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT &amp;&amp; logger.isWarnEnabled()) &#123; logger.warn(&quot;Custom isolation level specified but no actual transaction initiated; &quot; + &quot;isolation level will effectively be ignored: &quot; + def); &#125; boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); return prepareTransactionStatus(def, null, true, newSynchronization, debugEnabled, null); &#125;&#125; 先看这行代码 1234567891011121314// AbstractPlatformTransactionManager// 拿到DataSourceTransactionObject对象Object transaction = doGetTransaction();// DataSourceTransactionManagerObject doGetTransaction() &#123; DataSourceTransactionObject txObject = new DataSourceTransactionObject(); txObject.setSavepointAllowed(isNestedTransactionAllowed()); //这个代码是从ThreadLocal中获取到连接对象 ConnectionHolder conHolder = (ConnectionHolder) TransactionSynchronizationManager.getResource(obtainDataSource()); txObject.setConnectionHolder(conHolder, false); return txObject;&#125; 这里的DataSourceTransactionObject对象看下它有什么属性和方法。 这个对象从属性上看就是一个数据库链接对象的包装对象，包含了链接对象、只读标记、是否新建的Connect等信息；还有包含了创建savepoint方法、回滚某个savepoint方法等 看代码： 123//这个代码是从ThreadLocal中获取到连接对象ConnectionHolder conHolder = (ConnectionHolder) TransactionSynchronizationManager.getResource(obtainDataSource()); 先看obtainDataSource: 12345678910// DataSourceTransactionManagerprotected DataSource obtainDataSource() &#123; DataSource dataSource = getDataSource(); Assert.state(dataSource != null, &quot;No DataSource set&quot;); return dataSource;&#125;// 所以Spring的事物就是事物的搬运工而已，它不管处理逻辑public DataSource getDataSource() &#123; return this.dataSource;&#125; 这个dataSource，有很多来源，比如HikariCP、Druid、C3P0。而dataSource的初始化是PlatformTransactionManager的初始化时传入的。也就是 而TransactionSynchronizationManager.getResource这里简单的说就是从一个ThreadLocal中拿ConnectionHolder，ThreadLocal没有就返回null。而第一次进来的话是肯定为null的。 回到doGetTransaction‘ 123// DataSourceTransactionManager// DataSourceTransactionObjecttxObject.setConnectionHolder(conHolder, false); 通过这个方法，把前面拿到的ConnectionHolder设置进DataSourceTransactionObject中，并把DataSourceTransactionObject的newConnectHolder设置为false。最后doGetTransaction就把DataSourceTransactionObject对象返回。 回到AbstractPlatformTransactionManager#getTransaction方法。 现在Object transaction = doGetTransaction();代码返回了一个DataSourceTransactionObject对象。接着执行了下面代码: 1234567// AbstractPlatformTransactionManager// 第一次进来connectionHolder为空的，所以不存在事务if (isExistingTransaction(transaction)) &#123; // Existing transaction found -&gt; check propagation behavior to find out how to behave. // 如果不是第一次进来，则会走这个逻辑，重点看 return handleExistingTransaction(def, transaction, debugEnabled);&#125; 这个isExistingTransaction 1234567// DataSourceTransactionManager@Overrideprotected boolean isExistingTransaction(Object transaction) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; //如果事务对象中有连接且是active的，则说明之前存在事务 return (txObject.hasConnectionHolder() &amp;&amp; txObject.getConnectionHolder().isTransactionActive());&#125; 就是从两个维度判断的 判断ConnectionHolder是否为null 如果第一步不为null就判断ConnectionHolder的事物是否活跃。 第一次进来的话DataSourceTransactionObject的connecttionHolder是为null的，所以，这个方法会放回false的，所以先看getTransaction后面的代码 1234567891011121314151617181920// AbstractPlatformTransactionManager.getTransaction// 第一次进来大部分会走这里else if (def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRED || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; //先挂起，这里只对PROPAGATION_REQUIRES_NEW和PROPAGATION_NESTED有用 SuspendedResourcesHolder suspendedResources = suspend(null); if (debugEnabled) &#123; logger.debug(&quot;Creating new transaction with name [&quot; + def.getName() + &quot;]: &quot; + def); &#125; try &#123; return startTransaction(def, transaction, debugEnabled, suspendedResources); &#125; catch (RuntimeException | Error ex) &#123; resume(null, suspendedResources); throw ex; &#125;&#125;// 其他事物传播属性就不看了...... 这段代码就涉及到事物传播属性了。先看下这个事务传播属性 事务传播下一节讲 先看 1234567891011121314// AbstractPlatformTransactionManagerprivate TransactionStatus startTransaction(TransactionDefinition definition, Object transaction, boolean debugEnabled, @Nullable SuspendedResourcesHolder suspendedResources) &#123; boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); //创建一个新的事务状态，注意这里的 newTransaction 属性为 true了 DefaultTransactionStatus status = newTransactionStatus( definition, transaction, true, newSynchronization, debugEnabled, suspendedResources); //开启事务,重点看看 DataSourceTransactionObject doBegin(transaction, definition); //开启事务后，改变事务状态 prepareSynchronization(status, definition); return status;&#125; 先创建了一个DefaultTransactionStatus对象，表示当前事务的状态。这个对象你面有事务的状态属性，比如是否新的，是否挂起，是否readOnly，有一点要注意，在这里，DefaultTransactionStatus中的newTransaction&#x3D;true的，这一点很重要。接着就是doBegin，从方法名就知道，就是开启事物的。 doBegin1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// DataSourceTransactionManagerprotected void doBegin(Object transaction, TransactionDefinition definition) &#123; //事务对象 DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; Connection con = null; try &#123; // 判断事务对象中有没有连接 if (!txObject.hasConnectionHolder() || txObject.getConnectionHolder().isSynchronizedWithTransaction()) &#123; //事务管理器中拿dataSource对象，从dataSource中拿连接对象 。。这里可能是AbstractRoutingDataSource的数据源 Connection newCon = obtainDataSource().getConnection(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Acquired Connection [&quot; + newCon + &quot;] for JDBC transaction&quot;); &#125; //把连接对象设置到事务对象中 txObject.setConnectionHolder(new ConnectionHolder(newCon), true); &#125; //设置有事务标识 txObject.getConnectionHolder().setSynchronizedWithTransaction(true); con = txObject.getConnectionHolder().getConnection(); //设置是否只读连接和设置事务隔离级别 Integer previousIsolationLevel = DataSourceUtils.prepareConnectionForTransaction(con, definition); //设置该连接的上一个隔离级别 txObject.setPreviousIsolationLevel(previousIsolationLevel); txObject.setReadOnly(definition.isReadOnly()); //如果是自动提交 if (con.getAutoCommit()) &#123; // 设置标记，表示最后一定要把链接对象的自动提交打开 txObject.setMustRestoreAutoCommit(true); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Switching JDBC Connection [&quot; + con + &quot;] to manual commit&quot;); &#125; //把自动提交关闭，因为提交要交给spring来做 con.setAutoCommit(false); &#125; //执行只读事务命令 prepareTransactionalConnection(con, definition); //把事务状态设置为true txObject.getConnectionHolder().setTransactionActive(true); //获取事务超时时间 int timeout = determineTimeout(definition); if (timeout != TransactionDefinition.TIMEOUT_DEFAULT) &#123; txObject.getConnectionHolder().setTimeoutInSeconds(timeout); &#125; // Bind the connection holder to the thread. //如果是一个新事务，这建立数据源对象和连接对象的绑定关系.且把该绑定关系的map设置到ThreadLocal中 if (txObject.isNewConnectionHolder()) &#123; TransactionSynchronizationManager.bindResource(obtainDataSource(), txObject.getConnectionHolder()); &#125; &#125; catch (Throwable ex) &#123; ..... &#125;&#125; 这代码看注解就好了，我在这总结下 由于第一次进来的时候，没有从ThreadLocal中获取到链接，所以会通过通过dataSource获取链接对象，并把链接对象包装成ConnectionHolder，然后赋值给DataSourceTransactionObject，并把DataSourceTransactionObject的新创建的Connection属性newConnectionHolder设置为true(这里对象里的connectionHolder属性第一次进来的时候是null的) 设置是否只读和设置事务隔离级别，在设置事务隔离级别的时候会记录链接对象的事务隔离级别（数据库默认的），然后把这个隔离级别设置到DataSourceTransactionObject对象的previousIsolationLevel属性中（这是为了恢复链接对象的隔离级别）。 把自动提交关闭 执行只读事务命令stmt.executeUpdate(&quot;SET TRANSACTION READ ONLY&quot;); 把DataSourceTransactionObject对象的事务活跃标记设置为true 判断这个ConnectionHolder是否新建的（判断DataSourceTransactionObject.newConnectionHolder属性是否为true。在第一步设置为true了），如果是就建立&lt;dataSource, connectionHolder&gt;这种对应关系，并把这个对应关系放入到ThreadLocal resources中。 doBegin执行完后执行了prepareSynchronization，这个方法就是设置下TransactionSynchronizationManager中ThreadLocal的值，这样我们在业务代码中就可以使用TransactionSynchronizationManager来获取属性了。 123456789101112131415// AbstractPlatformTransactionManagerprepareSynchronization(status, definition);protected void prepareSynchronization(DefaultTransactionStatus status, TransactionDefinition definition) &#123; // DefaultTransactionStatus在第一次创建的时候，newSynchronization设置为true了 if (status.isNewSynchronization()) &#123; TransactionSynchronizationManager.setActualTransactionActive(status.hasTransaction()); TransactionSynchronizationManager.setCurrentTransactionIsolationLevel( definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT ? definition.getIsolationLevel() : null); TransactionSynchronizationManager.setCurrentTransactionReadOnly(definition.isReadOnly()); TransactionSynchronizationManager.setCurrentTransactionName(definition.getName()); TransactionSynchronizationManager.initSynchronization(); &#125;&#125; 最后把DefaultTransactionStatus对象返回。回到TransactionAspectSupport.createTransactionIfNecessary，把返回值对象设置给了status变量。最后执行了一步： 这个方法就是封装TransactionInfo对象。TransactionAspectSupport.createTransactionIfNecessary也执行完了，返回了一个TransactionInfo对象。 火炬传递 之前就说过，invocation &#x3D; invocation::proceed，所以这里的方法调用就是执行了invocation::proceed，这个方法的作用在AOP的调用中就已经说过了。最终会调用到目标对象的业务方法，执行事务逻辑。 事务提交在没错误发生时就进行了事物提交。 跟踪代码 事务提交方法中的回滚123456789101112131415161718192021222324252627282930313233343536373839// TransactionAspectSupportprotected void commitTransactionAfterReturning(@Nullable TransactionInfo txInfo) &#123; if (txInfo != null &amp;&amp; txInfo.getTransactionStatus() != null) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Completing transaction for [&quot; + txInfo.getJoinpointIdentification() + &quot;]&quot;); &#125; // 或返回DataSourceTransactionManager txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()); &#125;&#125;// AbstractPlatformTransactionManager@Overridepublic final void commit(TransactionStatus status) throws TransactionException &#123; if (status.isCompleted()) &#123; throw new IllegalTransactionStateException( &quot;Transaction is already completed - do not call commit or rollback more than once per transaction&quot;); &#125; DefaultTransactionStatus defStatus = (DefaultTransactionStatus) status; // 这里要注意，这里执行了回滚操作，因为有一个方法抛出错误了 if (defStatus.isLocalRollbackOnly()) &#123; if (defStatus.isDebug()) &#123; logger.debug(&quot;Transactional code has requested rollback&quot;); &#125; processRollback(defStatus, false); return; &#125; if (!shouldCommitOnGlobalRollbackOnly() &amp;&amp; defStatus.isGlobalRollbackOnly()) &#123; if (defStatus.isDebug()) &#123; logger.debug(&quot;Global transaction is marked as rollback-only but transactional code requested commit&quot;); &#125; processRollback(defStatus, true); return; &#125; //执行事务提交 processCommit(defStatus);&#125; 在这段代码中，发现有可能会执行一个方法processRollback。这个方法是用来做事务回滚的。居然在commit代码中会发生回滚，现在看下发生回滚的条件。 就是DefaultTransactionStatus中的rollbackOnly属性设置为true了。而只能通过DefaultTransactionStatus对象的setRollbackOnly()方法把rollbackOnly设置为true。 第二种情况需要满足两个条件。第一，需要检查AbstractPlatformTransactionManager#shouldCommitOnGlobalRollbackOnly() 方法返回值是否为false，该方法的默认实现的返回值为false。第二，通过DefaultTransactionStatus，检查DataSourceTransactionObject中的ConnectionHolder对象的rollbackOnly属性是否为true。 满足上边的两种情况的某一个，都会触发在conmit方法中调用rollback逻辑。而怎么触发条件，在事务传播属性中讲 从第二中情况看DefaultTransactionStatus、DataSourceTransactionObject和ConnectionHolder的关系。从包含关系看，它们的关系是这样的： 执行事务提交1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192// AbstractPlatformTransactionManagerprivate void processCommit(DefaultTransactionStatus status) throws TransactionException &#123; try &#123; boolean beforeCompletionInvoked = false; try &#123; boolean unexpectedRollback = false; prepareForCommit(status); //调用beforeCommit方法，事务提交之前做业务处理 triggerBeforeCommit(status); triggerBeforeCompletion(status); beforeCompletionInvoked = true; //如果有回滚点 if (status.hasSavepoint()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Releasing transaction savepoint&quot;); &#125; unexpectedRollback = status.isGlobalRollbackOnly(); status.releaseHeldSavepoint(); &#125; //如果是新事务，则提交事务 else if (status.isNewTransaction()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Initiating transaction commit&quot;); &#125; unexpectedRollback = status.isGlobalRollbackOnly(); doCommit(status); &#125; else if (isFailEarlyOnGlobalRollbackOnly()) &#123; unexpectedRollback = status.isGlobalRollbackOnly(); &#125; // Throw UnexpectedRollbackException if we have a global rollback-only // marker but still didn&#x27;t get a corresponding exception from commit. if (unexpectedRollback) &#123; throw new UnexpectedRollbackException( &quot;Transaction silently rolled back because it has been marked as rollback-only&quot;); &#125; &#125; catch (UnexpectedRollbackException ex) &#123; // can only be caused by doCommit triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK); throw ex; &#125; catch (TransactionException ex) &#123; // can only be caused by doCommit if (isRollbackOnCommitFailure()) &#123; doRollbackOnCommitException(status, ex); &#125; else &#123; triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN); &#125; throw ex; &#125; catch (RuntimeException | Error ex) &#123; if (!beforeCompletionInvoked) &#123; triggerBeforeCompletion(status); &#125; doRollbackOnCommitException(status, ex); throw ex; &#125; // Trigger afterCommit callbacks, with an exception thrown there // propagated to callers but the transaction still considered as committed. try &#123; //触发afterCommit方法，事务提交后做业务处理 triggerAfterCommit(status); &#125; finally &#123; triggerAfterCompletion(status, TransactionSynchronization.STATUS_COMMITTED); &#125; &#125; finally &#123; //处理挂起事务，在这里恢复绑定关系 cleanupAfterCompletion(status); &#125;&#125;protected void doCommit(DefaultTransactionStatus status) &#123; DataSourceTransactionManager.DataSourceTransactionObject txObject = (DataSourceTransactionManager.DataSourceTransactionObject)status.getTransaction(); Connection con = txObject.getConnectionHolder().getConnection(); try &#123; con.commit(); &#125; catch (SQLException var5) &#123; throw new TransactionSystemException(&quot;Could not commit JDBC transaction&quot;, var5); &#125;&#125; 这段代码就只有一处有doCommit，所以重点看这里（其他）： 这段代码很简单，就是判断DefaultTransactionStatus.newTransaction属性是否为true，为true就提交了。在上面已经强调过了DefaultTransactionStatus.newTransaction属性，在对象刚创建的时候就是设置为true的。而从代码也可以得出，DefaultTransactionStatus.newTransaction这个属性是用来控制事务提交的。 事务回滚回到TransactionAspectSupport，当火炬传递出错时，就会执行下面的代码 跟踪代码，到： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// AbstractPlatformTransactionManagerprivate void processRollback(DefaultTransactionStatus status, boolean unexpected) &#123; try &#123; boolean unexpectedRollback = unexpected; try &#123; triggerBeforeCompletion(status); //如果有回滚点，如果是嵌套事务 if (status.hasSavepoint()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Rolling back transaction to savepoint&quot;); &#125; status.rollbackToHeldSavepoint(); &#125; //如果是新事务，newTransaction 为true的情况下才回滚 else if (status.isNewTransaction()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Initiating transaction rollback&quot;); &#125; doRollback(status); &#125; else &#123; // Participating in larger transaction if (status.hasTransaction()) &#123; if (status.isLocalRollbackOnly() || isGlobalRollbackOnParticipationFailure()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Participating transaction failed - marking existing transaction as rollback-only&quot;); &#125; doSetRollbackOnly(status); &#125; else &#123; if (status.isDebug()) &#123; logger.debug(&quot;Participating transaction failed - letting transaction originator decide on rollback&quot;); &#125; &#125; &#125; else &#123; logger.debug(&quot;Should roll back transaction but cannot - no transaction available&quot;); &#125; // Unexpected rollback only matters here if we&#x27;re asked to fail early if (!isFailEarlyOnGlobalRollbackOnly()) &#123; unexpectedRollback = false; &#125; &#125; &#125; catch (RuntimeException | Error ex) &#123; triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN); throw ex; &#125; triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK); // Raise UnexpectedRollbackException if we had a global rollback-only marker if (unexpectedRollback) &#123; throw new UnexpectedRollbackException( &quot;Transaction rolled back because it has been marked as rollback-only&quot;); &#125; &#125; finally &#123; cleanupAfterCompletion(status); &#125;&#125;protected void doRollback(DefaultTransactionStatus status) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) status.getTransaction(); Connection con = txObject.getConnectionHolder().getConnection(); if (status.isDebug()) &#123; logger.debug(&quot;Rolling back JDBC transaction on Connection [&quot; + con + &quot;]&quot;); &#125; try &#123; con.rollback(); &#125; catch (SQLException ex) &#123; throw new TransactionSystemException(&quot;Could not roll back JDBC transaction&quot;, ex); &#125;&#125; 和提交一样，都是通过DefaultTransactionStatus.newTransaction属性来控制的。最后都是通过Connection对象的rollback方法回滚的。 但这里有一点要说下，就是异常校验 异常校验回到TransactionAspectSupport#completeTransactionAfterThrowing，到这里 txInfo.transactionAttribute就是封装了注解@Transactional的属性对象。在poincut解析@Transcation中可以知道，最后封装成了RuleBasedTransactionAttribute这个对象。我们看下这个rollbackOn方法。 1234567891011121314151617181920212223242526272829303132// RuleBasedTransactionAttribute@Overridepublic boolean rollbackOn(Throwable ex) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Applying rules to determine whether transaction should rollback on &quot; + ex); &#125; RollbackRuleAttribute winner = null; int deepest = Integer.MAX_VALUE; if (this.rollbackRules != null) &#123; for (RollbackRuleAttribute rule : this.rollbackRules) &#123; int depth = rule.getDepth(ex); if (depth &gt;= 0 &amp;&amp; depth &lt; deepest) &#123; deepest = depth; winner = rule; &#125; &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Winning rollback rule is: &quot; + winner); &#125; // User superclass behavior (rollback on unchecked) if no rule matches. if (winner == null) &#123; logger.trace(&quot;No relevant rollback rule found: applying default rules&quot;); return super.rollbackOn(ex); &#125; return !(winner instanceof NoRollbackRuleAttribute);&#125; 这段代码就是判断哪些错误需要回滚，哪些错误不需要回滚的 其核心代码有三部分 第一部分: 123456789if (this.rollbackRules != null) &#123; for (RollbackRuleAttribute rule : this.rollbackRules) &#123; int depth = rule.getDepth(ex); if (depth &gt;= 0 &amp;&amp; depth &lt; deepest) &#123; deepest = depth; winner = rule; &#125; &#125;&#125; 这段代码中rollbackRules集合的初始化看RuleBasedTransactionAttribute的初始化代码 这个集合只涉及到两个对象RollbackRuleAttribute和NoRollbackRuleAttribute。而NoRollbackRuleAttribute是RollbackRuleAttribute的父类，getDepth方法的逻辑都是在RollbackRuleAttribute中的，看代码 execptionName就是@Transcational的设置的值，就是异常类的完全限定名。而上边的代码就是检查异常类和异常类的父类是否和execptionName匹配，匹配的话就返回深度。如果没有匹配到的情况，那么就会走到第二部分代码 在这一部分代码中，会调用RuleBasedTransactionAttribute父类的rollbackOn方法。 第一部分如果匹配到的情况，就会检查 匹配到的RollbackRuleAttribute是否是NoRollbackRuleAttribute，如果是的话，那就意味着是不需要回滚的，所以最终会返回false。 整段代码的意思就是如如果抛出的异常和配置的异常不匹配，就找父类再和配置的异常匹配，如果这样一直下去还找不到，那就和RuntimeExection和Error比较。如果通过异常类有匹配到，那就检查是否NoRollbackRuleAttribute，如果是就意味着不需要回滚，不是的话就是需要回滚。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"11-Spring中的代理的TargetSource接口的使用","slug":"spring/11-Spring中的代理的TargetSource接口的使用","date":"2021-11-25T12:00:19.000Z","updated":"2022-03-23T09:03:55.254Z","comments":true,"path":"blog/spring/11-Spring中的代理的TargetSource接口的使用/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/11-Spring%E4%B8%AD%E7%9A%84%E4%BB%A3%E7%90%86%E7%9A%84TargetSource%E6%8E%A5%E5%8F%A3%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Spring中的代理在Spring中所有的代理类都是通过ProxyFactory类的getProxy来生成的，而这个getPorxy会分情况使用JdkDynamicAopProxy或者ObjenesisCglibAopProxy来生成代理对象。这个情况有两个 如果注解的proxyTargetClass = true 或者 proxyMode &#x3D; ScopedProxyMode.TARGET_CLASS&#96; 对象的类有实现接口 如果满足情况1或者两个情况都不瞒住那么就会使用ObjenesisCglibAopProxy（Cglib）来创建代理对象；如果情况1步满足，满足情况2那么会使用JdkDynamicAopProxy（jdk动态代理）来创建代理对象。 这里我只看JdkDynamicAopProxy就好了，ObjenesisCglibAopProxy的逻辑和JdkDynamicAopProxy一样。只是api不同。 看JdkDynamicAopProxy的getProxy方法 123456789101112131415@Overridepublic Object getProxy() &#123; return getProxy(ClassUtils.getDefaultClassLoader());&#125;@Overridepublic Object getProxy(@Nullable ClassLoader classLoader) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Creating JDK dynamic proxy: &quot; + this.advised.getTargetSource()); &#125; // 获取实现的接口 Class&lt;?&gt;[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this);&#125; 可以看到，这个Proxy就是使用了JDK的动态代理，而且InvocationHandler就是JdkDynamicAopProxy对象。 所以只要看JdkDynamicAopProxy的invoke方法就好了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object oldProxy = null; boolean setProxyContext = false; //从代理工厂中拿到TargetSource对象，该对象包装了被代理实例bean TargetSource targetSource = this.advised.targetSource; Object target = null; try &#123; //被代理对象的equals方法和hashCode方法是不能被代理的，不会走切面 if (!this.equalsDefined &amp;&amp; AopUtils.isEqualsMethod(method)) &#123; // The target does not implement the equals(Object) method itself. return equals(args[0]); &#125; else if (!this.hashCodeDefined &amp;&amp; AopUtils.isHashCodeMethod(method)) &#123; // The target does not implement the hashCode() method itself. return hashCode(); &#125; else if (method.getDeclaringClass() == DecoratingProxy.class) &#123; // There is only getDecoratedClass() declared -&gt; dispatch to proxy config. return AopProxyUtils.ultimateTargetClass(this.advised); &#125; else if (!this.advised.opaque &amp;&amp; method.getDeclaringClass().isInterface() &amp;&amp; method.getDeclaringClass().isAssignableFrom(Advised.class)) &#123; // Service invocations on ProxyConfig with the proxy config... return AopUtils.invokeJoinpointUsingReflection(this.advised, method, args); &#125; Object retVal; //如果该属性设置为true，则把代理对象设置到ThreadLocal中 if (this.advised.exposeProxy) &#123; // Make invocation available if necessary. oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; &#125; // Get as late as possible to minimize the time we &quot;own&quot; the target, // in case it comes from a pool. //这个target就是被代理实例 target = targetSource.getTarget(); Class&lt;?&gt; targetClass = (target != null ? target.getClass() : null); // Get the interception chain for this method. //从代理工厂中拿过滤器链 Object是一个MethodInterceptor类型的对象，其实就是一个advice对象 List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); // Check whether we have any advice. If we don&#x27;t, we can fallback on direct // reflective invocation of the target, and avoid creating a MethodInvocation. //如果该方法没有执行链，则说明这个方法不需要被拦截，则直接反射调用 if (chain.isEmpty()) &#123; // We can skip creating a MethodInvocation: just invoke the target directly // Note that the final invoker must be an InvokerInterceptor so we know it does // nothing but a reflective operation on the target, and no hot swapping or fancy proxying. Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); &#125; else &#123; // We need to create a method invocation... MethodInvocation invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); // Proceed to the joinpoint through the interceptor chain. retVal = invocation.proceed(); &#125; // Massage return value if necessary. Class&lt;?&gt; returnType = method.getReturnType(); if (retVal != null &amp;&amp; retVal == target &amp;&amp; returnType != Object.class &amp;&amp; returnType.isInstance(proxy) &amp;&amp; !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) &#123; // Special case: it returned &quot;this&quot; and the return type of the method // is type-compatible. Note that we can&#x27;t help if the target sets // a reference to itself in another returned object. retVal = proxy; &#125; else if (retVal == null &amp;&amp; returnType != Void.TYPE &amp;&amp; returnType.isPrimitive()) &#123; throw new AopInvocationException( &quot;Null return value from advice does not match primitive return type for: &quot; + method); &#125; return retVal; &#125; finally &#123; if (target != null &amp;&amp; !targetSource.isStatic()) &#123; // Must have come from TargetSource. targetSource.releaseTarget(target); &#125; if (setProxyContext) &#123; // Restore old proxy. AopContext.setCurrentProxy(oldProxy); &#125; &#125;&#125; 这个方法在[AOP的方法调用](.&#x2F;10-Spring AOP中的方法调用)中已经讲过了，整个逻辑就只有3部分 获取目标对象 Advice链的调用 非增强方法的调用 2和3步之前就已经讲过了。第一步其实就是一段简单的代码 1Object target = this.advised.targetSource.getTarget(); 这里就是TargetSource接口的应用。从这里就可以知道，TargetSource接口的的作用其实就是在代理对象中获取目标对象。而这个其实是Spring的一个扩展，这个扩展也对外提供了入口方法。 TargetSource接口在项目中的使用TargetSourceCreator对象的创建12345678910111213// AbstractBeanFactoryBasedTargetSourceCreator实现了TargetSourceCreator接口public class CustomTargetSourceCreator extends AbstractBeanFactoryBasedTargetSourceCreator &#123; @Override protected AbstractBeanFactoryBasedTargetSource createBeanFactoryBasedTargetSource(Class&lt;?&gt; beanClass, String beanName) &#123; if (getBeanFactory() instanceof ConfigurableListableBeanFactory) &#123; if(beanClass.isAssignableFrom(StudentServiceImpl.class)) &#123; return new CustomTargetSource(); &#125; &#125; return null; &#125;&#125; 可以加@Component注解 TargetSource接口的创建1234567// AbstractBeanFactoryBasedTargetSource实现了TargetSource接口public class CustomTargetSource extends AbstractBeanFactoryBasedTargetSource &#123; @Override public Object getTarget() throws Exception &#123; return getBeanFactory().getBean(getTargetBeanName()); &#125;&#125; 入口123456789101112131415161718192021222324252627@Componentpublic class SetCustomTargetSourceCreator implements BeanPostProcessor, PriorityOrdered, BeanFactoryAware &#123; private BeanFactory beanFactory; @Override public int getOrder() &#123; return 45; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if(bean instanceof AnnotationAwareAspectJAutoProxyCreator || bean instanceof InfrastructureAdvisorAutoProxyCreator) &#123; AbstractAdvisorAutoProxyCreator proxyCreator = (AbstractAdvisorAutoProxyCreator)bean; CustomTargetSourceCreator customTargetSourceCreator = new CustomTargetSourceCreator(); customTargetSourceCreator.setBeanFactory(beanFactory); proxyCreator.setCustomTargetSourceCreators(customTargetSourceCreator); &#125; return bean; &#125; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; this.beanFactory = beanFactory; &#125;&#125; 注意，这个入口类必须要考虑执行顺序，这个之后再讲。 自定义的TargetSource对象在Spring源码中的初始化自定义的TargetSource对象的初始化这里涉及到Spring Bean的初始化流程，初始化流程的入口方法看AbstractBeanFactory的getBean方法，跟踪代码到AbstractAutowireCapableBeanFactor.createBean中的这段代码，这段代码在Bean实例化前调用。 resolveBeforeInstantiation方法是BeanPostProcessor接口的运用，跟踪代码到applyBeanPostProcessorsBeforeInstantiation。 123456789101112131415161718192021222324252627282930313233// AbstractAutowireCapableBeanFactoryprotected Object resolveBeforeInstantiation(String beanName, RootBeanDefinition mbd) &#123; Object bean = null; if (!Boolean.FALSE.equals(mbd.beforeInstantiationResolved)) &#123; // Make sure bean class is actually resolved at this point. if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; Class&lt;?&gt; targetType = determineTargetType(beanName, mbd); if (targetType != null) &#123; bean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName); if (bean != null) &#123; bean = applyBeanPostProcessorsAfterInitialization(bean, beanName); &#125; &#125; &#125; mbd.beforeInstantiationResolved = (bean != null); &#125; return bean;&#125;// AbstractAutowireCapableBeanFactory@Nullableprotected Object applyBeanPostProcessorsBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; Object result = ibp.postProcessBeforeInstantiation(beanClass, beanName); if (result != null) &#123; return result; &#125; &#125; &#125; return null;&#125; 看这个实现 AbstractAutoProxyCreator的postProcessBeforeInstantiation方法 12345678910111213141516171819202122232425262728293031// AbstractAutoProxyCreator@Overridepublic Object postProcessBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) &#123; Object cacheKey = getCacheKey(beanClass, beanName); if (!StringUtils.hasLength(beanName) || !this.targetSourcedBeans.contains(beanName)) &#123; if (this.advisedBeans.containsKey(cacheKey)) &#123; return null; &#125; if (isInfrastructureClass(beanClass) || shouldSkip(beanClass, beanName)) &#123; this.advisedBeans.put(cacheKey, Boolean.FALSE); return null; &#125; &#125; // Create proxy here if we have a custom TargetSource. // Suppresses unnecessary default instantiation of the target bean: // The TargetSource will handle target instances in a custom fashion. TargetSource targetSource = getCustomTargetSource(beanClass, beanName); if (targetSource != null) &#123; if (StringUtils.hasLength(beanName)) &#123; this.targetSourcedBeans.add(beanName); &#125; Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(beanClass, beanName, targetSource); Object proxy = createProxy(beanClass, beanName, specificInterceptors, targetSource); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; &#125; return null;&#125; 这段代码是在bean实例化前调用的。这段代码重点看TargetSource targetSource = getCustomTargetSource(beanClass, beanName);。 12345678910111213141516171819202122// AbstractAutoProxyCreator@Nullableprotected TargetSource getCustomTargetSource(Class&lt;?&gt; beanClass, String beanName) &#123; // We can&#x27;t create fancy target sources for directly registered singletons. if (this.customTargetSourceCreators != null &amp;&amp; this.beanFactory != null &amp;&amp; this.beanFactory.containsBean(beanName)) &#123; for (TargetSourceCreator tsc : this.customTargetSourceCreators) &#123; TargetSource ts = tsc.getTargetSource(beanClass, beanName); if (ts != null) &#123; // Found a matching TargetSource. if (logger.isTraceEnabled()) &#123; logger.trace(&quot;TargetSourceCreator [&quot; + tsc + &quot;] found custom TargetSource for bean with name &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; return ts; &#125; &#125; &#125; // No custom TargetSource found. return null;&#125; 这段代码中customTargetSourceCreators的值是通过setCustomTargetSourceCreators方法设置的，而AbstractAutoProxyCreator又是AnnotationAwareAspectJAutoProxyCreator或者InfrastructureAdvisorAutoProxyCreator的父类，所以这里的customTargetSourceCreators就是CustomTargetSourceCreator了。所以现在只需要看CustomTargetSourceCreator的getTargetSource方法。而该方法的实现在其父类中，看源码 1234567891011121314151617181920212223242526272829303132// AbstractBeanFactoryBasedTargetSourceCreator@Override@Nullablepublic final TargetSource getTargetSource(Class&lt;?&gt; beanClass, String beanName) &#123; AbstractBeanFactoryBasedTargetSource targetSource = createBeanFactoryBasedTargetSource(beanClass, beanName); if (targetSource == null) &#123; return null; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Configuring AbstractBeanFactoryBasedTargetSource: &quot; + targetSource); &#125; DefaultListableBeanFactory internalBeanFactory = getInternalBeanFactoryForBean(beanName); // We need to override just this bean definition, as it may reference other beans // and we&#x27;re happy to take the parent&#x27;s definition for those. // Always use prototype scope if demanded. BeanDefinition bd = this.beanFactory.getMergedBeanDefinition(beanName); GenericBeanDefinition bdCopy = new GenericBeanDefinition(bd); if (isPrototypeBased()) &#123; bdCopy.setScope(BeanDefinition.SCOPE_PROTOTYPE); &#125; internalBeanFactory.registerBeanDefinition(beanName, bdCopy); // Complete configuring the PrototypeTargetSource. targetSource.setTargetBeanName(beanName); targetSource.setBeanFactory(internalBeanFactory); return targetSource;&#125; 第一行就是createBeanFactoryBasedTargetSource就是我们实现的，返回一个TargetSource对象。如果真的返回了TargetSource对象后，就会执行这块代码 12345678910TargetSource targetSource = getCustomTargetSource(beanClass, beanName);if (targetSource != null) &#123; if (StringUtils.hasLength(beanName)) &#123; this.targetSourcedBeans.add(beanName); &#125; Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(beanClass, beanName, targetSource); Object proxy = createProxy(beanClass, beanName, specificInterceptors, targetSource); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy;&#125; 来完成切面的收集和为目标对象进行切面匹配，最后通过匹配到的切面完成代理对象的创建，这时代理对象就包含了 TargetSource对象 切面链表 最后把对象返回，并且最后会执行InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation方法。最后createBean方法结束，然后把返回的Bean加入到一级缓存（如果是单例）中。 自定义的TargetSource对象的入口需要考虑顺序123456789101112131415161718192021222324252627@Componentpublic class SetCustomTargetSourceCreator implements BeanPostProcessor, PriorityOrdered, BeanFactoryAware &#123; private BeanFactory beanFactory; @Override public int getOrder() &#123; return 45; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if(bean instanceof AnnotationAwareAspectJAutoProxyCreator || bean instanceof InfrastructureAdvisorAutoProxyCreator) &#123; AbstractAdvisorAutoProxyCreator proxyCreator = (AbstractAdvisorAutoProxyCreator)bean; CustomTargetSourceCreator customTargetSourceCreator = new CustomTargetSourceCreator(); customTargetSourceCreator.setBeanFactory(beanFactory); proxyCreator.setCustomTargetSourceCreators(customTargetSourceCreator); &#125; return bean; &#125; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; this.beanFactory = beanFactory; &#125;&#125; 我写的入口方法是实现了BeanPostProcessor接口。 从上边的解析中知道，自定义的TragetSource对象的生成是使用了BeanPostProcessor的。而AnnotationAwareAspectJAutoProxyCreator或者instanceof InfrastructureAdvisorAutoProxyCreator这两个接口是实现了Ordered接口的。而且这些BeanPostProcessor接口的是有顺序的，在之前就解析过，这个顺序是在在AbstractApplicationContext的registerBeanPostProcessors是确定的。这个方法的执行逻辑如下 获取所有实现了BeanPostProcessor接口的BeanName 遍历，把实现了PriorityOrdered接口的beanName找出来，然后通过beanFactory.getBean(beanName)方法获取实例，让后放到列表中。便利结束后排序，然后添加到beanFactory的beanPostProcessors列表中。 遍历，把实现了Ordered接口的beanName找出来，然后通过beanFactory.getBean(beanName)方法获取实例，让后放到列表中。便利结束后排序，然后添加到beanFactory的beanPostProcessors列表中。 遍历，把没有实现PriorityOrdered和Ordered接口的beanName找出来，然后通过beanFactory.getBean(beanName)方法获取实例，让后放到列表中。便利结束后排序，然后添加到beanFactory的beanPostProcessors列表中。 遍历，把实现了接口MergedBeanDefinitionPostProcessor的beanName找出来，然后通过beanFactory.getBean(beanName)方法获取实例，让后放到列表中。便利结束后排序，然后添加到beanFactory的beanPostProcessors列表中。 最终的beanPostProcessors的顺序 1234List BeanPostProcessor,PriorityOrderedList BeanPostProcessor,OrderedList BeanPostProcessorList MergedBeanDefinitionPostProcessor 而AnnotationAwareAspectJAutoProxyCreator或者instanceof InfrastructureAdvisorAutoProxyCreator是在第二个List中，所以我的入门方法只需要保证在第一个List中就行了。但是不能是在第二个或之后的List中。因为如果在第二个List中时，beanFactory.getBean(beanName)时，此时的beanPostProcessors列表只包含第一List的也就是实现了PriorityOrdered接口的，只有第二list的BeanPostProcessor全部getBean排序后才会加入到postBeanProcessors列表中。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"11-Spring中多例TargetSource","slug":"spring/11-Spring中多例TargetSource","date":"2021-11-25T12:00:18.000Z","updated":"2022-03-23T09:03:55.242Z","comments":true,"path":"blog/spring/11-Spring中多例TargetSource/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/11-Spring%E4%B8%AD%E5%A4%9A%E4%BE%8BTargetSource/","excerpt":"","text":"在Spring中bean的初始化过程中简单的说过多例，就是每次beanFactory.getBean时都会生成一个实例，这点是没问题的。 那么现在有这样类的定义： 1234567891011121314151617181920@Component@Scope(value = DefaultListableBeanFactory.SCOPE_PROTOTYPE)public class ScopedProxyBean &#123; public void code() &#123; System.out.println(this.hashCode()); &#125;&#125;@Componentpublic class MyBean &#123; @Autowired private ScopedProxyBean scopedProxyBean; public void tet() &#123; scopedProxyBean.code(); &#125;&#125; 上面的代码很简单，就是把ScopedProxyBean设置成多例，然后MyBean中DI这个类。而在MyBean，中调用scopedProxyBean.code()来打印ScopedProxyBean的hashcode。现在有这样的测试代码： 12345678@Testpublic void test9() &#123; AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(ScanBean.class); MyBean bean = applicationContext.getBean(MyBean.class); for (int i = 0; i &lt; 10; i++) &#123; bean.tet(); &#125;&#125; 结果： 因为，对于一个多例的Spring Bean，MyBean在DI时调用了一次getBean已经获取到一个ScopedProxyBean实例了，那么在MyBean这个类中的ScopedProxyBean的对对象已经确定了，所以hashcode才会相同。而别的对象中，调用getBean又会获取不同的对象。 单例和多例的不同就是单例使用了缓存，让第二次getbean时从一级缓存stangleObjects这个Map中取对象，而多例没有使用缓存，所以每次getbean都会创建新的对象。 现在改下ScopedProxyBean： 12345678@Component@Scope(value = DefaultListableBeanFactory.SCOPE_PROTOTYPE, proxyMode = ScopedProxyMode.TARGET_CLASS)public class ScopedProxyBean &#123; public void code() &#123; System.out.println(this.hashCode()); &#125;&#125; 也就加了proxyMode = ScopedProxyMode.TARGET_CLASS 其他都一样。测试结果： 可以看到，每次答应的结果都不同，这是不是意味着MyBean中的ScopedProxyBean会变化？如果真的这样，就破坏了Spring的单例原则，而且在运行时每次都改变MyBean中的ScopedProxyBean引用指向的对象这种行为也不可能实现。造成上面的情况，只可能是使用了代理，在依赖注入的时候先为MyBean中的ScopedProxyBean的引用指向一个代理对象。而在运行时，通过代理对象调用方法时会新建一个对象来进行方法调用，只有通过这样的方法才会造成这种现象。现在看下MyBean中的ScopedProxyBean的引用指向的对象： 可以看到，是一个CGLIB的代理对象。可以确定在@Scope中加上 1@Scope(value = DefaultListableBeanFactory.SCOPE_PROTOTYPE, proxyMode = ScopedProxyMode.TARGET_CLASS) 会导致生成代理对象。那问题来了，是在什么时候检查到需要生成代理对象的。看扫描类生成BeanDefinition的源码（这里以注解的上下文对象为例），在ConfigurationClassPostProcessor中扫描@Component注解的逻辑中： 1AnnotationConfigUtils 1ScopedProxyUtils 好了，现在重点看下ScopedProxyUtils.createScopedProxy这个方法。 从上边可知，在这种情况下 1@Scope(value = DefaultListableBeanFactory.SCOPE_PROTOTYPE, proxyMode = ScopedProxyMode.TARGET_CLASS) proxyTargetClass为true。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//ScopedProxyUtilspublic static BeanDefinitionHolder createScopedProxy(BeanDefinitionHolder definition, BeanDefinitionRegistry registry, boolean proxyTargetClass) &#123; // 原来的beanName String originalBeanName = definition.getBeanName(); BeanDefinition targetDefinition = definition.getBeanDefinition(); // 把targetName设置成 scopedTarget. + beanName 这种模式，比如：scopedTarget.xyzBean String targetBeanName = getTargetBeanName(originalBeanName); // Create a scoped proxy definition for the original bean name, // &quot;hiding&quot; the target bean in an internal target definition. // 把BeanDefinition的目标对象的类型设置为ScopedProxyFactoryBean RootBeanDefinition proxyDefinition = new RootBeanDefinition(ScopedProxyFactoryBean.class); proxyDefinition.setDecoratedDefinition(new BeanDefinitionHolder(targetDefinition, targetBeanName)); proxyDefinition.setOriginatingBeanDefinition(targetDefinition); proxyDefinition.setSource(definition.getSource()); proxyDefinition.setRole(targetDefinition.getRole()); // 添加属性targetBeanName为targetBeanName // 通过这样设置会，在通过BeanDefintion创建对象时会调用ScopedProxyFactoryBean的setTargetBeanName方法。 proxyDefinition.getPropertyValues().add(&quot;targetBeanName&quot;, targetBeanName); if (proxyTargetClass) &#123; targetDefinition.setAttribute(AutoProxyUtils.PRESERVE_TARGET_CLASS_ATTRIBUTE, Boolean.TRUE); // ScopedProxyFactoryBean&#x27;s &quot;proxyTargetClass&quot; default is TRUE, so we don&#x27;t need to set it explicitly here. &#125; else &#123; proxyDefinition.getPropertyValues().add(&quot;proxyTargetClass&quot;, Boolean.FALSE); &#125; // Copy autowire settings from original bean definition. // 设置是否允许注入，默认值为true proxyDefinition.setAutowireCandidate(targetDefinition.isAutowireCandidate()); // 设置Primary proxyDefinition.setPrimary(targetDefinition.isPrimary()); if (targetDefinition instanceof AbstractBeanDefinition) &#123; proxyDefinition.copyQualifiersFrom((AbstractBeanDefinition) targetDefinition); &#125; // The target bean should be ignored in favor of the scoped proxy. targetDefinition.setAutowireCandidate(false); targetDefinition.setPrimary(false); // 注册旧的beanDefinition // Register the target bean as separate bean in the factory. registry.registerBeanDefinition(targetBeanName, targetDefinition); // Return the scoped proxy definition as primary bean definition // (potentially an inner bean). // 这里的BeanDefinitionHolder指向的beanDefinition为新的，而且originalBeanName就是我们自己定义的beanName return new BeanDefinitionHolder(proxyDefinition, originalBeanName, definition.getAliases());&#125; 可以看到，对于加上了ScopedProxyMode.TARGET_CLASS的@Scope，会在往BeanFactory注册BeanDefinition前会在扫描出来的BeanDefinition的基础上创建一个新的BeanDefinition，而新的BeanDefinition对在其属性中加上这个属性： 1org.springframework.aop.framework.autoproxy.AutoProxyUtils.preserveTargetClass=true 而且新的BeanDefinition的被允许为可以依赖注入并设置Primary属性，而且旧的BeanDefinition把是否允许注入设置为false了，这也就是说，旧的beanDefinition创建的对象是不会被注入到其他对象中的。 而且上边有4点很重要 就是beanName了，上边会有两个beanName，一个是originalBeanName，这个就是我们定义的benName（默认）；第二个就是targetBeanName，这个targetBeanName的值被设置成了scopedTarget. + originalBeanName这种模式。比如：scopedTarget.xyzBean。在最后注册BeanDefinition的时候，会生成如下的关系 12&lt;targetBeanName, 旧的BeanDefinition&gt;&lt;originalBeanName, 新的BeanDefinition&gt; 新建的BeanDefinition的目标对象是ScopedProxyFactoryBean这个对象 新建的BeanDefinition通过 1proxyDefinition.getPropertyValues().add(&quot;targetBeanName&quot;, targetBeanName); 设置了targetBeanName属性，这样设置后，在通过BeanDefinition创建对象的时候会调用调用ScopedProxyFactoryBean的setTargetBeanName方法。 新的BeanDefinition的scope值为默认值，也就是单例；而旧的BeanDefinition的scope为我们bean定义的值，在上边的例子中，就是多例 所以通过上边的4点可知，根据新的BeanDefinition创建的对象，就是我们在代码中通过依赖注入主人的对象，该对象的BeanName就是我们定义的。而通过旧的BeanDefinition创建的对象是多例的，而且不会被依赖注入，而且BanName为scopedTarget.$&#123;BeanName&#125;这种模式。所我我们现在只需要关注新BeanDefintion的定义所对应的类，也就是ScopedProxyFactoryBean。看代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public class ScopedProxyFactoryBean extends ProxyConfig implements FactoryBean&lt;Object&gt;, BeanFactoryAware, AopInfrastructureBean &#123; /** The TargetSource that manages scoping. */ private final SimpleBeanTargetSource scopedTargetSource = new SimpleBeanTargetSource(); /** The name of the target bean. */ @Nullable private String targetBeanName; /** The cached singleton proxy. */ @Nullable private Object proxy; /** * Create a new ScopedProxyFactoryBean instance. */ public ScopedProxyFactoryBean() &#123; setProxyTargetClass(true); &#125; /** * Set the name of the bean that is to be scoped. */ public void setTargetBeanName(String targetBeanName) &#123; this.targetBeanName = targetBeanName; this.scopedTargetSource.setTargetBeanName(targetBeanName); &#125; @Override public void setBeanFactory(BeanFactory beanFactory) &#123; if (!(beanFactory instanceof ConfigurableBeanFactory)) &#123; throw new IllegalStateException(&quot;Not running in a ConfigurableBeanFactory: &quot; + beanFactory); &#125; ConfigurableBeanFactory cbf = (ConfigurableBeanFactory) beanFactory; this.scopedTargetSource.setBeanFactory(beanFactory); ProxyFactory pf = new ProxyFactory(); pf.copyFrom(this); pf.setTargetSource(this.scopedTargetSource); Assert.notNull(this.targetBeanName, &quot;Property &#x27;targetBeanName&#x27; is required&quot;); Class&lt;?&gt; beanType = beanFactory.getType(this.targetBeanName); if (beanType == null) &#123; throw new IllegalStateException(&quot;Cannot create scoped proxy for bean &#x27;&quot; + this.targetBeanName + &quot;&#x27;: Target type could not be determined at the time of proxy creation.&quot;); &#125; if (!isProxyTargetClass() || beanType.isInterface() || Modifier.isPrivate(beanType.getModifiers())) &#123; pf.setInterfaces(ClassUtils.getAllInterfacesForClass(beanType, cbf.getBeanClassLoader())); &#125; // Add an introduction that implements only the methods on ScopedObject. ScopedObject scopedObject = new DefaultScopedObject(cbf, this.scopedTargetSource.getTargetBeanName()); pf.addAdvice(new DelegatingIntroductionInterceptor(scopedObject)); // Add the AopInfrastructureBean marker to indicate that the scoped proxy // itself is not subject to auto-proxying! Only its target bean is. pf.addInterface(AopInfrastructureBean.class); this.proxy = pf.getProxy(cbf.getBeanClassLoader()); &#125; @Override public Object getObject() &#123; if (this.proxy == null) &#123; throw new FactoryBeanNotInitializedException(); &#125; return this.proxy; &#125; @Override public Class&lt;?&gt; getObjectType() &#123; if (this.proxy != null) &#123; return this.proxy.getClass(); &#125; return this.scopedTargetSource.getTargetClass(); &#125; @Override public boolean isSingleton() &#123; return true; &#125;&#125; 可以看到，它是实现了 FactoryBean接口，而且新建的BeanDefinition能够被依赖注入，也就是说该类对象的生命周期被Spring控制，所以在依赖注入时，被注入的对象的类型是由getObjectType决定的，而ScopedProxyFactoryBean的getObjectType方法看下图： 现在看下ScopedProxyFactoryBean#getTargetClass。 12345678910111213141516171819202122232425//ScopedProxyFactoryBean@Overridepublic Class&lt;?&gt; getTargetClass() &#123; Class&lt;?&gt; targetClass = this.targetClass; if (targetClass != null) &#123; return targetClass; &#125; synchronized (this) &#123; // Full check within synchronization, entering the BeanFactory interaction algorithm only once... targetClass = this.targetClass; if (targetClass == null &amp;&amp; this.beanFactory != null) &#123; // Determine type of the target bean. targetClass = this.beanFactory.getType(this.targetBeanName); if (targetClass == null) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Getting bean with name &#x27;&quot; + this.targetBeanName + &quot;&#x27; for type determination&quot;); &#125; Object beanInstance = this.beanFactory.getBean(this.targetBeanName); targetClass = beanInstance.getClass(); &#125; this.targetClass = targetClass; &#125; return targetClass; &#125;&#125; 可以看到，会根据targetBeanName去获取目标类的class对象。从上边的ScopedProxyFactoryBean的BeanDefinition定义可知，在创建ScopedProxyFactoryBean对象的时候会调用其setTargetBeanName方法： 1234public void setTargetBeanName(String targetBeanName) &#123; this.targetBeanName = targetBeanName; this.scopedTargetSource.setTargetBeanName(targetBeanName);&#125; 所以，targetBeanName就有值了，而且该值的有个固定前缀scopedTarget.。所以通过getTargetClass获取到的就是我们定义的Bean的类型，以最开始的例子为例，targetClass = ScopedProxyBean.class。 而注入的对象是由getObject()方法决定的 通过上边的分析，已知ScopedProxyFactoryBean是能依赖注入的。以MyBean和ScopedProxyBean为例，getObjectType()返回的class对象就是ScopedProxyBean.class，而在依赖注入时getBean获取到了ScopedProxyFactoryBean这个类的实例，而由于这个类的实例是实现了FactoryBean接口，那么就会在缓存FactoryBeanRegistrySupport.factoryBeanObjectCache中拿目标对象，缓存中没有就调用getObject()方法，而在这里getObject()返回返回的是一个proxy，是一个代理对象。而这个代理对象是什么时候生成的，那么就要看ScopedProxyFactoryBean实现的另一个接口——BeanFactoryAware了。 BeanFactoryAware这个接口会在Bean创建完，并完成依赖注入后会调用，现在看这个接口的方法setBeanFactory(BeanFactory beanFactory) 1234567891011121314151617181920212223242526272829303132333435//ScopedProxyFactoryBean @Overridepublic void setBeanFactory(BeanFactory beanFactory) &#123; if (!(beanFactory instanceof ConfigurableBeanFactory)) &#123; throw new IllegalStateException(&quot;Not running in a ConfigurableBeanFactory: &quot; + beanFactory); &#125; ConfigurableBeanFactory cbf = (ConfigurableBeanFactory) beanFactory; this.scopedTargetSource.setBeanFactory(beanFactory); ProxyFactory pf = new ProxyFactory(); pf.copyFrom(this); pf.setTargetSource(this.scopedTargetSource); Assert.notNull(this.targetBeanName, &quot;Property &#x27;targetBeanName&#x27; is required&quot;); // 设置目标接口 Class&lt;?&gt; beanType = beanFactory.getType(this.targetBeanName); if (beanType == null) &#123; throw new IllegalStateException(&quot;Cannot create scoped proxy for bean &#x27;&quot; + this.targetBeanName + &quot;&#x27;: Target type could not be determined at the time of proxy creation.&quot;); &#125; if (!isProxyTargetClass() || beanType.isInterface() || Modifier.isPrivate(beanType.getModifiers())) &#123; pf.setInterfaces(ClassUtils.getAllInterfacesForClass(beanType, cbf.getBeanClassLoader())); &#125; // Add an introduction that implements only the methods on ScopedObject. ScopedObject scopedObject = new DefaultScopedObject(cbf, this.scopedTargetSource.getTargetBeanName()); pf.addAdvice(new DelegatingIntroductionInterceptor(scopedObject)); // Add the AopInfrastructureBean marker to indicate that the scoped proxy // itself is not subject to auto-proxying! Only its target bean is. pf.addInterface(AopInfrastructureBean.class); this.proxy = pf.getProxy(cbf.getBeanClassLoader());&#125; targetBeanName&#96;还是和上边的一样 这里就是整个@Scope(value = DefaultListableBeanFactory.SCOPE_PROTOTYPE, proxyMode = ScopedProxyMode.TARGET_CLASS)的关键，首先从整体看，它就是去创建一个代理对象的，但这里有一个非常关键的代码，就是 12ProxyFactory pf = new ProxyFactory();pf.setTargetSource(this.scopedTargetSource); 这段代码就是赠段代码的核心，但却很简单，就是往ProxyFactory设置了一个targetSource对象，这个对象就是SimpleBeanTargetSource。 这个对象是在创建 ScopedProxyFactoryBean的时候创建的 而且在创建的玩后会调用ScopedProxyFactoryBean#setTargetBeanName方法，会设置scopedTargetSource的targetBeanName。 这个SimpleBeanTargetSource最终会在代理对象的方法调用时被调用，但现在先跟踪下代理对象的创建过程也就是ProxyFactory.getProxy 12345678910111213141516171819202122232425262728public Object getProxy(@Nullable ClassLoader classLoader) &#123; return createAopProxy().getProxy(classLoader);&#125;protected final synchronized AopProxy createAopProxy() &#123; if (!this.active) &#123; activate(); &#125; return getAopProxyFactory().createAopProxy(this);&#125;//这里的AdvisedSupport config = this = 最开始的ProxyFactory pf = new ProxyFactory();public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) &#123; Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException(&quot;TargetSource cannot determine target class: &quot; + &quot;Either an interface or a target is required for proxy creation.&quot;); &#125; if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) &#123; return new JdkDynamicAopProxy(config); &#125; return new ObjenesisCglibAopProxy(config); &#125; else &#123; return new JdkDynamicAopProxy(config); &#125;&#125; 这里有两个代理，而从上文已知代理对象是使用了CGLIB的代理，但由于我更加熟悉jdk原生动态代理而且，有设置接口，所以这里就以JdkDynamicAopProxy为例，而CGLIB的调用逻辑也是一样的，就是API不同而已。 看JdkDynamicAopProxy.getProxy 123456789//JdkDynamicAopProxypublic Object getProxy(@Nullable ClassLoader classLoader) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Creating JDK dynamic proxy: &quot; + this.advised.getTargetSource()); &#125; Class&lt;?&gt;[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this);&#125; 从这里可知，InvocationHandler是this，也就是JdkDynamicAopProxy，现在看invoke方法，在代理对象调用方法时都会进入到这个方法，invoke在[10-Spring AOP中的方法调用](.&#x2F;10-Spring AOP中的方法调用)中对已经看过了，不过那时时关注的是切面的调用过程，和目标对象的方法的是怎么调用的，但没关注目标对象是怎么获取的。这里就看下目标对象是怎么获取的。 在invoke中先通过这段代码获取TargetSource 1TargetSource targetSource = this.advised.targetSource; 然后通过这段代码获取目对象: 1target = targetSource.getTarget(); 好了，这就结束。现在回忆下AOP的代理对象是怎么生成的，看代码AbstractAutoProxyCreator.createProxy: 生成的过程和多实例代理一样，不同点在于TargetSource不同，AOP中用到了SingletonTargetSource，这个其实很简单，就是把目标对象放进去就，然后get出来。而多实例的代理是SimpleBeanTargetSource，现在看SimpleBeanTargetSource的定义: 12345678public class SimpleBeanTargetSource extends AbstractBeanFactoryBasedTargetSource &#123; @Override public Object getTarget() throws Exception &#123; return getBeanFactory().getBean(getTargetBeanName()); &#125;&#125; 很简单，就是使用beanFactory重新去获取一个新的对象，所以这里就解析了为什么加了proxyMode = ScopedProxyMode.TARGET_CLASS每次调用对象都不同。因为在这里的targetBeanName的值是targetSource.scopedProxyBean，该对象的BeanDefinition是被设置成多例的，所以在getBean的时候就会新建一个对象。 注意SimpleBeanTargetSource中的getBeanFactory()是在运行时就已经设置进去了，就是TargetSource接口的初始化过程。这个过程和bean代理的提前生成有关。 CGLIB就类推就好了，逻辑都是一样的，就是API不同。 在Spring中，一个Bean的beanName有targetSource.这个前缀的话，就代表该Bean只是一个单纯的对象，不会参与到依赖注入，也就是说不会被其他对象引用，所以这种Bean从功能的角度看，没有任何作用。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"11-Spring-用接口实现AOP","slug":"spring/11-Spring-用接口实现AOP","date":"2021-11-25T12:00:17.000Z","updated":"2022-03-23T09:03:55.203Z","comments":true,"path":"blog/spring/11-Spring-用接口实现AOP/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/11-Spring-%E7%94%A8%E6%8E%A5%E5%8F%A3%E5%AE%9E%E7%8E%B0AOP/","excerpt":"","text":"感觉使用AOP接口更加的灵活。 Advisor123456789101112131415161718192021222324@Componentpublic class CustomAdvisor implements PointcutAdvisor &#123; @Autowired private CustomPointcut pointcut; @Autowired private CustomAdvice advice; @Override public Pointcut getPointcut() &#123; return pointcut; &#125; @Override public Advice getAdvice() &#123; return advice; &#125; @Override public boolean isPerInstance() &#123; return false; &#125;&#125; Advice1234567891011@Componentpublic class CustomAdvice implements MethodInterceptor &#123; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; System.out.println(&quot;我是自定义的AOP--before&quot;); Object obj = invocation.proceed(); System.out.println(&quot;我是自定义的AOP--after&quot;); return obj; &#125;&#125; Pointcut1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Componentpublic class CustomPointcut implements Pointcut, MethodMatcher &#123; /** * 所有类都匹配 * @return */ @Override public ClassFilter getClassFilter() &#123; return ClassFilter.TRUE; &#125; @Override public MethodMatcher getMethodMatcher() &#123; return this; &#125; /** * 所有方法都匹配 * @param method * @param targetClass * @return */ @Override public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; //拿原始方法对象，这个方法上才有注解 Method specificMethod = AopUtils.getMostSpecificMethod(method, targetClass); if(AnnotatedElementUtils.hasAnnotation(specificMethod, CustomCache.class)) &#123; return true; &#125; return false; &#125; @Override public boolean isRuntime() &#123; return true; &#125; @Override public boolean matches(Method method, Class&lt;?&gt; targetClass, Object... args) &#123; if (args.length &gt; 0 &amp;&amp; args[0] != null) &#123; return &quot;xyz&quot;.equalsIgnoreCase(args[0].toString()); &#125; return false; &#125;&#125; 自定义注解CustomCache123456@Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface CustomCache &#123;&#125; 测试12345678@Testpublic void test4() &#123; AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(ScanBean.class, BeanPostProcessorPro.class); AopBean1 aopBean1 = applicationContext.getBean(AopBean1.class); System.out.println(aopBean1); aopBean1.print(&quot;xx&quot;); aopBean1.print(&quot;xyz&quot;);&#125; 分析逻辑很简单，就是匹配方法是否有CustomCache这个注解和入参是否为”xyz”。 结果： 这里我分析些下CustomPointcut的方法 12345678910@Overridepublic boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; //拿原始方法对象，这个方法上才有注解 Method specificMethod = AopUtils.getMostSpecificMethod(method, targetClass); if(AnnotatedElementUtils.hasAnnotation(specificMethod, CustomCache.class)) &#123; return true; &#125; return false;&#125; 这里，我为什么不这样写呢？ 1234567@Overridepublic boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; if(AnnotatedElementUtils.hasAnnotation(method, CustomCache.class)) &#123; return true; &#125; return false;&#125; 首先，这个方法matches对于一个对象来说至少会被执行一次，对于一个需要被增强的对象来说至少被执行两次。 第一次是在生成代理类对象之前，第二次是在方法调用的过程中。 在这两次的过程，传入的Method是不同的。 第一次：看源码从这里开始 到这里结束： 这个class对象是通过bean.getClass()获取的，也就是说，第一个Method对象是从bean类型类中获取的。 第二次是通过动态代理对象中的的invoke获取的 那么这个invoke的Method对象是怎么来的，写了个测试方法 1234567891011121314@Testpublic void test5() &#123; byte[] $Proxy0s = ProxyGenerator.generateProxyClass(&quot;$Proxy0&quot;, new Class[]&#123;AopBean.class&#125;); try &#123; FileOutputStream fileOutputStream = new FileOutputStream(&quot;$Proxy0.class&quot;); fileOutputStream.write($Proxy0s); fileOutputStream.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 这个测试方法是把动态代理的class打印出来 看这个代理类的字节码文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public final class $Proxy0 extends Proxy implements AopBean &#123; private static Method m1; private static Method m2; private static Method m0; private static Method m3; public $Proxy0(InvocationHandler var1) throws &#123; super(var1); &#125; public final boolean equals(Object var1) throws &#123; try &#123; return (Boolean)super.h.invoke(this, m1, new Object[]&#123;var1&#125;); &#125; catch (RuntimeException | Error var3) &#123; throw var3; &#125; catch (Throwable var4) &#123; throw new UndeclaredThrowableException(var4); &#125; &#125; public final String toString() throws &#123; try &#123; return (String)super.h.invoke(this, m2, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final int hashCode() throws &#123; try &#123; return (Integer)super.h.invoke(this, m0, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final void print(String var1) throws &#123; try &#123; super.h.invoke(this, m3, new Object[]&#123;var1&#125;); &#125; catch (RuntimeException | Error var3) &#123; throw var3; &#125; catch (Throwable var4) &#123; throw new UndeclaredThrowableException(var4); &#125; &#125; static &#123; try &#123; m1 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;equals&quot;, Class.forName(&quot;java.lang.Object&quot;)); m2 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;toString&quot;); m0 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;hashCode&quot;); m3 = Class.forName(&quot;com.enjoy.xyz.aop.bean.AopBean&quot;).getMethod(&quot;print&quot;, Class.forName(&quot;java.lang.String&quot;)); &#125; catch (NoSuchMethodException var2) &#123; throw new NoSuchMethodError(var2.getMessage()); &#125; catch (ClassNotFoundException var3) &#123; throw new NoClassDefFoundError(var3.getMessage()); &#125; &#125;&#125; 可以看到，这个Method对象对象是通过 1Class.forName(&quot;com.enjoy.xyz.aop.bean.AopBean&quot;).getMethod(&quot;print&quot;, Class.forName(&quot;java.lang.String&quot;)); 这样获取的，也就是获取接口的Method对象。 所以，这两次由于获取Method对象的来源不同，导致这两个Method对象拥有的信息不一样，比如这个方法是有注解的，那么第一次的Method对象是能获取到注解的信息的，而第二次不行。 添加了这样的条件断点 注意，以上的情况只会出现在被代理对象使用了接口的情况。 所以才在matches中用这行代码来拿原始方法对象，这个方法上才有注解 1Method specificMethod = AopUtils.getMostSpecificMethod(method, targetClass); 这行代码的原理就是由于matches中targetClass是被代理对象的class对象，所以从这里获取的Method才是准的。 其他AopContext.currentProxy()在这样定义后 1@EnableAspectJAutoProxy(exposeProxy = true) 就能在项目中使用AopContext.currentProxy()来获取代理类。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"10-Spring AOP中的方法调用","slug":"spring/10-Spring AOP中的方法调用","date":"2021-11-25T12:00:16.000Z","updated":"2022-03-23T09:03:55.162Z","comments":true,"path":"blog/spring/10-Spring AOP中的方法调用/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/10-Spring%20AOP%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8/","excerpt":"","text":"在前面讲过，代理对象有两种生成方式（如果被代理对象没接口就只能用cglib） 在之前的对getProxy的代码分析中，已经知道，createAopProxy最终有代理类 JdkDynamicAopProxy ObjenesisCglibAopProxy JdkDynamicAopProxy在AOP解析已经了解代理对象的InvocationHandler就是JdkDynamicAopProxy 所以看invoke方法就行了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object oldProxy = null; boolean setProxyContext = false; //从代理工厂中拿到TargetSource对象，该对象包装了被代理实例bean TargetSource targetSource = this.advised.targetSource; Object target = null; try &#123; //被代理对象的equals方法和hashCode方法是不能被代理的，不会走切面 if (!this.equalsDefined &amp;&amp; AopUtils.isEqualsMethod(method)) &#123; // The target does not implement the equals(Object) method itself. return equals(args[0]); &#125; else if (!this.hashCodeDefined &amp;&amp; AopUtils.isHashCodeMethod(method)) &#123; // The target does not implement the hashCode() method itself. return hashCode(); &#125; else if (method.getDeclaringClass() == DecoratingProxy.class) &#123; // There is only getDecoratedClass() declared -&gt; dispatch to proxy config. return AopProxyUtils.ultimateTargetClass(this.advised); &#125; else if (!this.advised.opaque &amp;&amp; method.getDeclaringClass().isInterface() &amp;&amp; method.getDeclaringClass().isAssignableFrom(Advised.class)) &#123; // Service invocations on ProxyConfig with the proxy config... return AopUtils.invokeJoinpointUsingReflection(this.advised, method, args); &#125; Object retVal; //如果该属性设置为true，则把代理对象设置到ThreadLocal中 if (this.advised.exposeProxy) &#123; // Make invocation available if necessary. oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; &#125; //这个target就是被代理实例 target = targetSource.getTarget(); Class&lt;?&gt; targetClass = (target != null ? target.getClass() : null); //从代理工厂中拿过滤器链 Object是一个MethodInterceptor类型的对象，其实就是一个advice对象 List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); //如果该方法没有执行链，则说明这个方法不需要被拦截，则直接反射调用 if (chain.isEmpty()) &#123; Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); &#125; else &#123; // We need to create a method invocation... MethodInvocation invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); // Proceed to the joinpoint through the interceptor chain. retVal = invocation.proceed(); &#125; // Massage return value if necessary. Class&lt;?&gt; returnType = method.getReturnType(); if (retVal != null &amp;&amp; retVal == target &amp;&amp; returnType != Object.class &amp;&amp; returnType.isInstance(proxy) &amp;&amp; !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) &#123; retVal = proxy; &#125; else if (retVal == null &amp;&amp; returnType != Void.TYPE &amp;&amp; returnType.isPrimitive()) &#123; throw new AopInvocationException( &quot;Null return value from advice does not match primitive return type for: &quot; + method); &#125; return retVal; &#125; finally &#123; if (target != null &amp;&amp; !targetSource.isStatic()) &#123; // Must have come from TargetSource. targetSource.releaseTarget(target); &#125; if (setProxyContext) &#123; // Restore old proxy. AopContext.setCurrentProxy(oldProxy); &#125; &#125;&#125; 我们看重要的。先执行到这里 这里的意思就是如果在在这种情况 设置了exposeProxy&#x3D;true，那么可以就会把代理对象放入到一个ThreadLocal中。在方法中就可以通过 1AopContext.currentProxy() 获取到代理对象。这是可以解决这种[问题](.&#x2F;Sping AOP在开发中的问题) 下面就是AOP调用的核心逻辑了。 1List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass) 这个方法会去检查这个执行的方法是否被拦截，并会返回匹配到的advice对象。 如果没有advice被返回，那执行了 也就是直接通过反射，用用被代理对象来调用。 而如果通过该方法有匹配成功的Advisor，那就直接来过增强逻辑的调用流程了 下面先看下这个方法advised.getInterceptorsAndDynamicInterceptionAdvice 在方法调用时，获取匹配的Advisor跟踪代码到DefaultAdvisorChainFactory#getInterceptorsAndDynamicInterceptionAdvice 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465//DefaultAdvisorChainFactorypublic List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice( Advised config, Method method, @Nullable Class&lt;?&gt; targetClass) &#123; // This is somewhat tricky... We have to process introductions first, // but we need to preserve order in the ultimate list. AdvisorAdapterRegistry registry = GlobalAdvisorAdapterRegistry.getInstance(); //从代理工厂中获得该被代理类的所有切面advisor，config就是代理工厂对象 Advisor[] advisors = config.getAdvisors(); List&lt;Object&gt; interceptorList = new ArrayList&lt;&gt;(advisors.length); Class&lt;?&gt; actualClass = (targetClass != null ? targetClass : method.getDeclaringClass()); Boolean hasIntroductions = null; for (Advisor advisor : advisors) &#123; //大部分走这里 if (advisor instanceof PointcutAdvisor) &#123; // Add it conditionally. PointcutAdvisor pointcutAdvisor = (PointcutAdvisor) advisor; //如果切面的pointCut和被代理对象是匹配的，说明是切面要拦截的对象..先进行类匹配 if (config.isPreFiltered() || pointcutAdvisor.getPointcut().getClassFilter().matches(actualClass)) &#123; //先类匹配，然后在方法匹配 MethodMatcher mm = pointcutAdvisor.getPointcut().getMethodMatcher(); boolean match; if (mm instanceof IntroductionAwareMethodMatcher) &#123; if (hasIntroductions == null) &#123; hasIntroductions = hasMatchingIntroductions(advisors, actualClass); &#125; match = ((IntroductionAwareMethodMatcher) mm).matches(method, actualClass, hasIntroductions); &#125; else &#123; match = mm.matches(method, actualClass); &#125; //如果类和方法都匹配 if (match) &#123; //获取到切面advisor中的advice，并且包装成MethodInterceptor类型的对象 MethodInterceptor[] interceptors = registry.getInterceptors(advisor); if (mm.isRuntime()) &#123; // Creating a new object instance in the getInterceptors() method // isn&#x27;t a problem as we normally cache created chains. for (MethodInterceptor interceptor : interceptors) &#123; interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptor, mm)); &#125; &#125; else &#123; interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; &#125; &#125; //如果是引介切面 else if (advisor instanceof IntroductionAdvisor) &#123; IntroductionAdvisor ia = (IntroductionAdvisor) advisor; if (config.isPreFiltered() || ia.getClassFilter().matches(actualClass)) &#123; Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; else &#123; Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; return interceptorList;&#125; 先获取了 上述方法中的config就是 这个advisors就是在AOP解析最后匹配成功的Advisor 接着执行到这个for循环 上边的代码块就是进行了类匹配和方法匹配，如果匹配成功了会执行到 这里，下面就讲这代码块 getInterceptors看一个方法 可以看到，这个方法里会做一些类型的转变，就看一个AfterReturningAdviceAdapter 可以看到，这里会把AfterRetruningAdvice这个Advice封装成一个AfterReturningAdviceInterceptor，也就是存在这样的封装 AfterReturningAdvice——AfterReturningAdviceInterceptor MethodBeforeAdviceAdapter——MethodBeforeAdviceInterceptor ThrowsAdviceAdapter——ThrowsAdviceInterceptor getInterceptors方法执行完后getInterceptors方法执行完后有这样一个判断。 就是MethodMatcher.isRuntime放回true后，那么就会把MethodInterceptor封装成InterceptorAndDynamicMethodMatcher。这个的作用在这先透露下，就是在Advice执行前做方法的参数校验的。 赠强方法的调用过程——ReflectiveMethodInvocation#proceed回到JdkDynamicAopProxy的invoke方法。 现在假如在方法调用时，调用的方法找到匹配的Advice列表（一条责任链），会执行下面的代码块。这也是 先看下ReflectiveMethodInvocation的构造方法 123456789101112private int currentInterceptorIndex = -1;protected ReflectiveMethodInvocation( Object proxy, @Nullable Object target, Method method, @Nullable Object[] arguments, @Nullable Class&lt;?&gt; targetClass, List&lt;Object&gt; interceptorsAndDynamicMethodMatchers) &#123; this.proxy = proxy; this.target = target; this.targetClass = targetClass; this.method = BridgeMethodResolver.findBridgedMethod(method); this.arguments = AopProxyUtils.adaptArgumentsIfNecessary(method, arguments); this.interceptorsAndDynamicMethodMatchers = interceptorsAndDynamicMethodMatchers;&#125; 看下ReflectiveMethodInvocation#proceed方法 12345678910111213141516171819202122232425262728293031@Override@Nullablepublic Object proceed() throws Throwable &#123; // We start with an index of -1 and increment early. if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) &#123; return invokeJoinpoint(); &#125; Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) &#123; // Evaluate dynamic method matcher here: static part will already have // been evaluated and found to match. InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice; Class&lt;?&gt; targetClass = (this.targetClass != null ? this.targetClass : this.method.getDeclaringClass()); if (dm.methodMatcher.matches(this.method, targetClass, this.arguments)) &#123; return dm.interceptor.invoke(this); &#125; else &#123; // Dynamic matching failed. // Skip this interceptor and invoke the next in the chain. return proceed(); &#125; &#125; else &#123; // It&#x27;s an interceptor, so we just invoke it: The pointcut will have // been evaluated statically before this object was constructed. return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this); &#125;&#125; 在第一个if中，由于currentInterceptorIndex属性的默认值为-1，所以先不会执行到这。 接着执行这代码 12Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); 而这代码的意思就是获取匹配到的Advice中的第++this.currentInterceptorIndex个。 运行时参数匹配接着到下一个if 这个InterceptorAndDynamicMethodMatcher就是当MethodMatcher.isRuntime放回true后生成的。里面的逻辑挺简单的，就是在运行时进行参数匹配，比如这可以判断参数是等于某个值时才进行执行增强的方法。为ture时的代码方法到下面讲。else的时候就递归调用（跳过这个Advice）。 MethodInterceptor的invoke调用现在看回这段代码。 解析前，先说下在这段代码中涉及到的一些关系 @Around——在APO注解解析时——AspectJAroundAdvice @Before ——在APO注解解析时—— AspectJMethodBeforeAdvice——在方法调用时，增强逻辑执行前——MethodBeforeAdviceInterceptor @After —— 在APO注解解析时——AspectJAfterAdvice——在方法调用时，增强逻辑执行前——AfterReturningAdviceInterceptor @AfterReturning ——在APO注解解析时—— AspectJAfterReturningAdvice @AfterThrowing ——在APO注解解析时—— AspectJAfterThrowingAdvice——在方法调用时，增强逻辑执行前——ThrowsAdviceInterceptor 这些关系在之前就已经说明了。看会代码，这一段代码的意思就是执行增强逻辑的，比如下面的这些方法。 现在假设interceptorOrInterceptionAdvice的类型是MethodBeforeAdviceInterceptor 看MethodBeforeAdviceInterceptor 这个advice就是AspectJMethodBeforeAdvice 这个before就是一个反射调用方法，而这个方法就是增强的方法，过程就不看了。重点看before方法调用后的MethodInvocation.proceeed方法。我们再看下调用invoke的代码 传了一个this进来，也就是说这个mi就是ReflectiveMethodInvocation这个对象本身，所以mi.proceed就是一个递归调用的过程，由于前一次会下标进行了+1的操作，所以这次的调用的Advice变成了第二个，然后这个又重复。比如第二次调用的Advice是AspectJAroundAdvice 看AspectJAroundAdvice 其实这里和before那一样的，不同点在于 aroud多传了ProceedingJoinPoint这个参数。最后还是调用了增强的方法。看回上图，before1()方法调用完之后之所以能能调用下个Advice是因为他的MethodInvocation默认帮我们调用了proceed()。而around不同，看它的invoke只是调用了增强方法本身，并没有帮我们传递，所以对于aroud就必须在增强方法那调用 这个方法才能继续调用下一个Advice。 好了，其他的就不说的了，其它大致也差不多。 那问题来了，什么时候调用目标方法呢？ 目标方法的调用回到ReflectiveMethodInvocation#proceed 看这里，当Advisor调用完后，这个if返回true 从传参就可以猜到作用了，继续跟踪 最后通过反射调用了目标方法。也就是说，执行目标方法的条件就是匹配的切面都执行过了（注意不是执行完） 再谈ExposeInvocationInterceptor.ADVISOR前面已经说过了，在匹配到的Advisor中只要有用注解定义的切面，那每次都会在匹配的到的Advisors列表的头部添加上这个 这里看下这个切面有什么用。 在ExposeInvocationInterceptor中看到 切面的类型是DefaultPointcutAdvisor，看一个切面主要看两点，pointcut和Advice。 从构造函数可以看出，切点是Pointcut.TRUE，Advice是ExposeInvocationInterceptor。 切点无非就看两个，ClassFilter和MethodMatcher，跟踪代码看到 这里我不继续跟了，ClassFilter.TRUE和MethodMatcher.TRUE两个实际上不做什么，在它们的match方法上都返回true，也就是说对所有类和方法都通行。下面看Advice 看Advice，在Spring中实现一个Advice，有两种方法，实现Advice接口、另一个实现MethodInterceptor接口。用得最多的就是实现MethodInterceptor接口的。这ExposeInvocationInterceptor就是实现了MethodInterceptor接口，所以看它的作用就只需看invoke方法。 这个切面所做的增强只是在把MethodInvocation方法放入到了一个ThreadLocal中，并且把旧的删除。由于它是在切面数组的0号位置，所以会第一次执行（我们没有添加全局AOP时） 这样带来的便利就是在方法中，只需要通过 1ExposeInvocationInterceptor.currentInvocation() 就能获取到MethodInvocation。这个对象的类型为ReflectiveMethodInvocation。通过这个对象，可以获取目标对象，代理类，参数等。 如果全部的Advisor都是用接口实现的，这种情况下通过下面的方法会返回一个null值 总结现在假设我定义的切面入下 然后又调用 了一个测试方法 aopBean1这个就是代理对象，aopBean1.print(“xx”);调用后就执行了下边的流程。 这时开始调用前的链表顺序是这样的(不考虑全局的AOP和ExposeInvocationInterceptor.ADVISOR) 也就是说先调用了around1 System.out.println(&quot;around1----1&quot;); 然后调用了joinPoint.proceed();，这个方法的作用和before中的mi.proceed是一样的，就是又调用链中的下一个Advice，这时也就是执行around2，而around2中又执行了 System.out.println(&quot;around2---1&quot;);接着又调用了joinPoint.proceed();，又调用了链中的下一个Advice也就是before1的System.out.println(&quot;before1----1&quot;);。这时从上边可以知道，before1方法执行完后又调用会调用连中下一个，这时链中Advice已经调用完了，就执行了这行代码 这就是执行目标方法的，执行完后就返回了，这时before1返回了，接着before1出栈，又到aroud2执行了，around2执行了System.out.println(&quot;around2---2&quot;);后就出栈了，又到aroud1执行了 around1执行了System.out.println(&quot;around1----2&quot;);,aroud1又出栈了，接着其他方法也执行完了，那整个调用就完成了，打印应结果 和预测的一样，整个执行时序是 1234567891011121314151617181920212223242526272829动态代理的invoke()方法入栈：ReflectiveMethodInvocation.proceed()入栈：around1()方法入栈：around1:System.out.println(&quot;around1----1&quot;);around1:joinPoint.proceed();around2()方法入栈：around2:System.out.println(&quot;around2---1&quot;);around2:joinPoint.proceed();before1()方法入栈：before1: System.out.println(&quot;before1----1&quot;);目标方法入栈：目标方法执行目标方法出栈before1()方法出栈。around2:System.out.println(&quot;around2---2&quot;);around2()方法出栈。around1:System.out.println(&quot;around1----2&quot;);around1()方法出栈。ReflectiveMethodInvocation.proceed()出栈。动态代理的invoke()方法出栈。 这里我把joinPoint.proceed()称为火炬传递，看下Afiter，也是一样的，只是先火炬传递，然后再调用after的逻辑。 ​ 这其实和Netty中的pipeline一样的","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"09-Spring-AOP解析","slug":"spring/09-Spring-AOP解析","date":"2021-11-25T12:00:15.000Z","updated":"2022-03-23T09:03:55.040Z","comments":true,"path":"blog/spring/09-Spring-AOP解析/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/09-Spring-AOP%E8%A7%A3%E6%9E%90/","excerpt":"","text":"传统的xml在传统的xml中开启aop要这样 通过前面的只是，找到了解析类AspectJAutoProxyBeanDefinitionParser 源码： 12345public BeanDefinition parse(Element element, ParserContext parserContext) &#123; AopNamespaceUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(parserContext, element); extendBeanDefinition(element, parserContext); return null;&#125; 看第一行代码的源码： 方法registerAspectJAnnotationAutoProxyCreatorIfNecessary的作用就是去生成一个AnnotationAwareAspectJAutoProxyCreator类对应的BeanDefinition。并把这个BeanDefinition注册到registry中。 注解使用@EnableAspectJAutoProxy 该注解的定义： 有个@Import，它会把类引入到Spring中，看AspectJAutoProxyRegistrar 实现了ImportBeanDefinitionRegistrar那么在注解解析完并且BeanDefinition创建注册完后会调用方法registerBeanDefinitions。看到了一个熟悉的方法 1AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(registry); 在xml中已经解析过了，就是创建AnnotationAwareAspectJAutoProxyCreator的BeanDefinition，然后注册。剩下的代码就是AOP对应接口的检查和AOP注解的解析了。 Spring中的AOP使用在前上一节就已经讲了Spring中AOP的使用了。在使用注解模式的时候，其本质就是转化成接口模式。先有这个概念。 AnnotationAwareAspectJAutoProxyCreator现在看下AnnotationAwareAspectJAutoProxyCreator的类图 从类图上可知，AbstractAutoProxyCreator实现了SmartInstantiationAwareBeanPostProcessor接口，而这个接口是继承了BeanPostProcessor这节中点看postProcessAfterInitialization方法，这是AOP的入口方法。 wrapIfNecessary1234567891011121314151617181920212223242526272829//AnnotationAwareAspectJAutoProxyCreatorprotected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; if (StringUtils.hasLength(beanName) &amp;&amp; this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) &#123; return bean; &#125; if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; &#125; //创建当前bean的代理，如果这个bean有advice的话，重点看，重要程度5 // Create proxy if we have advice. Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); //如果有切面，则生成该bean的代理 if (specificInterceptors != DO_NOT_PROXY) &#123; this.advisedBeans.put(cacheKey, Boolean.TRUE); //把被代理对象bean实例封装到SingletonTargetSource对象中 Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; &#125; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean;&#125; 这个方法就是检查是否需要对类进行包装，如果需要，就生成一个代理类。而判断依据就是方法getAdvicesAndAdvisorsForBean方法的返回值。而getAdvicesAndAdvisorsForBean方法用一句话总结就是：获取切面或者增强对象。而这个方法是在AbstractAdvisorAutoProxyCreator中实现的。 获取匹配的切面——getAdvicesAndAdvisorsForBean12345678910111213141516171819202122232425//AnnotationAwareAspectJAutoProxyCreatorprotected Object[] getAdvicesAndAdvisorsForBean( Class&lt;?&gt; beanClass, String beanName, @Nullable TargetSource targetSource) &#123; //找到合格的切面，重点看 List&lt;Advisor&gt; advisors = findEligibleAdvisors(beanClass, beanName); if (advisors.isEmpty()) &#123; return DO_NOT_PROXY; &#125; return advisors.toArray();&#125;//AnnotationAwareAspectJAutoProxyCreatorprotected List&lt;Advisor&gt; findEligibleAdvisors(Class&lt;?&gt; beanClass, String beanName) &#123; //找到候选的切面,其实就是一个寻找有@Aspectj注解的过程，把工程中所有有这个注解的类封装成Advisor返回 List&lt;Advisor&gt; candidateAdvisors = findCandidateAdvisors(); //判断候选的切面是否作用在当前beanClass上面，就是一个匹配过程。。现在就是一个匹配 List&lt;Advisor&gt; eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); //针对@Aspect注解切面添加了一个默认的切面 DefaultPointcutAdvisor extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) &#123; //对有@Order@Priority进行排序 eligibleAdvisors = sortAdvisors(eligibleAdvisors); &#125; return eligibleAdvisors;&#125; 这个方法就是对切面Advisor的收集了。 findCandidateAdvisors——获取（创建）所有Advisor这两个方法有两个地方有实现 由于是通过AnnotationAwareAspectJAutoProxyCreator调用的，所以我们只看这个就好了。 AnnotationAwareAspectJAutoProxyCreator#findCandidateAdvisors 12345678910111213//AnnotationAwareAspectJAutoProxyCreator@Overrideprotected List&lt;Advisor&gt; findCandidateAdvisors() &#123; // Add all the Spring advisors found according to superclass rules. //找到所有的Advisor List&lt;Advisor&gt; advisors = super.findCandidateAdvisors(); // Build Advisors for all AspectJ aspects in the bean factory. //主要看这里，创建候选的切面 对@Aspect注解的类进行处理 if (this.aspectJAdvisorsBuilder != null) &#123; advisors.addAll(this.aspectJAdvisorsBuilder.buildAspectJAdvisors()); &#125; return advisors;&#125; 它先调了父类的findCandidateAdvisors，看父类也就是AbstractAdvisorAutoProxyCreator。 12345//AbstractAdvisorAutoProxyCreatorprotected List&lt;Advisor&gt; findCandidateAdvisors() &#123; Assert.state(this.advisorRetrievalHelper != null, &quot;。。。。&quot;); return this.advisorRetrievalHelper.findAdvisorBeans();&#125; 获取实现了Advisor接口的实例BeanFactoryAdvisorRetrievalHelper#findAdvisorBeans 1234567891011121314151617181920212223242526272829303132//BeanFactoryAdvisorRetrievalHelperpublic List&lt;Advisor&gt; findAdvisorBeans() &#123; String[] advisorNames = this.cachedAdvisorBeanNames; if (advisorNames == null) &#123; advisorNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors( this.beanFactory, Advisor.class, true, false); this.cachedAdvisorBeanNames = advisorNames; &#125; if (advisorNames.length == 0) &#123; return new ArrayList&lt;&gt;(); &#125; List&lt;Advisor&gt; advisors = new ArrayList&lt;&gt;(); for (String name : advisorNames) &#123; if (isEligibleBean(name)) &#123; if (this.beanFactory.isCurrentlyInCreation(name)) &#123; //去掉日志打印 。。。 &#125; else &#123; try &#123; advisors.add(this.beanFactory.getBean(name, Advisor.class)); &#125; catch (BeanCreationException ex) &#123; //去掉重要代码 。。。 &#125; &#125; &#125; &#125; return advisors;&#125; 先找到实现了Advisor接口的BeanName，后面的for循环就是根据名字beanFactory.getBean放入到一个集合中并且返回。 回到AnnotationAwareAspectJAutoProxyCreator#findCandidateAdvisors方法 getAdvisors——根据注解@Aspect生成一个个Advisor对象看BeanFactoryAspectJAdvisorsBuilder#buildAspectJAdvisors方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//BeanFactoryAspectJAdvisorsBuilder//省略了部分代码...List&lt;Advisor&gt; advisors = new ArrayList&lt;&gt;();aspectNames = new ArrayList&lt;&gt;();//获取spring容器中的所有bean的名称BeanNameString[] beanNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors( this.beanFactory, Object.class, true, false);for (String beanName : beanNames) &#123; if (!isEligibleBean(beanName)) &#123; continue; &#125; Class&lt;?&gt; beanType = this.beanFactory.getType(beanName); if (beanType == null) &#123; continue; &#125; //判断类上是否有@Aspect注解 if (this.advisorFactory.isAspect(beanType)) &#123; aspectNames.add(beanName); AspectMetadata amd = new AspectMetadata(beanType, beanName); if (amd.getAjType().getPerClause().getKind() == PerClauseKind.SINGLETON) &#123; //创建获取有@Aspect注解类的实例工厂，负责获取有@Aspect注解类的实例 MetadataAwareAspectInstanceFactory factory = new BeanFactoryAspectInstanceFactory(this.beanFactory, beanName); //创建切面advisor对象 List&lt;Advisor&gt; classAdvisors = this.advisorFactory.getAdvisors(factory); if (this.beanFactory.isSingleton(beanName)) &#123; this.advisorsCache.put(beanName, classAdvisors); &#125; else &#123; this.aspectFactoryCache.put(beanName, factory); &#125; advisors.addAll(classAdvisors); &#125; else &#123; // Per target or per this. if (this.beanFactory.isSingleton(beanName)) &#123; throw new IllegalArgumentException(&quot;Bean with name &#x27;&quot; + beanName + &quot;&#x27; is a singleton, but aspect instantiation model is not singleton&quot;); &#125; MetadataAwareAspectInstanceFactory factory = new PrototypeAspectInstanceFactory(this.beanFactory, beanName); this.aspectFactoryCache.put(beanName, factory); advisors.addAll(this.advisorFactory.getAdvisors(factory)); &#125; &#125;&#125;this.aspectBeanNames = aspectNames;return advisors;//省略了部分代码... 按功能看代码 先获取所有的beanName，然后遍历该接口。 判断有没有@Aspect这个注解。 后面就是根据注解生成切面的过程了，看源码： 创建了一个MetadataAwareAspectInstanceFactory工厂类对象，接着看核心代码 1List&lt;Advisor&gt; classAdvisors = this.advisorFactory.getAdvisors(factory); 这里的advisorFactory的对象是ReflectiveAspectJAdvisorFactory. 12345678910111213141516171819202122232425262728293031323334353637//ReflectiveAspectJAdvisorFactorypublic List&lt;Advisor&gt; getAdvisors(MetadataAwareAspectInstanceFactory aspectInstanceFactory) &#123; //从工厂中获取有@Aspect注解的类Class Class&lt;?&gt; aspectClass = aspectInstanceFactory.getAspectMetadata().getAspectClass(); //从工厂中获取有@Aspect注解的类的名称 String aspectName = aspectInstanceFactory.getAspectMetadata().getAspectName(); validate(aspectClass); //创建工厂的装饰类，获取实例只会获取一次 MetadataAwareAspectInstanceFactory lazySingletonAspectInstanceFactory = new LazySingletonAspectInstanceFactoryDecorator(aspectInstanceFactory); List&lt;Advisor&gt; advisors = new ArrayList&lt;&gt;(); //这里循环没有@Pointcut注解的方法 for (Method method : getAdvisorMethods(aspectClass)) &#123; //非常重要重点看看，重要程度 5 Advisor advisor = getAdvisor(method, lazySingletonAspectInstanceFactory, 0, aspectName); if (advisor != null) &#123; advisors.add(advisor); &#125; &#125; if (!advisors.isEmpty() &amp;&amp; lazySingletonAspectInstanceFactory.getAspectMetadata().isLazilyInstantiated()) &#123; Advisor instantiationAdvisor = new SyntheticInstantiationAdvisor(lazySingletonAspectInstanceFactory); advisors.add(0, instantiationAdvisor); &#125; //判断属性上是否有引介注解 for (Field field : aspectClass.getDeclaredFields()) &#123; //判断属性上是否有DeclareParents注解，如果有返回切面 Advisor advisor = getDeclareParentsAdvisor(field); if (advisor != null) &#123; advisors.add(advisor); &#125; &#125; return advisors;&#125; 看第一for循环 通过getAdvisorMethods(aspectClass)获取了没有@Pointcut注解的有注解的方法，接着遍历执行getAdvisor方法。 getAdvisor——Advisor（切面）的创建——InstantiationModelAwarePointcutAdvisorImpl看这个方法的源码 123456789101112131415161718//ReflectiveAspectJAdvisorFactorypublic Advisor getAdvisor(Method candidateAdviceMethod, MetadataAwareAspectInstanceFactory aspectInstanceFactory, int declarationOrderInAspect, String aspectName) &#123; validate(aspectInstanceFactory.getAspectMetadata().getAspectClass()); //获取pointCut对象，最重要的是从注解中获取表达式 AspectJExpressionPointcut expressionPointcut = getPointcut( candidateAdviceMethod, aspectInstanceFactory.getAspectMetadata().getAspectClass()); if (expressionPointcut == null) &#123; return null; &#125; //创建Advisor切面类，这才是真正的切面类，一个切面类里面肯定要有1、pointCut 2、advice //这里pointCut是expressionPointcut， advice 增强方法是 candidateAdviceMethod return new InstantiationModelAwarePointcutAdvisorImpl(expressionPointcut, candidateAdviceMethod, this, aspectInstanceFactory, declarationOrderInAspect, aspectName);&#125; 整段代码做了三件事： pointcut的创建 advice的创建 根据前两步的pointcut和advice创建一个InstantiationModelAwarePointcutAdvisorImpl切面Advisor对象 pointcut的创建——AspectJExpressionPointcut12345678910111213141516171819202122232425262728293031323334private AspectJExpressionPointcut getPointcut(Method candidateAdviceMethod, Class&lt;?&gt; candidateAspectClass) &#123; //从候选的增强方法里面 candidateAdviceMethod 找有有注解 //Pointcut.class, Around.class, Before.class, After.class, AfterReturning.class, AfterThrowing.class //并把注解信息封装成AspectJAnnotation对象 AspectJAnnotation&lt;?&gt; aspectJAnnotation = AbstractAspectJAdvisorFactory.findAspectJAnnotationOnMethod(candidateAdviceMethod); if (aspectJAnnotation == null) &#123; return null; &#125; //创建一个PointCut类，并且把前面从注解里面解析的表达式设置进去 AspectJExpressionPointcut ajexp = new AspectJExpressionPointcut(candidateAspectClass, new String[0], new Class&lt;?&gt;[0]); ajexp.setExpression(aspectJAnnotation.getPointcutExpression()); if (this.beanFactory != null) &#123; ajexp.setBeanFactory(this.beanFactory); &#125; return ajexp;&#125;protected static AspectJAnnotation&lt;?&gt; findAspectJAnnotationOnMethod(Method method) &#123; //Pointcut.class, Around.class, Before.class, After.class, AfterReturning.class, AfterThrowing.class for (Class&lt;?&gt; clazz : ASPECTJ_ANNOTATION_CLASSES) &#123; //找到Pointcut.class, Around.class, Before.class, After.class, AfterReturning.class, AfterThrowing.class //注解的方法，并且把注解里面的信息封装成AspectJAnnotation对象 AspectJAnnotation&lt;?&gt; foundAnnotation = findAnnotation(method, (Class&lt;Annotation&gt;) clazz); if (foundAnnotation != null) &#123; return foundAnnotation; &#125; &#125; return null;&#125;private static final Class&lt;?&gt;[] ASPECTJ_ANNOTATION_CLASSES = new Class&lt;?&gt;[] &#123; Pointcut.class, Around.class, Before.class, After.class, AfterReturning.class, AfterThrowing.class&#125;; 整个代码下来就做一件事，就是找方法上是否有 Around.class, Before.class, After.class, AfterReturning.class, AfterThrowing.class这些注解，有的话就把注解的信息封装成一个AspectJAnnotation对象，并在最后创建AspectJExpressionPointcut对象，并把AspectJAnnotation中的表达式赋值给AspectJExpressionPointcut，那么这时pointcut就已经创建了，而且这个pointcut的类型是AspectJExpressionPointcut 这里pointcut有两种情况： 注解中的值是表达式@Around(value = &quot;execution(public * com.enjoy.xyz.aop.bean.*.*(..))&quot;)这样的形式 注解中的值是符号引用 @Around(value = &quot;pc1()&quot;)这样的形式 对于这两种pointcut中的表达式（被代理方法的集合），第一种就是已经完成创建好pointcut了，第二种的情况创建完一半，剩下的一半需要把value中的符号变成确定的表达式。下边的讲解都一第二种情况为例 比如我定义这样一个类： 1234567891011121314@Component@Aspectpublic class AspectAnnotation &#123; @Pointcut(value = &quot;execution(public * com.enjoy.xyz.aop.bean.*.*(..))&quot;) public void pc1() &#123; &#125; @Around(value = &quot;pc1()&quot;) public void around1(ProceedingJoinPoint joinPoint) throws Throwable &#123; System.out.println(&quot;around1----1&quot;); joinPoint.proceed(); System.out.println(&quot;around1----2&quot;); &#125;&#125; 经过了 getPointcut后获取到数据入下图 现在已经有expression&#x3D;pc()了，也就是说我们只要根据这个expression&#x3D;pc()就能找到对应的方法了，找到方法后就能找到方法上对应的@Pointcut的信息了。 好了，现在已经有了pointcut了，就差Advite了 Advice的创建——AbstractAspectJAdvice一个Advisor中要有一个pointcut和一个Advice。在注解的模式中，advice对应着一个方法。下面就看Advice是怎么创建的 看代码 这个构造方法传入的Advice的方法。然后看最后 123456private Advice instantiateAdvice(AspectJExpressionPointcut pointcut) &#123; //创建Advice对象 Advice advice = this.aspectJAdvisorFactory.getAdvice(this.aspectJAdviceMethod, pointcut, this.aspectInstanceFactory, this.declarationOrder, this.aspectName); return (advice != null ? advice : EMPTY_ADVICE);&#125; 看ReflectiveAspectJAdvisorFactory#getAdvice 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public Advice getAdvice(Method candidateAdviceMethod, AspectJExpressionPointcut expressionPointcut, MetadataAwareAspectInstanceFactory aspectInstanceFactory, int declarationOrder, String aspectName) &#123; //获取有@Aspect注解的类 Class&lt;?&gt; candidateAspectClass = aspectInstanceFactory.getAspectMetadata().getAspectClass(); validate(candidateAspectClass); //找到candidateAdviceMethod方法上面的注解，并且包装成AspectJAnnotation对象，这个对象中就有注解类型 AspectJAnnotation&lt;?&gt; aspectJAnnotation = AbstractAspectJAdvisorFactory.findAspectJAnnotationOnMethod(candidateAdviceMethod); if (aspectJAnnotation == null) &#123; return null; &#125; // If we get here, we know we have an AspectJ method. // Check that it&#x27;s an AspectJ-annotated class if (!isAspect(candidateAspectClass)) &#123; throw new AopConfigException(&quot;Advice must be declared inside an aspect type: &quot; + &quot;Offending method &#x27;&quot; + candidateAdviceMethod + &quot;&#x27; in class [&quot; + candidateAspectClass.getName() + &quot;]&quot;); &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Found AspectJ method: &quot; + candidateAdviceMethod); &#125; AbstractAspectJAdvice springAdvice; //根据不同的注解类型创建不同的advice类实例 switch (aspectJAnnotation.getAnnotationType()) &#123; case AtPointcut: if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Processing pointcut &#x27;&quot; + candidateAdviceMethod.getName() + &quot;&#x27;&quot;); &#125; return null; case AtAround: //实现了MethodInterceptor接口 springAdvice = new AspectJAroundAdvice( candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); break; case AtBefore: //实现了MethodBeforeAdvice接口，没有实现MethodInterceptor接口 springAdvice = new AspectJMethodBeforeAdvice( candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); break; case AtAfter: //实现了MethodInterceptor接口 springAdvice = new AspectJAfterAdvice( candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); break; case AtAfterReturning: //实现了AfterReturningAdvice接口，没有实现MethodInterceptor接口 springAdvice = new AspectJAfterReturningAdvice( candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); AfterReturning afterReturningAnnotation = (AfterReturning) aspectJAnnotation.getAnnotation(); if (StringUtils.hasText(afterReturningAnnotation.returning())) &#123; springAdvice.setReturningName(afterReturningAnnotation.returning()); &#125; break; case AtAfterThrowing: //实现了MethodInterceptor接口 springAdvice = new AspectJAfterThrowingAdvice( candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); AfterThrowing afterThrowingAnnotation = (AfterThrowing) aspectJAnnotation.getAnnotation(); if (StringUtils.hasText(afterThrowingAnnotation.throwing())) &#123; springAdvice.setThrowingName(afterThrowingAnnotation.throwing()); &#125; break; default: throw new UnsupportedOperationException( &quot;Unsupported advice type on method: &quot; + candidateAdviceMethod); &#125; // Now to configure the advice... springAdvice.setAspectName(aspectName); springAdvice.setDeclarationOrder(declarationOrder); //获取注解中的argNames属性参数名称 String[] argNames = this.parameterNameDiscoverer.getParameterNames(candidateAdviceMethod); if (argNames != null) &#123; springAdvice.setArgumentNamesFromStringArray(argNames); &#125; //计算argNames和类型的对应关系 springAdvice.calculateArgumentBindings(); return springAdvice;&#125; 有多种Advice Around —— AspectJAroundAdvice Before —— AspectJMethodBeforeAdvice After —— AspectJAfterAdvice AfterReturning —— AspectJAfterReturningAdvice AfterThrowing —— AspectJAfterThrowingAdvice 上面的Advice都实现了MethodInterceptor接口 在Spring中实现一个Advice，有两种方法，实现Advice接口、另一个实现MethodInterceptor接口。用得最多的就是实现MethodInterceptor接口的。 MethodInterceptor的invoke方法就是增强逻辑的调用方法。和动态代理一样。 总结现在Pointcut是AspectJExpressionPointcut，Advite已经有了，最后一步就是把它们封装成一个切面Advisor。 InstantiationModelAwarePointcutAdvisorImpl是实现了PointcutAdvisor接口的，所以他是一个切面 经过getAdvisor，会获得到这样一个关系 最后就把这些新建的Advisor放入到集合中并返回。 排序在收集有注解的方法时就已经排序了 在这里会先根据上图的顺排序，相同的就根据方法的名字把方法排序好： 所以，在生成切面Advisor后的advisors集合就是有序的 总结findCandidateAdvisors方法就是找到候选的切面。 找实现了Advisor接口的类，获取其实例。 寻找有@Aspectj注解，并解析有@Around、@Before等注解的value，用来创建成一个piontCut对象，接着根据对应注解，把方法封装成对应的Advice对象，最终把Advice和pointCut封装成一个Advisor对象该对象类型是InstantiationModelAwarePointcutAdvisorImpl。然后缓存这个bean的。advisors集合，建立&lt;beanName, advisors&gt;这种关系。最后把工程中所有有的Advisor返回。 注意，在解析注解是，是没有处理@Pointcut这个注解的，至于为什么，后面讲 findAdvisorsThatCanApply——Advisor匹配过程上边是找到所用的Advisor，但不是所有的Advisor都适用在这个对象上面的，所以这一步就是一个匹配的过程。跟踪代码到： 12345678910111213141516171819202122232425//AopUtilspublic static List&lt;Advisor&gt; findAdvisorsThatCanApply(List&lt;Advisor&gt; candidateAdvisors, Class&lt;?&gt; clazz) &#123; if (candidateAdvisors.isEmpty()) &#123; return candidateAdvisors; &#125; List&lt;Advisor&gt; eligibleAdvisors = new ArrayList&lt;&gt;(); for (Advisor candidate : candidateAdvisors) &#123; //如果是引介切面并且匹配 if (candidate instanceof IntroductionAdvisor &amp;&amp; canApply(candidate, clazz)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; boolean hasIntroductions = !eligibleAdvisors.isEmpty(); //调用pointCut中的ClassFilter 和methodMatcher的match方法的过程 for (Advisor candidate : candidateAdvisors) &#123; if (candidate instanceof IntroductionAdvisor) &#123; // already processed continue; &#125; if (canApply(candidate, clazz, hasIntroductions)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; return eligibleAdvisors;&#125; 看canApply，继续跟踪代码到 1234567891011121314151617181920212223242526272829303132333435363738public static boolean canApply(Pointcut pc, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; Assert.notNull(pc, &quot;Pointcut must not be null&quot;); //调用ClassFilter的matches方法，判断类是否匹配 if (!pc.getClassFilter().matches(targetClass)) &#123; return false; &#125; MethodMatcher methodMatcher = pc.getMethodMatcher(); if (methodMatcher == MethodMatcher.TRUE) &#123; // No need to iterate the methods if we&#x27;re matching any method anyway... return true; &#125; IntroductionAwareMethodMatcher introductionAwareMethodMatcher = null; if (methodMatcher instanceof IntroductionAwareMethodMatcher) &#123; introductionAwareMethodMatcher = (IntroductionAwareMethodMatcher) methodMatcher; &#125; Set&lt;Class&lt;?&gt;&gt; classes = new LinkedHashSet&lt;&gt;(); if (!Proxy.isProxyClass(targetClass)) &#123; classes.add(ClassUtils.getUserClass(targetClass)); &#125; classes.addAll(ClassUtils.getAllInterfacesForClassAsSet(targetClass)); //判断类中方法是否匹配，，有些可能是方法上面有注解的拦截，所以需要判断方法是否匹配 for (Class&lt;?&gt; clazz : classes) &#123; Method[] methods = ReflectionUtils.getAllDeclaredMethods(clazz); for (Method method : methods) &#123; if (introductionAwareMethodMatcher != null ? introductionAwareMethodMatcher.matches(method, targetClass, hasIntroductions) : methodMatcher.matches(method, targetClass)) &#123; return true; &#125; &#125; &#125; return false;&#125; 这个方法就是尽心pointcut的匹配，只有类和方法匹配都通过了。就表示这个Advisor就是这个这个bean切面。 判断类是否匹配 pc是一个Pointcut，看这个，因为在前面创建的Pointcut的类型是这个 看AspectJExpressionPointcut#matches 123456789101112131415161718public boolean matches(Class&lt;?&gt; targetClass) &#123; //这里会根据@Around(&quot;pc1()&quot;)配置中的pc1找到pc1方法从而找到真正的pointCut表达式 PointcutExpression pointcutExpression = obtainPointcutExpression(); try &#123; try &#123; //匹配targetClass是否在表达式中 return pointcutExpression.couldMatchJoinPointsInType(targetClass); &#125; catch (ReflectionWorldException ex) &#123; //去掉不重要的代码 ..... &#125; &#125; catch (Throwable ex) &#123; logger.debug(&quot;PointcutExpression matching rejected target class&quot;, ex); &#125; return false;&#125; 先看 1PointcutExpression pointcutExpression = obtainPointcutExpression(); 这一行会根据Pointcut的expression获取到对应的方法，然后获取方法的@Pointcut的信息，最后封装成了PointcutExpression。也就是说，比如这时的Pointcut对象是从这个注解生成的@Around(&quot;pc1()&quot;)，那就会去找pc1()方法上@Pointcut注解信息。 现在有了@Pointcut的信息了，然后调用 1pointcutExpression.couldMatchJoinPointsInType(targetClass) 去检查类的全限定名是否符合expression就好了。流程就这样，这里的细节就不扣了。 判断类中方法是否匹配 走到这就要意味着类是匹配的，接着就看方法是否匹配了，这里逻辑和上边是一样的，只是可以记下 1Method[] methods = ReflectionUtils.getAllDeclaredMethods(clazz); Spring中ReflectionUtils工具类 总结就是通过pointcut进行类匹配和方法匹配，最后把匹配到的Advisor返回 extendAdvisors——添加ExposeInvocationInterceptor增强在执行extendAdvisors前 执行了extendAdvisors后 从上图可以看到，匹配的切面多了一个ExposeInvocationInterceptor.ADVISOR，而且这个切面放在第一的位置。跟踪源码 从isAspectJAdvice从源码可知，只有在使用注解创建切面后才需要引入这个默认的切面。应为InstantiationModelAwarePointcutAdvisor、AbstractAspectJAdvice和AspectJExpressionPointcut这三个都是在扫描到对应的aop注解后所生成的对应的类。 sortAdvisors做最后一次排序，这个排序是对有@Order @Priority进行排序 回到wrapIfNecessary 这行代码执行完了，已经得到了切面对象了，代码继续走 这段代码的，如果有切面的话就为bean生成一个代理对象。并返回代理对象。而且会把代理对象放入到一级缓存singletonObjects中。 创建代理对象——createProxy简单的看下createProxy源码 1234567891011121314151617181920212223242526272829303132333435//AbstractAutoProxyCreatorprotected Object createProxy(Class&lt;?&gt; beanClass, @Nullable String beanName, @Nullable Object[] specificInterceptors, TargetSource targetSource) &#123; if (this.beanFactory instanceof ConfigurableListableBeanFactory) &#123; AutoProxyUtils.exposeTargetClass((ConfigurableListableBeanFactory) this.beanFactory, beanName, beanClass); &#125; ProxyFactory proxyFactory = new ProxyFactory(); //把AnnotationAwareAspectJAutoProxyCreator中的某些属性copy到proxyFactory中 proxyFactory.copyFrom(this); if (!proxyFactory.isProxyTargetClass()) &#123; if (shouldProxyTargetClass(beanClass, beanName)) &#123; proxyFactory.setProxyTargetClass(true); &#125; else &#123; evaluateProxyInterfaces(beanClass, proxyFactory); &#125; &#125; //组装advisor Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); //把advisor加入到proxyFactory proxyFactory.addAdvisors(advisors); //把targetSource对象加入到proxyFactory proxyFactory.setTargetSource(targetSource); customizeProxyFactory(proxyFactory); proxyFactory.setFrozen(this.freezeProxy); if (advisorsPreFiltered()) &#123; proxyFactory.setPreFiltered(true); &#125; //获取代理对象 return proxyFactory.getProxy(getProxyClassLoader());&#125; 首先创建了一个ProxyFactory proxyFactory = new ProxyFactory(); 先下这个类的类图： 看 Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); 1234567891011121314151617181920212223242526272829protected Advisor[] buildAdvisors(@Nullable String beanName, @Nullable Object[] specificInterceptors) &#123; // Handle prototypes correctly... //设置自定义的MethodInterceptor 和 Advice Advisor[] commonInterceptors = resolveInterceptorNames(); List&lt;Object&gt; allInterceptors = new ArrayList&lt;&gt;(); if (specificInterceptors != null) &#123; allInterceptors.addAll(Arrays.asList(specificInterceptors)); if (commonInterceptors.length &gt; 0) &#123; if (this.applyCommonInterceptorsFirst) &#123; allInterceptors.addAll(0, Arrays.asList(commonInterceptors)); &#125; else &#123; allInterceptors.addAll(Arrays.asList(commonInterceptors)); &#125; &#125; &#125; if (logger.isTraceEnabled()) &#123; int nrOfCommonInterceptors = commonInterceptors.length; int nrOfSpecificInterceptors = (specificInterceptors != null ? specificInterceptors.length : 0); ..... &#125; Advisor[] advisors = new Advisor[allInterceptors.size()]; for (int i = 0; i &lt; allInterceptors.size(); i++) &#123; advisors[i] = this.advisorAdapterRegistry.wrap(allInterceptors.get(i)); &#125; return advisors;&#125; 添加全局的切面在这个方法中有一个有意思的方法resolveInterceptorNames，看下后面的代码 通过resolveInterceptorNames生成的Advisor会被添加到allInterceptors的首位。而在这个方法resolveInterceptorNames我们是能做一些操作的。比如添加一个[全局的拦截器](.&#x2F;Spring 全局（切面）的拦截器)。 也就是说，这样做了后只要通过代理对象调用被拦截的方法，那我们添加的都会被调用到。 创建代理——proxyFactory.getProxy(buildAdvisors执行完后就返回了所有需要被执行的切面，看createProxy的源码看到， 而且在最后会把这Advisor数组设置到ProxyFactory中。 最后就是生成代理了。有两种代理方式。 我门先跟踪下createAopProxy()的方法 1234567891011121314151617181920212223242526//ProxyCreatorSupportprotected final synchronized AopProxy createAopProxy() &#123; if (!this.active) &#123; activate(); &#125; return getAopProxyFactory().createAopProxy(this);&#125;//DefaultAopProxyFactory@Overridepublic AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) &#123; Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException(&quot;TargetSource cannot determine target class: &quot; + &quot;Either an interface or a target is required for proxy creation.&quot;); &#125; if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) &#123; return new JdkDynamicAopProxy(config); &#125; return new ObjenesisCglibAopProxy(config); &#125; else &#123; return new JdkDynamicAopProxy(config); &#125;&#125; 从源码发现，如果有接口的情况下会走JDK的代理，如果没有就走CGLIB的代理 可以看到，两种代理在创建的时候都会把AdvisedSupport对象作为入参传入，而这个AdvisedSupport对象就是之前创建的ProxyFactory。 我更熟悉JDK的动态代理，所以看下JdkDynamicAopProxy#getProxy方法（Cglib就是api不同，整体的逻辑都是一样的） 123456789@Overridepublic Object getProxy(@Nullable ClassLoader classLoader) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Creating JDK dynamic proxy: &quot; + this.advised.getTargetSource()); &#125; Class&lt;?&gt;[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this);&#125; 可以看到，这个动态代理的InvocationHandler是this，也就是JdkDynamicAopProxy。这个类还有一个重要的属性， 这个引用指向的是ProxyFactory，而这个ProxyFactory从这个creaetProxy的代码中可以发现，他的作用就是用来保存一些必要的信息的，最重要的一个信息就是这个类匹配到的切面列表。剩下的InvocationHandler的invoke方法是怎么实现的，就在下节讲。 总结整个流程就是判断这个类是否有切面，判断的逻辑就是先获取全部的切面。 实现了Advisor接口的类的实例——直接通过beanFactory.getBean获取 有@Aspect注解的类——从缓存拿，没有的话根据@Around、@Before等注解的信息生成一个pointcut对象，然后再把方法封装成对应的Advice，接着把pointcut和Advice封装成一个Advisor。最后把Advisor放入缓存。而这里的实现类分别如下 123456789Advisor——InstantiationModelAwarePointcutAdvisorImplPointcut——AspectJExpressionPointcutAdvice对应的类型有点过，不过这些都是AbstractAspectJAdvice的子类- Around —— AspectJAroundAdvice- Before —— AspectJMethodBeforeAdvice- After —— AspectJAfterAdvice- AfterReturning —— AspectJAfterReturningAdvice- AfterThrowing —— AspectJAfterThrowingAdvice 获取到全部切面后，再遍历Advisor对象，根据pointcut找到对应的方法并获取该方法的@Pointcut注解的信息，然后看类和方法是否匹配，如果匹配就放入到一个集合中。 接着如果使用了@AOP的注解，而且bean也匹配到了通过注解生成的Advisor对象，那会添加一个切面，这个切面的pointcut为Pointcut.TURE，advice为ExposeInvocationInterceptor 最后，如果返回的集合不为空，就为bean生成一个代理对象。在生成代理对象时，可以通过[这种模式](.&#x2F;Spring 全局（切面）的拦截器)来生成一个全局的切面","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"08-Spring-AOP概念","slug":"spring/08-Spring-AOP概念","date":"2021-11-25T12:00:14.000Z","updated":"2022-03-23T09:03:54.900Z","comments":true,"path":"blog/spring/08-Spring-AOP概念/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/08-Spring-AOP%E6%A6%82%E5%BF%B5/","excerpt":"","text":"AOP-概念连接点（Joinpoint）&#x3D;&#x3D; 一个连接点就对应着一个被代理方法程序执行的某个特定位置：如类开始初始化前、类初始化后、类某个方法调用前、调用后、方法抛出异常后。一个类或一段程序代码拥有一些具有边界性质的特定点，这些点中的特定点就称为“连接点”。Spring仅支持方法的连接点，即仅能在方法调用前、方法调用后、方法抛出异常时以及方法调用前后这些程序执行点织入增强。连接点由两个信息确定：第一是用方法表示的程序执行点；第二是用相对点表示的方位。 切点（Pointcut）&#x3D;&#x3D; 连接点的集合每个程序类都拥有多个连接点，如一个拥有两个方法的类，这两个方法都是连接点，即连接点是程序类中客观存在的事物。AOP通过“切点”定位特定的连接点。连接点相当于数据库中的记录，而切点相当于查询条件。切点和连接点不是一对一的关系，一个切点可以匹配多个连接点。在Spring中，切点通过org.springframework.aop.Pointcut接口进行描述，它使用类和方法作为连接点的查询条件，Spring AOP的规则解析引擎负责切点所设定的查询条件，找到对应的连接点。其实确切地说，不能称之为查询连接点，因为连接点是方法执行前、执行后等包括方位信息的具体程序执行点，而切点只定位到某个方法上，所以如果希望定位到具体连接点上，还需要提供方位信息。 增强（Advice） &#x3D;&#x3D; 具体增强的代码逻辑增强是织入到目标类连接点上的一段程序代码，在Spring中，增强除用于描述一段程序代码外，还拥有另一个和连接点相关的信息，这便是执行点的方位。结合执行点方位信息和切点信息，我们就可以找到特定的连接点。 目标对象（Target）增强逻辑的织入目标类。如果没有AOP，目标业务类需要自己实现所有逻辑，而在AOP的帮助下，目标业务类只实现那些非横切逻辑的程序逻辑，而性能监视和事务管理等这些横切逻辑则可以使用AOP动态织入到特定的连接点上。 引介（Introduction）和 织入（Weaving）引介（Introduction）引介是一种特殊的增强，它为类添加一些属性和方法。这样，即使一个业务类原本没有实现某个接口，通过AOP的引介功能，我们可以动态地为该业务类添加接口的实现逻辑，让业务类成为这个接口的实现类。 织入（Weaving）织入是将增强添加对目标类具体连接点上的过程。AOP像一台织布机，将目标类、增强或引介通过AOP这台织布机天衣无缝地编织到一起。根据不同的实现技术，AOP有三种织入的方式： 编译期织入，这要求使用特殊的Java编译器。 类装载期织入，这要求使用特殊的类装载器。 动态代理织入，在运行期为目标类添加增强生成子类的方式。 Spring采用动态代理织入，而AspectJ采用编译期织入和类装载期织入。 代理（Proxy） &#x3D;&#x3D; 动态生成的一个类被AOP织入增强后，就产出了一个结果类，它是融合了原类和增强逻辑的代理类。根据不同的代理方式，代理类既可能是和原类具有相同接口的类，也可能就是原类的子类，所以我们可以采用调用原类相同的方式调用代理类。 切面（Aspect、advisor） &#x3D;&#x3D; 一类功能（日志功能，事务功能，缓存功能）切面由切点和增强（引介）组成，它既包括了横切逻辑的定义，也包括了连接点的定义，Spring AOP就是负责实施切面的框架，它将切面所定义的横切逻辑织入到切面所指定的连接点中。advisor：必须要用 pointCut、advice Spring中的AOP概念在Spring中对AOP的概念做了简化。在Spring中只有切面、切点和增强 切面（Aspect、advisor）一类功能（日志功能，事务功能，缓存功能）必须要有Pointcut、Advice 切点（Pointcut）&#x3D;&#x3D; 被代理方法的集合作用：匹配、拦截目的： 为了生成代理对象 用代理对象调用的时候会匹配 一个Pointcut中会有这两个： ClassFiter &#x3D;&#x3D; 类是它拦截 MethodMatcher &#x3D;&#x3D; 方法是它匹配 增强（Advice） &#x3D;&#x3D; 具体增强的代码逻辑承载了具体的增强的代码逻辑 在Spring中使用有两种方法，一种是注解，一种是接口 注解12345678910111213141516171819202122232425262728293031323334@Component@Aspect@EnableAspectJAutoProxy()public class AspectAnnotation &#123; @Pointcut(value = &quot;execution(public * com.enjoy.xyz.aop.bean.*.*(..))&quot;) public void pc1() &#123; &#125; @Before(value = &quot;pc1()&quot;) public void before1() throws Throwable &#123; System.out.println(&quot;before1----1&quot;); &#125; @After(value = &quot;pc1()&quot;) public void after1() &#123; System.out.println(&quot;after1----1&quot;); &#125; @Around(value = &quot;pc1()&quot;) public void around1(ProceedingJoinPoint joinPoint) throws Throwable &#123; //可以通过这个或来去参数和方法 MethodInvocation methodInvocation = ExposeInvocationInterceptor.currentInvocation(); System.out.println(&quot;around1----1&quot;); joinPoint.proceed(); System.out.println(&quot;around1----2&quot;); &#125; @Around(value = &quot;pc1()&quot;) public void around2(ProceedingJoinPoint joinPoint) throws Throwable &#123; System.out.println(&quot;around2---1&quot;); joinPoint.proceed(); System.out.println(&quot;around2---2&quot;); &#125;&#125; 开启： 1234@Component@EnableAspectJAutoProxy()public class AopConfig &#123;&#125; 接口切面123456789101112131415161718192021222324252627@Componentpublic class CustomAdvisor implements PointcutAdvisor &#123; // 切点 @Autowired private CustomPointcut pointcut; // 增强 @Autowired private CustomAdvice advice; @Override public Pointcut getPointcut() &#123; return pointcut; &#125; @Override public Advice getAdvice() &#123; return advice; &#125; @Override public boolean isPerInstance() &#123; return false; &#125;&#125; 切点、增强切点要实现Pointcut接口、增强要实现MethodMatcher接口 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768@Componentpublic class CustomPointcut implements Pointcut, MethodMatcher &#123; /** * 所有类都匹配 * @return */ @Override public ClassFilter getClassFilter() &#123; return ClassFilter.TRUE; &#125; @Override public MethodMatcher getMethodMatcher() &#123; return this; &#125; /** * 所有方法都匹配 * @param method * @param targetClass * @return */ @Override public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; //拿原始方法对象，这个方法上才有注解 Method specificMethod = AopUtils.getMostSpecificMethod(method, targetClass); if(AnnotatedElementUtils.hasAnnotation(specificMethod, CustomCache.class)) &#123; method.getAnnotations(); return true; &#125; return false; &#125; @Override public boolean isRuntime() &#123; return true; &#125; /** * 参数匹配 * * @param method * @param targetClass * @param args * @return */ @Override public boolean matches(Method method, Class&lt;?&gt; targetClass, Object... args) &#123; if (args.length &gt; 0 &amp;&amp; args[0] != null) &#123; return &quot;xyz&quot;.equalsIgnoreCase(args[0].toString()); &#125; return false; &#125;&#125;@Componentpublic class CustomAdvice implements MethodInterceptor &#123; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; System.out.println(&quot;我是自定义的AOP--before&quot;); Object obj = invocation.proceed(); System.out.println(&quot;我是自定义的AOP--after&quot;); return obj; &#125;&#125;","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"07-Spring中基于注解--ConfigurationClassPostProcessor","slug":"spring/07-Spring中基于注解--ConfigurationClassPostProcessor","date":"2021-11-25T12:00:13.000Z","updated":"2022-03-23T09:03:54.899Z","comments":true,"path":"blog/spring/07-Spring中基于注解--ConfigurationClassPostProcessor/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/07-Spring%E4%B8%AD%E5%9F%BA%E4%BA%8E%E6%B3%A8%E8%A7%A3--ConfigurationClassPostProcessor/","excerpt":"","text":"基于注解的就是去处理这些注解 @PropertySources、@PropertySource、@ComponentScan、@Bean、@Import、@ImportResource、@Configuration、@Value 这些注解的处理都是类ConfigurationClassPostProcessor实现的。（@Value还会通过AutowiredAnnotationBeanPostProcessor处理） 之前讲bean的初始化阶段都是基于ClassPathXmlApplicationContext这个上下文对象的，现在讲注解了，使用下面的上下文类AnnotationConfigApplicationContext，看下该类的类图： 从类图可以看到，该上下文类继承了GenericApplicationContext，而GenericApplicationContext实现了BeanDefinitionRegistry接口，也就是说上下文类AnnotationConfigApplicationContext将作为BeanDefinition的注册器。但如果继续看GenericApplicationContext： 将会发现这里只是使用了代理模式而已，GenericApplicationContext作为DefaultListableBeanFactory的代理类，真正的BeanDefinition的注册器还是DefaultListableBeanFactory。 比如有下面这一代码段： 12345678910111213141516@ComponentScans(&#123;@ComponentScan(&quot;xx&quot;), @ComponentScan(&quot;aa&quot;)&#125;)@Component@ComponentScan(value = &quot;com.enjoy.jack&quot;/*,includeFilters = ,basePackages = */)//&lt;context:property-placeholder location=&quot;classpath:application.properties&quot;/&gt;@PropertySource(name = &quot;jack&quot;, value = &quot;classpath:application.properties&quot;)public class ScanBean &#123;&#125;@Testpublic void test2() &#123; AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(ScanBean.class); StudentService bean = applicationContext.getBean(StudentService.class); System.out.println(bean); bean.sleep(new ArrayList()); bean.eat(&quot;Jack1&quot;);&#125; 看AnnotationConfigApplicationContext的对应的构造方法 12345678910public AnnotationConfigApplicationContext() &#123; this.reader = new AnnotatedBeanDefinitionReader(this); this.scanner = new ClassPathBeanDefinitionScanner(this);&#125;public AnnotationConfigApplicationContext(Class&lt;?&gt;... componentClasses) &#123; this(); register(componentClasses); refresh();&#125; 上面创建了两个对象，分别是ClassPathBeanDefinitionScanner和AnnotatedBeanDefinitionReader。ClassPathBeanDefinitionScanner之前已经说过了，它的作用就是扫描包，找有@Component等注解的类。而AnnotatedBeanDefinitionReader将在下面介绍它的职能。 继续看下一行代码 1register(componentClasses); 实际的作用就是把ScanBean这个类封装成AnnotatedGenericBeanDefinition然后注册到BeanDefinitionRegistry，也就是AnnotationConfigApplicationContext中。 12345678@Overridepublic void register(Class&lt;?&gt;... componentClasses) &#123; Assert.notEmpty(componentClasses, &quot;At least one component class must be specified&quot;); this.reader.register(componentClasses);&#125; ---&gt;AnnotatedBeanDefinitionReader.register(Class&lt;?&gt;... componentClasses) ---&gt;AnnotatedBeanDefinitionReader.registerBean(Class&lt;?&gt; beanClass) ---&gt;AnnotatedBeanDefinitionReader.doRegisterBean 从上面的代码可以发现一个问题，就是AnnotationConfigApplicationContext将类的信息封装成BeanDefinition这一步时委托给了AnnotatedBeanDefinitionReader这个对象完成的。而且，AnnotatedBeanDefinitionReader.deRegisterBean也就是这节重点讲解的方法。 最后看 1refresh(); AnnotationConfigApplicationContext没有重写refresh方法，所以refresh方法是从父类继承过来的 AbstractApplicationContext#refresh，这段代码讲了很多次了。 AnnotationConfigApplicationContext执行了refresh后，代码执行到了obtainFreshBeanFactory方法 12345protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; //核心方法，必须读，重要程度：5 refreshBeanFactory(); return getBeanFactory();&#125; 其中refreshBeanFactory方法有两个实现。 看回AnnotationConfigApplicationContext的类图，它的父类是GenericApplicationContext，所以refreshBeanFactory方法是从父类GenericApplicationContext继承过来的。 现在看GenericApplicationContext#refreshBeanFactory方法。 它什么都不做，这里就和ClassPathXmlApplicationContext完全不同了，ClassPathXmlApplicationContext会在这里添加完成扫描和BeanDefinition的创建和注册，但在AnnotationConfigApplicationContext中，这里基本什么都不做。 那BeanDefinition的创建和注册是在什么时候完成的？ 回到构造方法： 在初始化AnnotationConfigApplicationContext创建了AnnotatedBeanDefinitionReader，在上边已经简单的说了该类的作用，但去看该类的构造方法时会发现该类还有另一个作用。看下这个类的构造方法： 123456789101112// AnnotatedBeanDefinitionReaderpublic AnnotatedBeanDefinitionReader(BeanDefinitionRegistry registry) &#123; this(registry, getOrCreateEnvironment(registry));&#125;public AnnotatedBeanDefinitionReader(BeanDefinitionRegistry registry, Environment environment) &#123; Assert.notNull(registry, &quot;BeanDefinitionRegistry must not be null&quot;); Assert.notNull(environment, &quot;Environment must not be null&quot;); this.registry = registry; this.conditionEvaluator = new ConditionEvaluator(registry, environment, null); AnnotationConfigUtils.registerAnnotationConfigProcessors(this.registry);&#125; 重点是这行代码。 1AnnotationConfigUtils.registerAnnotationConfigProcessors(this.registry); 还记得这行代码吗？在讲xml的自定义标签解析，解析这个时 讲过——在ClassPathBeanDefinitionScanner讲包的类扫描出来后，在对应的BeanDefinitionParser类——ComponentScanBeanDefinitionParser，把有对应的注解的类封装成ScannedGenericBeanDefinition后，在最后执行这一行代码: 还记得这个方法的作用吗？ 这个方法会把一些BeanPostProcessor封装成RootBeanDefinition，注册到BeanDefinitionRegistry中 之前我们重点关注的是 ConfigurationClassPostProcessor AutowiredAnnotationBeanPostProcessor CommonAnnotationBeanPostProcessor 在前面的章节已经讲过AutowiredAnnotationBeanPostProcessor和CommonAnnotationBeanPostProcessor，这两个类是在Bean的初始化步骤中，注解收集和依赖注入起到作用。剩下的ConfigurationClassPostProcessor还没讲过他的作用。我们看下ConfigurationClassPostProcessor的类图： 可以看到ConfigurationClassPostProcessor实现BeanDefinitionRegistryPostProcessor接口的，也就是说在AbstractApplicationContext#refresh方法执行到 1invokeBeanFactoryPostProcessors(beanFactory) 这行代码时会触发ConfigurationClassPostProcessor#postProcessBeanDefinitionRegistry方法的调用。通过之前对BeanDefinitionRegistryPostProcessor的讲解，已经知道了postProcessBeanDefinitionRegistry方法就是对BeanDefinition做一些操作的，比如添加也是可以的。下面看ConfigurationClassPostProcessor类。 ConfigurationClassPostProcessor 看ConfigurationClassPostProcessor#postProcessBeanDefinitionRegistry源码： 12345678910111213141516public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) &#123; int registryId = System.identityHashCode(registry); if (this.registriesPostProcessed.contains(registryId)) &#123; throw new IllegalStateException( &quot;postProcessBeanDefinitionRegistry already called on this post-processor against &quot; + registry); &#125; if (this.factoriesPostProcessed.contains(registryId)) &#123; throw new IllegalStateException( &quot;postProcessBeanFactory already called on this post-processor against &quot; + registry); &#125; this.registriesPostProcessed.add(registryId); //核心逻辑，重点看，重要程度5 processConfigBeanDefinitions(registry);&#125; ConfigurationClassPostProcessor#processConfigBeanDefinitions123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113public void processConfigBeanDefinitions(BeanDefinitionRegistry registry) &#123; List&lt;BeanDefinitionHolder&gt; configCandidates = new ArrayList&lt;&gt;(); //获取所有的beanNames String[] candidateNames = registry.getBeanDefinitionNames(); for (String beanName : candidateNames) &#123; BeanDefinition beanDef = registry.getBeanDefinition(beanName); //如果有该标识就不再处理 if (beanDef.getAttribute(ConfigurationClassUtils.CONFIGURATION_CLASS_ATTRIBUTE) != null) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Bean definition has already been processed as a configuration class: &quot; + beanDef); &#125; &#125; //判断是否是候选的需要处理的BeanDefinition，如果是则放入容器configCandidates else if (ConfigurationClassUtils.checkConfigurationClassCandidate(beanDef, this.metadataReaderFactory)) &#123; configCandidates.add(new BeanDefinitionHolder(beanDef, beanName)); &#125; &#125; // Return immediately if no @Configuration classes were found //如果容器为空，则直接返回 if (configCandidates.isEmpty()) &#123; return; &#125; // Sort by previously determined @Order value, if applicable //对需要处理的所有beanDefinition排序 configCandidates.sort((bd1, bd2) -&gt; &#123; int i1 = ConfigurationClassUtils.getOrder(bd1.getBeanDefinition()); int i2 = ConfigurationClassUtils.getOrder(bd2.getBeanDefinition()); return Integer.compare(i1, i2); &#125;); // Detect any custom bean name generation strategy supplied through the enclosing application context SingletonBeanRegistry sbr = null; if (registry instanceof SingletonBeanRegistry) &#123; sbr = (SingletonBeanRegistry) registry; if (!this.localBeanNameGeneratorSet) &#123; BeanNameGenerator generator = (BeanNameGenerator) sbr.getSingleton( AnnotationConfigUtils.CONFIGURATION_BEAN_NAME_GENERATOR); if (generator != null) &#123; this.componentScanBeanNameGenerator = generator; this.importBeanNameGenerator = generator; &#125; &#125; &#125; if (this.environment == null) &#123; this.environment = new StandardEnvironment(); &#125; //候选BeanDefinition的解析器 // Parse each @Configuration class ConfigurationClassParser parser = new ConfigurationClassParser( this.metadataReaderFactory, this.problemReporter, this.environment, this.resourceLoader, this.componentScanBeanNameGenerator, registry); Set&lt;BeanDefinitionHolder&gt; candidates = new LinkedHashSet&lt;&gt;(configCandidates); Set&lt;ConfigurationClass&gt; alreadyParsed = new HashSet&lt;&gt;(configCandidates.size()); do &#123; //解析核心流程，重点看，重要程度5 //其实就是把类上面的特殊注解解析出来最终封装成beanDefinition parser.parse(candidates); parser.validate(); Set&lt;ConfigurationClass&gt; configClasses = new LinkedHashSet&lt;&gt;(parser.getConfigurationClasses()); configClasses.removeAll(alreadyParsed); // Read the model and create bean definitions based on its content if (this.reader == null) &#123; this.reader = new ConfigurationClassBeanDefinitionReader( registry, this.sourceExtractor, this.resourceLoader, this.environment, this.importBeanNameGenerator, parser.getImportRegistry()); &#125; //@Bean @Import 内部类 @ImportedResource ImportBeanDefinitionRegistrar具体处理逻辑 this.reader.loadBeanDefinitions(configClasses); //已经解析完成了的类 alreadyParsed.addAll(configClasses); candidates.clear(); //比较差异又走一遍解析流程 if (registry.getBeanDefinitionCount() &gt; candidateNames.length) &#123; String[] newCandidateNames = registry.getBeanDefinitionNames(); Set&lt;String&gt; oldCandidateNames = new HashSet&lt;&gt;(Arrays.asList(candidateNames)); Set&lt;String&gt; alreadyParsedClasses = new HashSet&lt;&gt;(); for (ConfigurationClass configurationClass : alreadyParsed) &#123; alreadyParsedClasses.add(configurationClass.getMetadata().getClassName()); &#125; for (String candidateName : newCandidateNames) &#123; if (!oldCandidateNames.contains(candidateName)) &#123; BeanDefinition bd = registry.getBeanDefinition(candidateName); if (ConfigurationClassUtils.checkConfigurationClassCandidate(bd, this.metadataReaderFactory) &amp;&amp; !alreadyParsedClasses.contains(bd.getBeanClassName())) &#123; candidates.add(new BeanDefinitionHolder(bd, candidateName)); &#125; &#125; &#125; candidateNames = newCandidateNames; &#125; &#125; while (!candidates.isEmpty()); // Register the ImportRegistry as a bean in order to support ImportAware @Configuration classes if (sbr != null &amp;&amp; !sbr.containsSingleton(IMPORT_REGISTRY_BEAN_NAME)) &#123; sbr.registerSingleton(IMPORT_REGISTRY_BEAN_NAME, parser.getImportRegistry()); &#125; if (this.metadataReaderFactory instanceof CachingMetadataReaderFactory) &#123; // Clear cache in externally provided MetadataReaderFactory; this is a no-op // for a shared cache since it&#x27;ll be cleared by the ApplicationContext. ((CachingMetadataReaderFactory) this.metadataReaderFactory).clearCache(); &#125;&#125; 代码很长，这里我根据功能看. 对需要生成BeanDefinition的类进行收集 1beanDef.getAttribute(ConfigurationClassUtils.CONFIGURATION_CLASS_ATTRIBUTE) 这个方法的作用就是看该BeanDefinition的属性中有没有org.springframework.context.annotation.ConfigurationClassPostProcessor.configurationClass这个key对应的属性值。在第一次的情况下该判断会返回false，所以这段代码主要看这行代码 1ConfigurationClassUtils.checkConfigurationClassCandidate(beanDef, this.metadataReaderFactory) 看该方法的部分重要的代码段： 这里有两个逻辑，如果有@Configuration注解，就对BeanDefinition做如下标记: 12beanDef.setAttribute(CONFIGURATION_CLASS_ATTRIBUTE, CONFIGURATION_CLASS_FULL);org.springframework.context.annotation.ConfigurationClassPostProcessor.configurationClass=full 如果没有@Configuration但有@Component、@PropertySource、@ComponentScan、@Bean、@Import、@ImportResource注解的话就对BeanDefinition做如下标记: 12beanDef.setAttribute(CONFIGURATION_CLASS_ATTRIBUTE, CONFIGURATION_CLASS_LITE);org.springframework.context.annotation.ConfigurationClassPostProcessor.configurationClass=lite 这个标记的作用在后面会讲到。 最后： 如有有@Order注解，就取值并设置到BeanDefinition中。 其实ConfigurationClassUtils.checkConfigurationClassCandidate(beanDef, this.metadataReaderFactory) 整个流程就是判断为BeanDefinition是否有@Component、@PropertySource、@ComponentScan、@Bean、@Import、@ImportResource、@Configuration这类注解，有就做标记并返回true。true就代表这个BeanDefinition需要被&#96;&#96;ConfigurationClassPostProcessor&#96;进一步处理。 回到ConfigurationClassPostProcessor#processConfigBeanDefinitions 对需要处理的所有beanDefinition进行升序排序，接着就是重点了 ConfigurationClassParser#parse看ConfigurationClassParser#parse 123456789101112131415161718192021222324252627public void parse(Set&lt;BeanDefinitionHolder&gt; configCandidates) &#123; for (BeanDefinitionHolder holder : configCandidates) &#123; BeanDefinition bd = holder.getBeanDefinition(); try &#123; //扫描注解得到的BeanDefinition if (bd instanceof AnnotatedBeanDefinition) &#123; parse(((AnnotatedBeanDefinition) bd).getMetadata(), holder.getBeanName()); &#125; //非扫描注解得到的BeanDefinition else if (bd instanceof AbstractBeanDefinition &amp;&amp; ((AbstractBeanDefinition) bd).hasBeanClass()) &#123; parse(((AbstractBeanDefinition) bd).getBeanClass(), holder.getBeanName()); &#125; else &#123; parse(bd.getBeanClassName(), holder.getBeanName()); &#125; &#125; catch (BeanDefinitionStoreException ex) &#123; throw ex; &#125; catch (Throwable ex) &#123; throw new BeanDefinitionStoreException( &quot;Failed to parse configuration class [&quot; + bd.getBeanClassName() + &quot;]&quot;, ex); &#125; &#125; //这行代码不能忽视 this.deferredImportSelectorHandler.process();&#125; 该方法的入参，就是之前收集的，有@Component、@PropertySource、@ComponentScan、@Bean、@Import、@ImportResource、@Configuration这些注解的类进行遍历解析。 到了一个重点方法了，看parse方法 12345protected final void parse(@Nullable String className, String beanName) throws IOException &#123; Assert.notNull(className, &quot;No bean class name for configuration class bean definition&quot;); MetadataReader reader = this.metadataReaderFactory.getMetadataReader(className); processConfigurationClass(new ConfigurationClass(reader, beanName), DEFAULT_EXCLUSION_FILTER);&#125; 看processConfigurationClass ConfigurationClassParser#processConfigurationClass其实就是把类上面的注解解析出来最终封装成beanDefinition 12345678910111213141516171819protected void processConfigurationClass(ConfigurationClass configClass, Predicate&lt;String&gt; filter) throws IOException &#123; // 对@Condition注解的支持，过滤掉不需要实例化的类 if (this.conditionEvaluator.shouldSkip(configClass.getMetadata(), ConfigurationPhase.PARSE_CONFIGURATION)) &#123; return; &#125; //不关注的代码 .... //这个对象理解为跟类或者接口对应，然后把metadata对象包装进去了 // Recursively process the configuration class and its superclass hierarchy. SourceClass sourceClass = asSourceClass(configClass, filter); do &#123; //核心代码，认真读 sourceClass = doProcessConfigurationClass(configClass, sourceClass, filter); &#125; while (sourceClass != null); this.configurationClasses.put(configClass, configClass);&#125; 条件判断 12345678910111213141516171819202122232425262728293031323334353637383940public boolean shouldSkip(@Nullable AnnotatedTypeMetadata metadata, @Nullable ConfigurationPhase phase) &#123; if (metadata == null || !metadata.isAnnotated(Conditional.class.getName())) &#123; return false; &#125; if (phase == null) &#123; if (metadata instanceof AnnotationMetadata &amp;&amp; ConfigurationClassUtils.isConfigurationCandidate((AnnotationMetadata) metadata)) &#123; return shouldSkip(metadata, ConfigurationPhase.PARSE_CONFIGURATION); &#125; return shouldSkip(metadata, ConfigurationPhase.REGISTER_BEAN); &#125; List&lt;Condition&gt; conditions = new ArrayList&lt;&gt;(); //获取@Conditional注解的value值 for (String[] conditionClasses : getConditionClasses(metadata)) &#123; for (String conditionClass : conditionClasses) &#123; //反射实例化Condition对象 Condition condition = getCondition(conditionClass, this.context.getClassLoader()); conditions.add(condition); &#125; &#125; //排序 AnnotationAwareOrderComparator.sort(conditions); //调用每一个condition的matches方法 for (Condition condition : conditions) &#123; ConfigurationPhase requiredPhase = null; if (condition instanceof ConfigurationCondition) &#123; requiredPhase = ((ConfigurationCondition) condition).getConfigurationPhase(); &#125; //调用matches方法 if ((requiredPhase == null || requiredPhase == phase) &amp;&amp; !condition.matches(this.context, metadata)) &#123; return true; &#125; &#125; return false;&#125; 这段代码的作用就是，当我有这样的注解时 对应的类 123456public class CustomCondition implements Condition &#123; @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; return true; &#125;&#125; 也就是，直到CustomCondition的matchs返回为true上面的代码才会玩下走，也就是BeanDefinition才能生成。 也可以自定义条件注解 自定义条件注解12345678910@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(value = OnClassCondition.class)public @interface ConditionOnClass &#123; Class&lt;?&gt;[] value() default &#123;&#125;; String[] name() default &#123;&#125;;&#125; 12345678910111213141516public class OnClassCondition implements Condition &#123; @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; if(metadata.isAnnotated(ConditionOnClass.class.getName())) &#123; Map&lt;String, Object&gt; annotationAttributes = metadata.getAnnotationAttributes(ConditionOnClass.class.getName()); try &#123; ClassUtils.forName(annotationAttributes.get(&quot;name&quot;).toString(),ClassUtils.getDefaultClassLoader()); return true; &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); return false; &#125; &#125; return false; &#125;&#125; 使用： 1234567@Component@Conditional(value = &#123;CustomCondition.class,CustomCondition1.class&#125;)@ConditionOnClass(name = &quot;com.enjoy.jack.bean.circular.CircularRefConB&quot;)@ConditionOnProperty(name = &quot;cn.enjoy.flag&quot;)public class ConditionalBean &#123;&#125; 接着看processConfigurationClass方法 需要生成BeanDefinition的类的收集和一些接口的执行核心看ConfigurationClassParser#doProcessConfigurationClass 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798protected final SourceClass doProcessConfigurationClass( ConfigurationClass configClass, SourceClass sourceClass, Predicate&lt;String&gt; filter) throws IOException &#123; //判断类上面是否有Component注解 if (configClass.getMetadata().isAnnotated(Component.class.getName())) &#123; // Recursively process any member (nested) classes first //递归处理有@Component、@PropertySource、@ComponentScan、@Bean、@Import、@ImportResource注解的内部类 processMemberClasses(configClass, sourceClass, filter); &#125; // Process any @PropertySource annotations //处理PropertySources和 PropertySource注解 for (AnnotationAttributes propertySource : AnnotationConfigUtils.attributesForRepeatable( sourceClass.getMetadata(), PropertySources.class, org.springframework.context.annotation.PropertySource.class)) &#123; if (this.environment instanceof ConfigurableEnvironment) &#123; //核心逻辑 processPropertySource(propertySource); &#125; else &#123; logger.info(&quot;Ignoring @PropertySource annotation on [&quot; + sourceClass.getMetadata().getClassName() + &quot;]. Reason: Environment must implement ConfigurableEnvironment&quot;); &#125; &#125; // Process any @ComponentScan annotations //处理ComponentScans和ComponentScan注解 Set&lt;AnnotationAttributes&gt; componentScans = AnnotationConfigUtils.attributesForRepeatable( sourceClass.getMetadata(), ComponentScans.class, ComponentScan.class); //是否需要跳过 if (!componentScans.isEmpty() &amp;&amp; !this.conditionEvaluator.shouldSkip(sourceClass.getMetadata(), ConfigurationPhase.REGISTER_BEAN)) &#123; for (AnnotationAttributes componentScan : componentScans) &#123; // The config class is annotated with @ComponentScan -&gt; perform the scan immediately //这个parse里面的逻辑，基本上跟我们&lt;component-scan&gt;自定义标签解析的逻辑差不多 Set&lt;BeanDefinitionHolder&gt; scannedBeanDefinitions = this.componentScanParser.parse(componentScan, sourceClass.getMetadata().getClassName()); // Check the set of scanned definitions for any further config classes and parse recursively if needed //这里又去递归，扫描到@Component生成beanDefinition后，又递归去校验类上面是否有特殊注解 for (BeanDefinitionHolder holder : scannedBeanDefinitions) &#123; BeanDefinition bdCand = holder.getBeanDefinition().getOriginatingBeanDefinition(); if (bdCand == null) &#123; bdCand = holder.getBeanDefinition(); &#125; //判断是否是候选的BeanDefinition，如果是又parse if (ConfigurationClassUtils.checkConfigurationClassCandidate(bdCand, this.metadataReaderFactory)) &#123; parse(bdCand.getBeanClassName(), holder.getBeanName()); &#125; &#125; &#125; &#125; //处理@Import注解 getImports(sourceClass) 获取类上面的@Import注解并封装成SourceClass // Process any @Import annotations processImports(configClass, sourceClass, getImports(sourceClass), filter, true); //处理@ImportResource注解 ，没啥用，加载xml配置文件 // Process any @ImportResource annotations AnnotationAttributes importResource = AnnotationConfigUtils.attributesFor(sourceClass.getMetadata(), ImportResource.class); if (importResource != null) &#123; String[] resources = importResource.getStringArray(&quot;locations&quot;); Class&lt;? extends BeanDefinitionReader&gt; readerClass = importResource.getClass(&quot;reader&quot;); for (String resource : resources) &#123; String resolvedResource = this.environment.resolveRequiredPlaceholders(resource); //建立xml文件和reader的映射关系 configClass.addImportedResource(resolvedResource, readerClass); &#125; &#125; //处理@Bean注解，重点 // Process individual @Bean methods //收集有@bean 注解的方法 Set&lt;MethodMetadata&gt; beanMethods = retrieveBeanMethodMetadata(sourceClass); for (MethodMetadata methodMetadata : beanMethods) &#123; //加入到ConfigurationClass中 configClass.addBeanMethod(new BeanMethod(methodMetadata, configClass)); &#125; //处理接口里面方法有@Bean注解的，逻辑差不多 // Process default methods on interfaces processInterfaces(configClass, sourceClass); // Process superclass, if any if (sourceClass.getMetadata().hasSuperClass()) &#123; String superclass = sourceClass.getMetadata().getSuperClassName(); if (superclass != null &amp;&amp; !superclass.startsWith(&quot;java&quot;) &amp;&amp; !this.knownSuperclasses.containsKey(superclass)) &#123; this.knownSuperclasses.put(superclass, configClass); // Superclass found, return its annotation metadata and recurse return sourceClass.getSuperClass(); &#125; &#125; // No superclass -&gt; processing is complete return null;&#125;先跳过一些方法，先看这段代码 看源码前，先解析下两个了类 ConfigurationClass该类的是对类的一些配置信息的封装 SourceClass是对类的封装，就是把类和注解封装起来 从这两个类的定义看，SourceClass其实就是一个简单的封装，这个类的重点就是包含了AnnotationMetadata也就是注解元数据。而这个类在源码中创建时，source就是目标类的class对象。 ConfigurationClass从属性上看，这个类也能表示某个类，因为它有beanName。但从其他属性看，更像是对一些信息的收集。 回到doProcessConfigurationClass，我们按功能看 判断类上面是否有Component注解和递归处理有@Component注解的内部类 这段代码块就是判断类上面是否有Component注解和递归处理有@Component注解的内部类，看源码： 12345678910111213141516171819202122232425262728293031323334private void processMemberClasses(ConfigurationClass configClass, SourceClass sourceClass, Predicate&lt;String&gt; filter) throws IOException &#123; // 获取该类的内部类并又包装成sourceClass对象 Collection&lt;SourceClass&gt; memberClasses = sourceClass.getMemberClasses(); if (!memberClasses.isEmpty()) &#123; List&lt;SourceClass&gt; candidates = new ArrayList&lt;&gt;(memberClasses.size()); for (SourceClass memberClass : memberClasses) &#123; //如果类是候选的 if (ConfigurationClassUtils.isConfigurationCandidate(memberClass.getMetadata()) &amp;&amp; !memberClass.getMetadata().getClassName().equals(configClass.getMetadata().getClassName())) &#123; candidates.add(memberClass); &#125; &#125; //排序 OrderComparator.sort(candidates); //循环去处理每一个内部类 for (SourceClass candidate : candidates) &#123; if (this.importStack.contains(configClass)) &#123; this.problemReporter.error(new CircularImportProblem(configClass, this.importStack)); &#125; else &#123; this.importStack.push(configClass); try &#123; //candidate 子 configClass 父，candidate 是 configClass的内部类 processConfigurationClass(candidate.asConfigClass(configClass), filter); &#125; finally &#123; this.importStack.pop(); &#125; &#125; &#125; &#125;&#125; 先通过 1Collection&lt;SourceClass&gt; memberClasses = sourceClass.getMemberClasses(); 获取了内部类，接着执行了 看哪些内部类有@Component、@PropertySource、@ComponentScan、@Bean、@Import、@ImportResource这些注解 排序完后，就进行遍历了。这个就是解析的入口方法，所以这是一个递归的过程了。 继续看doProcessConfigurationClass 处理PropertySources和 PropertySource注解 12345678910111213141516171819202122232425262728293031private void processPropertySource(AnnotationAttributes propertySource) throws IOException &#123; String name = propertySource.getString(&quot;name&quot;); if (!StringUtils.hasLength(name)) &#123; name = null; &#125; String encoding = propertySource.getString(&quot;encoding&quot;); if (!StringUtils.hasLength(encoding)) &#123; encoding = null; &#125; //获取配置文件路径 String[] locations = propertySource.getStringArray(&quot;value&quot;); Assert.isTrue(locations.length &gt; 0, &quot;At least one @PropertySource(value) location is required&quot;); boolean ignoreResourceNotFound = propertySource.getBoolean(&quot;ignoreResourceNotFound&quot;); Class&lt;? extends PropertySourceFactory&gt; factoryClass = propertySource.getClass(&quot;factory&quot;); PropertySourceFactory factory = (factoryClass == PropertySourceFactory.class ? DEFAULT_PROPERTY_SOURCE_FACTORY : BeanUtils.instantiateClass(factoryClass)); for (String location : locations) &#123; try &#123; //替换占位符 String resolvedLocation = this.environment.resolveRequiredPlaceholders(location); //流的方式加载配置文件并封装成Resource对象 Resource resource = this.resourceLoader.getResource(resolvedLocation); //加载Resource中的配置属性封装成Properties对象中，并创建PropertySource对象加入到Environment对象中 addPropertySource(factory.createPropertySource(name, new EncodedResource(resource, encoding))); &#125; //去掉catch ... &#125;&#125; 通过执行了代码后 1factory.createPropertySource(name, new EncodedResource(resource, encoding)) 已经把配置信息封装成一个Map，并封装成一个PropertySource 接着在代码中 1addPropertySource(PropertySource&lt;?&gt; propertySource) 会把PropertySource添加到Environment中 处理ComponentScans和ComponentScan注解 先通过上边的代码获取ComponentScan的集合。如果有就进入该代码块： 对前面的结合遍历，而在在for循环中有这样的一个方法: 1Set&lt;BeanDefinitionHolder&gt; scannedBeanDefinitions = this.componentScanParser.parse(componentScan, sourceClass.getMetadata().getClassName()); componentScanParser指向的对象是ComponentScanAnnotationParser ComponentScanAnnotationParser#parse 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public Set&lt;BeanDefinitionHolder&gt; parse(AnnotationAttributes componentScan, final String declaringClass) &#123; ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(this.registry, componentScan.getBoolean(&quot;useDefaultFilters&quot;), this.environment, this.resourceLoader); Class&lt;? extends BeanNameGenerator&gt; generatorClass = componentScan.getClass(&quot;nameGenerator&quot;); boolean useInheritedGenerator = (BeanNameGenerator.class == generatorClass); scanner.setBeanNameGenerator(useInheritedGenerator ? this.beanNameGenerator : BeanUtils.instantiateClass(generatorClass)); ScopedProxyMode scopedProxyMode = componentScan.getEnum(&quot;scopedProxy&quot;); if (scopedProxyMode != ScopedProxyMode.DEFAULT) &#123; scanner.setScopedProxyMode(scopedProxyMode); &#125; else &#123; Class&lt;? extends ScopeMetadataResolver&gt; resolverClass = componentScan.getClass(&quot;scopeResolver&quot;); scanner.setScopeMetadataResolver(BeanUtils.instantiateClass(resolverClass)); &#125; scanner.setResourcePattern(componentScan.getString(&quot;resourcePattern&quot;)); for (AnnotationAttributes filter : componentScan.getAnnotationArray(&quot;includeFilters&quot;)) &#123; for (TypeFilter typeFilter : typeFiltersFor(filter)) &#123; scanner.addIncludeFilter(typeFilter); &#125; &#125; for (AnnotationAttributes filter : componentScan.getAnnotationArray(&quot;excludeFilters&quot;)) &#123; for (TypeFilter typeFilter : typeFiltersFor(filter)) &#123; scanner.addExcludeFilter(typeFilter); &#125; &#125; boolean lazyInit = componentScan.getBoolean(&quot;lazyInit&quot;); if (lazyInit) &#123; scanner.getBeanDefinitionDefaults().setLazyInit(true); &#125; Set&lt;String&gt; basePackages = new LinkedHashSet&lt;&gt;(); String[] basePackagesArray = componentScan.getStringArray(&quot;basePackages&quot;); for (String pkg : basePackagesArray) &#123; String[] tokenized = StringUtils.tokenizeToStringArray(this.environment.resolvePlaceholders(pkg), ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS); Collections.addAll(basePackages, tokenized); &#125; for (Class&lt;?&gt; clazz : componentScan.getClassArray(&quot;basePackageClasses&quot;)) &#123; basePackages.add(ClassUtils.getPackageName(clazz)); &#125; if (basePackages.isEmpty()) &#123; basePackages.add(ClassUtils.getPackageName(declaringClass)); &#125; scanner.addExcludeFilter(new AbstractTypeHierarchyTraversingFilter(false, false) &#123; @Override protected boolean matchClassName(String className) &#123; return declaringClass.equals(className); &#125; &#125;); return scanner.doScan(StringUtils.toStringArray(basePackages));&#125; 第一行就new ClassPathBeanDefinitionScanner了，这个ClassPathBeanDefinitionScanner就是负责包扫描的。 后面的代码其他都是向ClassPathBeanDefinitionScanner中填充注解属性 最后： 这个方法的详情看01-Spring中BeanDefinition的创建和注册 扫描完后返回了一个BeanDefinition集合，接着又有一个for 因为通过scaner扫描出来的只是有@Component注解类，类上还可能有其他注解，所以对于这种情况，如果有对应注解，这里通过循环调用parse，又把流程走一遍 处理@Import注解接着代码走到这 先通过getImports(sourceClass)拿到类上的@Import的值和类上其他注解中@Import的的值，返回一个SourceClass集合。 接着看processImports的核心代码段 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//如果没有@Import注解直接返回，不处理if (importCandidates.isEmpty()) &#123; return;&#125;//循环类上面的每一个@Importfor (SourceClass candidate : importCandidates) &#123; //如果Import进来的是一个ImportSelector类型 if (candidate.isAssignable(ImportSelector.class)) &#123; // Candidate class is an ImportSelector -&gt; delegate to it to determine imports Class&lt;?&gt; candidateClass = candidate.loadClass(); //反射实例化 ImportSelector selector = ParserStrategyUtils.instantiateClass(candidateClass, ImportSelector.class, this.environment, this.resourceLoader, this.registry); Predicate&lt;String&gt; selectorFilter = selector.getExclusionFilter(); if (selectorFilter != null) &#123; exclusionFilter = exclusionFilter.or(selectorFilter); &#125; //如果是一个DeferredImportSelector类型 if (selector instanceof DeferredImportSelector) &#123; //比较复杂，springboot中自动配置用到了 this.deferredImportSelectorHandler.handle(configClass, (DeferredImportSelector) selector); &#125; else &#123; //在这里调用selectImports方法，返回所有的需要import到spring容器的beanName String[] importClassNames = selector.selectImports(currentSourceClass.getMetadata()); Collection&lt;SourceClass&gt; importSourceClasses = asSourceClasses(importClassNames, exclusionFilter); //递归处理，有可能import进来的类又有@Import注解 processImports(configClass, currentSourceClass, importSourceClasses, exclusionFilter, false); &#125; &#125; //如果Import进来的是一个ImportBeanDefinitionRegistrar类型 else if (candidate.isAssignable(ImportBeanDefinitionRegistrar.class)) &#123; // Candidate class is an ImportBeanDefinitionRegistrar -&gt; // delegate to it to register additional bean definitions Class&lt;?&gt; candidateClass = candidate.loadClass(); //反射实例化 ImportBeanDefinitionRegistrar registrar = ParserStrategyUtils.instantiateClass(candidateClass, ImportBeanDefinitionRegistrar.class, this.environment, this.resourceLoader, this.registry); //加入到importBeanDefinitionRegistrars容器中，这里还没有调用registerBeanDefinitions configClass.addImportBeanDefinitionRegistrar(registrar, currentSourceClass.getMetadata()); &#125; else &#123; // Candidate class not an ImportSelector or ImportBeanDefinitionRegistrar -&gt; // process it as an @Configuration class this.importStack.registerImport( currentSourceClass.getMetadata(), candidate.getMetadata().getClassName()); //如果都不是，则走这里 processConfigurationClass(candidate.asConfigClass(configClass), exclusionFilter); &#125;&#125; importCandidates就是@Import中值。 从源码可知如果Import进来的是一个ImportSelector类型、DeferredImportSelector类型、ImportBeanDefinitionRegistrar类型的，都会在该方法中执行对应的逻辑，而如不是上边的类型，就会执行processConfigurationClass(candidate.asConfigClass(configClass), exclusionFilter)也就是递归的处理引入类的注解信息，这里有一点要注意，candidate.asConfigClass(configClass)candidate为被引入的类，configClass为有@Import的类，通过asConfigClass后，会把configClass添加到ConfigurationClass.importedBy集合中，这种表示该类是被某个类@Import进来的。 回到源码。 第一段代码就是如果没有@Import注解直接返回，不处理，有的话才会执行下面流程。假如有Import注解 看for的代码块，就是遍历@Import注解上的类，先看第一个if条件。 可以看到， 先实例话了对象，然后根据实现的接口走不同的逻辑。 如果实现了ImportSelector接口，就会先去创建对象，然后会看这个对象是否实现了DeferredImportSelector接口，如果没有实现，就会直接调用ImportSelector的selectImports方法，从源码可知，该方法的入参为有@Import注解的类。 执行完ImportSelector接口方法后，接口返回了一个类的全限定名数组，接着根据这个数组生成Collection&lt;SourceClass&gt; 1Collection&lt;SourceClass&gt; importSourceClasses = asSourceClasses(importClassNames, exclusionFilter); 最后，返回的类（Collection&lt;SourceClass&gt;）又有可能有Import注解，所以又调用了processImports递归的处理。 如果实现了DeferredImportSelector接口 就只是包装成DeferredImportSelectorHolder，然后放入到集合中 执行时机后边讲 接着看else if代码块: 如是实现的是mportBeanDefinitionRegistrars接口，先实例化，然后执行 12345configClass.addImportBeanDefinitionRegistrar(registrar, currentSourceClass.getMetadata());public void addImportBeanDefinitionRegistrar(ImportBeanDefinitionRegistrar registrar, AnnotationMetadata importingClassMetadata) &#123; this.importBeanDefinitionRegistrars.put(registrar, importingClassMetadata);&#125; 添加到configClass（ConfigurationClass）的importBeanDefinitionRegistrars容器中。 这个接口的作用相当于BeanDefinitionRegistryPostProcessor#postProcessBeanDefinitionRegistry方法，只是调用的时机不同，比如实现这样一个功能 1234567891011public class JamesImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; //自己创建beanDefinition对象，然后注册到BeanDefinitionRegistry中 GenericBeanDefinition genericBeanDefinition = new GenericBeanDefinition(); genericBeanDefinition.setBeanClass(BeanDefinitionBean.class); MutablePropertyValues propertyValues = genericBeanDefinition.getPropertyValues(); propertyValues.add(&quot;name&quot;,&quot;Jack&quot;); registry.registerBeanDefinition(&quot;beanDefinitionBean&quot;,genericBeanDefinition); &#125;&#125; 执行时机先不说 如果上面的接口都没有，情况下走到else了： candidate.asConfigClass(configClass)，这个方法的参数要搞清楚，configClass是有@Import注解的类，而candidate是被引入的类，这段代码的意思就是，把configClass添加到candidate的ConfigurationClass.importedBy集合中。这样，就能知道某个类是被谁impor的了。 接着调用了processConfigurationClass，因为被引入进来的类可能还有别的注解，所以流程又从头开始。 这个方法不像上边的Scan方法，这个方法中还没涉及BeanDefinition的创建，也没有注册，但会实例化对象，这点看上去有点迷惑，但如果看回入口方法ConfigurationClassParser#processConfigurationClass，在最后会把ConfigurationClass放入到一个Map中。从这点上看ConfigurationClass可以看做是BeanDefinition信息的载体。先看到这，等把下面的流程看完了， 再回到入口方法。 处理@ImportResource注解代码执行到这里了 再这里处理ImportResource注解。代码也很简单，只是在向ConfigurationClass的importedResources集合放入对应关系，也就是，这里和上边的一样，不会去实际处理。只是做了信息的收集。 处理@Bean注解 1retrieveBeanMethodMetadata(sourceClass) 这段代码的意思就是返回有@Bean的方法，并且把该方法封装成MethodMetadata，然后放入到ConfigurationClass的beanMethods集合中。还是做了信息的收集 重点的代码已经看完了，看回入口方法 再看ConfigurationClassParser#processConfigurationClass12345678SourceClass sourceClass = asSourceClass(configClass, filter);do &#123; //核心代码，认真读 sourceClass = doProcessConfigurationClass(configClass, sourceClass, filter);&#125;while (sourceClass != null);this.configurationClasses.put(configClass, configClass); 经过了doProcessConfigurationClass方法后，除了扫描的其他都是对注解信息的收集，并把信息放到了对应的ConfigurationClass类中，最后把ConfigurationClass放入了configurationClasses这个Map集合中。这个方法的逻辑就结束了。 也就是说configurationClasses这个集合是用来保存已经完成信息收集的ConfigurationClass类。 DeferredImportSelector类型的类的处理时机和作用DeferredImportSelector接口的作用、初始化和执行原理 再看ConfigurationClassPostProcessor#processConfigBeanDefinitions完整代码 上边的代码只是执行了parse方法，并把收集完信息的ConfigrationClass放入了ConfigurationClassParser#configurationClasses这个集合中，那下边剩下的逻辑就是处理这个集合中的configurationClasses了，看代码 继续跟踪代码到这里 BeanDefinition的创建——ConfigurationClassBeanDefinitionReader#loadBeanDefinitions 在其实已经有BeanDefinition的创建的创建了，就是@Component注解扫描。在这扫描其实已经把绝大部分的类的BeanDefinition都创建了。而到了loadBeanDefinitions这一步就是为剩余的类生成BeanDefinition。这些剩余类有 通过applicationContext.register()这样注册的 @Import进来的类，这里设计到的 @Bean注解 引入进来的xml文件的解析流程 ImportBeanDefinitionRegistrar 123456789101112131415161718192021222324252627282930313233343536//ConfigurationClassBeanDefinitionReaderpublic void loadBeanDefinitions(Set&lt;ConfigurationClass&gt; configurationModel) &#123; TrackedConditionEvaluator trackedConditionEvaluator = new TrackedConditionEvaluator(); for (ConfigurationClass configClass : configurationModel) &#123; loadBeanDefinitionsForConfigurationClass(configClass, trackedConditionEvaluator); &#125;&#125;private void loadBeanDefinitionsForConfigurationClass( ConfigurationClass configClass, TrackedConditionEvaluator trackedConditionEvaluator) &#123; //是否要跳过 // 判断哪些被@Improt引入的类中是有用@Contitional注解，有就执行方法判断条件是否通过 if (trackedConditionEvaluator.shouldSkip(configClass)) &#123; String beanName = configClass.getBeanName(); if (StringUtils.hasLength(beanName) &amp;&amp; this.registry.containsBeanDefinition(beanName)) &#123; this.registry.removeBeanDefinition(beanName); &#125; this.importRegistry.removeImportingClass(configClass.getMetadata().getClassName()); return; &#125; //@Import进来的类，和内部类走这里变成BeanDefinition，并注册 if (configClass.isImported()) &#123; registerBeanDefinitionForImportedConfigurationClass(configClass); &#125; //@Bean注解的方法变成BeanDefinition，并注册 for (BeanMethod beanMethod : configClass.getBeanMethods()) &#123; loadBeanDefinitionsForBeanMethod(beanMethod); &#125; //走xml解析流程 loadBeanDefinitionsFromImportedResources(configClass.getImportedResources()); //调用ImportBeanDefinitionRegistrar的方法 loadBeanDefinitionsFromRegistrars(configClass.getImportBeanDefinitionRegistrars());&#125; 根据@Conditional来判断按功能看代码 这里又是根据@Conditional来判断条件是否符合 @Import进来的类 先执行了configClass.isImported())看源码： 123public boolean isImported() &#123; return !this.importedBy.isEmpty();&#125; 也就是，这个if是去判断这个类是否被@Import注解引入的。 12345678910111213141516171819// ConfigurationClassBeanDefinitionReaderprivate void registerBeanDefinitionForImportedConfigurationClass(ConfigurationClass configClass) &#123; AnnotationMetadata metadata = configClass.getMetadata(); AnnotatedGenericBeanDefinition configBeanDef = new AnnotatedGenericBeanDefinition(metadata); ScopeMetadata scopeMetadata = scopeMetadataResolver.resolveScopeMetadata(configBeanDef); configBeanDef.setScope(scopeMetadata.getScopeName()); String configBeanName = this.importBeanNameGenerator.generateBeanName(configBeanDef, this.registry); AnnotationConfigUtils.processCommonDefinitionAnnotations(configBeanDef, metadata); BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(configBeanDef, configBeanName); definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry); this.registry.registerBeanDefinition(definitionHolder.getBeanName(), definitionHolder.getBeanDefinition()); configClass.setBeanName(configBeanName); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Registered bean definition for imported class &#x27;&quot; + configBeanName + &quot;&#x27;&quot;); &#125;&#125; 生成一个AnnotatedGenericBeanDefinition对象了，并且执行registry.registerBeanDefinition去注册BeanDefinition @Bean注解接着看代码 ConfigurationClass中的BeanMethods集合放的是有@Bean的方法的封装，所以这个for就是把@Bean的方法，变成BeanDefinition，看源码到这里 这时存在这样的关系 这个是真的和factoryMethod一样了。 经过上面的步骤后，被@Import引入的类和有@Bean修饰的方法，都已经变成一个BeanDefinition了，剩下的看下面 xml解析 这个步骤就看这里——Spring中BeanDefinition的创建和注册BeanDefinition已经都生成并注册完了，看最后的一段代码 ImportBeanDefinitionRegistrar的方法 在对@Impor的解析的时候，如果引入的类实现了接口ImportBeanDefinitionRegistrar接口，就实例化一个对象并放入了ConfigrationClass的importBeanDefinitionRegistrars中，而这段代码就是拿到这个集合，并调用对象的registerBeanDefinitions方法。 看到这，还记得一个接口BeanDefinitionRegistryPostProcesso这个接口吗，ImportBeanDefinitionRegistrar接口的作用和BeanDefinitionRegistryPostProcesso是一样的。但是执行的时机不同。BeanDefinitionRegistryPostProcesso接口的的postProcessBeanDefinitionRegistry这个方法BeanFactory创建完成后调用的，可以对BeanDefinition做任何操作。可以看BeanDefinitionRegistryPostProcessor调用时机可以知道，实现PriorityOrdered接口的是最先执行的，而ConfigurationClassPostProcessor实现了这个接口，但该类的getOrder()返回的是最低的优先级。 也就是说，从执行的顺序看 实现了BeanDefinitionRegistryPostProcessor和PriorityOrdered的先执行 ConfigurationClassPostProcessor执行 在ConfigurationClassPostProcessor执行的最后执行ImportBeanDefinitionRegistrar 执行实现了BeanDefinitionRegistryPostProcessor和Ordered的 执行只实现了BeanDefinitionRegistryPostProcessor 所以，ImportBeanDefinitionRegistrar接口的调用是发生在ConfigurationClassPostProcessor中，所以对于某些有先后关系的Beandefinition，这时就需要考虑优先级的问题。 ConfigurationClassParser#parse总结parser.parse(candidates) 先解析@Configuration、@PropertySources、@PropertySource、@ComponentScan、@Bean、@Import、@ImportResource。在解析@ComponentScan的时候就已经把BeanDefinition创建并且注册了，而在解析@Import的时候会调用ImportSelector接口的方法，其他的注解都只是做一个信息收集的过程。收集完注解的信息后，会把这些信息的封装对象ConfigurationClass放到一个收集完成集合中，根据之前收集的信息（ConfigurationClass）去创建并注册BeanDefinition，并且最后调用ImportBeanDefinitionRegistrar接口的方法。 @Configuration和@Component的问题从源码可以知道，使用@Component都能触发内部注解的解析，但为什么还要用@Configuration这个注解呢？ 比如这种情况 在使用@Component的时候，根据之前的Bean的初始化阶段factoryMethod的源码我们都知道，这种情况lison()会被调用了两次。也就是说，会产生两个Lison对象。 还有一种情况 12345678910111213141516public class LiLi implements FactoryBean &#123; @Override public Object getObject() throws Exception &#123; return new Docker(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return Docker.class; &#125; public void xae() &#123; System.out.println(&quot;xae&quot;); &#125;&#125; 也是使用了@Component，我通过applicationContext.getBean(Docker.class)获取的对象和通过lisonFactory()方法调用lili的getObject获取的对象是两个不同的对象。其实这样想，在lisonFactory()中就相当于获取了Lili中个工厂类，然后工程类在new一个。 上面两种情况都破坏了Spring的单例。 但是上面两种情况只要把@Component注解换成@Configuration注解，那么这时调用的都是获取同一个对象。@Configuration注解就是保证单例的。其实就是调用liLi方法时走的是代理对象的方法，代理对象会从缓存中拿值。 怎么实现的，看回源码 上面都是讲了ConfigurationClassPostProcessor的postProcessBeanDefinitionRegistry方法。用来生成并注册BeanDefinition。在处理完个方法后，还会执行一个方法，就是postProcessBeanFactory。现在看 ConfigurationClassPostProcessor#postProcessBeanFactory方法。 ConfigurationClassPostProcessor#postProcessBeanFactory 说代码前，先看下cglib的api 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public void enhanceConfigurationClasses(ConfigurableListableBeanFactory beanFactory) &#123; Map&lt;String, AbstractBeanDefinition&gt; configBeanDefs = new LinkedHashMap&lt;&gt;(); for (String beanName : beanFactory.getBeanDefinitionNames()) &#123; BeanDefinition beanDef = beanFactory.getBeanDefinition(beanName); Object configClassAttr = beanDef.getAttribute(ConfigurationClassUtils.CONFIGURATION_CLASS_ATTRIBUTE); MethodMetadata methodMetadata = null; if (beanDef instanceof AnnotatedBeanDefinition) &#123; methodMetadata = ((AnnotatedBeanDefinition) beanDef).getFactoryMethodMetadata(); &#125; if ((configClassAttr != null || methodMetadata != null) &amp;&amp; beanDef instanceof AbstractBeanDefinition) &#123; // Configuration class (full or lite) or a configuration-derived @Bean method // -&gt; resolve bean class at this point... AbstractBeanDefinition abd = (AbstractBeanDefinition) beanDef; if (!abd.hasBeanClass()) &#123; try &#123; abd.resolveBeanClass(this.beanClassLoader); &#125; catch (Throwable ex) &#123; throw new IllegalStateException( &quot;Cannot load configuration class: &quot; + beanDef.getBeanClassName(), ex); &#125; &#125; &#125; if (ConfigurationClassUtils.CONFIGURATION_CLASS_FULL.equals(configClassAttr)) &#123; //打印日志 ..... configBeanDefs.put(beanName, (AbstractBeanDefinition) beanDef); &#125; &#125; if (configBeanDefs.isEmpty()) &#123; // nothing to enhance -&gt; return immediately return; &#125; ConfigurationClassEnhancer enhancer = new ConfigurationClassEnhancer(); for (Map.Entry&lt;String, AbstractBeanDefinition&gt; entry : configBeanDefs.entrySet()) &#123; AbstractBeanDefinition beanDef = entry.getValue(); // If a @Configuration class gets proxied, always proxy the target class beanDef.setAttribute(AutoProxyUtils.PRESERVE_TARGET_CLASS_ATTRIBUTE, Boolean.TRUE); // Set enhanced subclass of the user-specified bean class Class&lt;?&gt; configClass = beanDef.getBeanClass(); Class&lt;?&gt; enhancedClass = enhancer.enhance(configClass, this.beanClassLoader); if (configClass != enhancedClass) &#123; if (logger.isTraceEnabled()) &#123; //打印日志 ..... &#125; beanDef.setBeanClass(enhancedClass); &#125; &#125;&#125; 在之前的代码中，检查到有@Configuration、@Componet等注解的时候会为BeanDefi&#96;nition打一个标记，现在在这里取出这个标记。 往下走到这 在前面已经讲过了@Configuration，把属性值设置为CONFIGURATION_CLASS_FULL=full，而这块代码也很简单，就是放入到configBeanDefs容器中。这时往下走到这段代码，就是对configBeanDefs集合的调用 在这个for循环中，生成了一个ConfigurationClassEnhancer，然后循环的 也就是说，如果有@Configuration这个注解，就把需要生成代理标记设置到beanDefinition的属性中。并把代理对象的class设置到BeanDefinition中。现在看下： ConfigurationClassEnhancer#enhance 1234567891011private Enhancer newEnhancer(Class&lt;?&gt; configSuperClass, @Nullable ClassLoader classLoader) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(configSuperClass); enhancer.setInterfaces(new Class&lt;?&gt;[] &#123;EnhancedConfiguration.class&#125;); enhancer.setUseFactory(false); enhancer.setNamingPolicy(SpringNamingPolicy.INSTANCE); enhancer.setStrategy(new BeanFactoryAwareGeneratorStrategy(classLoader)); enhancer.setCallbackFilter(CALLBACK_FILTER); enhancer.setCallbackTypes(CALLBACK_FILTER.getCallbackTypes()); return enhancer;&#125; 也就是说在根据BeanDefinition生成bean的过程中会生成一个代理对象，该代理对象继承了目标类 定义的Callback 可以看下 从cglib的api可知，代理对象的方法调用都会被CallbackFilter拦截，看上面的ConditionalCallbackFilter 1234567891011121314151617181920212223242526272829private static class ConditionalCallbackFilter implements CallbackFilter &#123; private final Callback[] callbacks; private final Class&lt;?&gt;[] callbackTypes; public ConditionalCallbackFilter(Callback[] callbacks) &#123; this.callbacks = callbacks; this.callbackTypes = new Class&lt;?&gt;[callbacks.length]; for (int i = 0; i &lt; callbacks.length; i++) &#123; this.callbackTypes[i] = callbacks[i].getClass(); &#125; &#125; @Override public int accept(Method method) &#123; for (int i = 0; i &lt; this.callbacks.length; i++) &#123; Callback callback = this.callbacks[i]; if (!(callback instanceof ConditionalCallback) || ((ConditionalCallback) callback).isMatch(method)) &#123; return i; &#125; &#125; throw new IllegalStateException(&quot;No callback available for method &quot; + method.getName()); &#125; public Class&lt;?&gt;[] getCallbackTypes() &#123; return this.callbackTypes; &#125;&#125; 在accept中，看哪个匹配就执行那个，我们现在关注的是@Bean注解，所以去isMatch中找吧，找到了BeanMethodInterceptor的isMatch匹配 也就是说调用的方法会进入到BeanMethodInterceptor的intercept方法中。看源码 这个有印象吗？就是在Spring bean的初始化时，对于@Bean、factoryMetoh这种通过调用方法来实例化的时候，Spring调用SimpleInstantiationStrategy.instantiate的源码： 在初始化阶段，这里先把有@Bean的方法对象放入到currentlyInvokedFactoryMethod这个threadLocal中，然后用反射调用方法，由于factoryBean这个是代理对象，所以这里的方法调用实际上就走到了这里： 也就是说，在Spring调用方法的时候实际上也是一个代理对象，这个时候由于这个ThreadLocal有值，所以Spring调用的时候会进去了if的代码块，通过调用被代理对象调用方法来生成对象。这是在spring的ioc初始化阶段走的逻辑。如果初始化阶段结束了，在程序运行节点调用有@bean的方法的时候，回到BeanMethodInterceptor的intercept方法，继续走 进去resolveBeanReference后继续走 走到这里意味着已经初始化完成了，所以通过beanFactory.getBean就是从一级缓存singletonObjects这个Map中拿数据了。接着代码继续走，由于Spring在调用代理方法的时候都会往一个TreadLocal中设置值，调用完后就移除了，走到这段代码时 会放回null，所以这时我们通过缓存获取的对象就直接返回了。所以在上边的两种情况下，调用了一个@Bean的方法，那就相当于从一级缓存中拿到数据了。而对于FactoryBean LiLi 这时是一个FactoryBean类型对象，但由于这个对象是cglib的代理对象，所以当调用object.getObject时，代理对象通过getBean在一级缓存singletonObjects中拿到FactoryBean类型对象，然后在返回前调用调用getObjectForBeanInstance，这个方法在讲实例化的最后——FactoryBean类型实例化就说过了，作用就是从factoryBeanObjectCache中拿目标对象。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"06-Spring自定义Scope","slug":"spring/06-Spring自定义Scope","date":"2021-11-25T12:00:12.000Z","updated":"2022-03-23T09:03:54.671Z","comments":true,"path":"blog/spring/06-Spring自定义Scope/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/06-Spring%E8%87%AA%E5%AE%9A%E4%B9%89Scope/","excerpt":"","text":"Bean 的多例作用域(如何管理 bean而已) 自定义作用域(Scope)自定义Scope要回到AbstractBeanFactory#doGetBean方法，源码 1234567891011121314151617181920212223242526272829protected &lt;T&gt; T doGetBean( String name, @Nullable Class&lt;T&gt; requiredType, @Nullable Object[] args, boolean typeCheckOnly) throws BeansException &#123; //去掉和这节没关的代码 ... String beanName = transformedBeanName(name); Object bean; //去掉和这节没关的代码 ... else &#123; String scopeName = mbd.getScope(); Scope scope = this.scopes.get(scopeName); try &#123; Object scopedInstance = scope.get(beanName, () -&gt; &#123; beforePrototypeCreation(beanName); try &#123; return createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; &#125;); bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); &#125; &#125; //去掉和这节没关的代码 ... return (T) bean;&#125; 自定义Scope： 1234567891011121314151617181920212223242526272829303132333435363738public class CustomScope implements Scope &#123; private ThreadLocal local = new ThreadLocal(); @Override public Object get(String name, ObjectFactory&lt;?&gt; objectFactory) &#123; if(local.get() != null) &#123; return local.get(); &#125; else &#123; //创建实例 // 其实就是调用createBean Object object = objectFactory.getObject(); local.set(object); return object; &#125; &#125; @Override public Object remove(String name) &#123; Object o = local.get(); local.remove(); return o; &#125; @Override public void registerDestructionCallback(String name, Runnable callback) &#123; &#125; @Override public Object resolveContextualObject(String key) &#123; return null; &#125; @Override public String getConversationId() &#123; return null; &#125;&#125; 自定义Scope注册: 12345678910111213@Componentpublic class CustomScopeRegistry implements BeanDefinitionRegistryPostProcessor &#123; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; &#125; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; beanFactory.registerScope(&quot;refreshScope&quot;,new CustomScope()); &#125;&#125; 使用自定义Scope： 1234567@Component@Scope(&quot;refreshScope&quot;)@Datapublic class CustomScopeBean &#123; private String username;&#125;","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"05-Spring配置的解析","slug":"spring/05-Spring配置的解析","date":"2021-11-25T12:00:11.000Z","updated":"2022-03-23T09:03:54.661Z","comments":true,"path":"blog/spring/05-Spring配置的解析/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/05-Spring%E9%85%8D%E7%BD%AE%E7%9A%84%E8%A7%A3%E6%9E%90/","excerpt":"","text":"在Spring中，Spring会把配置内容保存到Environment中。而这个Environment内容的对象类型为StandardEnvironment。的创建采用的延迟创建的模式，只有当点用ApplicationContext的getEnvironment方法时才会传遍。源码为 123456789101112//AbstractApplicationContext@Overridepublic ConfigurableEnvironment getEnvironment() &#123; if (this.environment == null) &#123; this.environment = createEnvironment(); &#125; return this.environment;&#125;protected ConfigurableEnvironment createEnvironment() &#123; return new StandardEnvironment();&#125; xml对配置的解析传统手艺xml 1&lt;context:property-placeholder location=&quot;classpath:application.properties&quot;/&gt; 现在找property-placeholder这个是那个类解析的，流程就是通过context，找到URI，通过URi找到对应的NamespaceHandlerSupport，再看该类的init方法，根据key找到BeanDefinitionParser 最终找到了property-placeholder的解析类是PropertyPlaceholderBeanDefinitionParser，看下该类的定义: 看getBeanClass，该方法会返回一个解析的关键类，会返回一个PropertySourcesPlaceholderConfigurer.class对象 为什么说它重要呢？因为在对自定义标签的解析中，会去到PropertyPlaceholderBeanDefinitionParser后会调用parser方法，在该类调用的parser方法中就是调用了getBeanClass方法，并且会创建一个GenericBeanDefinition然后注册到beanFactory中，那么在BeanDefinition初始化和注册完成后，就会去找实现了BeanFactoryPostProcessor接口的BeanDefinition，然后实例化并调用方法postProcessBeanFactory方法。 看PropertySourcesPlaceholderConfigurer的类图 实现了BeanFactoryPostProcessor接口，那么会在BeanDefinition初始化和注册完成后就会调用该接口的方法，看方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; if (this.propertySources == null) &#123; this.propertySources = new MutablePropertySources(); if (this.environment != null) &#123; //把environment对象封装成的PropertySource对象加入到 this.propertySources.addLast( //把environment对象封装成PropertySource对象MutablePropertySources中的list中 new PropertySource&lt;Environment&gt;(ENVIRONMENT_PROPERTIES_PROPERTY_SOURCE_NAME, this.environment) &#123; @Override @Nullable //source就是environment对象 public String getProperty(String key) &#123; return this.source.getProperty(key); &#125; &#125; ); &#125; try &#123; //加载本地配置文件中的属性值包装成properties对象后，最终包装成PropertySource对象 PropertySource&lt;?&gt; localPropertySource = new PropertiesPropertySource(LOCAL_PROPERTIES_PROPERTY_SOURCE_NAME, mergeProperties()); //加入到MutablePropertySources中的list中 if (this.localOverride) &#123; this.propertySources.addFirst(localPropertySource); &#125; else &#123; this.propertySources.addLast(localPropertySource); &#125; &#125; catch (IOException ex) &#123; throw new BeanInitializationException(&quot;Could not load properties&quot;, ex); &#125; &#125; processProperties(beanFactory, new PropertySourcesPropertyResolver(this.propertySources)); this.appliedPropertySources = this.propertySources;&#125;protected void processProperties(ConfigurableListableBeanFactory beanFactoryToProcess, final ConfigurablePropertyResolver propertyResolver) throws BeansException &#123; //设置占位符的前缀后缀 propertyResolver.setPlaceholderPrefix(this.placeholderPrefix); propertyResolver.setPlaceholderSuffix(this.placeholderSuffix); //设分割符 : propertyResolver.setValueSeparator(this.valueSeparator); //重点是这个匿名对象 @Value的依赖注入会掉过来 StringValueResolver valueResolver = strVal -&gt; &#123; String resolved = (this.ignoreUnresolvablePlaceholders ? propertyResolver.resolvePlaceholders(strVal) : propertyResolver.resolveRequiredPlaceholders(strVal)); if (this.trimValues) &#123; resolved = resolved.trim(); &#125; return (resolved.equals(this.nullValue) ? null : resolved); &#125;; //核心流程。把占位符$&#123;xxx&#125;替换成真正的值 doProcessProperties(beanFactoryToProcess, valueResolver);&#125;protected void doProcessProperties(ConfigurableListableBeanFactory beanFactoryToProcess, StringValueResolver valueResolver) &#123; //beanDefinition的修改者 BeanDefinitionVisitor visitor = new BeanDefinitionVisitor(valueResolver); //获取所有的beanNames String[] beanNames = beanFactoryToProcess.getBeanDefinitionNames(); for (String curName : beanNames) &#123; // Check that we&#x27;re not parsing our own bean definition, // to avoid failing on unresolvable placeholders in properties file locations. if (!(curName.equals(this.beanName) &amp;&amp; beanFactoryToProcess.equals(this.beanFactory))) &#123; //获取BeanDefinition对象 BeanDefinition bd = beanFactoryToProcess.getBeanDefinition(curName); try &#123; //修改BeanDefinition中的MutablePropertyValues中的每一个属性值，把属性值有$&#123;enjoy.name&#125;修改成真正的参数值 visitor.visitBeanDefinition(bd); &#125; catch (Exception ex) &#123; throw new BeanDefinitionStoreException(bd.getResourceDescription(), curName, ex.getMessage(), ex); &#125; &#125; &#125; // New in Spring 2.5: resolve placeholders in alias target names and aliases as well. beanFactoryToProcess.resolveAliases(valueResolver); //把内嵌的Value解析器设置到BeanFactory中..为@Value的依赖注入做准备 // New in Spring 3.0: resolve placeholders in embedded values such as annotation attributes. beanFactoryToProcess.addEmbeddedValueResolver(valueResolver);&#125; 看第一个if (this.environment !&#x3D; null)，这个environment对在 这 这里创建一个对象 之后执行 PropertySource&lt;?&gt; localPropertySource = new PropertiesPropertySource(LOCAL_PROPERTIES_PROPERTY_SOURCE_NAME, mergeProperties());就把项目属性设置好了 该方法核心其实就是走了这种操作 123Properties properties = new Properties();properties.load(new FileInputStream(&quot;/Volumes/vm/mq/rockermq/conf/broker.conf&quot;));System.out.println(properties); 最后调用 processProperties(beanFactory, new PropertySourcesPropertyResolver(this.propertySources)); 跟踪代码最后看核心源码： 就是遍历BeanDefinition，然后修改对应的值。 解析的逻辑在这个类中：PropertyPlaceholderHelper 感觉配置是如何解析的不是很重要，大致流程了解下即可，也就是BeanFactoryPostProcessor接口的应用。 通过配置文件实例化bean12345678910enjoy.name=jackenjoy.password=123enjoy.beanClass = $&#123;enjoy.placeHolderBean1&#125;,$&#123;enjoy.placeHolderBean2&#125;,$&#123;enjoy.placeHolderBean3&#125;enjoy.placeHolderBean1 = com.enjoy.jack.beanDefinitionPostProcessor.PlaceHolderBean1enjoy.placeHolderBean2 = com.enjoy.jack.beanDefinitionPostProcessor.PlaceHolderBean2enjoy.placeHolderBean3 = com.enjoy.jack.beanDefinitionPostProcessor.PlaceHolderBean3cn.enjoy.scan.packages=com.xiangxue.jackcn.enjoy.flag=true 比如配置有个文件这样 然后我自己写了个类： 123456789101112131415161718192021222324@Componentpublic class PropertyPlaceHolderBeanDefinition implements BeanDefinitionRegistryPostProcessor &#123; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; try &#123; Properties properties = PropertiesLoaderUtils.loadAllProperties(&quot;application.properties&quot;, ClassUtils.getDefaultClassLoader()); String property = properties.getProperty(&quot;enjoy.beanClass&quot;); String[] beanClasss = property.split(&quot;,&quot;); for (String classs : beanClasss) &#123; BeanDefinition beanDefinition = new GenericBeanDefinition(); beanDefinition.setBeanClassName(classs);// String beanName = BeanDefinitionReaderUtils.generateBeanName(beanDefinition,registry); registry.registerBeanDefinition(UUID.randomUUID().toString(),beanDefinition); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; &#125;&#125; @Value注解的依赖注入就看AbstarctAutowireCapobleBeanFactory#doCreateBean中的populateBean方法，该方法就是处理依赖注入的 到这里 跟踪代码： 1234AbstractAutowireCapableBeanFactory#populateBean ---&gt;AutowiredAnnotationBeanPostProcessor#postProcessProperties --&gt;InjectionMetadata#inject --&gt;AutowiredAnnotationBeanPostProcessor#inject @Autowire在这里会返回bean，但对于@Value不同，看代码： 在PropertySourcesPlaceholderConfigurer的源码中传入了一个匿名对象 那getAutowireCandidateResolver()这个就获取了匿名对象了。返回值后就和@Autowire一样了。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"04-Spring的Bean实例化","slug":"spring/04-Spring的Bean实例化","date":"2021-11-25T12:00:10.000Z","updated":"2022-03-23T09:03:54.612Z","comments":true,"path":"blog/spring/04-Spring的Bean实例化/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/04-Spring%E7%9A%84Bean%E5%AE%9E%E4%BE%8B%E5%8C%96/","excerpt":"","text":"实例化也是在refresh()方法中完成的。上refresh()源码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; //为容器初始化做准备，重要程度：0 // Prepare this context for refreshing. prepareRefresh(); /* 重要程度：5 1、创建BeanFactory对象 * 2、xml解析 * 传统标签解析：bean、import等 * 自定义标签解析 如：&lt;context:component-scan base-package=&quot;com.xiangxue.jack&quot;/&gt; * 自定义标签解析流程： * a、根据当前解析标签的头信息找到对应的namespaceUri * b、加载spring所有jar中的spring.handlers文件。并建立映射关系 * c、根据namespaceUri从映射关系中找到对应的实现了NamespaceHandler接口的类 * d、调用类的init方法，init方法是注册了各种自定义标签的解析类 * e、根据namespaceUri找到对应的解析类，然后调用paser方法完成标签解析 * * 3、把解析出来的xml标签封装成BeanDefinition对象 * */ // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); /* * 给beanFactory设置一些属性值，可以不看 * */ // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); /* * BeanDefinitionRegistryPostProcessor * BeanFactoryPostProcessor * 完成对这两个接口的调用 * */ // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); /* * 把实现了BeanPostProcessor接口的类实例化，并且加入到BeanFactory中 * */ // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); /* * 国际化,重要程度2 * */ // Initialize message source for this context. initMessageSource(); //初始化事件管理类 // Initialize event multicaster for this context. initApplicationEventMulticaster(); //这个方法着重理解模板设计模式，因为在springboot中，这个方法是用来做内嵌tomcat启动的 // Initialize other special beans in specific context subclasses. onRefresh(); /* * 往事件管理类中注册事件类 * */ // Check for listener beans and register them. registerListeners(); /* * 这个方法是spring中最重要的方法，没有之一 * 所以这个方法一定要理解要具体看 * 1、bean实例化过程 * 2、ioc * 3、注解支持 * 4、BeanPostProcessor的执行 * 5、Aop的入口 * */ // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset &#x27;active&#x27; flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring&#x27;s core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; Bean的实例话时在finishBeanFactoryInitialization(beanFactory);方法中完成的。 1234567891011121314protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; //省略不重要的方法 .... // Stop using the temporary ClassLoader for type matching. beanFactory.setTempClassLoader(null); // Allow for caching all bean definition metadata, not expecting further changes. beanFactory.freezeConfiguration(); //重点看这个方法，重要程度：5 // Instantiate all remaining (non-lazy-init) singletons. beanFactory.preInstantiateSingletons();&#125; 看最后的方法beanFactory.preInstantiateSingletons();（beanFactory终于不只是注册和查找了，终于有一个与它的名字相匹配的操作了）。方法翻译是实例化单例，那看看是怎么实例化Bean的吧。 DefaultListableBeanFactory#preInstantiateSingletons123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* * 具体实例化过程 * */@Overridepublic void preInstantiateSingletons() throws BeansException &#123; // Iterate over a copy to allow for init methods which in turn register new bean definitions. // While this may not be part of the regular factory bootstrap, it does otherwise work fine. // 把所有beanName都缓存到beanDefinitionNames了，这时这个BeanName所对应的是BeanDefinition List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames); // Trigger initialization of all non-lazy singleton beans... for (String beanName : beanNames) &#123; //把父BeanDefinition里面的属性拿到子BeanDefinition中 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); //如果不是抽象的，单例的，非懒加载的就实例化 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; //判断bean是否实现了FactoryBean接口，这里可以不看 &amp;factoryBeanDemo if (isFactoryBean(beanName)) &#123; Object bean = getBean(FACTORY_BEAN_PREFIX + beanName); if (bean instanceof FactoryBean) &#123; FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) bean; boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123; isEagerInit = AccessController.doPrivileged( (PrivilegedAction&lt;Boolean&gt;) ((SmartFactoryBean&lt;?&gt;) factory)::isEagerInit, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; if (isEagerInit) &#123; getBean(beanName); &#125; &#125; &#125; else &#123; //主要从这里进入，看看实例化过程 getBean(beanName); &#125; &#125; &#125; // Trigger post-initialization callback for all applicable beans... for (String beanName : beanNames) &#123; Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) &#123; SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; &#123; smartSingleton.afterSingletonsInstantiated(); return null; &#125;, getAccessControlContext()); &#125; else &#123; smartSingleton.afterSingletonsInstantiated(); &#125; &#125; &#125;&#125; 第一步就是 这个beanDefinitionNames就是BeanDefinition放入beanFactory的Map中后，还会把beanName放入到到beanFactory的一个list列表中，这个列表就是beanDefinitionNames。 然后就是遍历这个列表了。在for循环中第一行代码 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName);，它是把父BeanDefinition里面的属性拿到子BeanDefinition中，这是一个深度拷贝的过程，而且会返回一个RootBeanDefinition，表示该BeanDefinition需要实例化了 比如是这样定义 比如在多数据源时，有一个父的bean保存一个共有的信息，而一些独有的信息就放在自己的bean中，那这时是要这样定义就好了。 继续看代码，如果不是抽象的，是单例的，非懒加载的就实例化，那会进入到if中， 这里主要看else吧，if的话是对那些实现FactoryBean接口的类实例化的，这里之后再看。 AbstractBeanFactory#getBeangetBean方法是由父类AbstractBeanFactory实现，跟踪代码，最后看AbstractBeanFactory#doGetBean 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165// AbstractBeanFactoryprotected &lt;T&gt; T doGetBean( String name, @Nullable Class&lt;T&gt; requiredType, @Nullable Object[] args, boolean typeCheckOnly) throws BeansException &#123; // 这里会解析出真实的beanName，对于实现了FactoryBean的接口，在进入这个方法时会在beanName前加上&amp;这个符号 String beanName = transformedBeanName(name); Object bean; //从缓存中拿实例 // Eagerly check singleton cache for manually registered singletons. Object sharedInstance = getSingleton(beanName); //如果缓存里面能拿到实例 if (sharedInstance != null &amp;&amp; args == null) &#123; //该方法是FactoryBean接口的调用入口 bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); &#125; else &#123; //如果singletonObjects缓存里面没有，则走下来 // Fail if we&#x27;re already creating this bean instance: // We&#x27;re assumably within a circular reference. //如果是scope 是Prototype的，校验是否有出现循环依赖，如果有则直接报错 if (isPrototypeCurrentlyInCreation(beanName)) &#123; throw new BeanCurrentlyInCreationException(beanName); &#125; // Check if bean definition exists in this factory. BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; // Not found -&gt; check parent. String nameToLookup = originalBeanName(name); if (parentBeanFactory instanceof AbstractBeanFactory) &#123; return ((AbstractBeanFactory) parentBeanFactory).doGetBean( nameToLookup, requiredType, args, typeCheckOnly); &#125; else if (args != null) &#123; // Delegation to parent with explicit args. return (T) parentBeanFactory.getBean(nameToLookup, args); &#125; else if (requiredType != null) &#123; // No args -&gt; delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); &#125; else &#123; return (T) parentBeanFactory.getBean(nameToLookup); &#125; &#125; if (!typeCheckOnly) &#123; markBeanAsCreated(beanName); &#125; try &#123; //父子BeanDefinition合并 RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); //获取依赖对象属性，依赖对象要先实例化 // Guarantee initialization of beans that the current bean depends on. String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) &#123; for (String dep : dependsOn) &#123; if (isDependent(beanName, dep)) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Circular depends-on relationship between &#x27;&quot; + beanName + &quot;&#x27; and &#x27;&quot; + dep + &quot;&#x27;&quot;); &#125; registerDependentBean(dep, beanName); try &#123; //实例化 getBean(dep); &#125; catch (NoSuchBeanDefinitionException ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;&#x27;&quot; + beanName + &quot;&#x27; depends on missing bean &#x27;&quot; + dep + &quot;&#x27;&quot;, ex); &#125; &#125; &#125; //着重看，大部分是单例的情况 // Create bean instance. if (mbd.isSingleton()) &#123; sharedInstance = getSingleton(beanName, () -&gt; &#123; try &#123; return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; // Explicitly remove instance from singleton cache: It might have been put there // eagerly by the creation process, to allow for circular reference resolution. // Also remove any beans that received a temporary reference to the bean. destroySingleton(beanName); throw ex; &#125; &#125;); //该方法是FactoryBean接口的调用入口 bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); &#125; else if (mbd.isPrototype()) &#123; // It&#x27;s a prototype -&gt; create a new instance. Object prototypeInstance = null; try &#123; beforePrototypeCreation(beanName); prototypeInstance = createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; //该方法是FactoryBean接口的调用入口 bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); &#125; else &#123; String scopeName = mbd.getScope(); if (!StringUtils.hasLength(scopeName)) &#123; throw new IllegalStateException(&quot;No scope name defined for bean ´&quot; + beanName + &quot;&#x27;&quot;); &#125; Scope scope = this.scopes.get(scopeName); if (scope == null) &#123; throw new IllegalStateException(&quot;No Scope registered for scope name &#x27;&quot; + scopeName + &quot;&#x27;&quot;); &#125; try &#123; Object scopedInstance = scope.get(beanName, () -&gt; &#123; beforePrototypeCreation(beanName); try &#123; return createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; &#125;); bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); &#125; catch (IllegalStateException ex) &#123; throw new BeanCreationException(beanName, &quot;Scope &#x27;&quot; + scopeName + &quot;&#x27; is not active for the current thread; consider &quot; + &quot;defining a scoped proxy for this bean if you intend to refer to it from a singleton&quot;, ex); &#125; &#125; &#125; catch (BeansException ex) &#123; cleanupAfterBeanCreationFailure(beanName); throw ex; &#125; &#125; // Check if required type matches the type of the actual bean instance. if (requiredType != null &amp;&amp; !requiredType.isInstance(bean)) &#123; try &#123; T convertedBean = getTypeConverter().convertIfNecessary(bean, requiredType); if (convertedBean == null) &#123; throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); &#125; return convertedBean; &#125; catch (TypeMismatchException ex) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Failed to convert bean &#x27;&quot; + name + &quot;&#x27; to required type &#x27;&quot; + ClassUtils.getQualifiedName(requiredType) + &quot;&#x27;&quot;, ex); &#125; throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); &#125; &#125; return (T) bean;&#125; 先看 1String beanName = transformedBeanName(name); 这里会解析出真实的beanName，对于实现了FactoryBean的接口，在进入这个方法时会在beanName前加上&amp;这个符号。经过这个方法后，会去掉&amp;符号，所以进入了方法后对于FactoryBean的接口的BeanDefinition，会有两个名字，一个是 12name=&amp;beanNamebeanName=beanName 接着看 1Object sharedInstance = getSingleton(beanName); getSingleton方法是从DefaultSingletonBeanRegistry继承过来的，该方法会根据beanName从缓存中拿实例，进去看看getSingleton： 1234567891011121314151617181920212223242526protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; //根据beanName从缓存中拿实例 //先从一级缓存拿 Object singletonObject = this.singletonObjects.get(beanName); //如果bean还正在创建，还没创建完成，其实就是堆内存有了，属性还没有DI依赖注入 if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; //从二级缓存中拿 singletonObject = this.earlySingletonObjects.get(beanName); //如果还拿不到，并且允许bean提前暴露 if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; //从三级缓存中拿到对象工厂 ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; //从工厂中拿到对象 singletonObject = singletonFactory.getObject(); //升级到二级缓存 this.earlySingletonObjects.put(beanName, singletonObject); //删除三级缓存 this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return singletonObject;&#125; 就是从缓存一级一级找。回到开始的，看没拿到的情况，也就是else代码块的代码。 通过父BeanFactory获取bean 可以看到，这段代码先通过getParentBeanFactory()父BeanFactory，如果存在，并且当前的BeanFactory不存在对应beanName的BeanDefinition，那么就会尝试使用父BeanFactory获取Bean。 DefaultSingletonBeanRegistry#getSingleton先看下面的代码段： RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName);上面讲过了，看下面处理dependsOn的 比如这样定义一个bean 那这个student的实例化是要依赖james13的，也就是说james13要先实例话，student才能实例话。回到源码，逻辑就是上面的逻辑，也就是先去getBean(‘james13’)，然后在往下实例化student 接着看代码块： 从if就知道，这代码块就是创建单例的Bean，也就是默认的情况。 getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory)方法又是从DefaultSingletonBeanRegistry继承过来的 看DefaultSingletonBeanRegistry#getSingleton方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(beanName, &quot;Bean name must not be null&quot;); synchronized (this.singletonObjects) &#123; //如果缓存中有，则直接返回 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; //打印日志 ...... //把beanName添加到singletonsCurrentlyInCreation Set容器中，在这个集合里面的bean都是正在实例化的bean beforeSingletonCreation(beanName); boolean newSingleton = false; boolean recordSuppressedExceptions = (this.suppressedExceptions == null); if (recordSuppressedExceptions) &#123; this.suppressedExceptions = new LinkedHashSet&lt;&gt;(); &#125; try &#123; //如果这里有返回值，就代表这个bean已经结束创建了，已经完全创建成功 singletonObject = singletonFactory.getObject(); newSingleton = true; &#125; catch (IllegalStateException ex) &#123; // Has the singleton object implicitly appeared in the meantime -&gt; // if yes, proceed with it since the exception indicates that state. singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; throw ex; &#125; &#125; catch (BeanCreationException ex) &#123; if (recordSuppressedExceptions) &#123; for (Exception suppressedException : this.suppressedExceptions) &#123; ex.addRelatedCause(suppressedException); &#125; &#125; throw ex; &#125; finally &#123; if (recordSuppressedExceptions) &#123; this.suppressedExceptions = null; &#125; //bean创建完成后singletonsCurrentlyInCreation要删除该bean afterSingletonCreation(beanName); &#125; if (newSingleton) &#123; //创建对象成功时，把对象缓存到singletonObjects缓存中,bean创建完成时放入一级缓存 addSingleton(beanName, singletonObject); &#125; &#125; return singletonObject; &#125;&#125; 注这里的 123456789ObjectFactory&lt;?&gt; singletonFactory = &#123; try &#123; return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; ..... throw ex; &#125;&#125; 先从一级缓singletonObjects集合中存拿对象，看没拿到的情况就会进行创建逻辑 beforeSingletonCreation(beanName);，该方法的作用就是把beanName添加到singletonsCurrentlyInCreation容器中，在这个集合里面的bean都是正在实例化的bean。 接着到 这个singletonFactory是在进入方法时传的一个匿名对象 也就是说，调用getObject时，实际上时调用了createBean(beanName, mbd, args)方法。注意 getObject调用完成后，就意味着bean已经完成了创建 上面的createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args)方法是在AbstractBeanFactory中定义的，它是一个抽象方法，这个方法的实现由AbstractAutowireCapableBeanFactory实现。 看AbstractAutowireCapableBeanFactory#createBean 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051protected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException &#123; RootBeanDefinition mbdToUse = mbd; // Make sure bean class is actually resolved at this point, and // clone the bean definition in case of a dynamically resolved Class // which cannot be stored in the shared merged bean definition. Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName); if (resolvedClass != null &amp;&amp; !mbd.hasBeanClass() &amp;&amp; mbd.getBeanClassName() != null) &#123; mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); &#125; // Prepare method overrides. try &#123; mbdToUse.prepareMethodOverrides(); &#125; //去掉了catch语句块 ....... try &#123; /* * TargetSource接口的运用，可以在用改一个类实现该接口，然后在里面定义实例化对象的方式，然后返回 * 也就是说不需要spring帮助我们实例化对象 * * * 这里可以直接返回实例本身 * * 这个代码不用看，实际开发过程中用不到，我会做为一个甜点分享 * */ // Give BeanPostProcessors a chance to return a proxy instead of the target bean instance. Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) &#123; return bean; &#125; &#125; //去掉了catch语句块 ....... try &#123; //主要看这个方法，重要程度 5 Object beanInstance = doCreateBean(beanName, mbdToUse, args); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Finished creating instance of bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; return beanInstance; &#125; //去掉了catch语句块 .......&#125; 重点看这段代码块： AbstractAutowireCapableBeanFactory#doCreateBean1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798protected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException &#123; // Instantiate the bean. BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) &#123; instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); &#125; if (instanceWrapper == null) &#123; //创建实例,,重点看，重要程度：5 instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; Object bean = instanceWrapper.getWrappedInstance(); Class&lt;?&gt; beanType = instanceWrapper.getWrappedClass(); if (beanType != NullBean.class) &#123; mbd.resolvedTargetType = beanType; &#125; // Allow post-processors to modify the merged bean definition. synchronized (mbd.postProcessingLock) &#123; if (!mbd.postProcessed) &#123; try &#123; //CommonAnnotationBeanPostProcessor 支持了@PostConstruct，@PreDestroy,@Resource注解 //AutowiredAnnotationBeanPostProcessor 支持 @Autowired,@Value注解 //BeanPostProcessor接口的典型运用，这里要理解这个接口 //对类中注解的装配过程 //重要程度5，必须看 applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); &#125; catch (Throwable ex) &#123; //打印日志 ....... &#125; mbd.postProcessed = true; &#125; &#125; // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. //是否 单例bean提前暴露 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) &#123; if (logger.isTraceEnabled()) &#123; //打印日志 ...... &#125; //这里着重理解，对理解循环依赖帮助非常大，重要程度 5 添加三级缓存 addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); &#125; // Initialize the bean instance. Object exposedObject = bean; try &#123; //ioc di，依赖注入的核心方法，该方法必须看，重要程度：5 populateBean(beanName, mbd, instanceWrapper); //bean 实例化+ioc依赖注入完以后的调用，非常重要，重要程度：5 exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; catch (Throwable ex) &#123; //抛出BeanCreationException错误 ....... &#125; if (earlySingletonExposure) &#123; Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) &#123; if (exposedObject == bean) &#123; exposedObject = earlySingletonReference; &#125; else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) &#123; String[] dependentBeans = getDependentBeans(beanName); Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;&gt;(dependentBeans.length); for (String dependentBean : dependentBeans) &#123; if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) &#123; actualDependentBeans.add(dependentBean); &#125; &#125; if (!actualDependentBeans.isEmpty()) &#123; //抛出BeanCurrentlyInCreationException错误 ....... &#125; &#125; &#125; &#125; // Register bean as disposable. try &#123; //注册bean销毁时的类DisposableBeanAdapter registerDisposableBeanIfNecessary(beanName, bean, mbd); &#125; catch (BeanDefinitionValidationException ex) &#123; //抛出BeanCreationException错误 ....... &#125; return exposedObject;&#125; 这是一个非常非常重要的方法，这里包含一个Spring bean的整个初始化流程，重要的方法 BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) void applyMergedBeanDefinitionPostProcessors(RootBeanDefinition mbd, Class&lt;?&gt; beanType, String beanName) void addSingletonFactory(String beanName, ObjectFactory&lt;?&gt; singletonFactory) void populateBean(String beanName, RootBeanDefinition mbd, @Nullable BeanWrapper bw) Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) void registerDisposableBeanIfNecessary(String beanName, Object bean, RootBeanDefinition mbd) 下面我们一个个方法 createBeanInstance—-Bean 的实例化过程（控制反转IOC）先看这段代码，也就是开头的代码 真正是实例就是在这段代码里完成的。 先总结这个方法的作用： 源码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) &#123; // Make sure bean class is actually resolved at this point. Class&lt;?&gt; beanClass = resolveBeanClass(mbd, beanName); if (beanClass != null &amp;&amp; !Modifier.isPublic(beanClass.getModifiers()) &amp;&amp; !mbd.isNonPublicAccessAllowed()) &#123; //抛出BeanCreationException错误 ....... &#125; Supplier&lt;?&gt; instanceSupplier = mbd.getInstanceSupplier(); if (instanceSupplier != null) &#123; return obtainFromSupplier(instanceSupplier, beanName); &#125; //如果有FactoryMethodName属性 @Bean if (mbd.getFactoryMethodName() != null) &#123; return instantiateUsingFactoryMethod(beanName, mbd, args); &#125; // Shortcut when re-creating the same bean... boolean resolved = false; boolean autowireNecessary = false; if (args == null) &#123; synchronized (mbd.constructorArgumentLock) &#123; if (mbd.resolvedConstructorOrFactoryMethod != null) &#123; resolved = true; autowireNecessary = mbd.constructorArgumentsResolved; &#125; &#125; &#125; if (resolved) &#123; if (autowireNecessary) &#123; return autowireConstructor(beanName, mbd, null, null); &#125; else &#123; return instantiateBean(beanName, mbd); &#125; &#125; // Candidate constructors for autowiring? //寻找当前正在实例化的bean中有@Autowired注解的构造函数 Constructor&lt;?&gt;[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); if (ctors != null || mbd.getResolvedAutowireMode() == AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) &#123; //如果ctors不为空，就说明构造函数上有@Autowired注解 return autowireConstructor(beanName, mbd, ctors, args); &#125; // Preferred constructors for default construction? ctors = mbd.getPreferredConstructors(); if (ctors != null) &#123; return autowireConstructor(beanName, mbd, ctors, null); &#125; //无参构造函数的实例化,大部分的实例是采用的无参构造函数的方式实例化 // No special handling: simply use no-arg constructor. return instantiateBean(beanName, mbd);&#125; &lt;bean&gt;标签里面。配置了factory-method属性的情况先执行这里： 这段代码的if只有在下面的情况才会进去。 标签里面。配置了factory-method属性 或者方法上面加上@Bean注解 看下AbstractAutowireCapableBeanFactory#instantiateUsingFactoryMethod方法。 ConstructorResolver#instantiateUsingFactoryMethod方法:(不弄完整的代码了，只看核心的) 这样配置的情况 创建好了BeanWrapperImpl包装类后，调用beanFactory.getBean(factoryBeanName)，也就是说先去获取factoryBean(没有就执行创建流程)，获取到了方法所对应的对象，并且isStatic &#x3D; false。接着走 从所有factoryBean对应的类中拿到全部方法，并找到匹配的方法，而且由于isStatic &#x3D; false，所以不能拿到静态的方法。接着走 instantiate方法不用看了，就是用反射调用这种形式的api：Method.invoke(Object object, Object... args) 方法调用完后就按定义实力化bean了，并返回了。接着会把放回的bean放入到之前创建的BeanWrapperImpl的对象中。后面还有多个方法，不过我不想看了，最后还是只有一个方法会被调用。 不过除了上面的情况还，还可以这样配置 不过这就要求方法factoryMethod是静态的方法了。这种情况就会走到一开始的else情况，后面都一样 factory-method的处理结束了。 方法上面加上@Bean注解@Bean的方式是使用了factory-method的方式来实现的，它使用的是非静态方法的那种方式。 比如有这样的配置类 其实就是相当于 实例化带有@Autowired注解的构造函数回到AbstractAutowireCapableBeanFactory#createBeanInstance，代码一直走 这个方法的作用就是通过构造方法注入值，比如有这样一个类 那Spring会把sc对象，和cq对象传入AutowiredConsturctorBean对象中。看回源码。 代码determineConstructorsFromBeanPostProcessors getBeanPostProcessors()回获取BeanPostProcessor的实例，这些实例是在refresh()方法中，执行了BeanDefinitionRegistryPostProcessor这种后置处理器的方法后，调用了registryBeanPostProcessor方法去实例化对应的BeanPostProcessor的实例，并且注册到beanFacotry的beanPostProcessors中 回到for代码。这里会有多个BeanPostProcessor会被调用到，不过我们关注的是这个 AutowireAnnotationBeanPostProcessor，进去看determineCandidateConstructors方法 代码接着走 遍历构造函数，获取到构造函数上的@Autowired注解信息，接着 获取@Autowired的required的值，默认为true。这个值的作用就是为true时，如果注入的对象不存在就报错，为false的时候，如果如果注入的对象不存在，不报错(对应引用，还是使用默认值null)。 最后 放入candidateConstructorsCache后就返回了 回到AbstractAutowireCapableBeanFactory#createBeanInstance，假如返回的不为null，那看方法 autowireConstructor(beanName, mbd, ctors, args): 看ConstructorResolver#autowireConstructor方法 该方法其实是更具构造方法创建对象的过程，不过有个问题，就是构造方法的参数怎么获取。其实我猜测和factoryMethod一样。跟踪源码一直到 这方法的流程太长了，代码不看了，最后都会调用beanFactory.getBean方法。 有一种情况，多个构造函数上有@Autowire，默认情况下会报错，因为源码中 回到AutowireAnnotationBeanPostProcessor#determineConstructorsFromBeanPostProcessors方法，在 只要required&#x3D;false就不报错了。如果有多个构造函数，会根据构造函数参数个数排序，而且只会用一个构造函数。 实例化没有@Autowired的有参构造函数AutowireAnnotationBeanPostProcessor#determineConstructorsFromBeanPostProcessors方法在没有@Autowired是会根据情况返回构造函数的。之后都是调用autowireConstructor方法的。不过对于这种情况要注意一点，如果想这种情况走通，一定只能有一个带参数的构造函数。 实例化无参构造函数上面的情况都不符合的才会走到这里 总结其实就是如果有依赖别的bean的，先去创建别的bean。而且这里只是分配的内存空间，对象的属性还没有处理，也就是还没到依赖注入这阶段。 applyMergedBeanDefinitionPostProcessors—对类中注解的收集和装配过程createBeanInstance方法只是实例化对象(控制反转)，并没有处理依赖注入的问题。而applyMergedBeanDefinitionPostProcessors会对类中注解执行收集过程 又是getBeanPostProcessors()，在这里Spring会帮我们放入一些BeanPostProcessor，这里还有个要求，就是要实现的是MergedBeanDefinitionPostProcessor这个接口，看会01-Spring中BeanDefinition的创建和注册最后的3张类图，我们关注的3个BeanPostProcessor只有两个是实现这个接口的。这两个就是 CommonAnnotationBeanPostProcessor AutowiredAnnotationBeanPostProcessor 下面看这个两个类的postProcessMergedBeanDefinition方法 AutowiredAnnotationBeanPostProcessor#postProcessMergedBeanDefinition跟踪代码 123456789101112131415161718192021222324252627public void postProcessMergedBeanDefinition(RootBeanDefinition beanDefinition, Class&lt;?&gt; beanType, String beanName) &#123; InjectionMetadata metadata = findAutowiringMetadata(beanName, beanType, null); metadata.checkConfigMembers(beanDefinition);&#125;private InjectionMetadata findAutowiringMetadata(String beanName, Class&lt;?&gt; clazz, @Nullable PropertyValues pvs) &#123; // Fall back to class name as cache key, for backwards compatibility with custom callers. String cacheKey = (StringUtils.hasLength(beanName) ? beanName : clazz.getName()); // Quick check on the concurrent map first, with minimal locking. InjectionMetadata metadata = this.injectionMetadataCache.get(cacheKey); if (InjectionMetadata.needsRefresh(metadata, clazz)) &#123; synchronized (this.injectionMetadataCache) &#123; metadata = this.injectionMetadataCache.get(cacheKey); if (InjectionMetadata.needsRefresh(metadata, clazz)) &#123; if (metadata != null) &#123; metadata.clear(pvs); &#125; //主要看这个方法 metadata = buildAutowiringMetadata(clazz); this.injectionMetadataCache.put(cacheKey, metadata); &#125; &#125; &#125; return metadata;&#125; ---&gt;AutowiredAnnotationBeanPostProcessor#buildAutowiringMetadata findAutowiringMetadata的作用就是缓存和从缓存中拿InjectionMetadata对象，其核心就是调用了buildAutowiringMetadata。 看AutowiredAnnotationBeanPostProcessor#buildAutowiringMetadata 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private InjectionMetadata buildAutowiringMetadata(final Class&lt;?&gt; clazz) &#123; if (!AnnotationUtils.isCandidateClass(clazz, this.autowiredAnnotationTypes)) &#123; return InjectionMetadata.EMPTY; &#125; List&lt;InjectionMetadata.InjectedElement&gt; elements = new ArrayList&lt;&gt;(); Class&lt;?&gt; targetClass = clazz; do &#123; final List&lt;InjectionMetadata.InjectedElement&gt; currElements = new ArrayList&lt;&gt;(); //寻找field上面的@Autowired注解并封装成对象 ReflectionUtils.doWithLocalFields(targetClass, field -&gt; &#123; MergedAnnotation&lt;?&gt; ann = findAutowiredAnnotation(field); if (ann != null) &#123; if (Modifier.isStatic(field.getModifiers())) &#123; //去掉一些打印日志的 ..... return; &#125; boolean required = determineRequiredStatus(ann); currElements.add(new AutowiredFieldElement(field, required)); &#125; &#125;); //寻找Method上面的@Autowired注解并封装成对象 ReflectionUtils.doWithLocalMethods(targetClass, method -&gt; &#123; Method bridgedMethod = BridgeMethodResolver.findBridgedMethod(method); if (!BridgeMethodResolver.isVisibilityBridgeMethodPair(method, bridgedMethod)) &#123; return; &#125; MergedAnnotation&lt;?&gt; ann = findAutowiredAnnotation(bridgedMethod); if (ann != null &amp;&amp; method.equals(ClassUtils.getMostSpecificMethod(method, clazz))) &#123; //去掉一些打印日志的 ..... boolean required = determineRequiredStatus(ann); PropertyDescriptor pd = BeanUtils.findPropertyForMethod(bridgedMethod, clazz); currElements.add(new AutowiredMethodElement(method, required, pd)); &#125; &#125;); elements.addAll(0, currElements); targetClass = targetClass.getSuperclass(); &#125; while (targetClass != null &amp;&amp; targetClass != Object.class); return InjectionMetadata.forElements(elements, clazz);&#125; 这里就是收集这个类上，有某些注解的属性，而判断的的方法就是在findAutowiredAnnotation 123456789101112131415161718192021222324public AutowiredAnnotationBeanPostProcessor() &#123; this.autowiredAnnotationTypes.add(Autowired.class); this.autowiredAnnotationTypes.add(Value.class); try &#123; this.autowiredAnnotationTypes.add((Class&lt;? extends Annotation&gt;) ClassUtils.forName(&quot;javax.inject.Inject&quot;, AutowiredAnnotationBeanPostProcessor.class.getClassLoader())); logger.trace(&quot;JSR-330 &#x27;javax.inject.Inject&#x27; annotation found and supported for autowiring&quot;); &#125; catch (ClassNotFoundException ex) &#123; // JSR-330 API not available - simply skip. &#125;&#125;@Nullableprivate MergedAnnotation&lt;?&gt; findAutowiredAnnotation(AccessibleObject ao) &#123; MergedAnnotations annotations = MergedAnnotations.from(ao); for (Class&lt;? extends Annotation&gt; type : this.autowiredAnnotationTypes) &#123; MergedAnnotation&lt;?&gt; annotation = annotations.get(type); if (annotation.isPresent()) &#123; return annotation; &#125; &#125; return null;&#125; 可以看到，这里有3个注解。其中就有@Autowired和@Value 而且，会把有这些注解的Field（属性）和 required（是否必须，默认为true）值包装成一个AutowiredFieldElement对象。 后面一个doWithLocalMethods是找方法上有@Autowire的，把必要信息封装成AutowiredMethodElement： 也就是说，只要有@Value和@Autowire的注解的方法或属性，就会创建一个AutowiredMethodElement对象，然后放到一个集合中。 最后，将上边创建的AutowiredMethodElement对象集合封装成InjectionMetadata并返回： 该方法返回后，会把InjectionMetadata对象放到一个injectionMetadataCache缓存中。 CommonAnnotationBeanPostProcessor#postProcessMergedBeanDefinitionCommonAnnotationBeanPostProcessor的过程和上面基本一样，就是关注的注解不同。 不过这个类实际上只是处理了@Resource注解而已，真实的处理@PostConstruct和@PreDestroy注解的是在其父类InitDestroyAnnotationBeanPostProcessor上处理的。 12345public void postProcessMergedBeanDefinition(RootBeanDefinition beanDefinition, Class&lt;?&gt; beanType, String beanName) &#123; super.postProcessMergedBeanDefinition(beanDefinition, beanType, beanName); InjectionMetadata metadata = findResourceMetadata(beanName, beanType, null); metadata.checkConfigMembers(beanDefinition);&#125; 不过处理的逻辑都是一样，@PostConstruct和@PreDestroy只能在方法上。而@Resource能在属性和方法上。不过，对应注解封装成的对象不同而已，对于@PostConstruct和@PreDestroy的方法封装成LifecycleElement，对于@Resource，会封装成ResourceElement。 CommonAnnotationBeanPostProcessor其实还有其他注解的 WebServiceRef EJB 不过这些用都没用过，就不管了 总结这个方法就是去收集注解的 12CommonAnnotationBeanPostProcessor 收集了@PostConstruct，@PreDestroy,@Resource注解AutowiredAnnotationBeanPostProcessor 收集了 @Autowired,@Value注解 addSingletonFactory—对理解循环依赖帮助非常大 这里着重理解，对理解循环依赖帮助非常大，重要程度 5 添加三级缓存 这个方法在doCreateBean方法中的代码段是: 执行的前提是这个Bean的作用域是Singleton、beanFactory的allowCircularReferences为true（也就是开启循环依赖，默认开启）最后要isSingletonCurrentlyInCreation(beanName)。 这个isSingletonCurrentlyInCreation(beanName)是什么意识呢？DefaultSingletonBeanRegistry#getSingleton代码的（也就是doCreateBean的外层方法），调用了下面代码 1beforeSingletonCreation(beanName); 把beanName添加到singletonsCurrentlyInCreation Set容器中，在这个集合里面的bean都是正在实例化的bean 而isSingletonCurrentlyInCreation(beanName)就是判断这个beanName是否在singletonsCurrentlyInCreation Set容器的，也就是判断是否在创建中。正常情况会这个方法会返回true 条件都满足了就调用代码： 1addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); 这里先记下这个ObjectFactory&lt;?&gt; singletonFactory = () -&gt; getEarlyBeanReference(beanName, mbd, bean)对象记下，看addSingletonFactory源码: 12345678910111213protected void addSingletonFactory(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(singletonFactory, &quot;Singleton factory must not be null&quot;); synchronized (this.singletonObjects) &#123; //如果一级缓存不存在 if (!this.singletonObjects.containsKey(beanName)) &#123; //设置三级缓存 this.singletonFactories.put(beanName, singletonFactory); //删除二级缓存 this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); &#125; &#125;&#125; 代码很简单，重要的一个作用就是设置三级缓存。 首先singletonObjects如果存在，就表示这个对象已经创建完成了，没有的话就会去设置三级缓存，并删除二级缓存。 看下三级缓存的定义 也就是说设置三级缓存的意义就是建立beanName和这个对象——() -&gt; getEarlyBeanReference(beanName, mbd, bean)的映射关系。 这个方法的作用就是这个，没别的了。 populateBean—依赖注入（DI）的核心方法依赖注入的核心方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374protected void populateBean(String beanName, RootBeanDefinition mbd, @Nullable BeanWrapper bw) &#123; //去掉不重要的方法 .... //这里很有意思，写接口可以让所有类都不能依赖注入 if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; return; &#125; &#125; &#125; &#125; PropertyValues pvs = (mbd.hasPropertyValues() ? mbd.getPropertyValues() : null); int resolvedAutowireMode = mbd.getResolvedAutowireMode(); if (resolvedAutowireMode == AUTOWIRE_BY_NAME || resolvedAutowireMode == AUTOWIRE_BY_TYPE) &#123; MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // Add property values based on autowire by name if applicable. if (resolvedAutowireMode == AUTOWIRE_BY_NAME) &#123; autowireByName(beanName, mbd, bw, newPvs); &#125; // Add property values based on autowire by type if applicable. if (resolvedAutowireMode == AUTOWIRE_BY_TYPE) &#123; autowireByType(beanName, mbd, bw, newPvs); &#125; pvs = newPvs; &#125; boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); boolean needsDepCheck = (mbd.getDependencyCheck() != AbstractBeanDefinition.DEPENDENCY_CHECK_NONE); PropertyDescriptor[] filteredPds = null; //重点看这个if代码块，重要程度 5 if (hasInstAwareBpps) &#123; if (pvs == null) &#123; pvs = mbd.getPropertyValues(); &#125; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; //依赖注入过程，@Autowired的支持 PropertyValues pvsToUse = ibp.postProcessProperties(pvs, bw.getWrappedInstance(), beanName); if (pvsToUse == null) &#123; if (filteredPds == null) &#123; filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); &#125; //老版本用这个完成依赖注入过程，@Autowired的支持 pvsToUse = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvsToUse == null) &#123; return; &#125; &#125; pvs = pvsToUse; &#125; &#125; &#125; if (needsDepCheck) &#123; if (filteredPds == null) &#123; filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); &#125; checkDependencies(beanName, mbd, filteredPds, pvs); &#125; //这个方法很鸡肋了，建议不看，是老版本用&lt;property name=&quot;username&quot; value=&quot;Jack&quot;/&gt; //标签做依赖注入的代码实现，复杂且无用 if (pvs != null) &#123; applyPropertyValues(beanName, mbd, bw, pvs); &#125;&#125; 这里我们按功能来分析代码，先看下面代码块。 这里又是BeanPostProcessor接口的应用。 这里我们可以自己写个类实现InstantiationAwareBeanPostProcessor接口，postProcessAfterInstantiation方法返回一个false，就可以不走依赖注入了。比如弄个有条件的依赖注入，类似这样 感觉这个扩展没什么用，只是用来坑人用的。接着后面的代码 又是BeanPostProcessor接口的应用，虽然实现InstantiationAwareBeanPostProcessor的类比较多，不过我们还是只关注两个 AutowiredAnnotationBeanPostProcessor#postProcessProperties 这个方法之前已经说过了。 之前在applyMergedBeanDefinitionPostProcessors已经收集好了注解了，而且，做了一个beanName和InjectionMetadata的映射。所以这里只需要用beanName就可以获取到InjectionMetadata对象了，这个对象就包含了beanName中，对应的注解信息（@Autowite，@Value等）。之后执行metadata.inject，这个方法就是依赖注入的核心。 先看AutowiredFieldElement#inject 重点是这个方法，这个方法会触发beanFactory.getBean方法的，也就是说如果A-&gt;B-&gt;C。 A在这个方法中会触发beanFactory.getBean(B)。接着B又在这个方法中会触发beanFactory.getBean(C) 最后调用 第一个就是调用field.setAccessible(true);，然后通过反射向字段设置值，也就是说依赖的属性现在经过这样的处理后就完成了值的设置了。 DI已经完成了，原理是真的简单，重点只是在收集注解上而已。 方法的依赖注入都一样的。 CommonAnnotationBeanPostProcessor#postProcessProperties对于@Resource注解，和上面都是一样的。 引用循环依赖的解决过程在AutowiredFieldElement#inject的 这里会触发依赖对象的beanFactory.getBean，上面说的情况是没有循环依赖的。 不过如果有这样的一种依赖情况—— A-&gt;B-&gt;A，这种也就是循环依赖的情况。 依赖情况—— A-&gt;B-&gt;A 一开始调用了A的doGetBean方法，接着又触发了B的doGetBean，由于循环依赖的关系，B的doGetBean又触发了A的doGetBean。这个流程看上去是一个死循环。但是如果在最后的A的doGetBean时，直接返回A的对象，而不去触发创建操作那当B拿到A对象，方法就返回了，并完成了Spring Bean的初始化流程，那么当B的doGetBean返回后，A也就会继续走其他流程了。下面看Spring源码是怎么实现这一步骤的。 我建立如下的调用顺序： A的doGetBean B的doGetBean B的doGetBean又触发了A的doGetBean 由于1在执行populateBean方法之前已经创建A这个对象，而且还会调用方法addSingletonFactory，建立了Map&lt;A,() -&gt; getEarlyBeanReference(beanName, mbd, bean)&gt;这种映射关系。2步骤没问题，看3步骤： A的doGetBean时，会调用 12//从缓存中拿实例Object sharedInstance = getSingleton(beanName); 前面简单的讲过这个方法，现在看回这个方法的源码。 123456789101112131415161718192021222324252627282930public Object getSingleton(String beanName) &#123; return getSingleton(beanName, true);&#125;protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; //根据beanName从缓存中拿实例 //先从一级缓存拿 Object singletonObject = this.singletonObjects.get(beanName); //如果bean还正在创建，还没创建完成，其实就是堆内存有了，属性还没有DI依赖注入 if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; //从二级缓存中拿 singletonObject = this.earlySingletonObjects.get(beanName); //如果还拿不到，并且允许bean提前暴露 if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; //从三级缓存中拿到对象工厂 ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; //从工厂中拿到对象 singletonObject = singletonFactory.getObject(); //升级到二级缓存 this.earlySingletonObjects.put(beanName, singletonObject); //删除三级缓存 this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return singletonObject;&#125; 由于在1步骤中，A已经在三级缓存有这样一个对象了——() -&gt; getEarlyBeanReference(beanName, mbd, bean)。 注意，这里的bean的对象就是目标对象，也就是A对象。 也就是说，到这一步时 1ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); 得到的结果是singletonFactory &#x3D; () -&gt; getEarlyBeanReference(beanName, mbd, bean) 接着就执行了 1singletonObject = singletonFactory.getObject(); 也就是说，这里实际执行的getEarlyBeanReference(beanName, mbd, bean)方法，这个方法是在类AbstractAutowireCapableBeanFactory中的。看这个方法 123456789101112protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) &#123; Object exposedObject = bean; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof SmartInstantiationAwareBeanPostProcessor) &#123; SmartInstantiationAwareBeanPostProcessor ibp = (SmartInstantiationAwareBeanPostProcessor) bp; exposedObject = ibp.getEarlyBeanReference(exposedObject, beanName); &#125; &#125; &#125; return exposedObject;&#125; 又是BeanPostProcessor的应用，而之所以这样，是因为返回的对象不一定是对象本身，比如AOP，它就需要返回一个代理对象了，所以在这才又使用 了这种BeanPostProcessor的应用。 这里会返回A对象，那么回到getSingleton()方法，这时会把该对象放入到二级缓存中，并删除三级缓存。getSingleton()的流程结束了，看拿到A的对象后做了什么。 最后还调用了getObjectForBeanInstance方法，这个方法就是FactoryBean接口的调用入口，暂时不看。else 不进入的话就不会去创建了，所以这里返回了，整个流程就和我一开始说的一样。只要在3这步切断A的createBean步骤就可以切断这个闭环了。 构造函数的循环依赖Spring是解决不了这种循环依赖的，对于引用的情况，是先创建了对象后才设置引用的。而构造函数的循环依赖是因为这时对象还在创建中，依赖的对象又在创建中。 所以Spring对于构造函数的循环依赖，会直接在启动的时候报错。 不过如果在构造函数上加上@Lazy就可以，因为实例化时获取到的依赖对象是一个代理对象，所以不存在循环依赖。 不过如果@Lazy加载两个类上还会有问题。 Bean范围为Pototype的时候的循环依赖这时bean是不会使用到3级缓存的，在源码AbstractBeanFactory#getBean中是没有调用到DefaultSingletonBeanRegistry#getSingleton方法的，这个方法就是使用3级缓存的。 该代码块直接调用了createBean，那只会触发bean的创建流程。但出现循环依赖，对同一对象的第二次调用doGetBean时，会进入到这段代码块: 会直接报错。 initializeBean—-依赖注入完以后的调用bean 实例化+ioc依赖注入完以后，会调用某些接口的方法。而initializeBean就是处理这个逻辑： 1234567891011121314151617181920212223242526272829303132333435363738protected Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) &#123; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; &#123; invokeAwareMethods(beanName, bean); return null; &#125;, getAccessControlContext()); &#125; else &#123; //调用Aware方法 invokeAwareMethods(beanName, bean); &#125; Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) &#123; //对类中某些特殊方法的调用，比如@PostConstruct，Aware接口，非常重要 重要程度 ：5 //ApplicationContextAwareProcessor 对Aware接口的调用如： //EnvironmentAware EmbeddedValueResolverAware ResourceLoaderAware ApplicationEventPublisherAware MessageSourceAware ApplicationContextAware //ImportAwareBeanPostProcessor 对ImportAware的支持 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; try &#123; //InitializingBean接口，afterPropertiesSet，init-method属性调用,非常重要，重要程度：5 invokeInitMethods(beanName, wrappedBean, mbd); &#125; catch (Throwable ex) &#123; throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, &quot;Invocation of init method failed&quot;, ex); &#125; if (mbd == null || !mbd.isSynthetic()) &#123; //这个地方可能生出代理实例，是aop的入口 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean;&#125; 一个一个方法看。 invokeAwareMethods—-BeanNameAware、BeanClassLoaderAware和BeanFactoryAware12345678910111213141516private void invokeAwareMethods(String beanName, Object bean) &#123; if (bean instanceof Aware) &#123; if (bean instanceof BeanNameAware) &#123; ((BeanNameAware) bean).setBeanName(beanName); &#125; if (bean instanceof BeanClassLoaderAware) &#123; ClassLoader bcl = getBeanClassLoader(); if (bcl != null) &#123; ((BeanClassLoaderAware) bean).setBeanClassLoader(bcl); &#125; &#125; if (bean instanceof BeanFactoryAware) &#123; ((BeanFactoryAware) bean).setBeanFactory(AbstractAutowireCapableBeanFactory.this); &#125; &#125;&#125; 这个方法是对实现了这些接口的类的方法调用。 BeanNameAware BeanClassLoaderAware BeanFactoryAware applyBeanPostProcessorsBeforeInitialization—非常重要的一个方法对类中某些特殊方法的调用，比如@PostConstruct，Aware接口，非常重要 重要程度 ：5 12345678910111213public Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) throws BeansException &#123; Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) &#123; Object current = processor.postProcessBeforeInitialization(result, beanName); if (current == null) &#123; return result; &#125; result = current; &#125; return result;&#125; 又是BeanPostProcessor的运用，会调用BeanPostProcessor#postProcessBeforeInitialization方法 有很多，我们只关注一些类 InitDestroyAnnotationBeanPostProcessor——@PostConstruct 这里会去调用加了这个@PostConstruct的方法。 该类是CommonAnnotationBeanPostProcessor的父类，CommonAnnotationBeanPostProcessor会在AnnotationConfigUtils#registerAnnotationConfigProcessors方法中，加入到Spring中，也就是在创建BeanDefinition阶段。 ApplicationContextAwareProcessor——又是Aware接口调用 1234567891011121314151617181920private void invokeAwareInterfaces(Object bean) &#123; if (bean instanceof EnvironmentAware) &#123; ((EnvironmentAware) bean).setEnvironment(this.applicationContext.getEnvironment()); &#125; if (bean instanceof EmbeddedValueResolverAware) &#123; ((EmbeddedValueResolverAware) bean).setEmbeddedValueResolver(this.embeddedValueResolver); &#125; if (bean instanceof ResourceLoaderAware) &#123; ((ResourceLoaderAware) bean).setResourceLoader(this.applicationContext); &#125; if (bean instanceof ApplicationEventPublisherAware) &#123; ((ApplicationEventPublisherAware) bean).setApplicationEventPublisher(this.applicationContext); &#125; if (bean instanceof MessageSourceAware) &#123; ((MessageSourceAware) bean).setMessageSource(this.applicationContext); &#125; if (bean instanceof ApplicationContextAware) &#123; ((ApplicationContextAware) bean).setApplicationContext(this.applicationContext); &#125;&#125; 挺简单的，就是会去调用这些接口的方法 EnvironmentAware EmbeddedValueResolverAware ResourceLoaderAware ApplicationEventPublisherAware MessageSourceAware ApplicationContextAware 这个是在AbstractApplicationContext.refresh方法中调用的prepareBeanFactory方法中添加进去的 ImportAwareBeanPostProcessor 调用了ImportAware的方法 比如这样写了一个类 是触发不到这段代码的，需要这样写也就是说只有通过@Import()注解注入的类才能调用到ImportAware#setImportMetadata方法。为什么会这样，这就涉及到这个类ConfigurationClassPostProcessor。这个类在之后会说。 这个是类是通过ConfigurationClassPostProcessor的postProcessBeanFactory方法加入进来的，也就是在Spring bean初始化之前。而ConfigurationClassPostProcessor又是通过AnnotationConfigUtils#registerAnnotationConfigProcessors加入进来的。 ServletContextAwareProcessor——ServletContextAware，ServletConfigAware 这是使用在使用Spring MVC的时候才会被加入到Spring中。 invokeInitMethods—-InitializingBeanInitializingBean接口，afterPropertiesSet，init-method属性调用,非常重要，重要程度：5 123456789101112131415161718192021222324252627282930313233protected void invokeInitMethods(String beanName, Object bean, @Nullable RootBeanDefinition mbd) throws Throwable &#123; boolean isInitializingBean = (bean instanceof InitializingBean); if (isInitializingBean &amp;&amp; (mbd == null || !mbd.isExternallyManagedInitMethod(&quot;afterPropertiesSet&quot;))) &#123; ...... if (System.getSecurityManager() != null) &#123; try &#123; AccessController.doPrivileged((PrivilegedExceptionAction&lt;Object&gt;) () -&gt; &#123; ((InitializingBean) bean).afterPropertiesSet(); return null; &#125;, getAccessControlContext()); &#125; catch (PrivilegedActionException pae) &#123; throw pae.getException(); &#125; &#125; else &#123; //调用实现了InitializingBean接口的方法 ((InitializingBean) bean).afterPropertiesSet(); &#125; &#125; if (mbd != null &amp;&amp; bean.getClass() != NullBean.class) &#123; String initMethodName = mbd.getInitMethodName(); if (StringUtils.hasLength(initMethodName) &amp;&amp; !(isInitializingBean &amp;&amp; &quot;afterPropertiesSet&quot;.equals(initMethodName)) &amp;&amp; !mbd.isExternallyManagedInitMethod(initMethodName)) &#123; //调用init-method配置的方法 invokeCustomInitMethod(beanName, bean, mbd); &#125; &#125;&#125; 从源码可以看到，会调用InitializingBean接口的方法和bean标签的init-method方法。 applyBeanPostProcessorsAfterInitialization—-是aop的入口这个地方可能生出代理实例，是aop的入口 前面有个applyBeanPostProcessorsBeforeInitialization方法，这个叫AfterI，很明显又是BeanPostProcessor接口的应用。 看了这么多BeanPostProcessor了，都是在循环中的，而且都是对一个Bean循环的，可以看到，如果某个类对对应的方法感兴趣了，就在对应的方法了做一些对类的操作，不感兴趣了就直接返回就好了。而且，BeanPostProcessor这种使用方式感觉就是使用了装饰器模式，虽然没有装饰器模式的形，但有装饰器模式模式的魂—-给原始类添加增强功能 这里先讲一个类ApplicationListenerDetector。 我们之前讲过 ApplicationListener接口的，也就是监听类。这个监听类就是在ApplicationListenerDetector#postProcessAfterInitialization方法添加到applicationContext的事件管理类中的。（注意这里是实现接口，但注解的情况在这里没有处理，注解的情况后面会说） 还有一个AnnotationAwareAspectJAutoProxyCreator这个是AOP的入口方法，详情看这两节 08-Spring-AOP概念 09-Spring-AOP解析 registerDisposableBeanIfNecessary—注册DisposableBeanAdapter 1注册bean销毁时的类DisposableBeanAdapter 代码很简单registerDisposableBean就是在下面的Map中建立&lt;beanName, DisposableBeanAdapter&gt;这种映射关系。 DisposableBeanAdapter就是用于bean销毁（这里的销毁不是触发GC，而是Spring结束前对bean做的一些操作）的实例 DisposableBeanAdapter的构造函数做如下操作 是否有实现了接口DisposableBean和AutoCloseable 在bean标签中是否定义了destroy-method属性 获取到对象中有@PreDestory注解的方法。 什么时候触发的？手动这样触发的 Tomcat启动时，AbstractReactiveWebInitializer这个类里面会为Tomcat添加一个ServletContextListener，当tomcat触发close方法时，会调用contextDestroyed方法 123456789101112131415161718192021222324@Overridepublic void contextDestroyed(ServletContextEvent sce) &#123; this.applicationContext.close();&#125;@Overridepublic void close() &#123; synchronized (this.startupShutdownMonitor) &#123; // 这里会调用getBeanFactory().destroySingletons(); // destroySingletonsa会去触发DisposableBeanAdapter的destroy方法 doClose(); // If we registered a JVM shutdown hook, we don&#x27;t need it anymore now: // We&#x27;ve already explicitly closed the context. if (this.shutdownHook != null) &#123; try &#123; Runtime.getRuntime().removeShutdownHook(this.shutdownHook); &#125; catch (IllegalStateException ex) &#123; // ignore - VM is already shutting down &#125; &#125; &#125;&#125; DefaultSingletonBeanRegistry#getSingleton —-Bean创建完成后看DefaultSingletonBeanRegistry#getSingleton代码，上面的都只是这个调用的过程 12//把beanName添加到singletonsCurrentlyInCreation Set容器中，在这个集合里面的bean都是正在实例化的beanbeforeSingletonCreation(beanName); 接着调用 12345678afterSingletonCreation(beanName);protected void afterSingletonCreation(String beanName) &#123; if (!this.inCreationCheckExclusions.contains(beanName) &amp;&amp; !this.singletonsCurrentlyInCreation.remove(beanName)) &#123; //抛出IllegalStateException ..... &#125;&#125; 之前的beforeSingletonCreation是把beanName添加到singletonsCurrentlyInCreation Set容器中，表示这个Bean正在创建；而创建完成后，就调用了afterSingletonCreation方法，把beanName在singletonsCurrentlyInCreation Set容器中移除，表示Bean已经创建完成了。 最后调用了 12345678910111213141516if (newSingleton) &#123; //创建对象成功时，把对象缓存到singletonObjects缓存中,bean创建完成时放入一级缓存 addSingleton(beanName, singletonObject);&#125;protected void addSingleton(String beanName, Object singletonObject) &#123; synchronized (this.singletonObjects) &#123; //一级缓存 this.singletonObjects.put(beanName, singletonObject); //三级缓存 this.singletonFactories.remove(beanName); //二级缓存 this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); //这是设计到统计的 &#125;&#125; 把创建好的Bean放入到一级缓存singletonObjects中，并把二、三级缓存删掉。 总结下这个方法的作用，就是在缓存中没有对象的时候会去调用jvm的对象创建流程，创建完成成后，封装一个ObjectFactory并放入三级缓存中，在三级缓存建立&lt;beanName, ObjectFactory&gt;这样的对应关系，当处理IOC和DI时，如果出现循环依赖就通过三级缓存的ObjectFactory获取到之前已经创建好的jvm对象，并移出三级缓存进入二级缓存。最后IOC和DI处理完，并且对应接口的调用也调用完了，就把bean放入到一级缓存中，并移除二级、三级缓存对应的值。而对象创建的流程时通过方法createBean实现的，并不是getSingleton的逻辑。getSingleton的核心逻辑就是只是使用了singletonObjects 这个一级缓存。 放入一级缓存Bean初始化完成后，也就是AbstractBeanFactory#createBean方法调用完成后， 这时会返回对象，也就是DefaultSingletonBeanRegistry的getSingleton方法中 这代码执行完成了。最后就是执行了这些方法 1234567891011protected void addSingleton(String beanName, Object singletonObject) &#123; synchronized (this.singletonObjects) &#123; //一级缓存 this.singletonObjects.put(beanName, singletonObject); //三级缓存 this.singletonFactories.remove(beanName); //二级缓存 this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); //这是设计到统计的 &#125;&#125; 上边的是只有scope&#x3D;”Singleton”时才会执行到，也就是说只有时单例模式，spring才会缓存对象。如果是prototype，也就是多例的时候，是不会用到缓存的。看源码 1234567891011121314//AbstractBeanFactory#doGetBeanelse if (mbd.isPrototype()) &#123; // It&#x27;s a prototype -&gt; create a new instance. Object prototypeInstance = null; try &#123; beforePrototypeCreation(beanName); prototypeInstance = createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; //该方法是FactoryBean接口的调用入口 bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd);&#125; 可以看到，对于多例它是直接createBean的，没有通过getSingleton方法，来缓存。 实现FactoryBean接口的类的实例化这个和FactoryBean接口有关 FactoryBean接口的getObject在这个类实例化过程中是不会调用getObject方法的，只有在需要使用getObject返回对象时才会去调用这个方法。 回到refresh()方法。跟踪代码： 123AbstractApplicationContext#refresh --&gt;AbstractApplicationContext#finishBeanFactoryInitialization --&gt; DefaultListableBeanFactory#preInstantiateSingletons 看DefaultListableBeanFactory#preInstantiateSingletons部分源码 这个if就是判断是否实现了FactoryBean接口，这里就看if代码块的代码，先看 123String FACTORY_BEAN_PREFIX = &quot;&amp;&quot;;Object bean = getBean(FACTORY_BEAN_PREFIX + beanName); 也就是说，对于FactoryBean类，在调用getBean时会使用&amp;beanName这种形式去找bean 这时，经过transformedBeanName又去掉了&amp;符号，并把这个值赋给了新引用。 我们找的目标bean的名字就是beanName引用的指向的字符串。而且这个beanName是有对应的BeanDefinition，这个BeanDefinition指向的类是实现了FactoryBean接口的。所以如果走完Bean的初始化流程，那初始化出来的对象的类型肯定不是我们的目标类型。而是实现了FactoryBean接口类的类型 下图返回的是一个代理对象，之所以是代理是因为如果不是代理对象的情况下，直接返回FactoryBean接口对象，那么如果程序通过&amp;baenName获取到该FactoryBean对象，接着通过这个对象调用getObject()方法，那么就会产生一个新的目标对象，这和Spring中bean的范围为singleton相矛盾了，所以这里返回的代理对象，就算程序通过&amp;baenName获取FactoryBean对象，由于这个FactoryBean对象是代理对象，那么在调用getObject()时就能做一些前置处理，比如检查目标对象是否在缓存中存在，如果存在就直接返回就行了。 也就是说，在这个时候在一级缓存singletonObjects 这个Map中就存在一个这样的关系 1&lt;beanName, FactoryBean接口类（代理）&gt; 接着会执行这个方法 1AbstractBeanFactory.getObjectForBeanInstance(sharedInstance, name, beanName, mbd) 调用这个方法的时候name&#x3D;&amp;beanName， beanName&#x3D;beanName的，这从上图就已经证明。 1234567891011121314151617181920protected Object getObjectForBeanInstance( Object beanInstance, String name, String beanName, @Nullable RootBeanDefinition mbd) &#123; // Don&#x27;t let calling code try to dereference the factory if the bean isn&#x27;t a factory. // 检查是否以&amp;开头 if (BeanFactoryUtils.isFactoryDereference(name)) &#123; if (beanInstance instanceof NullBean) &#123; return beanInstance; &#125; if (!(beanInstance instanceof FactoryBean)) &#123; throw new BeanIsNotAFactoryException(beanName, beanInstance.getClass()); &#125; if (mbd != null) &#123; mbd.isFactoryBean = true; &#125; return beanInstance; &#125; .....&#125; 那么在上边的情况这时&#96;if (BeanFactoryUtils.isFactoryDereference(name)) 为true，就直接放回了。所以当通过&amp;beanName获取到的对象为FactoryBean的对象。此时，这个bean的初始化就已经结束了。下面看FactoryBean.getObject方法是什么时候调用的。 假如某个类的定义如下 123456789101112131415@Component(&quot;bObject&quot;)public AFactoryBean implements FactoryBean&lt;BObject&gt; &#123; ..... public BObject getObject() throws Exception &#123; renturn new BObject(); &#125; ....&#125;@Componentpublic C &#123; @Autowire private BObject bObject;&#125; 那么在该类C的DI阶段，就会调用beanFactory.getBean(“bObject”)，那么此时 1Object sharedInstance = getSingleton(&quot;bObject&quot;); 这代码会返回一个对象，而且这个对象是AFactoryBean，此时进入到getObjectForBeanInstance方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748protected Object getObjectForBeanInstance( Object beanInstance, String name, String beanName, @Nullable RootBeanDefinition mbd) &#123; // Don&#x27;t let calling code try to dereference the factory if the bean isn&#x27;t a factory. // 检查是否以&amp;开头 if (BeanFactoryUtils.isFactoryDereference(name)) &#123; if (beanInstance instanceof NullBean) &#123; return beanInstance; &#125; if (!(beanInstance instanceof FactoryBean)) &#123; throw new BeanIsNotAFactoryException(beanName, beanInstance.getClass()); &#125; if (mbd != null) &#123; mbd.isFactoryBean = true; &#125; return beanInstance; &#125; // Now we have the bean instance, which may be a normal bean or a FactoryBean. // If it&#x27;s a FactoryBean, we use it to create a bean instance, unless the // caller actually wants a reference to the factory. //如果实例不是FactoryBean类型的 if (!(beanInstance instanceof FactoryBean)) &#123; return beanInstance; &#125; //如果代码能走下来，则说明 beanName不是以&amp;开头，并且beanInstance是FactoryBean类型的 Object object = null; if (mbd != null) &#123; mbd.isFactoryBean = true; &#125; else &#123; //从缓存里面拿FactoryBean类型的实例 object = getCachedObjectForFactoryBean(beanName); &#125; if (object == null) &#123; // Return bean instance from factory. FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) beanInstance; // Caches object obtained from FactoryBean if it is a singleton. if (mbd == null &amp;&amp; containsBeanDefinition(beanName)) &#123; mbd = getMergedLocalBeanDefinition(beanName); &#125; boolean synthetic = (mbd != null &amp;&amp; mbd.isSynthetic()); //重点看 object = getObjectFromFactoryBean(factory, beanName, !synthetic); &#125; return object;&#125; 由于beanName是bObject，所以不会进入到if代码，那么此时最后会走到这代码块 重点看 1object = getObjectFromFactoryBean(factory, beanName, !synthetic); 先从缓存factoryBeanObjectCache 中拿，没有就执行下边的doGetObjectFromFactoryBean方法 这个方法放回后，想都不用想，就是把拿到的目标对象放到缓存factoryBeanObjectCache中。下次再通过beanName就可以直接在缓存factoryBeanObjectCache中获取了。 而如果这时时通过&amp;beanName去获取bean，那么进入doGetBean时会把&amp;去掉后获取到一个实现FactoryBean类型类的对象，之后进入getObjectForBeanInstance(sharedInstance, name, beanName, mbd)时由于BeanFactoryUtils.isFactoryDereference(name)为true，所以就直接返回了实现FactoryBean类型类的对象了（注意，这个是代理对象）。 FactoryBean的应用场景可以提供用户自己定义实例化 在Spring中如果你想创建一个对象，对象能被Spring管理 自定义BeanDefination，通过BeanDefinitionRegistryPostProcessor接口 实现FactoryBean @Bean Spring bean初始化完成后——放入到一级缓存之后，执行SmartInitializingSingleton接口回到DefaultListableBeanFactory#preInstantiateSingletons() 在这里会调用接口SmartInitializingSingleton的方法。 这里说一个类就是EventListenerMethodProcessor。 EventListenerMethodProcessor——@EventListener注解的解析[Spring Event事件通知机制](.&#x2F;Spring Event事件通知机制) Spring初始化流程中的三级缓存在Spring的初始化流程会使用到三级缓存 123private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap(256);private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap(16);private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap(16); 其中singletonObjects就是最缓存最终的对象的。而earlySingletonObjects和singletonFactories相互配合使用，完成了Bean的初始化流程 比如：存在这样一个类 1234class A &#123; @Autowrite A a;&#125; 首先一个，在beanFactory.getBean(A)的时候， new A 收集A中的注解信息（@Autowrite、@Resource、@Value和@PostConstruct等注解信息） 创建一个包含A对象引用的ObjectFactory对象，并把这个对象放入3级缓存中。 populateBean，完成Bean的填充（DI依赖注入）和@PostConstruct等注解的处理 在处理依赖注入时，本质上就是进行beanFactory#getBean，所以对于A对象的依赖注入，会进行beanFactory.getBean(a) 从三级缓存中一级一级的拿对象，最终在第三级singletonFactories中获取到一个ObjectFactory对象。然后会执行ObjectFactory#getObject。 会对ObjectFactory对象中的A对象进行AbstractAutoProxyCreator#getEarlyBeanReference，也就是进行切面的匹配，匹配到的话就生成代理对象。 把第二步中返回的A对象（或A对像的代理对象）放入二级缓存，并移除3级缓存（这时二级缓存已经是最终的对象了） 把第3步的对象返回，作为A对象中a属性引用的对象。 对A对象进行initializeBean，并返回A对象（或A对像的代理对象） BeanPostProcessor#postProcessBeforeInitialization的执行，包括对ApplicationEventPublisherAware、ApplicationContextAware等接口的处理和@PostConstruct的等注解的处理 执行InitializingBean#afterPropertiesSet BeanPostProcessor#postProcessAfterInitialization的执行，并返回A对象（或A对像的代理对象） 最重要的就是AbstractAutoProxyCreator，完成切面匹配和代理类生成。对于A对象，由于在4.2中已经执行过这一步了，所以这里不会重复执行，会直接返回A对象。 会从二级缓存拿对象，如果没有就直接返回A对象。如果有就返回二级缓存对象（A对象或者A对像的代理对象） 对于没有的情况，是不存在循环依赖才会出现这种情况，而对于有的情况，就意味着有循环引用。 对于A对象，由于在第4步中提前完成了切面匹配和代理对象的生成，所以这里会返回A对象（或A对像的代理对象） 把第6步返回的对象（A对象或者A对像的代理对象）放入到一级缓存中，并移除二三级缓存对应的内容。 从这个过程可以看出，三级缓存singletonFactories就是用来避免循环依赖时，重复触发creaetBean的问题，并且会对Bean提交进行包装（AOP）。而二级缓存earlySingletonObjects存放提前完成包装的对象（AOP），并负责提前暴露给其他bean。 Spring Bean初始化流程图 Spring Bean 循环依赖的流程图","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"03-Spring的Bean实例化前的方法","slug":"spring/03-Spring的Bean实例化前的方法","date":"2021-11-25T12:00:09.000Z","updated":"2022-03-23T09:03:54.250Z","comments":true,"path":"blog/spring/03-Spring的Bean实例化前的方法/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/03-Spring%E7%9A%84Bean%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95/","excerpt":"","text":"上节讲了BeanDefinitionRegistryPostProcessor的实例化和调用时机。 这节看Bean实例化前执行的一些重要方法。 回到AbstractApplicationContext#refresh()方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; //为容器初始化做准备，重要程度：0 // Prepare this context for refreshing. prepareRefresh(); /* 重要程度：5 1、创建BeanFactory对象 * 2、xml解析 * 传统标签解析：bean、import等 * 自定义标签解析 如：&lt;context:component-scan base-package=&quot;com.xiangxue.jack&quot;/&gt; * 自定义标签解析流程： * a、根据当前解析标签的头信息找到对应的namespaceUri * b、加载spring所有jar中的spring.handlers文件。并建立映射关系 * c、根据namespaceUri从映射关系中找到对应的实现了NamespaceHandler接口的类 * d、调用类的init方法，init方法是注册了各种自定义标签的解析类 * e、根据namespaceUri找到对应的解析类，然后调用paser方法完成标签解析 * * 3、把解析出来的xml标签封装成BeanDefinition对象 * */ // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); /* * 给beanFactory设置一些属性值，可以不看 * */ // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); /* * BeanDefinitionRegistryPostProcessor * BeanFactoryPostProcessor * 完成对这两个接口的调用 * */ // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); /* * 把实现了BeanPostProcessor接口的类实例化，并且加入到BeanFactory中 * */ // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); /* * 国际化,重要程度2 * */ // Initialize message source for this context. initMessageSource(); //初始化事件管理类 // Initialize event multicaster for this context. initApplicationEventMulticaster(); //这个方法着重理解模板设计模式，因为在springboot中，这个方法是用来做内嵌tomcat启动的 // Initialize other special beans in specific context subclasses. onRefresh(); /* * 往事件管理类中注册事件类 * */ // Check for listener beans and register them. registerListeners(); /* * 这个方法是spring中最重要的方法，没有之一 * 所以这个方法一定要理解要具体看 * 1、bean实例化过程 * 2、ioc * 3、注解支持 * 4、BeanPostProcessor的执行 * 5、Aop的入口 * */ // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset &#x27;active&#x27; flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring&#x27;s core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; registerBeanPostProcessors(beanFactory)该方法也是一个很重要的方法，这里会把实现了BeanPostProcessor接口的类实例化，并且加入到BeanFactory中，而BeanPostProcessor的就是spring提供的扩展点接口。 registerBeanPostProcessors(beanFactory)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, AbstractApplicationContext applicationContext) &#123; String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); // Register BeanPostProcessorChecker that logs an info message when // a bean is created during BeanPostProcessor instantiation, i.e. when // a bean is not eligible for getting processed by all BeanPostProcessors. int beanProcessorTargetCount = beanFactory.getBeanPostProcessorCount() + 1 + postProcessorNames.length; beanFactory.addBeanPostProcessor(new BeanPostProcessorChecker(beanFactory, beanProcessorTargetCount)); // Separate between BeanPostProcessors that implement PriorityOrdered, // Ordered, and the rest. List&lt;BeanPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;(); List&lt;BeanPostProcessor&gt; internalPostProcessors = new ArrayList&lt;&gt;(); List&lt;String&gt; orderedPostProcessorNames = new ArrayList&lt;&gt;(); List&lt;String&gt; nonOrderedPostProcessorNames = new ArrayList&lt;&gt;(); //提前实例化BeanPostProcessor类型的bean，然后bean进行排序 for (String ppName : postProcessorNames) &#123; if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; //getBean是实例化方法，后面我们在讲bean实例化过程是会着重讲到 BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); priorityOrderedPostProcessors.add(pp); //判断类型是否是MergedBeanDefinitionPostProcessor，如果是则代码是内部使用的 if (pp instanceof MergedBeanDefinitionPostProcessor) &#123; internalPostProcessors.add(pp); &#125; &#125; else if (beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; orderedPostProcessorNames.add(ppName); &#125; else &#123; nonOrderedPostProcessorNames.add(ppName); &#125; &#125; // First, register the BeanPostProcessors that implement PriorityOrdered. sortPostProcessors(priorityOrderedPostProcessors, beanFactory); //注册到BeanFactory中 registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors); // Next, register the BeanPostProcessors that implement Ordered. List&lt;BeanPostProcessor&gt; orderedPostProcessors = new ArrayList&lt;&gt;(orderedPostProcessorNames.size()); for (String ppName : orderedPostProcessorNames) &#123; BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); orderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) &#123; internalPostProcessors.add(pp); &#125; &#125; sortPostProcessors(orderedPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, orderedPostProcessors); // Now, register all regular BeanPostProcessors. List&lt;BeanPostProcessor&gt; nonOrderedPostProcessors = new ArrayList&lt;&gt;(nonOrderedPostProcessorNames.size()); for (String ppName : nonOrderedPostProcessorNames) &#123; BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); nonOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) &#123; internalPostProcessors.add(pp); &#125; &#125; registerBeanPostProcessors(beanFactory, nonOrderedPostProcessors); // Finally, re-register all internal BeanPostProcessors. sortPostProcessors(internalPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, internalPostProcessors); // Re-register post-processor for detecting inner beans as ApplicationListeners, // moving it to the end of the processor chain (for picking up proxies etc). beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext));&#125; 看String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); 该方法上节讲过了，就是在beanDefinition注册器(BeanFactory)中获取了实现了BeanPostProcessor接口的类的beanName。 还记得名字是这两个的BeanDefinition吗？ org.springframework.context.annotation.internalAutowiredAnnotationProcessor org.springframework.context.annotation.internalCommonAnnotationProcessor 这两个对应的BeanDefinition是RootBeanDefinition，是在扫描的最后一步，也就是扫描完类而且创建好了BeanDefinition和BeanDefinition已经注册完成后，Spring会把名字是这两个的RootBeanDefinition注册到注册器中，而且RootBeanDefinition里面对应的class对象是CommonAnnotationBeanPostProcessor.class和AutowiredAnnotationBeanPostProcessor.class 看下这两个类的类图: CommonAnnotationBeanPostProcessor: AutowiredAnnotationBeanPostProcessor: 好了代码继续走 这段代码会对实现了BeanPostProcessor的类实例化并进行分类 把实现了PriorityOrdered的分类一类 把实现了Ordered的分类一类 把除上边两种情况外的分为一类 然后会进行排序，排序规则和上一节的BeanDefinitionRegistryPostProcess的排序规则一样，在上边的分类下，对order进行升序排序。 接着执行： 1registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors); 12345678910111213public void addBeanPostProcessor(BeanPostProcessor beanPostProcessor) &#123; Assert.notNull(beanPostProcessor, &quot;BeanPostProcessor must not be null&quot;); this.beanPostProcessors.remove(beanPostProcessor); if (beanPostProcessor instanceof InstantiationAwareBeanPostProcessor) &#123; this.hasInstantiationAwareBeanPostProcessors = true; &#125; if (beanPostProcessor instanceof DestructionAwareBeanPostProcessor) &#123; this.hasDestructionAwareBeanPostProcessors = true; &#125; this.beanPostProcessors.add(beanPostProcessor);&#125; 就是把创建的BeanPostProcessor对象放入到下面这个集合中。 1private final List&lt;BeanPostProcessor&gt; beanPostProcessors = new CopyOnWriteArrayList(); 下面的代码也是同样的逻辑。 registerBeanPostProcessors(beanFactory);该方法的作用就是把实现了BeanPostProcessor接口的类实例化，并且加入到BeanFactory中，没别的了。 BeanPostProcessor的简单介绍看下BeanPostProcessor接口的定义 接口看上去很简单，但该接口有很多子接口，在子接口中一般情况下是会定义一些default方法，这些default方法是交给实现类来重写的。而通过定义不同的default方法和实现类实现不同的default方法这种方式，来提供丰厚的扩展点。 比如： [BeanPostProcessor 接口理解](.&#x2F;BeanPostProcessor 接口理解) Spring中重要的接口 initApplicationEventMulticaster();——事件处理器SimpleApplicationEventMulticaster的创建源码： 123456789101112131415protected void initApplicationEventMulticaster() &#123; ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (beanFactory.containsLocalBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME)) &#123; this.applicationEventMulticaster = beanFactory.getBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, ApplicationEventMulticaster.class); //打印日志 ...... &#125; else &#123; this.applicationEventMulticaster = new SimpleApplicationEventMulticaster(beanFactory); beanFactory.registerSingleton(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, this.applicationEventMulticaster); //打印日志 ...... &#125;&#125; 该方法会创建一个SimpleApplicationEventMulticaster对象，并把对象注册到了Spring容器的singletonObjects中（singletonObjects可以叫一级缓存或单例池）。而SimpleApplicationEventMulticaster是Spring的默认事件管理器。 详情看[Spring Event事件通知机制](.&#x2F;Spring Event事件通知机制) onRefresh();该方法在Spring中没用，是一个空方法，但在Spring boot中有用。 在spring boot中，这个方法是用来做内嵌Servlet容器（默认是tomcat）的启动的 registerListeners()——事件监听器注册源码: 1234567891011121314151617181920212223protected void registerListeners() &#123; // Register statically specified listeners first. for (ApplicationListener&lt;?&gt; listener : getApplicationListeners()) &#123; getApplicationEventMulticaster().addApplicationListener(listener); &#125; // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let post-processors apply to them! String[] listenerBeanNames = getBeanNamesForType(ApplicationListener.class, true, false); for (String listenerBeanName : listenerBeanNames) &#123; getApplicationEventMulticaster().addApplicationListenerBean(listenerBeanName); &#125; // 触发一些应用早期事件 // Publish early application events now that we finally have a multicaster... Set&lt;ApplicationEvent&gt; earlyEventsToProcess = this.earlyApplicationEvents; this.earlyApplicationEvents = null; if (!CollectionUtils.isEmpty(earlyEventsToProcess)) &#123; for (ApplicationEvent earlyEvent : earlyEventsToProcess) &#123; getApplicationEventMulticaster().multicastEvent(earlyEvent); &#125; &#125;&#125; 这个方法是添加一些ApplicationListener和触发一些early application events。 对于Spring 的事件通知可以看[Spring Event事件通知机制](.&#x2F;Spring Event事件通知机制) 对于 123for (ApplicationListener&lt;?&gt; listener : getApplicationListeners()) &#123; getApplicationEventMulticaster().addApplicationListener(listener);&#125; 可以通过下面的方式来添加ApplicationListener 对于 12345String[] listenerBeanNames = getBeanNamesForType(ApplicationListener.class, true, false);for (String listenerBeanName : listenerBeanNames) &#123; getApplicationEventMulticaster().addApplicationListenerBean(listenerBeanName);&#125; 这里只是添加了实现了ApplicationListener接口的beanName，这里会在multicastEvent方法中，触发通过beanName，从Spring容器中获取对象的方法。也就是getBean。 而注册的事件是怎么触发的？可以通过下面的代码触发。 12ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;);applicationContext.publishEvent(new EnjoyEvent(&quot;Jack&quot;,&quot;enjoyEvent&quot;)); Spring的事件通知涉及到观察者设计模式。 比如： 可以看下publishEvent源码: 可以看到，逻辑就是遍历全部的ApplicationListener，最后执行invokeListener(listener, event)方法。 看下invokeListener(listener, event)方法 Spring也会发布事件，有这5种标准事件： 比如第一事件ContextRefreshedEvent，在refresh()方法的最后的finishRefresh就有发布这个事件。 所以如果我们自己写一个订阅该事件的类，比如： 也就可以在容器初始化完时做一些操作。 第二个事件ContextStartedEvent 第三个事件ContextStoppedEvent","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"02-Spinrg的后置处理器-BeanDefinitionRegistryPostProcessor调用时机","slug":"spring/02-Spinrg的后置处理器-BeanDefinitionRegistryPostProcessor调用时机","date":"2021-11-25T12:00:08.000Z","updated":"2022-03-23T09:03:54.145Z","comments":true,"path":"blog/spring/02-Spinrg的后置处理器-BeanDefinitionRegistryPostProcessor调用时机/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/02-Spinrg%E7%9A%84%E5%90%8E%E7%BD%AE%E5%A4%84%E7%90%86%E5%99%A8-BeanDefinitionRegistryPostProcessor%E8%B0%83%E7%94%A8%E6%97%B6%E6%9C%BA/","excerpt":"","text":"以ClassPathXmlApplicationContext为ApplicationContext 1234567891011121314151617181920212223242526272829303132333435363738394041public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; //x去掉了一些和这节无关的代码 ..... /* * 重要程度：5 * 1、创建BeanFactory对象 * 2、xml解析 * 传统标签解析：bean、import等 * 自定义标签解析 如：&lt;context:component-scan base-package=&quot;com.xiangxue.jack&quot;/&gt; * 自定义标签解析流程： * a、根据当前解析标签的头信息找到对应的namespaceUri * b、加载spring所有jar中的spring.handlers文件。并建立映射关系 * c、根据namespaceUri从映射关系中找到对应的实现了NamespaceHandler接口的类 * d、调用类的init方法，init方法是注册了各种自定义标签的解析类 * e、根据namespaceUri找到对应的解析类，然后调用paser方法完成标签解析 * * 3、把解析出来的xml标签封装成BeanDefinition对象 * */ // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); /* * 给beanFactory设置一些属性值，可以不看 * */ // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); /* * BeanDefinitionRegistryPostProcessor * BeanFactoryPostProcessor * 完成对这两个接口的调用 * */ // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); &#125;&#125; 上一节讲了在XML配置下的BeanDefinition初始化和注册 1ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); 这一节看 1invokeBeanFactoryPostProcessors(beanFactory); invokeBeanFactoryPostProcessors从方法的翻译—-调用Bean Factory后处理器看，它是做一些后置处理的。 1234protected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) &#123; PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors()); ....&#125; 这里的getBeanFactoryPostProcessors会返回一个List&lt;BeanFactoryPostProcessor&gt;，这个集合是通过ApplicationContext的addBeanFactoryPostProcessor方法来添加值的 看invokeBeanFactoryPostProcessors： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145public static void invokeBeanFactoryPostProcessors( ConfigurableListableBeanFactory beanFactory, List&lt;BeanFactoryPostProcessor&gt; beanFactoryPostProcessors) &#123; // Invoke BeanDefinitionRegistryPostProcessors first, if any. Set&lt;String&gt; processedBeans = new HashSet&lt;&gt;(); // beanFactory是DefaultListableBeanFactory，它实现了BeanDefinitionRegistry接口 if (beanFactory instanceof BeanDefinitionRegistry) &#123; BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; List&lt;BeanFactoryPostProcessor&gt; regularPostProcessors = new ArrayList&lt;&gt;(); List&lt;BeanDefinitionRegistryPostProcessor&gt; registryProcessors = new ArrayList&lt;&gt;(); // 默认情况下beanFactoryPostProcessors会为空，可以通过addBeanFactoryPostProcessor方法来添加自己的BeanFactoryPostProcessor for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) &#123; if (postProcessor instanceof BeanDefinitionRegistryPostProcessor) &#123; BeanDefinitionRegistryPostProcessor registryProcessor = (BeanDefinitionRegistryPostProcessor) postProcessor; registryProcessor.postProcessBeanDefinitionRegistry(registry); registryProcessors.add(registryProcessor); &#125; else &#123; regularPostProcessors.add(postProcessor); &#125; &#125; // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the bean factory post-processors apply to them! // Separate between BeanDefinitionRegistryPostProcessors that implement // PriorityOrdered, Ordered, and the rest. List&lt;BeanDefinitionRegistryPostProcessor&gt; currentRegistryProcessors = new ArrayList&lt;&gt;(); //获取实现了BeanDefinitionRegistryPostProcessor接口的所有类的BeanDefinition对象的beanName // First, invoke the BeanDefinitionRegistryPostProcessors that implement PriorityOrdered. String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) &#123; if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); &#125; &#125; //排序 sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); //调用过程 invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); // Next, invoke the BeanDefinitionRegistryPostProcessors that implement Ordered. postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) &#123; //判断是否是实现的Ordered接口 if (!processedBeans.contains(ppName) &amp;&amp; beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); &#125; &#125; sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); //没实现排序接口的调用 // Finally, invoke all other BeanDefinitionRegistryPostProcessors until no further ones appear. boolean reiterate = true; while (reiterate) &#123; reiterate = false; postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) &#123; if (!processedBeans.contains(ppName)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); reiterate = true; &#125; &#125; sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); &#125; //调用postProcessBeanFactory方法 // Now, invoke the postProcessBeanFactory callback of all processors handled so far. invokeBeanFactoryPostProcessors(registryProcessors, beanFactory); invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory); &#125; else &#123; // Invoke factory processors registered with the context instance. invokeBeanFactoryPostProcessors(beanFactoryPostProcessors, beanFactory); &#125; // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the bean factory post-processors apply to them! String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false); // Separate between BeanFactoryPostProcessors that implement PriorityOrdered, // Ordered, and the rest. List&lt;BeanFactoryPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;(); List&lt;String&gt; orderedPostProcessorNames = new ArrayList&lt;&gt;(); List&lt;String&gt; nonOrderedPostProcessorNames = new ArrayList&lt;&gt;(); for (String ppName : postProcessorNames) &#123; if (processedBeans.contains(ppName)) &#123; // skip - already processed in first phase above &#125; //实现了PriorityOrdered接口的 else if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class)); &#125; //实现了Ordered接口的 else if (beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; orderedPostProcessorNames.add(ppName); &#125; else &#123; //没实现接口的 nonOrderedPostProcessorNames.add(ppName); &#125; &#125; //排序 // First, invoke the BeanFactoryPostProcessors that implement PriorityOrdered. sortPostProcessors(priorityOrderedPostProcessors, beanFactory); //调用 invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory); // Next, invoke the BeanFactoryPostProcessors that implement Ordered. List&lt;BeanFactoryPostProcessor&gt; orderedPostProcessors = new ArrayList&lt;&gt;(orderedPostProcessorNames.size()); for (String postProcessorName : orderedPostProcessorNames) &#123; orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); &#125; sortPostProcessors(orderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory); // Finally, invoke all other BeanFactoryPostProcessors. List&lt;BeanFactoryPostProcessor&gt; nonOrderedPostProcessors = new ArrayList&lt;&gt;(nonOrderedPostProcessorNames.size()); for (String postProcessorName : nonOrderedPostProcessorNames) &#123; nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); &#125; invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory); // Clear cached merged bean definitions since the post-processors might have // modified the original metadata, e.g. replacing placeholders in values... beanFactory.clearMetadataCache();&#125; 代码很长，不过跟着代码一步步走，beanFactory的实例类型是DefaultListableBeanFactory，而该类是实现了BeanDefinitionRegistry接口的，所以会进入第一个if。跟着代码一步步走，先看这段代码块 12String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); 这代码的意思就是在之前注册的beanDefinition中，找到有实现了BeanDefinitionRegistryPostProcessor接口的beanDefinition，最后返回这些beanDefinition的beanName，看上图可以看到有名字返回。 请注意最后一个org.springframework.context.annotation.internalConfigurationAnnotationProcessor这个对应的beanDefinition就是在扫描注解创建完beanDefinition并注册后，Spring自己加进来的RootBeanDefinition，该BeanDefinition指代的类为ConfigurationClassPostProcessor.class&#96;这里只是回忆下，现在还没涉及到这个class对象。 下面先介绍下BeanDefinitionRegistryPostProcessor接口的作用。 BeanDefinitionRegistryPostProcessor的作用。 先看下定义： 从接口BeanDefinitionRegistryPostProcessor定义的抽象方法可以看到，带着注册中心，也就是说，这个接口的实现类完全可以获取到之前创建并注册完成的BeanDefinition！那我们可以做一些扩展，比如自己再注册个BeanDefinition或者修改下BeanDefinition也可以去实现扫描我们自定义的注解！也就是要增删改查BeanDefinition时可以实现这个接口！ 而BeanFactoryPostProcessor的定义方法是需要传BeanFactory的，所以该方法更加适合做修改应用上下文，但由于该方法的入参会是DefaultListableBeanFactory，该类也实现了BeanDefinitionRegistry，所以这也能对BeanDefinition做增删改 使用场景： 实现自己的自定义注解，并且注入到Spring容器中 这里定义了两个抽象方法，不过这两个方法的调用时机是不同的。从方法名可以了解一些信息 postProcessBeanDefinitionRegistry的重点是对BeanDefinition的处理 postProcessBeanFactory的重点是对BeanFactory对象属性的修改 参考类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Componentpublic class BeanPro implements BeanDefinitionRegistryPostProcessor, PriorityOrdered, Ordered &#123; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; //查询BeanDefinition final String[] beanDefinitionNames = registry.getBeanDefinitionNames(); for (String beanDefinitionName : beanDefinitionNames) &#123; BeanDefinition beanDefinition = registry.getBeanDefinition(beanDefinitionName); System.out.println(beanDefinition); &#125; GenericBeanDefinition genericBeanDefinition = new GenericBeanDefinition(); genericBeanDefinition.setBeanClass(BeanDefinitionBean.class); MutablePropertyValues propertyValues = genericBeanDefinition.getPropertyValues(); propertyValues.add(&quot;name&quot;,&quot;Jack&quot;); registry.registerBeanDefinition(&quot;beanDefinitionBean&quot;,genericBeanDefinition); //也可以做自定义注解扫描// ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(registry);// //需要过滤的注解// scanner.addIncludeFilter(new AnnotationTypeFilter(MyService.class));// scanner.scan(&quot;com.enjoy.jack.customBean&quot;); &#125; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; String[] beanDefinitionNames = registry.getBeanDefinitionNames(); DefaultListableBeanFactory beanFactory1 = (DefaultListableBeanFactory)beanFactory; beanFactory1.setAllowBeanDefinitionOverriding(true); beanFactory1.setAllowCircularReferences(true); beanFactory1.setAllowRawInjectionDespiteWrapping(true); &#125; @Override public int getOrder() &#123; return 0; &#125;&#125;@Datapublic class BeanDefinitionBean &#123; private String name = &quot;test&quot;;&#125; 回到invokeBeanFactoryPostProcessors 12String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); 已经讲了，看for，for里面就是是判断下是否实现了PriorityOrdered这个排序接口，先看下我关注的类ConfigurationClassPostProcessor，看下该类的类图： 他有实现PriorityOrdered接口，所以会进入到方法 1beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class) beanFactory的对象是DefaultListableBeanFactory，这个getBean方法会触发Spring Bean的初始化，作用就是是实例化ConfigurationClassPostProcessor对象，并完成控制反转（IOC）、依赖注入（DI）。更进一步说，就是实例化实现了BeanDefinitionRegistryPostProcessor接口和PriorityOrdered接口的类。 接着把实例化的对象放入到 1List&lt;BeanDefinitionRegistryPostProcessor&gt; currentRegistryProcessors = new ArrayList&lt;&gt;(); 这个集合中。最后把beanName放入 1Set&lt;String&gt; processedBeans = new HashSet&lt;&gt;(); 这个集合中。代码再往下走。 根据PriorityOrdered接口对currentRegistryProcessors集合排下序，接着把currentRegistryProcessors放到了这个集合中 1List&lt;BeanDefinitionRegistryPostProcessor&gt; registryProcessors = new ArrayList&lt;&gt;(); 接着就是重点方法了， 1invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); 这个方法就是去对调用刚刚创建的实现了BeanDefinitionRegistryPostProcessor接口的对象的postProcessBeanDefinitionRegistry方法，这里只调用了一个方法，还有另一个方法postProcessBeanFactory会在之后被调用。 上面讲的是实现了BeanDefinitionRegistryPostProcessor接口和PriorityOrdered接口的，没实现的呢？继续看代码： 还是同样的味道，同样的套路。。。。。。。代码继续走 之前的都是调用了BeanDefinitionRegistryPostProcessor的postProcessBeanBefinitionRegistry方法，那最后就是调用BeanDefinitionRegistryPostProcessor的postProcessBeanFactory方法了。都是和上边同样的流程， 不过这里需要注意，最后的集合regularPostProcessors在默认的情况下是没有的，因为它需要手动调用ApplicationContext.addBeanFactoryPostProcessor方法才会有。 代码继续走，这次是获取实现了BeanFactoryPostProcessor接口的类了 之后的逻辑和排序都和上边的一样。 总结这个方法方法很重要，甚至比前面第一节中，BeanFactory的创建还重要。因为在第一节中的讲的BeanDefinition的创建和注册是在obtainFreshBeanFactory完成的，这只是针对基于xml文件的，但现在随着注解的形式已经成为了主流，而基于注解的BeanDefinition的创建和注册不是在obtainFreshBeanFactory完成的，而是在这个方法中调用了ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry完成了BeanDefinition的创建和注册的。所以这个方法是一个非常非常重要的方法！！ 看下默认排序的OrderComparator： 下面对这个方法中的BeanDefinitionRegistyPostProcess的调用顺序进行总结： PriorityOrdered &gt; Ordered &gt; 非PriorityOrdered非Ordered order越小越先被执行 还有一个AnnotationAwareOrderComparator.INSTANCE，这个排序是OrderComparator的子类，它支持@Priority、和@Order注解，排序规则还是和上边一样。 流程图","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"01-Spring中BeanDefinition的创建和注册","slug":"spring/01-Spring中BeanDefinition的创建和注册","date":"2021-11-25T12:00:07.000Z","updated":"2022-03-23T09:03:54.081Z","comments":true,"path":"blog/spring/01-Spring中BeanDefinition的创建和注册/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/01-Spring%E4%B8%ADBeanDefinition%E7%9A%84%E5%88%9B%E5%BB%BA%E5%92%8C%E6%B3%A8%E5%86%8C/","excerpt":"","text":"以ClassPathXmlApplicationContext为ApplicationContext 1234方法重要程度： 0：不重要，可以不看 1：一般重要，可看可不看 5：非常重要，一定要看 必须读的 ：重要程度 5 12345678910public ClassPathXmlApplicationContext( String[] configLocations, boolean refresh, @Nullable ApplicationContext parent) throws BeansException &#123; super(parent); setConfigLocations(configLocations); if (refresh) &#123; refresh(); &#125;&#125; refresh()方法是重要方法，ClassPathXmlApplicationContext没有重写该方法所以用的是从父类AbstractApplicationContext继承过来的refresh()， 该方法是spring容器启动的核心方法，该类是一个典型的父类模板设计模式的运用，提供一个钩子方法，通过继承，子类实现对应的钩子方法来完成不同的功能。 根据不同的上下文对象，会调用不同的上下文对象子类方法中 核心上下文子类有： ClassPathXmlApplicationContext FileSystemXmlApplicationContext AnnotationConfigApplicationContext AnnotationConfigWebApplicationContext EmbeddedWebApplicationContext(springboot) …. 这里关注的重点只有两个ClassPathXmlApplicationContext和AnnotationConfigApplicationContext。先看第一ClassPathXmlApplicationContext。ClassPathXmlApplicationContext的类图： 看回refresh()方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; //为容器初始化做准备，重要程度：0 // Prepare this context for refreshing. prepareRefresh(); /* * 重要程度：5 * */ // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); /* * 给beanFactory设置一些属性值，可以不看 * */ // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); /* * BeanDefinitionRegistryPostProcessor * BeanFactoryPostProcessor * 完成对这两个接口的调用 * */ // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); /* * 把实现了BeanPostProcessor接口的类实例化，并且加入到BeanFactory中 * */ // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); /* * 国际化,重要程度2 * */ // Initialize message source for this context. initMessageSource(); //初始化事件管理类 // Initialize event multicaster for this context. initApplicationEventMulticaster(); //这个方法着重理解模板设计模式，因为在springboot中，这个方法是用来做内嵌tomcat启动的 // Initialize other special beans in specific context subclasses. onRefresh(); /* * 往事件管理类中注册事件类 * */ // Check for listener beans and register them. registerListeners(); /* * 这个方法是spring中最重要的方法，没有之一 * 所以这个方法一定要理解要具体看 * 1、bean实例化过程 * 2、ioc * 3、注解支持 * 4、BeanPostProcessor的执行 * 5、Aop的入口 * */ // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset &#x27;active&#x27; flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring&#x27;s core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; 首先refresh()方法是AbstractApplicationContext的核心，从上面源码看到，该方法里面定义的Spring整个初始化阶段的操作。而且提供一些钩子方法，让不同的类去重写钩子方法，实现不同的功能。该方法是线程安全的，不过该方法也就掉用哥一次，没谁调用多次 吧。 这一节看 1ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); 对于ClassPathXmlApplicationContext这个方法的作用： 创建BeanFactory对象 xml解析 传统标签解析：bean、import等自定义标签解析 如：&lt;context:component-scan base-package&#x3D;”com.xiangxue.jack”&#x2F;&gt;自定义标签解析流程： a、根据当前解析标签的头信息找到对应的namespaceUri b、加载spring所有jar中的spring.handlers文件。并建立映射关系 c、根据namespaceUri从映射关系中找到对应的实现了NamespaceHandler接口的类 d、调用类的init方法，init方法是注册了各种自定义标签的解析类 e、根据namespaceUri找到对应的解析类，然后调用paser方法完成标签解析 把解析出来的xml标签封装成BeanDefinition对象 开始看源码，该方法是在AbstractApplicationContext中的， 12345protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; //核心方法，必须读，重要程度：5 refreshBeanFactory(); return getBeanFactory();&#125; refreshBeanFactory()方法是一个抽象的钩子方法，它的实现是交由子类来完成的， AbstractRefreshableApplicationContext#refreshBeanFactory： 12345678910111213141516171819202122232425protected final void refreshBeanFactory() throws BeansException &#123; //如果BeanFactory不为空，则清除BeanFactory和里面的实例 if (hasBeanFactory()) &#123; destroyBeans(); closeBeanFactory(); &#125; try &#123; //BeanFactory 实例工厂 DefaultListableBeanFactory beanFactory = createBeanFactory(); beanFactory.setSerializationId(getId()); //设置是否可以循环依赖 allowCircularReferences //是否允许使用相同名称重新注册不同的bean实现. customizeBeanFactory(beanFactory); //解析xml，并把xml中的标签封装成BeanDefinition对象 loadBeanDefinitions(beanFactory); this.beanFactory = beanFactory; &#125; catch (IOException ex) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + getDisplayName(), ex); &#125;&#125;protected DefaultListableBeanFactory createBeanFactory() &#123; return new DefaultListableBeanFactory(getInternalParentBeanFactory());&#125; DefaultListableBeanFactory beanFactory = createBeanFactory();返回的是DefaultListableBeanFactory customizeBeanFactory(beanFactory);这里面主要设置两个参数： 12345678910protected void customizeBeanFactory(DefaultListableBeanFactory beanFactory) &#123; if (this.allowBeanDefinitionOverriding != null) &#123; //设置允许Bean定义覆盖 beanFactory.setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); &#125; //设置是否可以循环依赖 if (this.allowCircularReferences != null) &#123; beanFactory.setAllowCircularReferences(this.allowCircularReferences); &#125;&#125; 重点看loadBeanDefinitions(beanFactory)，该方法又是钩子方法，它由AbstractXmlApplicationContext实现： 123456789101112131415161718protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // Create a new XmlBeanDefinitionReader for the given BeanFactory. //创建xml的解析器，这里是一个委托模式 XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // Configure the bean definition reader with this context&#x27;s // resource loading environment. beanDefinitionReader.setEnvironment(this.getEnvironment()); //这里传一个this进去，因为ApplicationContext是实现了ResourceLoader接口的 beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // Allow a subclass to provide custom initialization of the reader, // then proceed with actually loading the bean definitions. initBeanDefinitionReader(beanDefinitionReader); //主要看这个方法 重要程度 5 loadBeanDefinitions(beanDefinitionReader);&#125; XmlBeanDefinitionReader是xml的解析器，DefaultListableBeanFactory不完成XML解析的工作，该工作委托给了XmlBeanDefinitionReader来完成，看下类图： XmlBeanDefinitionReader上面的代码的重点方法是AbstractXmlApplicationContext.loadBeanDefinitions(beanDefinitionReader)： 1234567891011protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; Resource[] configResources = getConfigResources(); if (configResources != null) &#123; reader.loadBeanDefinitions(configResources); &#125; //获取需要加载的xml配置文件 String[] configLocations = getConfigLocations(); if (configLocations != null) &#123; reader.loadBeanDefinitions(configLocations); &#125;&#125; configLocations就是一个资源数组，里面保存的就是指定解析的xml配置文件： 经过调用 12XmlBeanDefinitionReader.loadBeanDefinitions(configLocations) ---&gt;AbstractBeanDefinitionReader.loadBeanDefinitions(String location, @Nullable Set&lt;Resource&gt; actualResources) AbstractBeanDefinitionReader.loadBeanDefinitions： 1234567891011121314151617181920212223242526272829303132public int loadBeanDefinitions(String location, @Nullable Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException &#123; ResourceLoader resourceLoader = getResourceLoader(); if (resourceLoader == null) &#123; ......//不重要代码 &#125; if (resourceLoader instanceof ResourcePatternResolver) &#123; // Resource pattern matching available. try &#123; //把字符串类型的xml文件路径，形如：classpath*:user/**/*-context.xml,转换成Resource对象类型，其实就是用流 //的方式加载配置文件，然后封装成Resource对象，不重要，可以不看 Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); //主要看这个方法 ** 重要程度 5 int count = loadBeanDefinitions(resources); if (actualResources != null) &#123; Collections.addAll(actualResources, resources); &#125; //不重要代码 ...... return count; &#125; catch (IOException ex) &#123; //不重要代码 ...... &#125; &#125; else &#123; //不重要代码 ...... return count; &#125;&#125; 参数location我们已经知道了，是一个字符串，而且是xml文件名字活匹配串，该方法中调用了 1Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); 该方法的实际作用就是通过流的方法去获取到xml文件，并且封装成一个Resource对象。而之所以是数组，是应为 可能会是这样的匹配串： 1classpath*:user/**/*-context.xml 好了，到重点方法int count &#x3D; loadBeanDefinitions(resources);了 12345678910@Overridepublic int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException &#123; Assert.notNull(resources, &quot;Resource array must not be null&quot;); int count = 0; for (Resource resource : resources) &#123; //模板设计模式，调用到子类中的方法 count += loadBeanDefinitions(resource); &#125; return count;&#125; loadBeanDefinitions又是钩子方法，它的实现由子类XmlBeanDefinitionReader完成 12345@Overridepublic int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException &#123; //EncodedResource带编码的对Resource对象的封装 return loadBeanDefinitions(new EncodedResource(resource));&#125; loadBeanDefinitions(EncodedResource encodedResource): 12345678910111213141516171819202122232425262728293031323334public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException &#123; Assert.notNull(encodedResource, &quot;EncodedResource must not be null&quot;); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Loading XML bean definitions from &quot; + encodedResource); &#125; Set&lt;EncodedResource&gt; currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (!currentResources.add(encodedResource)) &#123; throw new BeanDefinitionStoreException( &quot;Detected cyclic loading of &quot; + encodedResource + &quot; - check your import definitions!&quot;); &#125; //获取Resource对象中的xml文件流对象 try (InputStream inputStream = encodedResource.getResource().getInputStream()) &#123; //InputSource是jdk中的sax xml文件解析对象 InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; //主要看这个方法 ** 重要程度 5 return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; catch (IOException ex) &#123; ........ &#125; finally &#123; currentResources.remove(encodedResource); if (currentResources.isEmpty()) &#123; this.resourcesCurrentlyBeingLoaded.remove(); &#125; &#125;&#125; 这里就是开始加载xml文件，为xml文件生成一个InputStream。 重点是doLoadBeanDefinitions(inputSource, encodedResource.getResource());方法 1234567891011121314151617181920212223protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException &#123; try &#123; //把inputSource 封装成Document文件对象，这是jdk的API Document doc = doLoadDocument(inputSource, resource); //主要看这个方法，根据解析出来的document对象，拿到里面的标签元素封装成BeanDefinition int count = registerBeanDefinitions(doc, resource); //....log return count; &#125; ..........&#125;public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; //又来一记委托模式，BeanDefinitionDocumentReader委托这个类进行document的解析 BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); int countBefore = getRegistry().getBeanDefinitionCount(); //主要看这个方法，createReaderContext(resource) XmlReaderContext上下文，封装了XmlBeanDefinitionReader对象 documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); return getRegistry().getBeanDefinitionCount() - countBefore;&#125; 上面兜兜转转，XmlBeanDefinitionReader其实就一个目的，读取xml的内容，并封装成一个Document对象，在 doLoadDocument(inputSource, resource)里完成这一过程。那有了xml文件的信息了，接下来就是去解析xml信息并封装成对象了，从单一职责原则看，XmlBeanDefinitionReader的职责已经完成了，所以它创建了BeanDefinitionDocumentReader类，将解析工作交给它。 BeanDefinitionDocumentReader该接口的实现类为DefaultBeanDefinitionDocumentReader，registerBeanDefinitions就是该类的核心方法 12345public void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; this.readerContext = readerContext; //主要看这个方法，把root节点传进去 doRegisterBeanDefinitions(doc.getDocumentElement());&#125; doc.getDocumentElement()，用来获取XMl的root节点，也就是spring配置文件的beans元素。 1234567891011protected void doRegisterBeanDefinitions(Element root) &#123; BeanDefinitionParserDelegate parent = this.delegate; this.delegate = createDelegate(getReaderContext(), root, parent); ........ preProcessXml(root); //主要看这个方法，标签具体解析过程 parseBeanDefinitions(root, this.delegate); postProcessXml(root); this.delegate = parent;&#125; 注意，这里又是一个委托，看createDelegate，这个方法返回了BeanDefinitionParserDelegate，从名字上看该类是真正将xml的信息封装成BeanDefinition的。往下看， 有两个空方法preProcessXml和postProcessXml，这两个在默认的实现类中什么都不做，这是为了扩展才提供了，看重点方法parseBeanDefinitions(root, this.delegate); 12345678910111213141516171819202122protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; if (delegate.isDefaultNamespace(root)) &#123; NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) &#123; //默认标签解析 parseDefaultElement(ele, delegate); &#125; else &#123; //自定义标签解析 delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125;&#125; 该方法就是遍历beans里面的元素。而这里又分为默认标签解析和自定义标签解析 默认标签解析传统标签解析：bean、beans、import和alias parseDefaultElement(ele, delegate)，解析默认标签解析： 12345678910111213141516171819private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; //import标签解析 重要程度 1 ，可看可不看 if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; importBeanDefinitionResource(ele); &#125; //alias标签解析 别名标签 重要程度 1 ，可看可不看 else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; processAliasRegistration(ele); &#125; //bean标签，重要程度 5，必须看 else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; processBeanDefinition(ele, delegate); &#125; // 这里是解析beans，其实可以看作是遍历bean而已 else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123; // recurse doRegisterBeanDefinitions(ele); &#125;&#125; 我们值看bean标签的解析，processBeanDefinition： 123456789101112131415161718protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; //重点看这个方法，重要程度 5 ，解析document，封装成BeanDefinition BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; //该方法功能不重要，设计模式重点看一下，装饰者设计模式，加上SPI设计思想 bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; //完成document到BeanDefinition对象转换后，对BeanDefinition对象进行缓存注册 // Register the final decorated instance. BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; ......... &#125; // Send registration event. getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; 现在已经有xml的bean标签的对应的对象了，如果是我做的会解析该xml的bean标签的信息并把这些信息封装到一个对象中，而spring也是这样做的，spring会把信息封装成BeanDefinition 重点看BeanDefinitionHolder bdHolder &#x3D; delegate.parseBeanDefinitionElement(ele);，这个方法就是去解析document，封装成BeanDefinition的，跟踪代码到BeanDefinitionParserDelegate.parseBeanDefinitionElement： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Nullablepublic BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) &#123; String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List&lt;String&gt; aliases = new ArrayList&lt;&gt;(); if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; ........ &#125; //检查beanName是否重复 if (containingBean == null) &#123; checkNameUniqueness(beanName, aliases, ele); &#125; //&lt;bean&gt;标签解析的核心方法，重要程度5 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); if (beanDefinition != null) &#123; if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123; beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; beanName = this.readerContext.generateBeanName(beanDefinition); String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; aliases.add(beanClassName); &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Neither XML &#x27;id&#x27; nor &#x27;name&#x27; specified - &quot; + &quot;using generated bean name [&quot; + beanName + &quot;]&quot;); &#125; &#125; catch (Exception ex) &#123; error(ex.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; return null;&#125; 首先获取id和name（也就是bean的别名），别名逗号切割，放入List&lt;String&gt; aliases中 而id会赋值给beanName引用，这也是bean的名字，而name会作为bean的别名，一直到 1AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); 该方法是&lt;bean&gt;标签解析的核心方法，会创建一个BeanDefinition对象，这个BeanDefinition的类型为GenericBeanDefinition parseBeanDefinitionElement： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374 public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, @Nullable BeanDefinition containingBean) &#123; this.parseState.push(new BeanEntry(beanName)); String className = null; if (ele.hasAttribute(CLASS_ATTRIBUTE)) &#123; className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); &#125; String parent = null; if (ele.hasAttribute(PARENT_ATTRIBUTE)) &#123; parent = ele.getAttribute(PARENT_ATTRIBUTE); &#125; try &#123; //创建GenericBeanDefinition对象 AbstractBeanDefinition bd = createBeanDefinition(className, parent); //解析bean标签的属性，并把解析出来的属性设置到BeanDefinition对象中 parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); //解析bean中的meta标签 parseMetaElements(ele, bd); //解析bean中的lookup-method标签 重要程度：2，可看可不看 parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); //解析bean中的replaced-method标签 重要程度：2，可看可不看 parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); //解析bean中的constructor-arg标签 重要程度：2，可看可不看 parseConstructorArgElements(ele, bd); //解析bean中的property标签 重要程度：2，可看可不看 parsePropertyElements(ele, bd); //可以不看，用不到 parseQualifierElements(ele, bd); bd.setResource(this.readerContext.getResource()); bd.setSource(extractSource(ele)); return bd; &#125; ...... finally &#123; this.parseState.pop(); &#125; return null; &#125;//createBeanDefinition(className, parent) protected AbstractBeanDefinition createBeanDefinition(@Nullable String className, @Nullable String parentName) throws ClassNotFoundException &#123; return BeanDefinitionReaderUtils.createBeanDefinition( parentName, className, this.readerContext.getBeanClassLoader()); &#125;//BeanDefinitionReaderUtils.createBeanDefinition(// parentName, className, this.readerContext.getBeanClassLoader()); public static AbstractBeanDefinition createBeanDefinition( @Nullable String parentName, @Nullable String className, @Nullable ClassLoader classLoader) throws ClassNotFoundException &#123; GenericBeanDefinition bd = new GenericBeanDefinition(); bd.setParentName(parentName); if (className != null) &#123; if (classLoader != null) &#123; bd.setBeanClass(ClassUtils.forName(className, classLoader)); &#125; else &#123; bd.setBeanClassName(className); &#125; &#125; return bd; &#125; 好了，终于看到GenericBeanDefinition bd = new GenericBeanDefinition()了 下图是GenericBeanDefinition的重要属性属性 好了，现在已经获取到了BeanDefinition，并把BeanDefinition在封装成了BeanDefinitionHolder了，那最后就是注册了吧，这步放到自定义标签（包扫描）上讲。 自定义标签自定义标签解析 如：&lt;context:component-scan base-package&#x3D;”com.xiangxue.jack”&#x2F;&gt;自定义标签解析流程： a、根据当前解析标签的头信息找到对应的namespaceUri b、加载spring所有jar中的spring.handlers文件。并建立映射关系 c、根据namespaceUri从映射关系中找到对应的实现了NamespaceHandler接口的类 d、调用NamespaceHandler类的init方法，init方法是注册了各种自定义标签的解析类 e、根据namespaceUri找到对应的解析类，然后调用paser方法完成标签解析 回到DefaultBeanDefinitionDocumentReader#parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate)， 12345678910111213141516171819202122protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; if (delegate.isDefaultNamespace(root)) &#123; NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) &#123; //默认标签解析 parseDefaultElement(ele, delegate); &#125; else &#123; //自定义标签解析 delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125;&#125; 看delegate.parseCustomElement(ele)，该方法就是去解析自定义标签的。 可以看到自定义标签的解析是交给了BeanDefinitionParserDelegate完成，看 BeanDefinitionParserDelegate#parseCustomElement 123456789101112131415public BeanDefinition parseCustomElement(Element ele) &#123; return parseCustomElement(ele, null);&#125;public BeanDefinition parseCustomElement(Element ele, @Nullable BeanDefinition containingBd) &#123; String namespaceUri = getNamespaceURI(ele); if (namespaceUri == null) &#123; return null; &#125; NamespaceHandler handler = this.readerContext.getNamespaceHandlerResolver().resolve(namespaceUri); if (handler == null) &#123; ....... &#125; return handler.parse(ele, new ParserContext(this.readerContext, this, containingBd));&#125; 有个namespaceUri，这个就是这些值 看下方法 1NamespaceHandler handler = this.readerContext.getNamespaceHandlerResolver().resolve(namespaceUri); NamespaceHandlerResolver的默认实现是DefaultNamespaceHandlerResolver DefaultNamespaceHandlerResolver#resolve： 1234567891011121314151617181920212223242526272829public NamespaceHandler resolve(String namespaceUri) &#123; //获取spring中所有jar包里面的 &quot;META-INF/spring.handlers&quot;文件，并且建立映射关系 Map&lt;String, Object&gt; handlerMappings = getHandlerMappings(); //根据namespaceUri：http://www.springframework.org/schema/context，获取到这个命名空间的处理类 Object handlerOrClassName = handlerMappings.get(namespaceUri); ........ else &#123; String className = (String) handlerOrClassName; try &#123; Class&lt;?&gt; handlerClass = ClassUtils.forName(className, this.classLoader); if (!NamespaceHandler.class.isAssignableFrom(handlerClass)) &#123; ...... &#125; NamespaceHandler namespaceHandler = (NamespaceHandler) BeanUtils.instantiateClass(handlerClass); //调用处理类的init方法，在init方法中完成标签元素解析类的注册 namespaceHandler.init(); //这里还做了缓存，第一次获取的是clasName，通过className来创建对象，第二次去获取的时候就能直接获取到对应的namespaceHandler对象了 handlerMappings.put(namespaceUri, namespaceHandler); return namespaceHandler; &#125; catch (ClassNotFoundException ex) &#123; ...... &#125; catch (LinkageError err) &#123; ...... &#125; &#125;&#125; 方法：Map&lt;String, Object&gt; handlerMappings = getHandlerMappings();该方法会去获所有的”META-INF&#x2F;spring.handlers”文件，并解析，然后建立映射关系(这让我想到java SPI和Dubbo SPI) 看下该方法的返回值： 好了这就是Spring的SPI了，和dubbo一样，都是使用key，value的方式 看下spring.handlers: 这些类都是继承了NamespaceHandlerSupport 方法registerBeanDefinitionParser的定义 可以看到registerBeanDefinitionParser其实就是把BeanDefinitionParser放入到了一个parsers的Map中。这些解析类是什么时候调用的呢？往下看。 上面已经获取了得到了对应关系&lt;自定义标签名,NamespaceHandler&gt;，接着执行了方法： 1Object handlerOrClassName = handlerMappings.get(namespaceUri); 就可以根据namespaceUri从映射关系中找到对应的实现了NamespaceHandler接口的类，一开始时，返回值是类的全限定名，这时就需要创建并缓存了。 接着就会创建该NamespaceHandler对象并执行到了NamespaceHandler的init方法 123456789101112try &#123; Class&lt;?&gt; handlerClass = ClassUtils.forName(className, this.classLoader); if (!NamespaceHandler.class.isAssignableFrom(handlerClass)) &#123; ...... &#125; NamespaceHandler namespaceHandler = (NamespaceHandler) BeanUtils.instantiateClass(handlerClass); //调用处理类的init方法，在init方法中完成标签元素解析类的注册 namespaceHandler.init(); handlerMappings.put(namespaceUri, namespaceHandler); return namespaceHandler;&#125; 从上边的介绍可知init方法是注册了各种自定义标签的解析类，把这些标签的解析类入了一个Map中。 好了，现在已经获取了自定义标签的NamespaceHandler，而且已经把对应的解析类都注册了，也就是说，比如xml文件这样写了 12345&lt;!--自定义标签--&gt;&lt;context:component-scan base-package=&quot;com.enjoy.jack&quot;&gt;&lt;/context:component-scan&gt;&lt;context:property-placeholder location=&quot;classpath:application.properties&quot;/&gt; 经过上面的步骤，我通过context，得到了uri，也就是这个 然后根据URI，并使用了SPI获取到了这个URI对应的NamespaceHandlerSupport，也就是ContextNamespaceHandler，而ContextNamespaceHandler的定义也很简单，就是重写了init方法， 接着就创建ContextNamespaceHandler对象并调用了这个init方法，把component-scan、property-placeholder等这些对应的BeanDefinitionParser做了一个对应关系，并放入了一个Map中。 最后就只剩下解析了，回到BeanDefinitionParserDelegate#parseCustomElement方法。该方法最后执行了 1handler.parse(ele, new ParserContext(this.readerContext, this, containingBd)); handler就是之前获取的ContextNamespaceHandler，也就是ContextNamespaceHandler。上面介绍了ContextNamespaceHandler的定义，他是没有重写parse方法的，所以这方法是从父类NamespaceHandlerSupport中继承过来的 看NamespaceHandlerSupport#parse 也就是根据标签获取对应的BeanDefinitionParser，接着执行了parse方法。 我们最关注的肯定是扫描标签: 所以根据上面的介绍可知component-scan对应的解析类是ComponentScanBeanDefinitionParser ComponentScanBeanDefinitionParser#parse 1234567891011121314151617public BeanDefinition parse(Element element, ParserContext parserContext) &#123; //获取basePackage属性 String basePackage = element.getAttribute(BASE_PACKAGE_ATTRIBUTE); basePackage = parserContext.getReaderContext().getEnvironment().resolvePlaceholders(basePackage); //可以用逗号分开 String[] basePackages = StringUtils.tokenizeToStringArray(basePackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS); //创建注解扫描器 // Actually scan for bean definitions and register them. ClassPathBeanDefinitionScanner scanner = configureScanner(parserContext, element); //扫描并把扫描的类封装成beanDefinition对象 核心方法，重要程度 5 Set&lt;BeanDefinitionHolder&gt; beanDefinitions = scanner.doScan(basePackages); registerComponents(parserContext.getReaderContext(), beanDefinitions, element); return null;&#125; 先获取basePackage属性，就是包路径，然后调用configureScanner，创建注解扫描器ClassPathBeanDefinitionScanner。 包扫描——ClassPathBeanDefinitionScanner看下该类的类图： 该类会去扫描有这些注解的类： @Component @Service @Repository @Configuration 看下ClassPathBeanDefinitionScanner的创建，useDefaultFilters默认就是true，不信自己看源码吧 123456789101112public ClassPathBeanDefinitionScanner(BeanDefinitionRegistry registry, boolean useDefaultFilters, Environment environment, @Nullable ResourceLoader resourceLoader) &#123; Assert.notNull(registry, &quot;BeanDefinitionRegistry must not be null&quot;); this.registry = registry; if (useDefaultFilters) &#123; registerDefaultFilters(); &#125; setEnvironment(environment); setResourceLoader(resourceLoader);&#125; useDefaultFilters默认情况下为true，重点看下registerDefaultFilters： 123456protected void registerDefaultFilters() &#123; this.includeFilters.add(new AnnotationTypeFilter(Component.class)); ClassLoader cl = ClassPathScanningCandidateComponentProvider.class.getClassLoader(); //不重要的方法 .........&#125; 逻辑很简单，就是把需要扫描的注解的class对象封装成AnnotationTypeFilter，然后放到了一个内部的集合includeFilters中 回到代码，这里就放了Component.class这个注解！和上面说的不同，那我们就看下其他注解的定义 其他3个注解都加了Component注解！也就是这些注解都能被识别 回到 ComponentScanBeanDefinitionParser.parse(Element element, ParserContext parserContext)方法 123456//创建注解扫描器// Actually scan for bean definitions and register them.ClassPathBeanDefinitionScanner scanner = configureScanner(parserContext, element);//扫描并把扫描的类封装成beanDefinition对象 核心方法，重要程度 5Set&lt;BeanDefinitionHolder&gt; beanDefinitions = scanner.doScan(basePackages);registerComponents(parserContext.getReaderContext(), beanDefinitions, element); 创建完了ClassPathBeanDefinitionScanner后就执行了doScan，方法，看下该方法： 1234567891011protected Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123; Assert.notEmpty(basePackages, &quot;At least one base package must be specified&quot;); Set&lt;BeanDefinitionHolder&gt; beanDefinitions = new LinkedHashSet&lt;&gt;(); for (String basePackage : basePackages) &#123; //扫描到有注解的类并封装成BeanDefinition对象 Set&lt;BeanDefinition&gt; candidates = findCandidateComponents(basePackage); ..... //还有其他代码，放在后面讲 &#125; return beanDefinitions;&#125; 重点看findCandidateComponents，该方法会扫描到有Component注解的类并封装成BeanDefinition对象 好，看scanCandidateComponents方法 先创建了一个Set&lt;BeanDefinition&gt; candidates = new LinkedHashSet&lt;&gt;(); 作用猜都猜到了，就是放包含注解的BeanDefintition的，看下resources有什么。 可以看到，resources就是指向一个类文件的对象。接着就是去遍历这些文件，并检查是否有Component注解。 看for循环的代码主要代码。 1234567891011121314151617MetadataReader metadataReader = getMetadataReaderFactory().getMetadataReader(resource);//如果类上面有includeFilters注解if (isCandidateComponent(metadataReader)) &#123; ScannedGenericBeanDefinition sbd = new ScannedGenericBeanDefinition(metadataReader); sbd.setSource(resource); if (isCandidateComponent(sbd)) &#123; if (debugEnabled) &#123; logger.debug(&quot;Identified candidate component class: &quot; + resource); &#125; candidates.add(sbd); &#125; else &#123; if (debugEnabled) &#123; logger.debug(&quot;Ignored because not a concrete top-level class: &quot; + resource); &#125; &#125;&#125; 代码很好理解，就是把类的的信息封装成MetadataReader对象，然后检查是否有对应的注解，有就创建一个ScannedGenericBeanDefinition对象。 这里先重点说下这段代码 1MetadataReader metadataReader = getMetadataReaderFactory().getMetadataReader(resource); getMetadataReaderFactory()返回的是CachingMetadataReaderFactory 接着getMetadataReader方法，这里 里面的源码不说，不过这里有个疑问，spring会不会去触发类的加载，如果有触发，那传个ClassLoader进去是可以理解的，因为一个类的最终确定，是要类加载器和类的全限定名来确定的，而如果有触发，那就会把类的静态存储结构转化为方法区的运行时数据结构，也就是说会占用内存，如果有大量的类是不使用的，那就浪费资源了。 我稍微看了这个getMetadataReader，没细看，而且之前生产BeanDefinition的时候是没触发类的加载的。所以猜测是用了和javap类似的技术，去解析类的静态存储结构里的数据。 回到遍历的代码，现在看下MetadataReader对象的数据 回到代码， isCandidateComponent(metadataReader)这个方法匹配会匹配是否有对应的注解，有就会去创建ScannedGenericBeanDefinition 看下ScannedGenericBeanDefinition的类图: 它是继承了GenericBeanDefinition，里面就多了个AnnotationMetadata属性，也就是保存了注解的属性 注意在这一步只是传入AnnotationMetadata属性后就放入之前创建的集合中了，类中注解里面的属性值和其他注解信息还没设置。 回到ClassPathBeanDefinitionScanner#doScan方法 12345678910111213141516171819202122232425protected Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123; //省略了部分代码 ..... //扫描到有注解的类并封装成BeanDefinition对象 Set&lt;BeanDefinition&gt; candidates = findCandidateComponents(basePackage); for (BeanDefinition candidate : candidates) &#123; //bean的范围 ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(candidate); candidate.setScope(scopeMetadata.getScopeName()); String beanName = this.beanNameGenerator.generateBeanName(candidate, this.registry); if (candidate instanceof AbstractBeanDefinition) &#123; postProcessBeanDefinition((AbstractBeanDefinition) candidate, beanName); &#125; // 用注解的信息来完善BeanDefinition if (candidate instanceof AnnotatedBeanDefinition) &#123; //支持了@Lazy @DependOn注解 AnnotationConfigUtils.processCommonDefinitionAnnotations((AnnotatedBeanDefinition) candidate); &#125; //省略的部分代码 .... 代码1 &#125; //省略了部分代码 .....&#125; 可以看到，之前讲bean标签的时候没说，String中bean的范围默认是singleton也就是单例。 上面的代码就是解析注解，然后对BeanDefinition的属性数据填充，bean的范围啊、是否懒加载。具体看BeanDefinition的信息图 好了，BeanDefinition的初始化就完成了，现在还差个注册。 beanDefinition注册到BeanFactory对象中 这里的BeanFactory真实类型就是一开始创建测DefaultListableBeanFactory，而该类又实现了BeanDefinitionRegistry接口，所以这里涉及到的BeanDefinitionRegistry的对应引用都是DefaultListableBeanFactory类型的对象 默认标签的BeanDefinition和自定义标签的BeanDefinition在创建完后都要注册器BeanFactory中。 这里以自定义标签的BeanDefinition的注册来讲。 代码接着上面doScan代码 最后检查BeanDefinition是否已经已经注册了，没有就去注册。执行代码1(是在doScan的for循环里面的) 12345678// 检查是否已经注册过了if (checkCandidate(beanName, candidate)) &#123; BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(candidate, beanName); definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry); beanDefinitions.add(definitionHolder); //BeanDefinition注册 registerBeanDefinition(definitionHolder, this.registry);&#125; 又是包装，这里会把把BeanDefinition，包装成了一个BeanDefinitionHolder，然后执行了 registerBeanDefinition(definitionHolder, this.registry); 这个BeanDefinitionRegistry的实现是DefaultListableBeanFactory！终于看到BeanFactory了！ DefaultListableBeanFactory实现了BeanDefinitionRegistry这个接口，看名字就知道作用是什么了。 看DefaultListableBeanFactory#registerBeanDefinition方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Overridepublic void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123; //省略校验代码 ..... //先判断BeanDefinition是否已经注册 BeanDefinition existingDefinition = this.beanDefinitionMap.get(beanName); if (existingDefinition != null) &#123; //是否允许覆盖 if (!isAllowBeanDefinitionOverriding()) &#123; throw new BeanDefinitionOverrideException(beanName, beanDefinition, existingDefinition); &#125; //只是打印信息而已 ..... this.beanDefinitionMap.put(beanName, beanDefinition); &#125; else &#123; if (hasBeanCreationStarted()) &#123; // Cannot modify startup-time collection elements anymore (for stable iteration) synchronized (this.beanDefinitionMap) &#123; this.beanDefinitionMap.put(beanName, beanDefinition); List&lt;String&gt; updatedDefinitions = new ArrayList&lt;&gt;(this.beanDefinitionNames.size() + 1); updatedDefinitions.addAll(this.beanDefinitionNames); updatedDefinitions.add(beanName); this.beanDefinitionNames = updatedDefinitions; removeManualSingletonName(beanName); &#125; &#125; else &#123; //把beanDefinition缓存到map中 // Still in startup registration phase this.beanDefinitionMap.put(beanName, beanDefinition); //把beanName放到beanDefinitionNames list中，这个list着重记住，bean实例化的时候需要用到 this.beanDefinitionNames.add(beanName); removeManualSingletonName(beanName); &#125; this.frozenBeanDefinitionNames = null; &#125; if (existingDefinition != null || containsSingleton(beanName)) &#123; resetBeanDefinition(beanName); &#125; else if (isConfigurationFrozen()) &#123; clearByTypeCache(); &#125;&#125; 注册实际上就是放入DefaultListableBeanFactory中的beanDefinitionMap中，并且把beanDefinition对应的名字放入DefaultListableBeanFactory的beanDefinitionNames的集合中。而这个beanName，默认规则是类的名字，头字母小写。 这里涉及到两个集合beanDefinitionMap和beanDefinitionNames，看下这两个属性的定义 好了，现在BeanDefinition已经注册到beanFactory中了，看下之后的代码 别名也去注册了，不过别名是和beanName做映射的。(这让我想起了Innodb中聚集索引和非聚集索引的关系) 也就是说，通过别名找bean，实际上是找到了bean的beanName，然后才找到bean的 现在已经完成了BeanDefinition的注册了，回到调用的地方ComponentScanBeanDefinitionParser#parse 注册完BeanDefinition后，还执行了一行 1registerComponents(parserContext.getReaderContext(), beanDefinitions, element); 这代码的核心是在 由于这是扫描注解的，annotationConfig是为true的，所以会执行AnnotationConfigUtils#registerAnnotationConfigProcessors添加各种的PostProcessor，而重点的PostProcessor如下： 添加ConfigurationClassPostProcessor 添加AutowiredAnnotationBeanPostProcessor 添加CommonAnnotationBeanPostProcessor 上面的3处代码都会检查下刚刚注册的beanDefinition中有没有名字为下面的： org.springframework.context.annotation.internalConfigurationBeanNameGenerator org.springframework.context.annotation.internalAutowiredAnnotationProcessor org.springframework.context.annotation.internalCommonAnnotationProcessor 这些的BeanDefinition如果没有就创建对应的RootBeanDefinition 123RootBeanDefinition def = new RootBeanDefinition(AutowiredAnnotationBeanPostProcessor.class);RootBeanDefinition def = new RootBeanDefinition(CommonAnnotationBeanPostProcessor.class);RootBeanDefinition def = new RootBeanDefinition(ConfigurationClassPostProcessor.class); 然后把创建好的RootBeanDefinition作为后置处理器（BeanDefinitionRegistryPostProcessor）注册到注册器beanFactory中。 这三个类很重要，和之后的流程有关，这里就记下是在什么时候到把这些PostProcessor注册到beanFactory中的： CommonAnnotationBeanPostProcessor.class AutowiredAnnotationBeanPostProcessor.class ConfigurationClassPostProcessor.class 看下各个类的类图 xml解析和BeanDefinition封装核心方法 扫描的流程图","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"00在SpringBoot项目中实现自己的WebMvcConfigurer需要注意的","slug":"spring/00在SpringBoot项目中实现自己的WebMvcConfigurer需要注意的","date":"2021-11-25T12:00:06.000Z","updated":"2022-03-23T09:03:53.875Z","comments":true,"path":"blog/spring/00在SpringBoot项目中实现自己的WebMvcConfigurer需要注意的/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/00%E5%9C%A8SpringBoot%E9%A1%B9%E7%9B%AE%E4%B8%AD%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%B7%B1%E7%9A%84WebMvcConfigurer%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84/","excerpt":"","text":"我在一个Spring boot的项目中实现了自己的WebMvcConfigurer，并重写了自己的configureMessageConverters。如下 123456789101112131415161718public class WebConfig implements WebMvcConfigurer &#123; @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; FastJsonHttpMessageConverter converter = new FastJsonHttpMessageConverter(); FastJsonConfig config = converter.getFastJsonConfig(); config.setDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); config.setSerializerFeatures(SerializerFeature.PrettyFormat); converter.setDefaultCharset(UTF_8); List&lt;MediaType&gt; supportedMediaTypes = new ArrayList&lt;&gt;(); supportedMediaTypes.add(MediaType.APPLICATION_JSON); supportedMediaTypes.add(MediaType.APPLICATION_JSON_UTF8); converter.setSupportedMediaTypes(supportedMediaTypes);// converters.add(new StringHttpMessageConverter()); converters.add(converter); &#125;&#125; 但返回结果中仍然把null都返回了，比如: 1234&#123; &quot;res&quot;: null, &quot;data&quot;: 1&#125; 也就是在对返回结果解析时使用的HttpMessageConverter并不是我定义的FastJsonHttpMessageConverter。结合SpringMVC的知识（只关注注解的），参数解析器和结果解析器中的用到的HttpMessageConverter是在RequestMappingHandlerAdapter创建的时候set进去的： 而configureMessageConverters是钩子方法，他的实现是在DelegatingWebMvcConfiguration中的，无论Spring还是Spring boot都是一样的。debug： 可以看到，这时已经有值了，也就是说在在执行WebConfig的configureMessageConverters之前，已经执行了某个WebMvcConfigurer的configureMessageConverters了，这就去看DelegatingWebMvcConfiguration的configureMessageConverters了： 可以看到，我定义的WebMvcConfigurer前面有个WebMvcAutoConfiguration，就是先执行了这个类的configureMessageConverters方法，添加了很多默认的HttpMessageConverter，而这个类中有这样一个注解： 是用来排序的，也就是说，如果想要我定义的在第一位，那只需要添加@Order(Ordered.HIGHEST_PRECEDENCE)上这个注解就好了 @Order的值可以是 &lt; Ordered.HIGHEST_PRECEDENCE+10 也可以修改自己的configureMessageConverters方法，最后代码改成这样converters.add(0, converter);修改后： 123456789101112131415@Overridepublic void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; FastJsonHttpMessageConverter converter = new FastJsonHttpMessageConverter(); FastJsonConfig config = converter.getFastJsonConfig(); config.setDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); config.setSerializerFeatures(SerializerFeature.PrettyFormat); converter.setDefaultCharset(UTF_8); List&lt;MediaType&gt; supportedMediaTypes = new ArrayList&lt;&gt;(); supportedMediaTypes.add(MediaType.APPLICATION_JSON); supportedMediaTypes.add(MediaType.APPLICATION_JSON_UTF8); converter.setSupportedMediaTypes(supportedMediaTypes); converters.add(0, converter);&#125;","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"00-Spring中重要的接口","slug":"spring/00-Spring中重要的接口","date":"2021-11-25T12:00:05.000Z","updated":"2022-03-23T09:03:53.853Z","comments":true,"path":"blog/spring/00-Spring中重要的接口/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/00-Spring%E4%B8%AD%E9%87%8D%E8%A6%81%E7%9A%84%E6%8E%A5%E5%8F%A3/","excerpt":"","text":"BeanDefinitionRegistryPostProcessor解析看这里02-Spinrg的后置处理器-BeanDefinitionRegistryPostProcessor调用时机 先看下定义： 从refresh，可以看到在执行invokeBeanFactoryPostProcessors方法的时候，这个时间短只是完成了BeanDefinition的创建和注册到BeanFactory中而已。而且从接口BeanDefinitionRegistryPostProcessor定义的抽象方法可以看到，它们都是带着注册中心的，也就是说，这个借口的实现类完全可以获取到之前创建并注册完成的BeanDefinition！那我们可以做一些扩展，比如自己再注册个BeanDefinition或者修改下BeanDefinition也可以去实现扫描我们自定义的注解！也就是要增删改查BeanDefinition时可以实现这个接口！ 这里定义了两个抽象方法，不过这两个方法的调用时机是不同的。从方法名可以了解一些信息 postProcessBeanDefinitionRegistry的重点是对BeanDefinition的处理 postProcessBeanFactory的重点是对BeanFactory对象属性的修改 参考类： 12345678910111213141516171819202122232425262728293031323334353637383940@Componentpublic class BeanPro implements BeanDefinitionRegistryPostProcessor, PriorityOrdered, Ordered &#123; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; //查询BeanDefinition final String[] beanDefinitionNames = registry.getBeanDefinitionNames(); for (String beanDefinitionName : beanDefinitionNames) &#123; BeanDefinition beanDefinition = registry.getBeanDefinition(beanDefinitionName); System.out.println(beanDefinition); &#125; GenericBeanDefinition genericBeanDefinition = new GenericBeanDefinition(); genericBeanDefinition.setBeanClass(BeanDefinitionBean.class); MutablePropertyValues propertyValues = genericBeanDefinition.getPropertyValues(); propertyValues.add(&quot;name&quot;,&quot;Jack&quot;); registry.registerBeanDefinition(&quot;beanDefinitionBean&quot;,genericBeanDefinition);// ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(registry);// //需要过滤的注解// scanner.addIncludeFilter(new AnnotationTypeFilter(MyService.class));// scanner.scan(&quot;com.enjoy.jack.customBean&quot;); &#125; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; String[] beanDefinitionNames = registry.getBeanDefinitionNames(); DefaultListableBeanFactory beanFactory1 = (DefaultListableBeanFactory)beanFactory; beanFactory1.setAllowBeanDefinitionOverriding(true); beanFactory1.setAllowCircularReferences(true); beanFactory1.setAllowRawInjectionDespiteWrapping(true); &#125; @Override public int getOrder() &#123; return 0; &#125;&#125; SmartInstantiationAwareBeanPostProcessor这个是看这里实例化带有@Autowired注解的构造函数 这个接口出了上边的作用还有 引用循环依赖的解决过程 MergedBeanDefinitionPostProcessor这个是看这里收集注解–applyMergedBeanDefinitionPostProcessors InstantiationAwareBeanPostProcessor这个看这里ioc di（控制反转），依赖注入的核心方法—-populateBean DestructionAwareBeanPostProcessor看registerDisposableBeanIfNecessary—bean销毁前需要做的操作类DisposableBeanAdapter AbstractAutowireCapableBeanFactory记住这个类，它定义基于注解的的bean的初始化阶段的整个流程。 BeanPostProcessor非常重要的一个接口，贯穿了Spring的bean的初始化流程和功能的扩展 看这这么多BeanPostProcessor了，都是在循环中的，而且都是对一个Bean循环的，可以看到，如果某个类对对应的方法感兴趣了，就在对应的方法了做一些对类的操作，不感兴趣了就直接返回就好了。而且，BeanPostProcessor这种使用方式感觉就是使用了装饰器模式，虽然没有装饰器模式的形，但有装饰器模式模式的魂—-给原始类添加增强功能 ApplicationListener12345678910@Component@Order(2)public class EnjoyApplicationListener1 implements ApplicationListener &#123; @Override public void onApplicationEvent(ApplicationEvent event) &#123; if(event instanceof EnjoyEvent) &#123; System.out.println(&quot;==EnjoyApplicationListener1&quot;); &#125; &#125;&#125; 或者使用注解 Spring Bean完成依赖注入后调用的接口按顺序（有部顺序是不对的）： BeanNameAware BeanClassLoaderAware BeanFactoryAware InitDestroyAnnotationBeanPostProcessor#postProcessBeforeInitialization—-处理@PostConstruct EnvironmentAware EmbeddedValueResolverAware ResourceLoaderAware ApplicationEventPublisherAware MessageSourceAware ApplicationContextAware ImportAware比如这样写了一个类 是触发不到这段代码的，需要这样写也就是说只有通过@Import()注解注入的类才能调用到ImportAware#setImportMetadata方法。这个原因就是在收集注解的BeanPostProcessor，也就是ConfigurationClassPostProcessor来完成收集，这个过程是发生在BeanDefinition创建阶段的。完成了Spring bean的依赖注入（DI)阶段后，也就是调用AbstractAutowireCapableBeanFactory#initializeBean中时，会调用ImportAwareBeanPostProcessor接口的postProcessBeforeInitialization方法来完成ImportAware接口的调用。 ServletContextAware，ServletConfigAware InitializingBean 调用init-method配置的方法 调用方法ApplicationListenerDetector#postProcessAfterInitialization把之前注册的事件–ApplicationListener注册到事件管理器中 DisposableBean，AutoCloseable在JVM关闭前，调用的方法。 Spring Bean初始化完成后调用的接口方法 SmartInitializingSingleton 在初始化入口方法DefaultListableBeanFactory#preInstantiateSingletons 执行完getBean后： ImportSelector、DeferredImportSelector和ImportBeanDefinitionRegistrar看07-Spring中基于注解–ConfigurationClassPostProcessor 都是引入类，交给Spring管理","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"00-Spring中的BeanDefinition","slug":"spring/00-Spring中的BeanDefinition","date":"2021-11-25T12:00:04.000Z","updated":"2022-03-23T09:03:53.777Z","comments":true,"path":"blog/spring/00-Spring中的BeanDefinition/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/00-Spring%E4%B8%AD%E7%9A%84BeanDefinition/","excerpt":"","text":"BeanDefinition的重要属性 这里解析下BeanDefinition，BeanDefinition在Spring中有很多实现，先下图： 每个BeanDefinition，有些许差别，但目的都是一样的，就是作为spring bean实例化前的载体，保存bean的原始信息。 RootBeanDefinition是在需要实例化时，就会把其他类型的BeanDefinition转化成该类。相当于其他BeanDefinition的汇总。在AbstractBeanFactory#doGetBean（也就是beanFactory.getBean）时会调用 GenericBeanDefinition 在默认标签bean解析完后把对应的信息封装成该对象，这个是一个通用的BeanDefinition，其他BeanDefinition都会继承它 ScannedGenericBeanDefinition 通过扫描得到就会创建该对象，特点就是有注解的信息 AnnotatedGenericBeanDefinition 需要获取注解信息时会把类封装成这个BeanDefinition @Import进来的和内部类都会把这些封装成这个BeanDefinition ConfigurationClassBeanDefinition这个是在@Bean封装成BeanDefinition时会创建这个，不过这个类是私有的内部类。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"00-Spring中一些重要的图","slug":"spring/00-Spring中一些重要的图","date":"2021-11-25T12:00:03.000Z","updated":"2022-03-23T09:03:53.747Z","comments":true,"path":"blog/spring/00-Spring中一些重要的图/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/00-Spring%E4%B8%AD%E4%B8%80%E4%BA%9B%E9%87%8D%E8%A6%81%E7%9A%84%E5%9B%BE/","excerpt":"","text":"","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"00-DeferredImportSelector接口的作用、初始化和执行原理","slug":"spring/00-DeferredImportSelector接口的作用、初始化和执行原理","date":"2021-11-25T12:00:02.000Z","updated":"2022-03-23T09:03:53.716Z","comments":true,"path":"blog/spring/00-DeferredImportSelector接口的作用、初始化和执行原理/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/00-DeferredImportSelector%E6%8E%A5%E5%8F%A3%E7%9A%84%E4%BD%9C%E7%94%A8%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86/","excerpt":"","text":"DeferredImportSelector接口的作用、初始化和执行原理实例化该接口是在实例化是发生在ConfigurationClassParser#processImports方法中的。在该方法执行了来实例化 12ImportSelector selector = ParserStrategyUtils.instantiateClass(candidateClass, ImportSelector.class, this.environment, this.resourceLoader, this.registry); ImportSelector是DeferredImportSelector的父类 看ParserStrategyUtils.instantiateClass的源码 123456789101112131415161718// ParserStrategyUtilsstatic &lt;T&gt; T instantiateClass(Class&lt;?&gt; clazz, Class&lt;T&gt; assignableTo, Environment environment, ResourceLoader resourceLoader, BeanDefinitionRegistry registry) &#123; Assert.notNull(clazz, &quot;Class must not be null&quot;); Assert.isAssignable(assignableTo, clazz); if (clazz.isInterface()) &#123; throw new BeanInstantiationException(clazz, &quot;Specified class is an interface&quot;); &#125; ClassLoader classLoader = (registry instanceof ConfigurableBeanFactory ? ((ConfigurableBeanFactory) registry).getBeanClassLoader() : resourceLoader.getClassLoader()); // 这个方法会检查clazz的构造函数是否有一个，如果是一个的话，还会检查该构造的参数是否有 // BeanFactory(也就是BeanDefinitionRegistry)，Environment和ResourceLoader // 有的话就会把对应的对象作为参数传入来完成对应的实例化 T instance = (T) createInstance(clazz, environment, resourceLoader, registry, classLoader); ParserStrategyUtils.invokeAwareMethods(instance, environment, resourceLoader, registry, classLoader); return instance;&#125; 通过createInstance方法可知，通过定义如下类 123456789101112131415161718192021// ImportSelector接口和ImportBeanDefinitionRegistrar接口的实现类也支持这中模式public class DeferredImportSelectorDemo implements DeferredImportSelector &#123; /** * 可以通过强制类型转换，转成需要的beanFactory */ private BeanFactory beanFactory; private Environment environment; public DeferredImportSelectorDemo(BeanFactory beanFactory, Environment environment) &#123; this.beanFactory = beanFactory; this.environment = environment; &#125; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; System.out.println(&quot;=====DeferredImportSelectorDemo.selectImports&quot;); return new String[]&#123;SelectImportBean.class.getName()&#125;; &#125;&#125; 通过提供一个带有BeanFactory或Environment参数的构造方法，来注入BeanFactory和Environment。通过这种模式和DeferredImportSelector接口方法的执行时机，可以实现一些特殊的功能 注意，这种注入方式要生效的前提是——只能提供一个构造方法 ImportSelector接口和ImportBeanDefinitionRegistrar接口的实现类也支持这中模式 或者ParserStrategyUtils.invokeAwareMethods可知： 1234567891011121314151617181920212223242526272829303132@Orderpublic class DeferredImportSelectorDemo implements DeferredImportSelector, BeanClassLoaderAware, BeanFactoryAware, EnvironmentAware, Ordered &#123; /** * 可以通过强制类型转换，转成需要的beanFactory */ private BeanFactory beanFactory; private Environment environment; @Override public void setBeanClassLoader(ClassLoader classLoader) &#123; &#125; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; this.beanFactory = beanFactory; &#125; @Override public void setEnvironment(Environment environment) &#123; this.environment = environment; &#125; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; System.out.println(&quot;=====DeferredImportSelectorDemo.selectImports&quot;); return new String[]&#123;SelectImportBean.class.getName()&#125;; &#125;&#125; 也可以通过如上的定义来完成BeanFactory和Environment的注入。 作用作用有两点： 对于有依赖关系的类，延迟一些类的BeanDefinition创建时机。也就是说实现如下流程。 这个作用之所以能实现，是由于该接口的方法的执行时机。这一点后面讲。 12检查某个类的BeanDefinition是否存在，存在的话就不引入某个类，或者把引入的类作为某个类的属性。通过某个环境变量的或配置的值，来判断某个类是否需要引入 单从该接口提供的方法看，是完成不了上边的流程的，因为没有BeanFactory和Environment这个入参。但可以通过提供提供一个构造方法，该构造方法有BeanFactory和Environment这两个参数，通过这种模式来引入这两个对象。还可以通过实现BeanClassLoaderAware, BeanFactoryAware, EnvironmentAware接口来引入对应的属性 按组来引入类，而且这些组也是支持排序的，只需在定义的类上加上@Order或实现Ordered等排序结构，就支持组的排序了。 这样就可以控制方法调用的顺序 ，从而控制类注册到Spring容器的时机。这样就可以起到了控制类间的依赖关系。 调用时机该接口的方法调用时机是发生在ConfigurationClassParser.parse方法的最用： 123456//ConfigurationClassParser public void parse(Set&lt;BeanDefinitionHolder&gt; configCandidates) &#123; .... // 这行代码不能忽视 this.deferredImportSelectorHandler.process();&#125; 在这时调用就意味着配置已经加载完成；已经通过包扫描，把扫描到的类有生成BeanDefinition，并注册到Spring容器中了；已经把某些注解比如@Import、@bean和@ImportResource等注解的信息保存到ConfigurationClass中，但还没有执行对应方法。 执行原理12345678910111213141516// DeferredImportSelectorHandlerpublic void process() &#123; List&lt;DeferredImportSelectorHolder&gt; deferredImports = this.deferredImportSelectors; this.deferredImportSelectors = null; try &#123; if (deferredImports != null) &#123; DeferredImportSelectorGroupingHandler handler = new DeferredImportSelectorGroupingHandler(); deferredImports.sort(DEFERRED_IMPORT_COMPARATOR); deferredImports.forEach(handler::register); handler.processGroupImports(); &#125; &#125; finally &#123; this.deferredImportSelectors = new ArrayList&lt;&gt;(); &#125;&#125; DeferredImportSelectorHolder其实就是DeferredImportSelector和ConfigurationClass的包装类 创建完DeferredImportSelectorGroupingHandler后，对之前收集的DeferredImportSelectorHolder列表做排序，排序个规则依赖于该Hodler里面的DeferredImportSelector对象 123private static final Comparator&lt;DeferredImportSelectorHolder&gt; DEFERRED_IMPORT_COMPARATOR = (o1, o2) -&gt; AnnotationAwareOrderComparator.INSTANCE.compare(o1.getImportSelector(), o2.getImportSelector());也就是说，依赖于@Order注解、Order等排序接口 排序完后对DeferredImportSelectorHolder列表遍历 遍历执行DeferredImportSelectorGroupingHandler.register 123456789101112// DeferredImportSelectorGroupingHandlerpublic void register(DeferredImportSelectorHolder deferredImport) &#123; //调用getImportGroup方法，返回实现了Group接口的类 Class&lt;? extends Group&gt; group = deferredImport.getImportSelector().getImportGroup(); //建立实现了Group接口类和DeferredImportSelectorGrouping的映射关系 DeferredImportSelectorGrouping grouping = this.groupings.computeIfAbsent( (group != null ? group : deferredImport), key -&gt; new DeferredImportSelectorGrouping(createGroup(group))); grouping.add(deferredImport); this.configurationClasses.put(deferredImport.getConfigurationClass().getMetadata(), deferredImport.getConfigurationClass());&#125; 这代码很简单，做了两件事 分组。先通过DeferredImportSelector.getImportGroup获取到分组的依据，如果没有重写该方法，那么就使用DeferredImportSelectorHolder对象本身。最终形成的结果就是形成如下关系： 1234567891011121314151617181920212223242526272829Class&lt;Group1&gt;=&gt;DeferredImportSelectorGrouping(Group1) &#123; Group group = Group1; List&lt;DeferredImportSelectorHolder&gt; deferredImports = &#123; Group1.DeferredImportSelectorHolder1, Group1.DeferredImportSelectorHolder2, &#125;&#125;;Class&lt;Group2&gt;=&gt;DeferredImportSelectorGrouping(Group2) &#123; Group group = Group2; List&lt;DeferredImportSelectorHolder&gt; deferredImports = &#123; Group2.DeferredImportSelectorHolder2, Group2.DeferredImportSelectorHolder3, &#125;&#125;;DeferredImportSelectorHolder1=&gt;DeferredImportSelectorGrouping(null) &#123; Group group = DefaultDeferredImportSelectorGroup1; List&lt;DeferredImportSelectorHolder&gt; deferredImports = &#123; DeferredImportSelectorHolder1, &#125;&#125;;DeferredImportSelectorHolder2=&gt;DeferredImportSelectorGrouping(null) &#123; Group group = DefaultDeferredImportSelectorGroup2; List&lt;DeferredImportSelectorHolder&gt; deferredImports = &#123; DeferredImportSelectorHolder2, &#125;&#125; 最后把这个关系保存在groupings这个Map中 通过DeferredImportSelectorHolder获取把这个DeferredImportSelector对象对应的ConfigurationClass对象后，保存在了configurationClasses这个Map中。为之后的生成BeanDefinition做准备 最后执行了DeferredImportSelectorGroupingHandler.processGroupImports方法 12345678910111213141516171819202122232425262728293031323334// DeferredImportSelectorGroupingHandlerpublic void processGroupImports() &#123; for (DeferredImportSelectorGrouping grouping : this.groupings.values()) &#123; Predicate&lt;String&gt; exclusionFilter = grouping.getCandidateFilter(); //这里调用了 group.selectImports() grouping.getImports().forEach(entry -&gt; &#123; ConfigurationClass configurationClass = this.configurationClasses.get(entry.getMetadata()); try &#123; //又递归处理每一个返回的Entry processImports(configurationClass, asSourceClass(configurationClass, exclusionFilter), Collections.singleton(asSourceClass(entry.getImportClassName(), exclusionFilter)), exclusionFilter, false); &#125; catch (BeanDefinitionStoreException ex) &#123; throw ex; &#125; catch (Throwable ex) &#123; throw new BeanDefinitionStoreException( &quot;Failed to process import candidates for configuration class [&quot; + configurationClass.getMetadata().getClassName() + &quot;]&quot;, ex); &#125; &#125;); &#125;&#125;// DeferredImportSelectorGroupingpublic Iterable&lt;Group.Entry&gt; getImports() &#123; for (DeferredImportSelectorHolder deferredImport : this.deferredImports) &#123; this.group.process(deferredImport.getConfigurationClass().getMetadata(), deferredImport.getImportSelector()); &#125; //在这里调用了实现了Group接口的selectImports方法 return this.group.selectImports();&#125; 上边方法的核心就是getImports方法 这个方法遍历上边初始化的groupingsMap。 了解了groupings这个Map的关系后，那processGroupImports的源码就很好理解了，就是按组来完成Group对象的process方法调用，然后调用Group.selectImports的调用，遍历该方法的返回值，最后把类的信息封保存到ConfigurationClass对象，收集完信息后，保存在了configurationClasses这个Map中。为之后的BeanDefinition做准备。 这里有两种情况，也就是对应下面的关系 1234567891011121314Class&lt;Group1&gt;=&gt;DeferredImportSelectorGrouping(Group1) &#123; Group group = Group1; List&lt;DeferredImportSelectorHolder&gt; deferredImports = &#123; Group1.DeferredImportSelectorHolder1, Group1.DeferredImportSelectorHolder2, &#125;&#125;;DeferredImportSelectorHolder1=&gt;DeferredImportSelectorGrouping() &#123; Group group = DefaultDeferredImportSelectorGroup1; List&lt;DeferredImportSelectorHolder&gt; deferredImports = &#123; DeferredImportSelectorHolder1, &#125;&#125;; 如果DeferredImportSelector的实现类没有重写getImportGroup，对应的groupings&#96;关系如下 123456DeferredImportSelectorHolder1=&gt;DeferredImportSelectorGrouping() &#123; Group group = DefaultDeferredImportSelectorGroup1; List&lt;DeferredImportSelectorHolder&gt; deferredImports = &#123; DeferredImportSelectorHolder1, &#125;&#125;; 那么在这里的Group等于DefaultDeferredImportSelectorGroup1，看下该类的逻辑 12345678910111213141516private static class DefaultDeferredImportSelectorGroup implements Group &#123; private final List&lt;Entry&gt; imports = new ArrayList&lt;&gt;(); @Override public void process(AnnotationMetadata metadata, DeferredImportSelector selector) &#123; for (String importClassName : selector.selectImports(metadata)) &#123; this.imports.add(new Entry(metadata, importClassName)); &#125; &#125; @Override public Iterable&lt;Entry&gt; selectImports() &#123; return this.imports; &#125;&#125; 其实就是执行了该组的DeferredImportSelector的selectImports方法，把这些方法的返回值收集起来，最后把收集的结果作为Group.selectImports的返回值。 总结：在这种关系下，需要引入的类依赖于DeferredImportSelector类的selectImports方法的返回值 如果DeferredImportSelector的实现类重写getImportGroup方法，并返回了一个Group接口的实现类 ，对应的关系如下 1234567Class&lt;Group1&gt;=&gt;DeferredImportSelectorGrouping(Group1) &#123; Group group = Group1; List&lt;DeferredImportSelectorHolder&gt; deferredImports = &#123; Group1.DeferredImportSelectorHolder1, Group1.DeferredImportSelectorHolder2, &#125;&#125;; 这种关系下，需要引入的类就依赖于Group接口的实现类的selectImports方法了。而该方法的返回值时可以使用Group#process方法来进行一些处理。 总结 12345678910111213@Component//Import虽然是实例化一个类，Import进来的类可以实现一些接口@Import(&#123;DeferredImportSelectorDemo.class,AwareBean.class&#125;)//@ImportResource(&quot;classpath:spring.xml&quot;)public class ImportBean &#123;&#125;public class DeferredImportSelectorDemo implements DeferredImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; System.out.println(&quot;=====DeferredImportSelectorDemo.selectImports&quot;); return new String[]&#123;SelectImportBean.class.getName()&#125;; &#125;&#125; 这种情况下SelectImportBean类就能被SpringIOC容器管理 12345678910111213141516171819202122232425262728293031323334353637public class DeferredImportSelectorDemo implements DeferredImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; System.out.println(&quot;=====DeferredImportSelectorDemo.selectImports&quot;); return new String[]&#123;SelectImportBean.class.getName()&#125;; &#125; /** * 要返回一个实现了Group接口的类 */ @Override public Class&lt;? extends Group&gt; getImportGroup() &#123; return DeferredImportSelectorGroupDemo.class; &#125; private static class DeferredImportSelectorGroupDemo implements DeferredImportSelector.Group &#123; List&lt;Entry&gt; list = new ArrayList&lt;&gt;(); /** 收集需要实例化的类 */ @Override public void process(AnnotationMetadata metadata, DeferredImportSelector selector) &#123; System.out.println(&quot;=====DeferredImportSelectorGroupDemo.process&quot;); String[] strings = selector.selectImports(metadata); for (String string : strings) &#123; list.add(new Entry(metadata,string)); &#125; &#125; @Override public Iterable&lt;Entry&gt; selectImports() &#123; System.out.println(&quot;=====DeferredImportSelectorGroupDemo.selectImports&quot;); return list; &#125; &#125;&#125; 这种情况下，引入的类就看DeferredImportSelectorGroupDemo的selectImports的返回值。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"00 Spring源码中经常出现的工具类","slug":"spring/00 Spring源码中经常出现的工具类","date":"2021-11-25T12:00:01.000Z","updated":"2022-03-23T09:03:53.713Z","comments":true,"path":"blog/spring/00 Spring源码中经常出现的工具类/","link":"","permalink":"http://sv.pointcut.cc/blog/spring/00%20Spring%E6%BA%90%E7%A0%81%E4%B8%AD%E7%BB%8F%E5%B8%B8%E5%87%BA%E7%8E%B0%E7%9A%84%E5%B7%A5%E5%85%B7%E7%B1%BB/","excerpt":"","text":"BeanUtils 对Bean的操作类 12345# 对根据class对象实例化对象instantiateClasscopyProperties(Object source, Object target, String... ignoreProperties)findPropertyType(String propertyName, Class&lt;?&gt;... beanClasses)findPropertyForMethod(Method method, Class&lt;?&gt; clazz) BeanDefinitionBuilder 用来创建BeanDefinition的工具类 AnnotationUtils 注解工具类 AnnotationAttributes 用来保存注解的属性 PropertiesLoaderUtils 用来读取配置文件 ReflectionUtils 这个是Spring中的反射工具类 ClassUtils 12//这个是用来获取原始方法，因为method有可能是接口的方法，而通过这个方法能获取到具体类上的方法。Method specificMethod = ClassUtils.getMostSpecificMethod(method, targetClass) AopUtils 12345//拿原始方法对象，这个方法上才有注解Method specificMethod = AopUtils.getMostSpecificMethod(method, targetClass);//选择可调用的方法Method invocableMethod = AopUtils.selectInvocableMethod(method, userType); AnnotatedElementUtils 这个可以获取注解中的属性值 1234567// 判断Clazz对象中是否有Controller注解。AnnotatedElementUtils.hasAnnotation(beanType, Controller.class)//获取类上注解的信息RequestMapping requestMapping = AnnotatedElementUtils.findMergedAnnotation(element, RequestMapping.class);//或者AnnotationAttributes attributes = AnnotatedElementUtils.getMergedAnnotationAttributes(target, Switch.class); 获取注解信息——AnnotationMetadata 在源码中，通过ClassPathBeanDefinitionScanner扫描类时会获取到类的注解信息，而这些类的注解信息Spring是通过类SimpleMetadataReaderFactory获取的，而SimpleMetadataReaderFactory的内部又是这样获取的 12345private static final int PARSING_OPTIONS = ClassReader.SKIP_DEBUG | ClassReader.SKIP_CODE | ClassReader.SKIP_FRAMES;SimpleAnnotationMetadataReadingVisitor visitor = new SimpleAnnotationMetadataReadingVisitor(classLoader);new ClassReader(InputStream).accept(visitor, PARSING_OPTIONS);AnnotationMetadata annotationMetadata = visitor.getMetadata(); 在开发中可以使用Spring已经封装好的类这样获取： 12345FileSystemResource fileSystemResource = new FileSystemResource(&quot;class文件的绝对路径&quot;);//这个是带缓存的MetadataReaderFactory，如果不需要缓存可以使用CachingMetadataReaderFactory cachingMetadataReaderFactory = new CachingMetadataReaderFactory();MetadataReader metadataReader = cachingMetadataReaderFactory.getMetadataReader(fileSystemResource);AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata(); 看测试方法： 123456789101112131415161718192021222324@Testpublic void test1() &#123; FileSystemResource fileSystemResource = new FileSystemResource(&quot;E:\\\\idea\\\\xiangxueedu-vip-3\\\\spring-source\\\\target\\\\classes\\\\com\\\\enjoy\\\\jack\\\\bean\\\\Jack.class&quot;); CachingMetadataReaderFactory cachingMetadataReaderFactory = new CachingMetadataReaderFactory(); try &#123; MetadataReader metadataReader = cachingMetadataReaderFactory.getMetadataReader(fileSystemResource); System.out.println(metadataReader); Set&lt;MethodMetadata&gt; annotatedMethods = metadataReader.getAnnotationMetadata().getAnnotatedMethods(Component.class.getName()); for (MethodMetadata annotatedMethod : annotatedMethods) &#123; System.out.println(annotatedMethod.getMethodName() + &quot;--&quot; + annotatedMethod.getReturnTypeName()); &#125; Map&lt;String, Object&gt; annotationAttributes = metadataReader.getAnnotationMetadata().getAnnotationAttributes(Component.class.getName(), false); System.out.println(annotationAttributes); MergedAnnotations annotations = metadataReader.getAnnotationMetadata().getAnnotations(); System.out.println(annotations); MergedAnnotation&lt;Component&gt; componentMergedAnnotation = annotations.get(Component.class); System.out.println(componentMergedAnnotation); AnnotationAttributes annotationAttributes1 = componentMergedAnnotation.asAnnotationAttributes(MergedAnnotation.Adapt.ANNOTATION_TO_MAP); System.out.println(annotationAttributes1); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 也可以使用这种模式 123AnnotationMetadata metadata = AnnotationMetadata.introspect(ScanBean.class);Set&lt;String&gt; sets = metadata.getAnnotationTypes();Map&lt;String, Object&gt; map = metadata.getAnnotationAttributes(PropertySource.class.getName(), false); 获取类上注解的属性 AnnotationConfigUtils在源码中，通过ClassPathBeanDefinitionScanner扫描类时同这样的方式获取注解上的属性： 123456789101112131415161718//AnnotationScopeMetadataResolverpublic ScopeMetadata resolveScopeMetadata(BeanDefinition definition) &#123; ScopeMetadata metadata = new ScopeMetadata(); if (definition instanceof AnnotatedBeanDefinition) &#123; AnnotatedBeanDefinition annDef = (AnnotatedBeanDefinition) definition; AnnotationAttributes attributes = AnnotationConfigUtils.attributesFor( annDef.getMetadata(), this.scopeAnnotationType); if (attributes != null) &#123; metadata.setScopeName(attributes.getString(&quot;value&quot;)); ScopedProxyMode proxyMode = attributes.getEnum(&quot;proxyMode&quot;); if (proxyMode == ScopedProxyMode.DEFAULT) &#123; proxyMode = this.defaultProxyMode; &#125; metadata.setScopedProxyMode(proxyMode); &#125; &#125; return metadata;&#125; 或者，某些注解在使用上有父子关系的 比如ComponentScans和ComponentScan，可以通过 12Set&lt;AnnotationAttributes&gt; componentScans = AnnotationConfigUtils.attributesForRepeatable( sourceClass.getMetadata(), ComponentScans.class, ComponentScan.class) 来获取ComponentScan注解集合。 TransactionSynchronizationManager——这个是在使用@Transaction注解的时候，在方法内能通过TransactionSynchronizationManager来获取链接的信息，或链接等，比如： 1ConnectionHolder connectionHolder = (ConnectionHolder)TransactionSynchronizationManager.getResource(dataSource); 更多的看这里14-Spring中使用事物 RequestContextHolder——这个是在SpringMVC中，用来获取Request对象的 123ServletRequestAttributes servletRequestAttributes = (ServletRequestAttributes)RequestContextHolder.getRequestAttributes();HttpServletRequest request = servletRequestAttributes.getRequest(); WebUtils——SpringWeb中的工具类，获取请求链接或者Cookie等 12static Cookie getCookie(HttpServletRequest request, String name) static String findParameterValue(ServletRequest request, String name) UrlPathHelper——SpringWeb中的工具类，有很多对HttpServletRequest操作的方法比如： 123UrlPathHelper uriPathHelper = new UrlPathHelper();//获取请求路径，比如http://127.0.0.1:9090/common/query2?areaCode=1，通过这个方法后返回/common/query2String lookupPath = uriPathHelper.getLookupPathForRequest(request); DefaultParameterNameDiscoverer——获取参数名称 这个类实现了接口ParameterNameDiscoverer，这个接口的定义也很简单 123456789public interface ParameterNameDiscoverer &#123; @Nullable String[] getParameterNames(Method method); @Nullable String[] getParameterNames(Constructor&lt;?&gt; ctor);&#125; 就是获取方法的参数名称，和构造器的参数名称 使用也很简单 1ParameterNameDiscoverer parameterNameDiscoverer = new DefaultParameterNameDiscoverer(); CollectionUtils 看名字就知道，就是对集合的操作工具类 MethodIntrospector——可以用来获取某个类的Method对象。使用：(SpringMVC中的AbstractHandlerMethodMapping中) 123456789101112Map&lt;Method, T&gt; methods = MethodIntrospector.selectMethods(userType, (MethodIntrospector.MetadataLookup&lt;T&gt;) method -&gt; &#123; try &#123; return getMappingForMethod(method, userType); &#125; catch (Throwable ex) &#123; 。。。。。。 &#125; &#125;);if (logger.isTraceEnabled()) &#123; logger.trace(formatMappings(userType, methods));&#125; BeanFactoryUtils AnnotationUtils ProxyFactory 创建代理对象，看源码AbstractAutoProxyCreator的createProxy方法 12345678910111213141516171819202122232425262728293031323334protected Object createProxy(Class&lt;?&gt; beanClass, @Nullable String beanName, @Nullable Object[] specificInterceptors, TargetSource targetSource) &#123; if (this.beanFactory instanceof ConfigurableListableBeanFactory) &#123; AutoProxyUtils.exposeTargetClass((ConfigurableListableBeanFactory) this.beanFactory, beanName, beanClass); &#125; ProxyFactory proxyFactory = new ProxyFactory(); //把AnnotationAwareAspectJAutoProxyCreator中的某些属性copy到proxyFactory中 proxyFactory.copyFrom(this); if (!proxyFactory.isProxyTargetClass()) &#123; if (shouldProxyTargetClass(beanClass, beanName)) &#123; proxyFactory.setProxyTargetClass(true); &#125; else &#123; evaluateProxyInterfaces(beanClass, proxyFactory); &#125; &#125; //组装advisor Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); //把advisor加入到proxyFactory proxyFactory.addAdvisors(advisors); //把targetSource对象加入到proxyFactory proxyFactory.setTargetSource(targetSource); customizeProxyFactory(proxyFactory); proxyFactory.setFrozen(this.freezeProxy); if (advisorsPreFiltered()) &#123; proxyFactory.setPreFiltered(true); &#125; //获取代理对象 return proxyFactory.getProxy(getProxyClassLoader());&#125; MethodParameter 获取方法的参数 12MethodParameter par = nwe MethodParameter(method, 0) ResolvableType 获取方法参数的范型类型 12MethodParameter returnType = nwe MethodParameter(method, -1)Class&lt;?&gt; bodyType = ResolvableType.forMethodParameter(returnType).getGeneric().resolve(); StandardAnnotationMetadata 1new StandardAnnotationMetadata(Object.class); 获取类的注解信息","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"}]},{"title":"Netty内存管理","slug":"netty/netty源码/Netty内存管理","date":"2021-11-23T12:00:22.000Z","updated":"2022-03-23T09:03:58.246Z","comments":true,"path":"blog/netty/netty源码/Netty内存管理/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/netty%E6%BA%90%E7%A0%81/Netty%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","excerpt":"","text":"为了提高内存的使用效率，Netty引入了jemalloc内存分配算法。Netty内存管理层级结构如图6-1所示，其中右边是内存管理的3个层级，分别是本地线程缓存、分配区arena、系统内存；左边是内存块区域，不同大小的内存块对应不同的分配区，总体的分配策略如下。 为了避免线程间锁的竞争和同步，每个I&#x2F;O线程都对应一个PoolThreadCache，负责当前线程使用非大内存的快速申请和释放。 当从PoolThreadCache中获取不到内存时，就从PoolArena的内存池中分配。当内存使用完并释放时，会将其放到PoolThreadCache中，方便下次使用。若从PoolArena的内存池中分配不到内存，则从堆内外内存中申请，申请到的内存叫PoolChunk。当内存块的大小默认为12MB时，会被放入PoolArea的内存池中，以便重复利用。当申请大内存时（超过了PoolChunk的默认内存大小12MB），直接在堆外或堆内内存中创建（不归PoolArea管理），用完后直接回收。 PooledByteBufPooledByteBuf 是池化的 ByteBuf，提高了内存分配与释放的速度，它本身是一个抽象泛 型类，有三个子类:PooledDirectByteBuf、PooledHeapByteBuf、PooledUnsafeDirectByteBuf。 三个子类在操作上和其他的 ByteBuf 没有太大的区别，关键在于内存池化技术上。 Jemalloc算法Netty 的 PooledByteBuf 采用与 jemalloc 一致的内存分配算法。基本思路可用这样的情景 类比，想像一下当前电商的配送流程。当顾客采购小件商品(比如书籍)时，直接从同城仓 库送出;当顾客采购大件商品(比如电视)时，从区域仓库送出;当顾客采购超大件商品(比 如汽车)时，则从全国仓库送出。Netty 的分配算法与此相似。 Netty 中，Tiny 和 Small 类型的请求都首先从同城仓库(ThreadCache-tcache)送出;如 果同城仓库没有，则会从区域仓库(PoolArena)送出，Normal 类型的请求则从区域仓库 (PoolArena)送出，Huge 类型的请求则从全国仓库(系统内存)送出。 Netty 中规定: 内存分配的最小单位为 16B。 &lt; 512B 的请求为 Tiny，&lt; 512B&lt;X&lt; 8KB(PageSize)的请求为 Small， 8KB&lt;&#x3D;X&lt;&#x3D; 16MB(ChunkSize)的请求为 Normal，&gt; 16MB(ChunkSize)的请求为 Huge。 Tiny、Small、Normal、Huge 中还有细层级，&lt; Tiny 的请求以 16B 为起点每次增加 16B 作为一个层级，也就是， Tiny 中还有 16B、32B、48B、……480B、496B 的层级; Small 中还有 512B、1KB、2KB、4KB 的层级; Normal 中还有 8KB、16KB、32KB、……8MB、16MB 的层级; Huge 中还有 32MB、64KB……的层级。 不管请求的大小，都会将向上规范化，比如:请求分配 511B、512B、513B，将依次 规范化为 512B、512B、1KB。 为了提高内存分配效率并减少内部碎片，jemalloc 算法将 Arena 切分为小块 Chunk，根 据每块的内存使用率又将小块组合为以下几种状态:QINIT，Q0，Q25，Q50，Q75，Q100。 Chunk 块可以在这几种状态间随着内存使用率的变化进行转移，内存使用率和状态转移可参 见下图: 其中横轴表示内存使用率(百分比)，纵轴表示状态，可以看到: QINIT 的内存使用率为[0,25)、Q0 为(0,50)、Q100 为[100,100]等等。 Chunk 块的初始状态为 QINIT，当使用率达到 25 时转移到 Q0 状态，再次达到 50 时转 移到 Q25，依次类推直到 Q100;当内存释放时又从 Q100 转移到 Q75，直到 Q0 状态且内存 使用率为 0 时，该 Chunk 从 Arena 中删除。 像 qInit、q000、q075 因为本身要维护很多 Chunk 块，所以内部是以链表的形式来组织 Chunk 块，同时 qInit、q000、q075 本身又组织为一个近似的双向链表，如图: 虽然已将 Arena 切分为小块 Chunk，但实际上 Chunk 是相当大的内存块，在 Netty 默认 使用 16MB。为了进一步提高内存利用率并减少内部碎片，需要继续将 Chunk 切分为小的块 Page。一个典型的切分将 Chunk 切分为 2048 块，可知 Page 的大小为:16MB&#x2F;2048&#x3D;8KB。一 个好的内存分配算法，应使得已分配内存块尽可能保持连续，这将大大减少内部碎片，由此 jemalloc 使用伙伴分配算法尽可能提高连续性。 伙伴分配算法的基本思想是: 我们知道，一个 Chunk 切分为 2048 块 Page，将这些 Page 作为叶子节点，然后组织起一个满二叉树 然后按层分配满足要求的内存块。 以待分配序列 8KB、16KB、8KB 为例分析分配过程(每个 Page 大小 8KB): 8KB–需要一个 Page，第 11 层满足要求，故分配 2048 节点即 Page0; 16KB–需要两个 Page，故需要在第 10 层进行分配，而 1024 的子节点 2048 已分配，从 左到右找到满足要求的 1025 节点，故分配节点 1025 即 Page2 和 Page3; 8KB–需要一个Page，第11层满足要求，2048已分配，从左到右找到2049节点即Page1 进行分配。 分配结束后，已分配连续的 Page0-Page3，这样的连续内存块，大大减少内部碎片并提 高内存使用率。 为了实现伙伴算法，Netty 中使用了 使用两个字节数组 memoryMap 和 depthMap 来表示两棵二叉树，其中 MemoryMap 存 放分配信息，depthMap 存放节点的高度信息。 左图表示每个节点的编号，注意从 1 开始，省略 0 是因为这样更容易计算父子关系:子 节点加倍，父节点减半，比如 512 的子节点为 1024&#x3D;512 * 2。右图表示每个节点的深度，注 意从 0 开始。在代表二叉树的数组中，左图中节点上的数字作为数组索引即 id，右图节点上的数字作为值。初始状态时，memoryMap 和 depthMap 相等，可知一个 id 为 512 节点的初 始值为 9 memoryMap[512] &#x3D; depthMap[512] &#x3D; 9; depthMap 的值初始化后不再改变，memoryMap 的值则随着节点分配而改变。当一个节 点被分配以后，该节点的值设置为 12(最大高度+1)表示不可用，并且会更新祖先节点的 值。下图表示随着 4 号节点分配而更新祖先节点的过程，其中每个节点的第一个数字表示节 点编号，第二个数字表示节点高度值。 分配过程如下: 4 号节点被完全分配，将高度值设置为 12 表示不可用。 4 号节点的父亲节点即 2 号节点，将高度值更新为两个子节点的较小值;其他祖先节点 亦然，直到高度值更新至根节点。 可推知，memoryMap 数组的值有如下三种情况: memoryMap[id] &#x3D; depthMap[id] – 该节点没有被分配 memoryMap[id] &gt; depthMap[id] – 至少有一个子节点被分配，不能再分配该高度满足的内存，但可以根据实际分配较小一些的内存。比如，上图中分配了 4 号子节点，2号节点，值从 1 更新为 2，表示该节点不能再分配 8MB 的只能最大分配 4MB 内存，因为分配了 4 号 节点后只剩下 5 号节点可用。 mempryMap[id] &#x3D; 最大高度 + 1(本例中 12) – 该节点及其子节点已被完全分配， 没有剩余空间。 前面我们说过，一个 page 是 8KB，但是 Netty 又支持 Tiny、Small 这种小于 8KB，最小 可达 16B 的内存分配请求，每次都分配一个 page，很浪费。为了应对这种需求，需要进一 步切分 Page 成更小的 SubPage。SubPage 是 jemalloc 中内存分配的最小单位，不能再进行切 分。SubPage 切分的单位并不固定，以第一次请求分配的大小为单位(最小切分单位为 16B)。 比如，第一次请求分配 32B，则 Page 按照 32B 均等切分为 256 块;第一次请求 16B，则 Page 按照 16B 均等切分为 512 块。为了便于内存分配和管理，根据 SubPage 的切分单位进行分组， d 对每个组而言，Arena 会以双向链表的形式进行管理。 那么根据切分的单位的大小和 Page 的大小，SubPage 分为 2 类:tinySubpagePools 和 smallSubpagePools，tinySubpagePools 中的 SubPage 的大小，从 16 字节到 496 个字节，共有 32 个元素，smallSubpagePools 则有 512 字节、1024、2048、4096，共有 4 个元素。 总的来说，Arena 中维护的数据结构如下: 在 Arena 数量上，为了减少各个线程进行内存分配时竞争，Netty 中会有多个 Arena， 默认的数量与处理器的个数有关。线程首次分配内存时，首先会为其分配一个固定的 Arena。 PoolThreadCache同时在 Netty 中为了提升性能，并不会一开始就从 PoolArena 中分配，因为 Arena 为几 个线程共享，而是先从每个线程自己的 PoolThreadCache 中去获取。当然开始的时候，这些 Cache 里面都是没有值的，要先从 PoolArena 中获取，当释放 Buf 的时候，才会把之前分配 的内存大小放到该 cache 里面，当下次要申请内存的时候，就会先从 PoolThreadCache 中找。 PoolThreadCache 中则维护了 6 个这样的线程缓存区域，3 个堆内存相关，3 个直接内存 相关，分别对应着三种分配内存的大小。 small 类型数组的大小(为 4), 而 tiny、normal 数组的大小分别分 32、 3。 smallSubPageHeapCaches 数组长度为 4， 依次缓存[512K, 1024k, 2048k, 4096k]大小的缓 存, 每个的元素对应的缓存 queue 中元素个数不能超过 256 个; 而 tinySubPageHeapCaches 数组缓存的是[16B, 32B, … , 496B]大小的内存块, 每个元素对应的缓存 queue 中元素个数 不能超过 512 个。normalHeapCaches 数组结构相同, 但是只缓存[8k, 16k, 32k]大小的内存块, 每个元素对应的缓存 queue 中元素个数不超过 64 个。 每一个 MemoryRegionCache 中又包含一个队列，队列中的每个元素类型为 Entry，Entry 中又包含了一个 PoolChunk，以方便对内存的管理。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"Netty 源码分析之 四 Promise 与 Future","slug":"netty/netty源码/Netty 源码分析之 四 Promise 与 Future","date":"2021-11-23T12:00:21.000Z","updated":"2022-03-23T09:03:58.225Z","comments":true,"path":"blog/netty/netty源码/Netty 源码分析之 四 Promise 与 Future/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/netty%E6%BA%90%E7%A0%81/Netty%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8B%20%E5%9B%9B%20Promise%20%E4%B8%8E%20Future/","excerpt":"","text":"Future和Promise的关系 Java原生FutureJava并发编程包下提供了Future接口。Future在异步编程中表示该异步操作的结果，通过Future的内部方法可以实现状态检查、取消执行、获取执行结果等操作。内部的方法如下： 1234567891011// 尝试取消执行 boolean cancel(boolean mayInterruptIfRunning); // 是否已经被取消执行 boolean isCancelled(); // 是否已经执行完毕 boolean isDone(); // 阻塞获取执行结果 V get() throws InterruptedException, ExecutionException; // 阻塞获取执行结果或超时后返回 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; Netty对Future的扩展原生的Future功能比较有限，Netty扩展了Future并增加了以下方法： 增加了更加丰富的状态判断方法 1234// 判断是否执行成功boolean isSuccess();// 判断是否可以取消执行boolean isCancellable(); 支持获取导致I&#x2F;O操作异常 1Throwable cause(); 增加了监听回调有关方法，支持future完成后执行用户指定的回调方法 12345678// 增加回调方法Future&lt;V&gt; addListener(GenericFutureListener&lt;? extends Future&lt;? super V&gt;&gt; listener);// 增加多个回调方法Future&lt;V&gt; addListeners(GenericFutureListener&lt;? extends Future&lt;? super V&gt;&gt;... listeners);// 删除回调方法Future&lt;V&gt; removeListener(GenericFutureListener&lt;? extends Future&lt;? super V&gt;&gt; listener);// 删除多个回调方法Future&lt;V&gt; removeListeners(GenericFutureListener&lt;? extends Future&lt;? super V&gt;&gt;... listeners); 增加了更丰富的阻塞等待结果返回的两类方法。其中一类是sync方法，阻塞等待结果且如果执行失败后向外抛出导致失败的异常；另外一类是await方法，仅阻塞等待结果返回，不向外抛出异常。 123456789// 阻塞等待，且如果失败抛出异常Future&lt;V&gt; sync() throws InterruptedException;// 同上，区别是不可中断阻塞等待过程Future&lt;V&gt; syncUninterruptibly(); // 阻塞等待Future&lt;V&gt; await() throws InterruptedException;// 同上，区别是不可中断阻塞等待过程Future&lt;V&gt; awaitUninterruptibly(); PromisePromise接口继续继承了Future，并增加若干个设置状态并回调的方法： 12345678// 设置成功状态并回调Promise&lt;V&gt; setSuccess(V result);boolean trySuccess(V result);// 设置失败状态并回调Promise&lt;V&gt; setFailure(Throwable cause);boolean tryFailure(Throwable cause);// 设置为不可取消状态boolean setUncancellable(); 可见，Promise作为一个特殊的Future，只是增加了一些状态设置方法。所以它常用于传入I&#x2F;O业务代码中，用于I&#x2F;O结束后设置成功（或失败）状态，并回调方法。 通过Promise设置I&#x2F;O执行结果以客户端连接的注册过程为例，调用链路如下： 12345io.netty.bootstrap.Bootstrap.connect()--&gt; io.netty.bootstrap.Bootstrap.doResolveAndConnect()----&gt;io.netty.bootstrap.AbstractBootstrap.initAndRegister()------&gt;io.netty.channel.MultithreadEventLoopGroup.register()--------&gt;io.netty.channel.SingleThreadEventLoop.register() 一直跟踪到SingleThreadEventLoop中，会看到这段代码： 1234@Overridepublic ChannelFuture register(Channel channel) &#123; return register(new DefaultChannelPromise(channel, this));&#125; 此处新建了一个DefaultChannelPromise，构造函数传入了当前的channel以及当前所在的线程this。从第一节的类图我们知道，DefaultChannelPromise同时实现了Future和Promise，具有上述提到的所有方法。 然后继续将该promise传递进另外一个register方法中： 123456@Overridepublic ChannelFuture register(final ChannelPromise promise) &#123; ObjectUtil.checkNotNull(promise, &quot;promise&quot;); promise.channel().unsafe().register(this, promise); return promise;&#125; 在该register方法中，继续将promise传递到Unsafe的register方法中，而立即返回了以ChannelFuture的形式返回了该promise。显然这里是一个异步回调处理：上层的业务可以拿到返回的ChannelFuture阻塞等待结果或者设置回调方法，而继续往下传的Promise可以用于设置执行状态并且回调设置的方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public final void register(EventLoop eventLoop, final ChannelPromise promise) &#123; ObjectUtil.checkNotNull(eventLoop, &quot;eventLoop&quot;); if (isRegistered()) &#123; // 如果已经注册过，则置为失败 promise.setFailure(new IllegalStateException(&quot;registered to an event loop already&quot;)); return; &#125; if (!isCompatible(eventLoop)) &#123; // 如果线程类型不兼容，则置为失败 promise.setFailure( new IllegalStateException(&quot;incompatible event loop type: &quot; + eventLoop.getClass().getName())); return; &#125; AbstractChannel.this.eventLoop = eventLoop; if (eventLoop.inEventLoop()) &#123; register0(promise); &#125; else &#123; try &#123; eventLoop.execute(new Runnable() &#123; @Override public void run() &#123; register0(promise); &#125; &#125;); &#125; catch (Throwable t) &#123; logger.warn( &quot;Force-closing a channel whose registration task was not accepted by an event loop: &#123;&#125;&quot;, AbstractChannel.this, t); closeForcibly(); closeFuture.setClosed(); // 出现异常情况置promise为失败 safeSetFailure(promise, t); &#125; &#125;&#125;private void register0(ChannelPromise promise) &#123; try &#123; // 注册之前，先将promise置为不可取消转态 if (!promise.setUncancellable() || !ensureOpen(promise)) &#123; return; &#125; boolean firstRegistration = neverRegistered; doRegister(); neverRegistered = false; registered = true; pipeline.invokeHandlerAddedIfNeeded(); // promise置为成功 safeSetSuccess(promise); pipeline.fireChannelRegistered(); if (isActive()) &#123; if (firstRegistration) &#123; pipeline.fireChannelActive(); &#125; else if (config().isAutoRead()) &#123; beginRead(); &#125; &#125; &#125; catch (Throwable t) &#123; closeForcibly(); closeFuture.setClosed(); // 出现异常情况置promise为失败 safeSetFailure(promise, t); &#125;&#125; 可见，底层的I&#x2F;O操作成功与否都可以通过Promise设置状态，并使得外层的ChannelFuture可以感知得到I&#x2F;O操作的结果。 通过ChannelFuture获取I&#x2F;O执行结果我们再来看看被返回的ChannelFuture的用途： 123456789101112131415// io.netty.bootstrap.AbstractBootstrap.java final ChannelFuture initAndRegister() &#123; //... ChannelFuture regFuture = config().group().register(channel); // 如果异常不为null，则意味着底层的I/O已经失败，并且promise设置了失败异常 if (regFuture.cause() != null) &#123; if (channel.isRegistered()) &#123; channel.close(); &#125; else &#123; channel.unsafe().closeForcibly(); &#125; &#125; return regFuture; &#125; 这里通过检查失败异常栈是否为空，可以提前检查到I&#x2F;O是否失败。继续回溯，还可以看到： 1234567891011121314151617 private ChannelFuture doBind(final SocketAddress localAddress) &#123; final ChannelFuture regFuture = initAndRegister(); final Channel channel = regFuture.channel(); if (regFuture.cause() != null) &#123; return regFuture; &#125; if (regFuture.isDone()) &#123; // 如果注册已经成功 ChannelPromise promise = channel.newPromise(); doBind0(regFuture, channel, localAddress, promise); return promise; &#125; else &#123; // 如果注册尚未完成 // ... &#125;&#125; 此处，通过ChannelFuture#isDone()方法可以知道底层的注册是否完成，如果完成，则继续进行bind操作。 但是因为注册是个异步操作，如果此时注册可能还没完成，那就会进入如下逻辑： 1234567891011121314151617181920212223//...else &#123; // Registration future is almost always fulfilled already, but just in case it&#x27;s not. final PendingRegistrationPromise promise = new PendingRegistrationPromise(channel); regFuture.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; Throwable cause = future.cause(); if (cause != null) &#123; // Registration on the EventLoop failed so fail the ChannelPromise directly to not cause an // IllegalStateException once we try to access the EventLoop of the Channel. promise.setFailure(cause); &#125; else &#123; // Registration was successful, so set the correct executor to use. // See https://github.com/netty/netty/issues/2586 promise.registered(); doBind0(regFuture, channel, localAddress, promise); &#125; &#125; &#125;); return promise;&#125; 这里新建了一个新的PendingRegistrationPromise，并为原来的ChannelFuture对象添加了一个回调方法，并在回调中更改PendingRegistrationPromise的状态，而且PendingRegistrationPromise会继续被传递到上层。当底层的Promise状态被设置并且回调，就会进入该回调方法。从而将I&#x2F;O状态继续向外传递。 DefaultChannelPromise的结果传递实现原理我们已经了解清楚了Promise和Future的异步模型。再来看看底层是如何实现的。以最常用的DefaultChannelPromise为例，内部非常简单，我们主要看它的父类DefaultPromise： 123456789101112// result字段的原子更新器@SuppressWarnings(&quot;rawtypes&quot;)private static final AtomicReferenceFieldUpdater&lt;DefaultPromise, Object&gt; RESULT_UPDATER = AtomicReferenceFieldUpdater.newUpdater(DefaultPromise.class, Object.class, &quot;result&quot;);// 缓存执行结果的字段private volatile Object result;// promise所在的线程private final EventExecutor executor;// 一个或者多个回调方法private Object listeners;// 阻塞线程数量计数器 private short waiters; 设置状态以设置成功状态为例（setSuccess）： 123456789101112131415161718192021222324252627282930@Overridepublic Promise&lt;V&gt; setSuccess(V result) &#123; if (setSuccess0(result)) &#123; // 调用回调方法 notifyListeners(); return this; &#125; throw new IllegalStateException(&quot;complete already: &quot; + this);&#125;private boolean setSuccess0(V result) &#123; return setValue0(result == null ? SUCCESS : result);&#125;private boolean setValue0(Object objResult) &#123; // 原子修改result字段为objResult if (RESULT_UPDATER.compareAndSet(this, null, objResult) || RESULT_UPDATER.compareAndSet(this, UNCANCELLABLE, objResult)) &#123; checkNotifyWaiters(); return true; &#125; return false;&#125;private synchronized void checkNotifyWaiters() &#123; if (waiters &gt; 0) &#123; // 如果有其他线程在等待该promise的结果，则唤醒他们 notifyAll(); &#125;&#125; 设置promise的状态其实就是原子地修改result字段为传入的执行结果。值得注意的是，result字段带有volatile关键字来确保多线程之间的可见性。另外，设置完毕状态后，会尝试唤醒所有在阻塞等待该promise返回结果的线程，而且会调用notifyListeners： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061private void notifyListeners() &#123; EventExecutor executor = executor(); if (executor.inEventLoop()) &#123; final InternalThreadLocalMap threadLocals = InternalThreadLocalMap.get(); final int stackDepth = threadLocals.futureListenerStackDepth(); if (stackDepth &lt; MAX_LISTENER_STACK_DEPTH) &#123; threadLocals.setFutureListenerStackDepth(stackDepth + 1); try &#123; notifyListenersNow(); &#125; finally &#123; threadLocals.setFutureListenerStackDepth(stackDepth); &#125; return; &#125; &#125; safeExecute(executor, new Runnable() &#123; @Override public void run() &#123; notifyListenersNow(); &#125; &#125;);&#125;private void notifyListenersNow() &#123; Object listeners; synchronized (this) &#123; // Only proceed if there are listeners to notify and we are not already notifying listeners. if (notifyingListeners || this.listeners == null) &#123; return; &#125; notifyingListeners = true; listeners = this.listeners; this.listeners = null; &#125; for (;;) &#123; if (listeners instanceof DefaultFutureListeners) &#123; notifyListeners0((DefaultFutureListeners) listeners); &#125; else &#123; notifyListener0(this, (GenericFutureListener&lt;?&gt;) listeners); &#125; synchronized (this) &#123; if (this.listeners == null) &#123; // Nothing can throw from within this method, so setting notifyingListeners back to false does not // need to be in a finally block. notifyingListeners = false; return; &#125; listeners = this.listeners; this.listeners = null; &#125; &#125;&#125;private void notifyListeners0(DefaultFutureListeners listeners) &#123; GenericFutureListener&lt;?&gt;[] a = listeners.listeners(); int size = listeners.size(); for (int i = 0; i &lt; size; i ++) &#123; notifyListener0(this, a[i]); &#125;&#125; 会调用添加的Listeners 其他设置状态方法不再赘言，基本上大同小异。 阻塞线程以等待执行结果上文提到其他线程会阻塞等待该promise返回结果，具体实现以sync方法为例： 12345678910111213141516171819202122232425262728293031323334353637@Overridepublic Promise&lt;V&gt; sync() throws InterruptedException &#123; // 阻塞等待 await(); // 如果有异常则抛出 rethrowIfFailed(); return this;&#125;@Overridepublic Promise&lt;V&gt; await() throws InterruptedException &#123; if (isDone()) &#123; // 如果已经完成，直接返回 return this; &#125; // 可以被中断 if (Thread.interrupted()) &#123; throw new InterruptedException(toString()); &#125; //检查死循环 checkDeadLock(); synchronized (this) &#123; while (!isDone()) &#123; // 递增计数器（用于记录有多少个线程在等待该promise返回结果） incWaiters(); try &#123; // 阻塞等待结果 wait(); &#125; finally &#123; // 递减计数器 decWaiters(); &#125; &#125; &#125; return this;&#125; 所有调用sync方法的线程，都会被阻塞，直到promise被设置为成功或者失败。这也解释了为何Netty客户端或者服务端启动的时候一般都会调用sync方法，本质上都是阻塞当前线程而异步地等待I&#x2F;O结果返回 回调机制12345678910111213141516171819202122232425262728@Overridepublic Promise&lt;V&gt; addListener(GenericFutureListener&lt;? extends Future&lt;? super V&gt;&gt; listener) &#123; checkNotNull(listener, &quot;listener&quot;); synchronized (this) &#123; // 添加回调方法 addListener0(listener); &#125; if (isDone()) &#123; // 如果I/O操作已经结束，直接触发回调 notifyListeners(); &#125; return this;&#125;private void addListener0(GenericFutureListener&lt;? extends Future&lt;? super V&gt;&gt; listener) &#123; if (listeners == null) &#123; // 只有一个回调方法直接赋值 listeners = listener; &#125; else if (listeners instanceof DefaultFutureListeners) &#123; // 将回调方法添加到DefaultFutureListeners内部维护的listeners数组中 ((DefaultFutureListeners) listeners).add(listener); &#125; else &#123; // 如果有多个回调方法，新建一个DefaultFutureListeners以保存更多的回调方法 listeners = new DefaultFutureListeners((GenericFutureListener&lt;?&gt;) listeners, listener); &#125;&#125; 从上边可以看到，添加回调方法完成之后，会立即检查promise是否已经完成；如果promise已经完成，则马上调用回调方法。 总结Netty的Promise和Future机制是基于Java并发包下的Future开发的。其中Future支持阻塞等待、添加回调方法、判断执行状态等，而Promise主要是支持状态设置相关方法。当底层I&#x2F;O操作通过Promise改变执行状态，我们可以通过同步等待的Future立即得到结果。 因此，就像永顺大牛标题所言，在Netty的异步模型里，Promise和Future就像是双子星一般紧密相连。但我觉得这两者更像是量子纠缠里的两个电子，因为改变其中一个方的状态，另外一方能够马上感知。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"Netty 源码分析之 二 ChannelPipeline","slug":"netty/netty源码/Netty 源码分析之 二 ChannelPipeline","date":"2021-11-23T12:00:20.000Z","updated":"2022-03-23T09:03:58.224Z","comments":true,"path":"blog/netty/netty源码/Netty 源码分析之 二 ChannelPipeline/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/netty%E6%BA%90%E7%A0%81/Netty%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8B%20%E4%BA%8C%20ChannelPipeline/","excerpt":"","text":"Channel 与 ChannelPipeline相信大家都知道了, 在 Netty 中每个 Channel 都有且仅有一个 ChannelPipeline 与之对应, 它们的组成关系如下: 通过上图我们可以看到, 一个 Channel 包含了一个 ChannelPipeline, 而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表. 这个链表的头是 HeadContext, 链表的尾是 TailContext, 并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler. 在Bootstrap的讲解中，已经讲解了Channel的初始化基本过程，下面的代码是 AbstractChannel 构造器: 12345protected AbstractChannel(Channel parent) &#123; this.parent = parent; unsafe = newUnsafe(); pipeline = new DefaultChannelPipeline(this);&#125; AbstractChannel 有一个 pipeline 字段, 在构造器中会初始化它为 DefaultChannelPipeline的实例. 这里的代码就印证了一点: 每个 Channel 都有一个 ChannelPipeline.首先进入到 DefaultChannelPipeline 构造器中: 123456789101112public DefaultChannelPipeline(AbstractChannel channel) &#123; if (channel == null) &#123; throw new NullPointerException(&quot;channel&quot;); &#125; this.channel = channel; tail = new TailContext(this); head = new HeadContext(this); head.next = tail; tail.prev = head;&#125; 在 DefaultChannelPipeline 构造器中, 首先将与之关联的 Channel 保存到字段 channel 中, 然后实例化两个 ChannelHandlerContext, 一个是 HeadContext 实例 head, 另一个是 TailContext 实例 tail. 接着将 head 和 tail 互相指向, 构成一个双向链表.特别注意到, 我们在开始的示意图中, head 和 tail 并没有包含 ChannelHandler, 这是因为 HeadContext 和 TailContext 继承于 AbstractChannelHandlerContext 的同时也实现了 ChannelHandler 接口了, 因此它们有 Context 和 Handler 的双重属性. ChannelInitializer 的添加还记得我们调用Bootstrap#connect后到了Channel的初始化吗: 123456final ChannelFuture initAndRegister() &#123; // 去掉非关键代码 final Channel channel = channelFactory().newChannel(); init(channel); ChannelFuture regFuture = config().group().register(channel);&#125; 重点就在init(channel); 1234567void init(Channel channel) &#123; ChannelPipeline p = channel.pipeline(); p.addLast(config.handler()); setChannelOptions(channel, newOptionsArray(), logger); setAttributes(channel, newAttributesArray());&#125; 在上面的代码中config.handler()就返回我们在上面创建的ChannelInitializer对象，并把它放入了Pipeline中，这个addLast就不在这里说的，执行完后Pipeline变成了这样。 那ChannelInitializer.channelRegistered这个方法又是有什么时候执行的呢？ 自定义 ChannelHandler 的添加过程在上一小节中, 我们已经分析了一个 ChannelInitializer 如何插入到 Pipeline 中的, 接下来就来探讨一下 ChannelInitializer 在哪里被调用, ChannelInitializer 的作用, 以及我们自定义的 ChannelHandler 是如何插入到 Pipeline 中的. 在 Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端) 一章的 channel 的注册过程 小节中, 我们已经分析过 Channel 的注册过程了, 这里我们再简单地复习一下: 首先在 AbstractBootstrap.initAndRegister中, 通过 group().register(channel), 调用 MultithreadEventLoopGroup.register 方法 在MultithreadEventLoopGroup.register 中, 通过 next() 获取一个可用的 SingleThreadEventLoop, 然后调用它的 register 在 SingleThreadEventLoop.register 中, 通过 channel.unsafe().register(this, promise) 来获取 channel 的 unsafe() 底层操作对象, 然后调用它的 register. 在 AbstractUnsafe.register 方法中, 调用 register0 方法注册 Channel 在 AbstractUnsafe.register0 中, 调用 AbstractNioChannel#doRegister 方法 AbstractNioChannel.doRegister 方法通过 javaChannel().register(eventLoop().selector, 0, this) 将 Channel 对应的 Java NIO SockerChannel 注册到一个 eventLoop 的 Selector 中, 并且将当前 Channel 作为 attachment. 那ChannelInitializer.channelRegistered这个方法又是有什么时候执行的呢？ 在channel 的注册过程的倒数第二步的代码中AbstractUnsafe.register0： 1234567891011121314151617181920212223242526private void register0(ChannelPromise promise) &#123; try &#123; if (!promise.setUncancellable() || !ensureOpen(promise)) &#123; return; &#125; boolean firstRegistration = neverRegistered; doRegister(); neverRegistered = false; registered = true; pipeline.invokeHandlerAddedIfNeeded(); safeSetSuccess(promise); pipeline.fireChannelRegistered(); if (isActive()) &#123; if (firstRegistration) &#123; pipeline.fireChannelActive(); &#125; else if (config().isAutoRead()) &#123; beginRead(); &#125; &#125; &#125; catch (Throwable t) &#123; closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); &#125;&#125; 在把Channel中的SocketChannel注册到EventLoop中的Selector中后，执行了pipeline.fireChannelRegistered(); 12345@Overridepublic final ChannelPipeline fireChannelRegistered() &#123; AbstractChannelHandlerContext.invokeChannelRegistered(head); return this;&#125; 关于上面代码的 head.fireXXX 的调用形式, 是 Netty 中 Pipeline 传递事件的常用方式, 我们以后会经常看到. AbstractChannelHandlerContext.invokeChannelRegistered(head)： 123456789101112131415161718192021222324static void invokeChannelRegistered(final AbstractChannelHandlerContext next) &#123; EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; next.invokeChannelRegistered(); &#125; else &#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; next.invokeChannelRegistered(); &#125; &#125;); &#125;&#125;private void invokeChannelRegistered() &#123; if (invokeHandler()) &#123; try &#123; ((ChannelInboundHandler) handler()).channelRegistered(this); &#125; catch (Throwable t) &#123; invokeExceptionCaught(t); &#125; &#125; else &#123; fireChannelRegistered(); &#125;&#125; HeadContext.handler(): 1234@Overridepublic ChannelHandler handler() &#123; return this;&#125; HeadContext.channelRegistered(): 12345@Overridepublic void channelRegistered(ChannelHandlerContext ctx) &#123; invokeHandlerAddedIfNeeded(); ctx.fireChannelRegistered();&#125; AbstractChannelHandlerContext.fireChannelRegistered： 12345@Overridepublic ChannelHandlerContext fireChannelRegistered() &#123; invokeChannelRegistered(findContextInbound(MASK_CHANNEL_REGISTERED)); return this;&#125; findContextInbound的作用就是找到当前结点的下一个可达结点。 12345678private AbstractChannelHandlerContext findContextInbound(int mask) &#123; AbstractChannelHandlerContext ctx = this; EventExecutor currentExecutor = executor(); do &#123; ctx = ctx.next; &#125; while (skipContext(ctx, currentExecutor, mask, MASK_ONLY_INBOUND)); return ctx;&#125; 一开始执行了pipeline.fireChannelRegistered()启用了head(HeadContext)的invokeChannelRegistered方法，紧接着调用了自己的channelRegistered，并把自己作为参数传进去了。 然后在channelRegistered中，HeadContext调用了fireChannelRegistered方法，在fireChannelRegistered的代码中看到，就是找到HeadContext的下一个可达结点，没错，在这个启动阶段，这个结点就是我们创建的ChannelInitializer对象，接着ChannelInitializer有调用了自己的channelRegistered这个方法初始化pipeline中的自定义handler然后调用了 AbstractChannelHandlerContext.invokeChannelRegistered这个方法了，上面的流程又来了一次，直道到TailContext为止 因此当调用了这个方法后, 我们自定义的 ChannelHandler 就插入到 Pipeline 了, 此时的 Pipeline 如下图所示: 当添加了自定义的 ChannelHandler 后, 会删除 ChannelInitializer 这个 ChannelHandler, 即 “ctx.pipeline().remove(this)”, 因此最后的 Pipeline 如下: ChannelHandler 的名字我们注意到, pipeline.addXXX 都有一个重载的方法, 例如 addLast, 它有一个重载的版本是: 1ChannelPipeline addLast(String name, ChannelHandler handler); 第一个参数指定了所添加的 handler 的名字(更准确地说是 ChannelHandlerContext 的名字, 不过我们通常是以 handler 作为叙述的对象, 因此说成 handler 的名字便于理解). 那么 handler 的名字有什么用呢? 如果我们不设置name, 那么 handler 会有怎样的名字? 我们还是以 addLast 方法为例: 1234@Overridepublic ChannelPipeline addLast(String name, ChannelHandler handler) &#123; return addLast(null, name, handler);&#125; 这个方法会调用重载的 addLast 方法: 1234567891011@Overridepublic ChannelPipeline addLast(EventExecutorGroup group, final String name, ChannelHandler handler) &#123; synchronized (this) &#123; checkDuplicateName(name); AbstractChannelHandlerContext newCtx = new DefaultChannelHandlerContext(this, group, name, handler); addLast0(name, newCtx); &#125; return this;&#125; 第一个参数被设置为 null, 我们不关心它. 第二参数就是这个 handler 的名字. 看代码可知, 在添加一个 handler 之前, 需要调用 checkDuplicateName 方法来确定此 handler 的名字是否和已添加的 handler 的名字重复. 而这个 checkDuplicateName 方法我们在前面已经有提到, 这里再回顾一下: 12345private void checkDuplicateName(String name) &#123; if (name2ctx.containsKey(name)) &#123; throw new IllegalArgumentException(&quot;Duplicate handler name: &quot; + name); &#125;&#125; Netty 判断一个 handler 的名字是否重复的依据很简单: DefaultChannelPipeline 中有一个 类型为 Map&lt;String, AbstractChannelHandlerContext&gt; 的 name2ctx 字段, 它的 key 是一个 handler 的名字, 而 value 则是这个 handler 所对应的 ChannelHandlerContext. 每当新添加一个 handler 时, 就会 put 到 name2ctx 中. 因此检查 name2ctx 中是否包含这个 name 即可.当没有重名的 handler 时, 就为这个 handler 生成一个关联的 DefaultChannelHandlerContext 对象, 然后就将 name 和 newCtx 作为 key-value 对 放到 name2Ctx 中. 自动生成 handler的名字如果我们调用的是如下的 addLast 方法 1ChannelPipeline addLast(ChannelHandler... handlers); 那么 Netty 会调用 generateName 为我们的 handler 自动生成一个名字: 1234567891011121314151617181920212223242526272829private String generateName(ChannelHandler handler) &#123; WeakHashMap&lt;Class&lt;?&gt;, String&gt; cache = nameCaches[(int) (Thread.currentThread().getId() % nameCaches.length)]; Class&lt;?&gt; handlerType = handler.getClass(); String name; synchronized (cache) &#123; name = cache.get(handlerType); if (name == null) &#123; name = generateName0(handlerType); cache.put(handlerType, name); &#125; &#125; synchronized (this) &#123; // It&#x27;s not very likely for a user to put more than one handler of the same type, but make sure to avoid // any name conflicts. Note that we don&#x27;t cache the names generated here. if (name2ctx.containsKey(name)) &#123; String baseName = name.substring(0, name.length() - 1); // Strip the trailing &#x27;0&#x27;. for (int i = 1;; i ++) &#123; String newName = baseName + i; if (!name2ctx.containsKey(newName)) &#123; name = newName; break; &#125; &#125; &#125; &#125; return name;&#125; 而 generateName 会接着调用 generateName0 来实际产生一个 handler 的名字: 123private static String generateName0(Class&lt;?&gt; handlerType) &#123; return StringUtil.simpleClassName(handlerType) + &quot;#0&quot;;&#125; 自动生成的名字的规则很简单, 就是 handler 的简单类名加上 “#0”, 因此我们的 EchoClientHandler 的名字就是 “EchoClientHandler#0”, 这一点也可以通过调试窗口佐证: 关于 Pipeline 的事件传输机制handler加入pipeline时后把handler包装成一个DefaultChannelHandlerContext对象，代表着handler和pipeline的绑定。而该context对象中的executionMask属性使用了前16位置表示了这个hander所属是inbound还是outbound，能执行哪些方法。 Netty 的事件可以分为 Inbound 和 Outbound 事件. 如下是从 Netty 官网上拷贝的一个图示: 1234567891011121314151617181920212223242526272829303132333435363738 I/O Request via Channel or ChannelHandlerContext |+---------------------------------------------------+---------------+| ChannelPipeline | || \\|/ || +---------------------+ +-----------+----------+ || | Inbound Handler N | | Outbound Handler 1 | || +----------+----------+ +-----------+----------+ || /|\\ | || | \\|/ || +----------+----------+ +-----------+----------+ || | Inbound Handler N-1 | | Outbound Handler 2 | || +----------+----------+ +-----------+----------+ || /|\\ . || . . || ChannelHandlerContext.fireIN_EVT() ChannelHandlerContext.OUT_EVT()|| [ method call] [method call] || . . || . \\|/ || +----------+----------+ +-----------+----------+ || | Inbound Handler 2 | | Outbound Handler M-1 | || +----------+----------+ +-----------+----------+ || /|\\ | || | \\|/ || +----------+----------+ +-----------+----------+ || | Inbound Handler 1 | | Outbound Handler M | || +----------+----------+ +-----------+----------+ || /|\\ | |+---------------+-----------------------------------+---------------+ | \\|/+---------------+-----------------------------------+---------------+| | | || [ Socket.read() ] [ Socket.write() ] || || Netty Internal I/O Threads (Transport Implementation) |+-------------------------------------------------------------------+ 从上图可以看出, inbound 事件和 outbound 事件的流向是不一样的, inbound 事件的流行是从下至上, 而 outbound 刚好相反, 是从上到下. 并且 inbound 的传递方式是通过调用相应的 ChannelHandlerContext.fireIN_EVT() 方法, 而 outbound 方法的的传递方式是通过调用 ChannelHandlerContext.OUT_EVT() 方法. 例如 ChannelHandlerContext.fireChannelRegistered() 调用会发送一个 ChannelRegistered 的 inbound 给下一个ChannelHandlerContext, 而 ChannelHandlerContext.bind 调用会发送一个 bind 的 outbound 事件给 下一个 ChannelHandlerContext. Inbound 事件传播方法有: 123456789ChannelHandlerContext.fireChannelRegistered()ChannelHandlerContext.fireChannelActive()ChannelHandlerContext.fireChannelRead(Object)ChannelHandlerContext.fireChannelReadComplete()ChannelHandlerContext.fireExceptionCaught(Throwable)ChannelHandlerContext.fireUserEventTriggered(Object)ChannelHandlerContext.fireChannelWritabilityChanged()ChannelHandlerContext.fireChannelInactive()ChannelHandlerContext.fireChannelUnregistered() Oubound 事件传输方法有: 1234567ChannelHandlerContext.bind(SocketAddress, ChannelPromise)ChannelHandlerContext.connect(SocketAddress, SocketAddress, ChannelPromise)ChannelHandlerContext.write(Object, ChannelPromise)ChannelHandlerContext.flush()ChannelHandlerContext.read()ChannelHandlerContext.disconnect(ChannelPromise)ChannelHandlerContext.close(ChannelPromise) 注意, 如果我们捕获了一个事件, 并且想让这个事件继续传递下去, 那么需要调用 Context 相应的传播方法. 如果希望将事件继续传播下去, 那么需要接着调用 ctx.fireChannelActive(). Outbound 操作(outbound operations of a channel)Outbound 事件都是请求事件(request event), 即请求某件事情的发生, 然后通过 Outbound 事件进行通知.Outbound 事件的传播方向是 tail -&gt; customContext -&gt; head. 我们接下来以 connect 事件为例, 分析一下 Outbound 事件的传播机制. Inbound 事件nbound 事件和 Outbound 事件的处理过程有点镜像.Inbound 事件是一个通知事件, 即某件事已经发生了, 然后通过 Inbound 事件进行通知. Inbound 通常发生在 Channel 的状态的改变或 IO 事件就绪.Inbound 的特点是它传播方向是 head -&gt; customContext -&gt; tail. 总结对于 Outbound事件: Outbound 事件是请求事件(由 Connect 发起一个请求, 并最终由 unsafe 处理这个请求) Outbound 事件的发起者是 Channel Outbound 事件的处理者是 unsafe Outbound 事件在 Pipeline 中的传输方向是 tail -&gt; head. 在 ChannelHandler 中处理事件时, 如果这个 Handler 不是最后一个 Hnalder, 则需要调用 ctx.xxx (例如 ctx.connect) 将此事件继续传播下去. 如果不这样做, 那么此事件的传播会提前终止. Outbound 事件流: Context.OUT_EVT -&gt; Connect.findContextOutbound -&gt; nextContext.invokeOUT_EVT -&gt; nextHandler.OUT_EVT -&gt; nextContext.OUT_EVT 对于 Inbound 事件: Inbound 事件是通知事件, 当某件事情已经就绪后, 通知上层. Inbound 事件发起者是 unsafe Inbound 事件的处理者是 Channel, 如果用户没有实现自定义的处理方法, 那么Inbound 事件默认的处理者是 TailContext, 并且其处理方法是空实现. Inbound 事件在 Pipeline 中传输方向是 head -&gt; tail 在 ChannelHandler 中处理事件时, 如果这个 Handler 不是最后一个 Hnalder, 则需要调用 ctx.fireIN_EVT (例如 ctx.fireChannelActive) 将此事件继续传播下去. 如果不这样做, 那么此事件的传播会提前终止. Outbound 事件流: Context.fireIN_EVT -&gt; Connect.findContextInbound -&gt; nextContext.invokeIN_EVT -&gt; nextHandler.IN_EVT -&gt; nextContext.fireIN_EVT","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"Netty 源码分析之 三 EventLoop","slug":"netty/netty源码/Netty 源码分析之 三 EventLoop","date":"2021-11-23T12:00:19.000Z","updated":"2022-03-23T09:03:58.219Z","comments":true,"path":"blog/netty/netty源码/Netty 源码分析之 三 EventLoop/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/netty%E6%BA%90%E7%A0%81/Netty%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8B%20%E4%B8%89%20EventLoop/","excerpt":"","text":"Netty 是 Reactor 模型的一个实现, 那么首先从 Reactor 的线程模型开始吧. 关于 Reactor 的线程模型首先我们来看一下 Reactor 的线程模型.[Reactor 的线程模型有三种](..&#x2F;..&#x2F;java原生（入门）&#x2F;Scalable IO in Java——多Reactor的代码实现.md): 单线程 Reactor模型 单线程 Reactor，工作者线程池 多 Reactor 线程模式 NioEventLoopGroup 与 Reactor 线程模型的对应我们介绍了三种 Reactor 的线程模型, 那么它们和 NioEventLoopGroup 又有什么关系呢? 其实, 不同的设置 NioEventLoopGroup 的方式就对应了不同的 Reactor 的线程模型. 单线程模型来看一下下面的例子: 12345EventLoopGroup bossGroup = new NioEventLoopGroup(1);ServerBootstrap b = new ServerBootstrap();b.group(bossGroup) .channel(NioServerSocketChannel.class) ... 多 Reactor 线程模式123456EventLoopGroup bossGroup = new NioEventLoopGroup(1);EventLoopGroup workerGroup = new NioEventLoopGroup();ServerBootstrap b = new ServerBootstrap();b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) ... EventLoop中的线程先回想下NioEventLoopGroup的初始化过程，跟踪代码一直到MultithreadEventExecutorGroup： 12345678//省略了很多非关键代码protected MultithreadEventExecutorGroup(int nThreads, Executor executor,EventExecutorChooserFactory chooserFactory, Object... args) &#123; executor = new ThreadPerTaskExecutor(newDefaultThreadFactory()); children = new EventExecutor[nThreads]; for (int i = 0; i &lt; nThreads; i ++) &#123; children[i] = newChild(executor, args); &#125;&#125; 上面列出了关键的代码。ThreadPerTaskExecutor： 123456789101112public final class ThreadPerTaskExecutor implements Executor &#123; private final ThreadFactory threadFactory; public ThreadPerTaskExecutor(ThreadFactory threadFactory) &#123; this.threadFactory = ObjectUtil.checkNotNull(threadFactory, &quot;threadFactory&quot;); &#125; @Override public void execute(Runnable command) &#123; threadFactory.newThread(command).start(); &#125;&#125; 从方法可以看出，这个Executor的左右就是启动一个线程的。 newChild是由NioEventLoopGroup重写的： 123456@Overrideprotected EventLoop newChild(Executor executor, Object... args) throws Exception &#123; EventLoopTaskQueueFactory queueFactory = args.length == 4 ? (EventLoopTaskQueueFactory) args[3] : null; return new NioEventLoop(this, executor, (SelectorProvider) args[0], ((SelectStrategyFactory) args[1]).newSelectStrategy(), (RejectedExecutionHandler) args[2], queueFactory);&#125; 上面代码可以看到，创建的executor现在只有创建线程和启动线程功能，每个Group中都有一个EventLoop数组children，而newChild创建出来的EventLoop是NioEventLoop。 一个客户端最开始的操作是connect，要链接到服务器才能做操作。所以先看客户端的connect，代码跟踪到了 123456789101112131415private static void doConnect( final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise connectPromise) &#123; final Channel channel = connectPromise.channel(); channel.eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; if (localAddress == null) &#123; channel.connect(remoteAddress, connectPromise); &#125; else &#123; channel.connect(remoteAddress, localAddress, connectPromise); &#125; connectPromise.addListener(ChannelFutureListener.CLOSE_ON_FAILURE); &#125; &#125;);&#125; 在执行doConnect之前已经初始化了Channel和为Channel分配好了EventLoop了，再结合EventLoopGroup的初始化，可以得出channel.eventLoop()返回的是一个NioEventLoop类型的对象。由于NioEventLoop没有重写void execute(Runnable task)方法，所以用了夫类SingleThreadEventExecutor的void execute(Runnable task)方法。调用如下: 12SingleThreadEventExecutor.execute(Runnable task) -&gt; SingleThreadEventExecutor.execute(Runnable task, boolean immediate) 看下SingleThreadEventExecutor.execute(Runnable task, boolean immediate)方法： 12345678910111213141516171819202122232425262728293031//该方法中的addTaskWakesUp属性在默认情况下是是为false的，可以看NioEventLoop的构造方法。private void execute(Runnable task, boolean immediate) &#123; //这个方法用来判断执行execute方法的线程和EventLoop对应的线程是否同一个 boolean inEventLoop = inEventLoop(); //这个方法就是把Runnable对象放入到taskQueue队列中 addTask(task); //从服务端或者客户端的启动流程来说，在启动阶段执行这个方法时，肯定不是同一个线程的。因为在第一次调用时EventLoop的线程还没有初始化 //之后的多次调用由于EventLoop的线程已经通过ThreadPerTaskExecutor创建了新的线程，所以在启动流程中，inEventLoop()总是返回为false的 if (!inEventLoop) &#123; startThread(); if (isShutdown()) &#123; boolean reject = false; try &#123; if (removeTask(task)) &#123; reject = true; &#125; &#125; catch (UnsupportedOperationException e) &#123; // The task queue does not support removal so the best thing we can do is to just move on and // hope we will be able to pick-up the task before its completely terminated. // In worst case we will log on termination. &#125; if (reject) &#123; reject(); &#125; &#125; &#125; if (!addTaskWakesUp &amp;&amp; immediate) &#123; wakeup(inEventLoop); &#125;&#125; 任务先入taskQueue队列，由于这个时候调用的线程还是业务线程，不是netty创建的IO线程，所以inEventLoop肯定为false的，然后执行了startThread()方法，一直到doStartThread()方法: 123456789101112131415private void doStartThread() &#123; //去掉很多非关键代码 executor.execute(new Runnable() &#123; @Override public void run() &#123; thread = Thread.currentThread(); if (interrupted) &#123; thread.interrupt(); &#125; ... SingleThreadEventExecutor.this.run(); ... &#125; &#125;)&#125; 可以看到这时使用到了executor，这个executor就是初始化NioEventLoopGroup时创建的ThreadPerTaskExecutor类型的对象，这个类的作用就是启动一个线程。 注意，executor实际上不是ThreadPerTaskExecutor，因为在构造方法中，对传入的executor做了包装，而这个包装类实际的作用就是对Runnable对应做一层包装，看源码： 12345678public void run() &#123; setCurrentEventExecutor(eventExecutor); try &#123; command.run(); &#125; finally &#123; setCurrentEventExecutor(null); &#125;&#125; 这种包装实际上就是使用了静态代理，本质上功能还是一样的，就是创建线程，并启动。 所以在doStartThread方法中执行了executor.execute的方法后，实际上就是创建并启动了一个线程。而这个线程的任务，就是把刚刚创建的线程对象赋值给EventLoop中的thread引用。然后执行SingleThreadEventExecutor.this.run()方法。而这个run方法，就是NioEventLoop中的run方法。 NioEventLoop的事件循环——run方法先看 NioEventLoop的类图： 在 Netty 中, 一个 EventLoop 需要负责两个工作, 第一个是作为 IO 线程, 负责相应的 IO 操作; 第二个是作为任务线程, 执行3个队列中的任务.而这3个队列分别是:（从优先级） SingleThreadEventExecutor中的taskQueue队列，这也是最重要的队列，通过execute方法添加进去，负责保存用户的任务。同时也是定时任务的载体。 AbstractScheduledEventExecutor中的scheduledTaskQueue队列，用来保存定时处理的任务，通过schedule等方法添加进去。而任务的处理处理也很是先从这个队列中取出到期的需要执行的任务，并把这些任务添加到taskQueue队列中，所以在第一点中才会出现最后的一句话。具体的在下面讲解。 SingleThreadEventLoop中的tailTasks队列，通过方法executeAfterEventLoopIteration把任务添加进去。而这个队列的就是保存当IO事件处和第一第二队列的任务处理完或者到达指定的期限后，需要执行的任务。所以在executeAfterEventLoopIteration的注解就是 12Adds a task to be run once at the end of next (or current) &#123;@code eventloop&#125; iteration.添加要在下一次(或当前)&#123;@code eventloop&#125;迭代结束时运行一次的任务。 接下来我们先从 IO 操纵方面入手, 看一下 TCP 数据是如何从 Java NIO Socket 传递到我们的 handler 中的. 回顾Netty的启动回顾一下在 Java NIO 中所讲述的 Selector 的使用流程: 通过 Selector.open() 打开一个 Selector. 将 Channel 注册到 Selector 中, 并设置需要监听的事件(interest set) 不断重复: 调用 select() 方法 调用 selector.selectedKeys() 获取 selected keys 迭代每个 selected key: 从 selected key 中获取 对应的 Channel 和附加信息(如果有的话) 判断是哪些 IO 事件已经就绪了, 然后处理它们. 如果是 OP_ACCEPT 事件, 则调用 “SocketChannel clientChannel &#x3D; ((ServerSocketChannel) key.channel()).accept()” 获取 SocketChannel, 并将它设置为 非阻塞的, 然后将这个 Channel 注册到 Selector 中. 根据需要更改 selected key 的监听事件（这里大部分情况是值OP_WRITE和OP_READ）. 将已经处理过的 key 从 selected keys 集合中删除. 上面操作的第一步 通过 Selector.open() 打开一个 Selector 我们已经在第一章的 [Channel 实例化](.&#x2F;Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端)#Channel 实例化) 这一小节中已经提到了, Netty 中是通过调用 SelectorProvider.openSocketChannel() 来打开一个新的 Java NIO SocketChannel: 1234private static SocketChannel newSocket(SelectorProvider provider) &#123; ... return provider.openSocketChannel();&#125; 第二步 将 Channel 注册到 Selector 中, 并设置需要监听的事件(interest set) 的操作我们在第一章 [channel 的注册过程](.&#x2F;Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端)#channel 的注册过程) 中也分析过了, 我们在来回顾一下, 在客户端的 Channel 注册过程中（同时也适用服务端）, 会有如下调用链: 12345678Bootstrap.initAndRegister -&gt; AbstractBootstrap.initAndRegister -&gt; MultithreadEventLoopGroup.register -&gt; MultithreadEventLoopGroup.next(选择EventLoop) --&gt; SingleThreadEventLoop.register -&gt; AbstractUnsafe.register -&gt; AbstractUnsafe.register0 -&gt; AbstractNioChannel.doRegister 在 AbstractUnsafe.register 方法中调用了 register0 方法: 123456@Overridepublic final void register(EventLoop eventLoop, final ChannelPromise promise) &#123; // 省略条件判断和错误处理 AbstractChannel.this.eventLoop = eventLoop; register0(promise);&#125; register0 方法代码如下: 12345678private void register0(ChannelPromise promise) &#123; boolean firstRegistration = neverRegistered; doRegister(); neverRegistered = false; registered = true; safeSetSuccess(promise); pipeline.fireChannelRegistered();&#125; register0 又调用了 AbstractNioChannel.doRegister: 12345@Overrideprotected void doRegister() throws Exception &#123; // 省略错误处理 selectionKey = javaChannel().register(eventLoop().selector, 0, this);&#125; 在这里 javaChannel() 返回的是一个 Java NIO SocketChannel（或者ServerSocketChannel） 对象, 我们将此 SocketChannel 注册到前面第一步获取的 Selector 中. 然后就是设置感兴趣的事件了。 客户端客户端通过这样的调用，来完成connect操作 12345Bootstrap.doConnect -&gt; AbstractChannel.connect -&gt; HeadContext.connect -&gt; AbstractNioUnsafe.connect -&gt; NioSocketChannel.doConnect 通过上述的调用，完成了SocketChannel.connect的操作。然后通过 12345678910111213141516AbstractNioUnsafe.fulfillConnectPromise(在上述的AbstractNioUnsafe.connect方法作为入口) -&gt; DefaultChannelPipeline.fireChannelActive -&gt; HeadContext.channelActive -&gt; HeadContext.readIfIsAutoRead -&gt; AbstractChannel.read -&gt; DefaultChannelPipeline.read -&gt; HeadContext.read -&gt; AbstractUnsafe.beginRead -&gt; AbstractNioChannel.doBeginRead或者NioEventLoop.run -&gt; NioEventLoop.processSelectedKey -&gt; AbstractNioUnsafe.finishConnect -&gt; 和上边的流程一样了 之所以有两种是因为，SocketConnect.connect不是马上成功，所以对于第一种情况，是方法直接链接成功了。第二种就是不是马上成功，需要处理OP_CONNECT事件，所以在NioSocketChannel.doConnect后就注册了一个OP_CONNECT事件。 通过上边的流程来完成了OP_READ事件的添加。 服务端123456ServerBootstrap.bind -&gt; AbstractBootstrap.doBind0 -&gt; AbstractChannel.bind -&gt; HeadContext.bind -&gt; AbstractUnfase.bind -&gt; NioServerSocketChannel.doBind 通过上边的调用，完成了ServerSocketChannel.bind操作。然后通过 123456789DefaultChannelPipeline.fireChannelActive -&gt; HeadContext.channelActive -&gt; HeadContext.readIfIsAutoRead -&gt; AbstractChannel.read -&gt; DefaultChannelPipeline.read -&gt; HeadContext.read -&gt; AbstractUnsafe.beginRead -&gt; AbstractNioMessageChannel.doBeginRead -&gt; AbstractNioChannel.doBeginRead 通过上边的调用，完成了OP_ACCEPT的注册。 那么接下来的第三步的循环是在哪里实现的呢? 第三步的操作就是NioEventLoop的关键。 NioEventLoop 的 run 循环在EventLoop中的线程中我们已经了解到了, 当 EventLoop.execute 第一次被调用时, 就会触发 startThread() 的调用, 进而导致了 EventLoop 所对应的 Java 线程的启动. 接着在启动的线程中就执行了SingleThreadEventExecutor.this.run()方法。 123456789101112131415thread = threadFactory.newThread(new Runnable() &#123; @Override public void run() &#123; boolean success = false; updateLastExecutionTime(); try &#123; SingleThreadEventExecutor.this.run(); success = true; &#125; catch (Throwable t) &#123; logger.warn(&quot;Unexpected exception from an event executor: &quot;, t); &#125; finally &#123; ... &#125; &#125; &#125;); 继续跟踪到 NioEventLoop.run() 方法, 其源码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//这里去掉了很多非关键的代码，使得代码结构更加简单清晰@Overrideprotected void run() &#123; int selectCnt = 0; for (;;) &#123; try &#123; int strategy; try&#123; strategy = selectStrategy.calculateStrategy(selectNowSupplier, hasTasks()); &#125;catch() .... selectCnt++; cancelledKeys = 0; needsToSelectAgain = false; final int ioRatio = this.ioRatio; boolean ranTasks; if (ioRatio == 100) &#123; try &#123; if (strategy &gt; 0) &#123; processSelectedKeys(); &#125; &#125; finally &#123; ranTasks = runAllTasks(); &#125; &#125; else if (strategy &gt; 0) &#123; final long ioStartTime = System.nanoTime(); try &#123; processSelectedKeys(); &#125; finally &#123; final long ioTime = System.nanoTime() - ioStartTime; ranTasks = runAllTasks(ioTime * (100 - ioRatio) / ioRatio); &#125; &#125; else &#123; ranTasks = runAllTasks(0); // This will run the minimum number of tasks &#125; ... &#125; catch (CancelledKeyException e) &#123; &#125; ... &#125; finally &#123; try &#123; if (isShuttingDown()) &#123; closeAll(); if (confirmShutdown()) &#123; return; &#125; &#125; &#125; catch (Error e) &#123; &#125; catch (Throwable t) &#123; &#125; &#125; &#125;&#125; 看到了上面代码的 for(;;) 所构成的死循环了没? NioEventLoop 事件循环的核心就是这里！现在我们把上面所提到的 Selector 使用步骤的第三步的部分也找到了. 这个 run 方法可以说是 Netty NIO 的核心 IO 事件的轮询首先, 在 run 方法中, 第一步是调用 hasTasks() 方法来判断当前任务队列中是否有任务: 1234567protected boolean hasTasks() &#123; return super.hasTasks() || !tailTasks.isEmpty();&#125;protected boolean hasTasks() &#123; assert inEventLoop(); return !taskQueue.isEmpty();&#125; 这个方法很简单, 仅仅是检查了一下 taskQueue 或者tailTasks是否为空， taskQueue 来自父类 SingleThreadEventExecutor，保存各种任务 tailTask 来自父类 SingleThreadEventLoop，用于每次事件循环后置任务 处理 接着执行 1selectStrategy.calculateStrategy(selectNowSupplier, hasTasks()); 如果hasTasks返回true，跟踪代码后实际执行的是: 123int selectNow() throws IOException &#123; return selector.selectNow();&#125; 如果是false，返回的是 1SelectStrategy.SELECT = -1 看下对strategy的处理 123456789101112131415161718192021222324252627282930strategy = selectStrategy.calculateStrategy(selectNowSupplier, hasTasks());switch (strategy) &#123; case SelectStrategy.CONTINUE: continue; case SelectStrategy.BUSY_WAIT: case SelectStrategy.SELECT: long curDeadlineNanos = nextScheduledTaskDeadlineNanos(); if (curDeadlineNanos == -1L) &#123; curDeadlineNanos = NONE; // nothing on the calendar &#125; nextWakeupNanos.set(curDeadlineNanos); try &#123; if (!hasTasks()) &#123; strategy = select(curDeadlineNanos); &#125; &#125; finally &#123; nextWakeupNanos.lazySet(AWAKE); &#125; default:&#125;private int select(long deadlineNanos) throws IOException &#123; if (deadlineNanos == NONE) &#123; return selector.select(); &#125; // Timeout will only be 0 if deadline is within 5 microsecs long timeoutMillis = deadlineToDelayNanos(deadlineNanos + 995000L) / 1000000L; return timeoutMillis &lt;= 0 ? selector.selectNow() : selector.select(timeoutMillis);&#125; 这其实也很好理解: 当 taskQueue 中没有任务时, 那么 Netty 可以阻塞地等待 IO 就绪事件; 而当 taskQueue 中有任务时, 我们自然地希望所提交的任务可以尽快地执行, 因此 Netty 会调用非阻塞的 selectNow() 方法, 以保证 taskQueue 中的任务尽快可以执行. IO 事件的处理在 NioEventLoop.run() 方法中, 第一步是通过 select&#x2F;selectNow 调用查询当前是否有就绪的 IO 事件. 那么当有 IO 事件就绪时, 第二步自然就是处理这些 IO 事件啦. 首先让我们来看一下 NioEventLoop.run 中循环的剩余部分: 12345678910111213141516171819if (ioRatio == 100) &#123; try &#123; if (strategy &gt; 0) &#123; processSelectedKeys(); &#125; &#125; finally &#123; ranTasks = runAllTasks(); &#125;&#125; else if (strategy &gt; 0) &#123; final long ioStartTime = System.nanoTime(); try &#123; processSelectedKeys(); &#125; finally &#123; final long ioTime = System.nanoTime() - ioStartTime; ranTasks = runAllTasks(ioTime * (100 - ioRatio) / ioRatio); &#125;&#125; else &#123; ranTasks = runAllTasks(0);&#125; 上面列出的代码中, 有两个关键的调用, 第一个是 processSelectedKeys() 调用, 根据字面意思, 我们可以猜出这个方法肯定是查询就绪的 IO 事件, 然后处理它; 第二个调用是 runAllTasks(), 这个方法我们也可以一眼就看出来它的功能就是运行 taskQueue 中的任务. 这里的代码还有一个十分有意思的地方, 即 ioRatio. 那什么是 ioRatio呢? 它表示的是此线程分配给 IO 操作所占的时间比(即运行 processSelectedKeys 耗时在整个循环中所占用的时间). 例如 ioRatio 默认是 50, 则表示 IO 操作和执行 task 的所占用的线程执行时间比是 1 : 1. 当知道了 IO 操作耗时和它所占用的时间比, 那么执行 task 的时间就可以很方便的计算出来了: 123456设 IO 操作耗时为 ioTime, ioTime 占的时间比例为 ioRatio, 则:ioTime / ioRatio = taskTime / taskRatiotaskRatio = 100 - ioRatio =&gt; taskTime = ioTime * (100 - ioRatio) / ioRatio 根据上面的公式, 当我们设置 ioRate &#x3D; 70 时, 则表示 IO 运行耗时占比为70%, 即假设某次循环一共耗时为 100ms, 那么根据公式, 我们知道 processSelectedKeys() 方法调用所耗时大概为70ms(即 IO 耗时), 而 runAllTasks() 耗时大概为 30ms(即执行 task 耗时). 当 ioRatio 为 100 时, Netty 就不考虑 IO 耗时的占比, 而是分别调用 processSelectedKeys()、runAllTasks(); 当 ioRatio 不为 100时, 则执行到 else 分支, 在这个分支中, 首先记录下 processSelectedKeys() 所执行的时间(即 IO 操作的耗时), 然后根据公式, 计算出执行 task 所占用的时间, 然后以此为参数, 调用 runAllTasks(). 我们这里先分析一下 processSelectedKeys() 方法调用, runAllTasks() 我们留到下一节再分析.processSelectedKeys() 方法的源码如下: 1234567private void processSelectedKeys() &#123; if (selectedKeys != null) &#123; processSelectedKeysOptimized(); &#125; else &#123; processSelectedKeysPlain(selector.selectedKeys()); &#125;&#125; processSelectedKeysPlain(Set selectedKeys): 123456789101112131415161718192021222324252627282930313233private void processSelectedKeysPlain(Set&lt;SelectionKey&gt; selectedKeys) &#123; if (selectedKeys.isEmpty()) &#123; return; &#125; Iterator&lt;SelectionKey&gt; i = selectedKeys.iterator(); for (;;) &#123; final SelectionKey k = i.next(); final Object a = k.attachment(); i.remove(); if (a instanceof AbstractNioChannel) &#123; processSelectedKey(k, (AbstractNioChannel) a); &#125; else &#123; @SuppressWarnings(&quot;unchecked&quot;) NioTask&lt;SelectableChannel&gt; task = (NioTask&lt;SelectableChannel&gt;) a; processSelectedKey(k, task); &#125; if (!i.hasNext()) &#123; break; &#125; if (needsToSelectAgain) &#123; selectAgain(); selectedKeys = selector.selectedKeys(); // Create the iterator again to avoid ConcurrentModificationException if (selectedKeys.isEmpty()) &#123; break; &#125; else &#123; i = selectedKeys.iterator(); &#125; &#125; &#125;&#125; 好了都看到熟悉的代码了，在这里就是迭代事件的，重点看 processSelectedKey(k, (AbstractNioChannel) a): 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) &#123; final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe(); if (!k.isValid()) &#123; final EventLoop eventLoop; try &#123; eventLoop = ch.eventLoop(); &#125; catch (Throwable ignored) &#123; // If the channel implementation throws an exception because there is no event loop, we ignore this // because we are only trying to determine if ch is registered to this event loop and thus has authority // to close ch. return; &#125; // Only close ch if ch is still registered to this EventLoop. ch could have deregistered from the event loop // and thus the SelectionKey could be cancelled as part of the deregistration process, but the channel is // still healthy and should not be closed. // See https://github.com/netty/netty/issues/5125 if (eventLoop == this) &#123; // close the channel if the key is not valid anymore unsafe.close(unsafe.voidPromise()); &#125; return; &#125; try &#123; int readyOps = k.readyOps(); // We first need to call finishConnect() before try to trigger a read(...) or write(...) as otherwise // the NIO JDK channel implementation may throw a NotYetConnectedException. if ((readyOps &amp; SelectionKey.OP_CONNECT) != 0) &#123; // remove OP_CONNECT as otherwise Selector.select(..) will always return without blocking // See https://github.com/netty/netty/issues/924 int ops = k.interestOps(); ops &amp;= ~SelectionKey.OP_CONNECT; k.interestOps(ops); unsafe.finishConnect(); &#125; // Process OP_WRITE first as we may be able to write some queued buffers and so free memory. if ((readyOps &amp; SelectionKey.OP_WRITE) != 0) &#123; // Call forceFlush which will also take care of clear the OP_WRITE once there is nothing left to write ch.unsafe().forceFlush(); &#125; // Also check for readOps of 0 to workaround possible JDK bug which may otherwise lead // to a spin loop if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) &#123; unsafe.read(); &#125; &#125; catch (CancelledKeyException ignored) &#123; unsafe.close(unsafe.voidPromise()); &#125;&#125; 这个代码是不是很熟悉啊? 完全是 Java NIO 的 Selector 的那一套处理流程 processSelectedKey 中处理了三个事件, 分别是: OP_READ, 可读事件, 即 Channel 中收到了新数据可供上层读取. OP_WRITE, 可写事件, 即上层可以向 Channel 写入数据. OP_CONNECT, 连接建立事件, 即 TCP 连接已经建立, Channel 处于 active 状态. Netty 的任务队列机制在Netty 中, 一个 NioEventLoop 通常需要肩负起两种任务, 第一个是作为 IO 线程, 处理 IO 操作; 第二个就是作为任务线程, 处理 taskQueue 中的任务. 这一节的重点就是分析一下 NioEventLoop 的任务队列机制的. Task 的添加NioEventLoop 继承于 SingleThreadEventExecutor, 而 SingleThreadEventExecutor 中有一个 Queue taskQueue 字段, 用于存放添加的 Task. 在 Netty 中, 每个 Task 都使用一个实现了 Runnable 接口的实例来表示.例如当我们需要将一个 Runnable 添加到 taskQueue 中时, 我们可以进行如下操作: 1234567EventLoop eventLoop = channel.eventLoop();eventLoop.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;Hello, Netty!&quot;); &#125;&#125;); 当调用 execute 后, 实际上是调用到了 SingleThreadEventExecutor.execute() 方法,上面已经讲过，会把这个task放入到taskQueue中。 123456protected void addTask(Runnable task) &#123; ObjectUtil.checkNotNull(task, &quot;task&quot;); if (!offerTask(task)) &#123; reject(task); &#125;&#125; 因此实际上, taskQueue 是存放着待执行的任务的队列. schedule 任务除了通过 execute 添加普通的 Runnable 任务外, 我们还可以通过调用 eventLoop.scheduleXXX 之类的方法来添加一个定时任务. EventLoop 中实现任务队列的功能在超类 SingleThreadEventExecutor 实现的, 而 schedule 功能的实现是在 SingleThreadEventExecutor 的父类, 即 AbstractScheduledEventExecutor 中实现的.在 AbstractScheduledEventExecutor 中, 有以 scheduledTaskQueue 字段: 1PriorityQueue&lt;ScheduledFutureTask&lt;?&gt;&gt; scheduledTaskQueue; scheduledTaskQueue 是一个队列(Queue), 其中存放的元素是 ScheduledFutureTask. 而 ScheduledFutureTask 我们很容易猜到, 它是对 Schedule 任务的一个抽象.我们来看一下 AbstractScheduledEventExecutor 所实现的 schedule 方法吧: 1234567891011121314@Overridepublic ScheduledFuture&lt;?&gt; schedule(Runnable command, long delay, TimeUnit unit) &#123; ObjectUtil.checkNotNull(command, &quot;command&quot;); ObjectUtil.checkNotNull(unit, &quot;unit&quot;); if (delay &lt; 0) &#123; delay = 0; &#125; validateScheduled0(delay, unit); return schedule(new ScheduledFutureTask&lt;Void&gt;( this, command, deadlineNanos(unit.toNanos(delay))));&#125; 这是其中一个重载的 schedule, 当一个 Runnable 传递进来后, 会被封装为一个 ScheduledFutureTask 对象, 这个对象会记录下这个 Runnable 在何时运行、已何种频率运行等信息.当构建了 ScheduledFutureTask 后, 会继续调用 另一个重载的 schedule 方法: 12345678910111213141516171819private &lt;V&gt; ScheduledFuture&lt;V&gt; schedule(final ScheduledFutureTask&lt;V&gt; task) &#123; if (inEventLoop()) &#123; scheduleFromEventLoop(task); &#125; else &#123; final long deadlineNanos = task.deadlineNanos(); // task will add itself to scheduled task queue when run if not expired if (beforeScheduledTaskSubmitted(deadlineNanos)) &#123; execute(task); &#125; else &#123; lazyExecute(task); // Second hook after scheduling to facilitate race-avoidance if (afterScheduledTaskSubmitted(deadlineNanos)) &#123; execute(WAKEUP_TASK); &#125; &#125; &#125; return task;&#125; 在这个方法中, ScheduledFutureTask 对象就会被添加到 scheduledTaskQueue 中了. 任务的执行当一个任务被添加到 taskQueue 后, 它是怎么被 EventLoop 执行的呢? 让我们回到 NioEventLoop.run() 方法中, 在这个方法里, 会分别调用 processSelectedKeys() 和 runAllTasks() 方法, 来进行 IO 事件的处理和 task 的处理. processSelectedKeys() 方法我们已经分析过了, 下面我们来看一下 runAllTasks() 中到底有什么名堂吧. runAllTasks 方法有两个重载的方法, 一个是无参数的, 另一个有一个参数的. 首先来看一下无参数的 runAllTasks: 123456789101112131415161718192021222324252627282930313233343536373839404142protected boolean runAllTasks() &#123; assert inEventLoop(); boolean fetchedAll; boolean ranAtLeastOne = false; do &#123; //先把定时任务中可执行的任务方法taskQueue中。 fetchedAll = fetchFromScheduledTaskQueue(); if (runAllTasksFrom(taskQueue)) &#123; ranAtLeastOne = true; &#125; &#125; while (!fetchedAll); // keep on processing until we fetched all scheduled tasks. if (ranAtLeastOne) &#123; lastExecutionTime = ScheduledFutureTask.nanoTime(); &#125; afterRunningAllTasks(); return ranAtLeastOne;&#125;protected final boolean runAllTasksFrom(Queue&lt;Runnable&gt; taskQueue) &#123; Runnable task = pollTaskFrom(taskQueue); if (task == null) &#123; return false; &#125; for (;;) &#123; safeExecute(task); task = pollTaskFrom(taskQueue); if (task == null) &#123; return true; &#125; &#125;&#125;protected static void safeExecute(Runnable task) &#123; try &#123; task.run(); &#125; catch (Throwable t) &#123; logger.warn(&quot;A task raised an exception. Task: &#123;&#125;&quot;, task, t); &#125;&#125; 我们前面已经提到过, EventLoop 可以通过调用 EventLoop.execute 来将一个 Runnable 提交到 taskQueue 中, 也可以通过调用 EventLoop.schedule 来提交一个 schedule 任务到 scheduledTaskQueue 中. 在此方法的一开始调用的 fetchFromScheduledTaskQueue() 其实就是将 scheduledTaskQueue 中已经可以执行的(即定时时间已到的 schedule 任务) 拿出来并添加到 taskQueue 中, 作为可执行的 task 等待被调度执行. 1234567891011121314151617181920212223242526272829private boolean fetchFromScheduledTaskQueue() &#123; if (scheduledTaskQueue == null || scheduledTaskQueue.isEmpty()) &#123; return true; &#125; long nanoTime = AbstractScheduledEventExecutor.nanoTime(); for (;;) &#123; Runnable scheduledTask = pollScheduledTask(nanoTime); if (scheduledTask == null) &#123; return true; &#125; if (!taskQueue.offer(scheduledTask)) &#123; // No space left in the task queue add it back to the scheduledTaskQueue so we pick it up again. scheduledTaskQueue.add((ScheduledFutureTask&lt;?&gt;) scheduledTask); return false; &#125; &#125;&#125;protected final Runnable pollScheduledTask(long nanoTime) &#123; assert inEventLoop(); ScheduledFutureTask&lt;?&gt; scheduledTask = peekScheduledTask(); if (scheduledTask == null || scheduledTask.deadlineNanos() - nanoTime &gt; 0) &#123; return null; &#125; scheduledTaskQueue.remove(); scheduledTask.setConsumed(); return scheduledTask;&#125; 接下来 runAllTasks() 方法就会不断调用 task &#x3D; pollTask() 从 taskQueue 中获取一个可执行的 task, 然后调用它的 run() 方法来运行此 task. 而对于runAllTasks带参数的方法，执行流程都一样，只是多了一个执行期限，这里看下源码和注释就好了 12345678910111213141516171819202122232425262728293031323334353637383940414243protected boolean runAllTasks(long timeoutNanos) &#123; fetchFromScheduledTaskQueue(); Runnable task = pollTask(); if (task == null) &#123; afterRunningAllTasks(); return false; &#125; // 这个计算逻辑可以这样理解，ScheduledFutureTask.nanoTime()返回当前的EventLoop年龄，而+timeoutNanos就得出了一个界限 // 这个界限表示在deadline之前的还能执行任务。 // 用事例举例就是我现在27岁，学习完后就取考试。 // 那结果就是我提前完成，然后做别的事了；或者到期我还未完成，生活好要继续，那我只能暂时先放下，做别的事了，在某个时间再继续。 final long deadline = timeoutNanos &gt; 0 ? ScheduledFutureTask.nanoTime() + timeoutNanos : 0; long runTasks = 0; //这个参数可以认为是年龄 long lastExecutionTime; for (;;) &#123; safeExecute(task); runTasks ++; // Check timeout every 64 tasks because nanoTime() is relatively expensive. // XXX: Hard-coded value - will make it configurable if it is really a problem. //每执行64个任务，就检查下是否超出了界限，超出了就结束，没有就继续。 if ((runTasks &amp; 0x3F) == 0) &#123; lastExecutionTime = ScheduledFutureTask.nanoTime(); if (lastExecutionTime &gt;= deadline) &#123; break; &#125; &#125; task = pollTask(); if (task == null) &#123; lastExecutionTime = ScheduledFutureTask.nanoTime(); break; &#125; &#125; afterRunningAllTasks(); this.lastExecutionTime = lastExecutionTime; return true;&#125; 注意, 因为 EventLoop 既需要执行 IO 操作, 又需要执行 task, 因此我们在调用 EventLoop.execute 方法提交任务时, 不要提交耗时任务, 更不能提交一些会造成阻塞的任务, 不然会导致我们的 IO 线程得不到调度, 影响整个程序的并发量。如果有写操作是耗时操作，建议在添加handler的时候使用一些接受一个EventExecutorGroup的add()方法。如果一个事件被传递给一个自定义的EventExecutorGroup,它将被包含在这个 EventExecutorGroup 中的某个 EventExecutor 所处理,从而被从该Channel 本身的 EventLoop 中移除。对于这种用例,Netty 提供了一个叫 DefaultEventExecutorGroup 的默认实现。意思就是在这个handler的操作交给别的线程执行执行，之后的事件传递都交给这个线程执行了，那原来的那个线程就可以执行别的任务了。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"Netty 源码分析之 一 Bootstrap(服务器端)","slug":"netty/netty源码/Netty 源码分析之 一 Bootstrap(服务器端)","date":"2021-11-23T12:00:18.000Z","updated":"2022-03-23T09:03:58.217Z","comments":true,"path":"blog/netty/netty源码/Netty 源码分析之 一 Bootstrap(服务器端)/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/netty%E6%BA%90%E7%A0%81/Netty%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8B%20%E4%B8%80%20Bootstrap(%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF)/","excerpt":"","text":"服务器端在分析客户端的代码时, 我们已经对 Bootstrap 启动 Netty 有了一个大致的认识, 那么接下来分析服务器端时, 就会相对简单一些了.首先还是来看一下服务器端的启动代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public final class EchoServer &#123; static final boolean SSL = System.getProperty(&quot;ssl&quot;) != null; static final int PORT = Integer.parseInt(System.getProperty(&quot;port&quot;, &quot;8007&quot;)); public static void main(String[] args) throws Exception &#123; // Configure SSL. final SslContext sslCtx; if (SSL) &#123; SelfSignedCertificate ssc = new SelfSignedCertificate(); sslCtx = SslContextBuilder.forServer(ssc.certificate(), ssc.privateKey()).build(); &#125; else &#123; sslCtx = null; &#125; // Configure the server. EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 100) .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline p = ch.pipeline(); if (sslCtx != null) &#123; p.addLast(sslCtx.newHandler(ch.alloc())); &#125; //p.addLast(new LoggingHandler(LogLevel.INFO)); p.addLast(new EchoServerHandler()); &#125; &#125;); // Start the server. ChannelFuture f = b.bind(PORT).sync(); // Wait until the server socket is closed. f.channel().closeFuture().sync(); &#125; finally &#123; // Shut down all event loops to terminate all threads. bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; 和客户端的代码相比, 没有很大的差别, 基本上也是进行了如下几个部分的初始化: EventLoopGroup: 不论是服务器端还是客户端, 都必须指定 EventLoopGroup. 在这个例子中, 指定了 NioEventLoopGroup, 表示一个 NIO 的EventLoopGroup, 不过服务器端需要指定两个 EventLoopGroup, 一个是 bossGroup, 用于处理客户端的连接事件; 另一个是 workerGroup, 用于处理与各个客户端连接的 IO 操作. ChannelType: 指定 Channel 的类型. 因为是服务器端, 因此使用了 NioServerSocketChannel. Handler: 设置数据的处理器. 不同的是，在服务端多需要两个EvetnLoopGroup和多了childHandler。 Channel 的初始化过程我们在分析客户端的 Channel 初始化过程时, 已经提到, Channel 是对 Java 底层 Socket 连接的抽象, 并且知道了客户端的 Channel 的具体类型是 NioSocketChannel, 那么自然的, 服务器端的 Channel 类型就是 NioServerSocketChannel 了.那么接下来我们按照分析客户端的流程对服务器端的代码也同样地分析一遍, 这样也方便我们对比一下服务器端和客户端有哪些不一样的地方. NioServerSocketChannel 的实例化过程客户端启动是通过bootstrap的connect，而服务端则使用ServerBootstrap.bind方法。这两个方法都是AbstractBootstrap的方法。这两个方法的代码结构很像。都是会调用AbstractBootstrap的initAndRegister方法。这段代码在客户端中已经讲过了，不同的是返回的Channel对象不同，对于服务端，返回的是NioServerSocketChannel。 下面是 NioServerSocketChannel 的类层次结构图: NioServerSocketChannel和NioSocketChannel很像，不同的是，NioSocketChannel的父类是AbstractNioByteChannel而NioServerSocketChannel的父类是AbstractNioMessageChannel也正因为这个父类的不同，就导致了两个Channel完成不同的功能。 首先, 我们来看一下它的默认的构造器. 和 NioSocketChannel 类似, 构造器都是调用了 newSocket 来打开一个 Java 的 NIO Socket, 不过需要注意的是, 客户端的 newSocket 调用的是 openSocketChannel, 而服务器端的 newSocket 调用的是 openServerSocketChannel. 顾名思义, 一个是客户端的 Java SocketChannel, 一个是服务器端的 Java ServerSocketChannel. 1234567private static ServerSocketChannel newSocket(SelectorProvider provider) &#123; return provider.openServerSocketChannel();&#125;public NioServerSocketChannel() &#123; this(newSocket(DEFAULT_SELECTOR_PROVIDER));&#125; 接下来会调用重载的构造器: 1234public NioServerSocketChannel(ServerSocketChannel channel) &#123; super(null, channel, SelectionKey.OP_ACCEPT); config = new NioServerSocketChannelConfig(this, javaChannel().socket());&#125; 这个构造其中, 调用父类构造器时, 传入的参数是 SelectionKey.OP_ACCEPT. 作为对比, 我们回想一下, 在客户端的 Channel 初始化时, 传入的参数是 SelectionKey.OP_READ. 有 Java NIO Socket 开发经验的朋友就知道了, Java NIO 是一种 Reactor 模式, 我们通过 selector 来实现 I&#x2F;O 的多路复用复用. 在一开始时, 服务器端需要监听客户端的连接请求, 因此在这里我们设置了 SelectionKey.OP_ACCEPT, 即通知 selector 我们对客户端的连接请求事件感兴趣. 接着和客户端的分析一样, 会逐级地调用父类的构造器 1NioServerSocketChannel -&gt; AbstractNioMessageChannel -&gt;AbstractNioChannel -&gt;AbstractChannel. 同样的, 在 AbstractChannel 中会实例化一个 unsafe 和 pipeline: 123456protected AbstractChannel(Channel parent) &#123; this.parent = parent; id = newId(); unsafe = newUnsafe(); pipeline = newChannelPipeline();&#125; 不过, 这里有一点需要注意的是, 客户端的 unsafe 是一个 AbstractNioByteChannel#NioByteUnsafe 的实例, 而在服务器端时, 因为 AbstractNioMessageChannel 重写了newUnsafe 方法: 1234@Overrideprotected AbstractNioUnsafe newUnsafe() &#123; return new NioMessageUnsafe();&#125; 因此在服务器端, unsafe 字段其实是一个 AbstractNioMessageChannel#AbstractNioUnsafe 的实例. 我们来总结一下, 在 NioServerSocketChannsl 实例化过程中, 所需要做的工作: 调用 NioServerSocketChannel.newSocket(DEFAULT_SELECTOR_PROVIDER) 打开一个新的 Java NIO ServerSocketChannel AbstractChannel(Channel parent) 中初始化 AbstractChannel 的属性: parent 属性置为 null unsafe 通过newUnsafe() 实例化一个 unsafe 对象, 它的类型是 AbstractNioMessageChannel#AbstractNioUnsafe 内部类 pipeline 是 new DefaultChannelPipeline(this) 新创建的实例. AbstractNioChannel 中的属性: SelectableChannel ch 被设置为 Java ServerSocketChannel, 即 NioServerSocketChannel#newSocket 返回的 Java NIO ServerSocketChannel. readInterestOp 被设置为 SelectionKey.OP_ACCEPT SelectableChannel ch 被配置为非阻塞的 ch.configureBlocking(false) NioServerSocketChannel 中的属性: ServerSocketChannelConfig config &#x3D; new NioServerSocketChannelConfig(this, javaChannel().socket()) Channel 的注册服务器端和客户端的 Channel 的注册过程一致, 因此就不再单独分析了. NioServerSocketChannel的端口绑定和OP_ACCEPT事件注册从ServerBootstrap的bind方法开始，到AbstractBootstrap的doBind方法 123456789101112131415private static void doBind0( final ChannelFuture regFuture, final Channel channel, final SocketAddress localAddress, final ChannelPromise promise) &#123; channel.eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; if (regFuture.isSuccess()) &#123; channel.bind(localAddress, promise).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); &#125; else &#123; promise.setFailure(regFuture.cause()); &#125; &#125; &#125;);&#125; 此时的channel是NioServerSocketChannel，而该类没有重写bind方法，所以看AbstractChannel#bind方法： 1234@Overridepublic ChannelFuture bind(SocketAddress localAddress, ChannelPromise promise) &#123; return pipeline.bind(localAddress, promise);&#125; 继续跟踪 1234@Overridepublic final ChannelFuture bind(SocketAddress localAddress, ChannelPromise promise) &#123; return tail.bind(localAddress, promise);&#125; 可以看到，这又是使得bind作为一个出站事件，流经pipilinele，最终到达HeadContext的bind方法。 12345@Overridepublic void bind( ChannelHandlerContext ctx, SocketAddress localAddress, ChannelPromise promise) &#123; unsafe.bind(localAddress, promise);&#125; 此时的unsafe类型是NioMessageUnsafe。该类没有重写bind方法，所以看AbstractUnsafe的bind方法。 1234567891011121314151617181920212223242526272829303132333435@Overridepublic final void bind(final SocketAddress localAddress, final ChannelPromise promise) &#123; assertEventLoop(); ..... boolean wasActive = isActive(); try &#123; doBind(localAddress); &#125; catch (Throwable t) &#123; safeSetFailure(promise, t); closeIfClosed(); return; &#125; if (!wasActive &amp;&amp; isActive()) &#123; // This method is used by outbound operation implementations to trigger an inbound event later. // They do not trigger an inbound event immediately because an outbound operation might have been // triggered by another inbound event handler method. If fired immediately, the call stack // will look like this for example: // // handlerA.inboundBufferUpdated() - (1) an inbound handler method closes a connection. // -&gt; handlerA.ctx.close() // -&gt; channel.unsafe.close() // -&gt; handlerA.channelInactive() - (2) another inbound handler method called while in (1) yet // // which means the execution of two inbound handler methods of the same handler overlap undesirably. eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; pipeline.fireChannelActive(); &#125; &#125;); &#125; safeSetSuccess(promise);&#125; 这里的doBind方法是这个方法的核心方法，这个方法是类AbstractChannel的抽象方法，它由类NioServerSocketChanel实现。 12345678@Overrideprotected void doBind(SocketAddress localAddress) throws Exception &#123; if (PlatformDependent.javaVersion() &gt;= 7) &#123; javaChannel().bind(localAddress, config.getBacklog()); &#125; else &#123; javaChannel().socket().bind(localAddress, config.getBacklog()); &#125;&#125; 好了，看到绑定端口的方法了，这也是java nio 中serverSocketChannel的标准代码。 现在serverSocketChannel是已经绑定了一个本地的端口，而且已经注册到一个Selector中了。但此时关注的事件是0（Channel的注册流程讲过了）。也就是，此时的serverSocketChannel是不能接受任何的链接的。我们继续看bind方法后面的 123456789//这里是调用了this.invokeLater(Runnable)方法，不过，这个方法实际上就是eventLoop().execute，所以这里就这样写了。if (!wasActive &amp;&amp; isActive()) &#123; eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; pipeline.fireChannelActive(); &#125; &#125;);&#125; !wasActive &amp;&amp; isActive()在执行完doBind后，就肯定返回位true了。 这段代码中，先不管eventLoop().execute(Runnable)是怎么运行和什么时候运行的，有一点可以肯定的是，参数Runnable的run方法是肯定会被eventLoop运行的。所以直接看pipeline.fireChannelActive()。而这行代码的意思就是使得ChannelActivie事件能流经pipeline节点中的channelActive方法。第一个流经的就是HeadContext。 12345public void channelActive(ChannelHandlerContext ctx) &#123; ctx.fireChannelActive(); readIfIsAutoRead();&#125; ctx.fireChannelActive();只是使得之后的节点能执行channelActive方法。重点方法是readIfIsAutoRead(). 123456private void readIfIsAutoRead() &#123; //默认情况下返回true if (channel.config().isAutoRead()) &#123; channel.read(); &#125;&#125; 该方法中的channel就是NioServerSocketChannel，它没有重写read方法，所以看AbstractChannel的read方法。 123456789101112@Overridepublic Channel read() &#123; pipeline.read(); return this;&#125;//DefaultChannelPipeline@Overridepublic final ChannelPipeline read() &#123; tail.read(); return this;&#125; 又是这种操作。。。已经看到很多了 根据之前的内容，可以确定，是看HeadContext的read方法 1234@Overridepublic void read(ChannelHandlerContext ctx) &#123; unsafe.beginRead();&#125; 这个方法在客户端中也看过了，这个方法在客户端中的作用就是告诉selector，SocketChannel关注OP_READ事件的。这里的作用也差不多，就是告诉Selector，ServerSocketChanel关注OP_ACCEPT事件。真如我的？看代码： 其实，代码在客户端中已经讲过了，服务端也是一样的流程。 这里的unsafe是NioMessageUnsafe，不过它没有重写这个方法，所以看AbstractUnsafe的beginRead方法 123456789101112131415public final void beginRead() &#123; assertEventLoop(); try &#123; doBeginRead(); &#125; catch (final Exception e) &#123; invokeLater(new Runnable() &#123; @Override public void run() &#123; pipeline.fireExceptionCaught(e); &#125; &#125;); close(voidPromise()); &#125;&#125; doBeginRead是AbstractChannel的抽象方法。这里实现了这个方法的类是AbstractNioMessageChannel，而这个类就是NioServerSocketChannel的父类。看类图： 代码： 1234567891011121314151617181920212223242526//AbstractNioMessageChannel@Overrideprotected void doBeginRead() throws Exception &#123; if (inputShutdown) &#123; return; &#125; super.doBeginRead();&#125;//AbstractNioChannel@Overrideprotected void doBeginRead() throws Exception &#123; // Channel.read() or ChannelHandlerContext.read() was called final SelectionKey selectionKey = this.selectionKey; if (!selectionKey.isValid()) &#123; return; &#125; readPending = true; // readInterestOp的值是在NioSocketChannel初始化时指定的，值就是OP_READ // readInterestOp的值是在NioSocketChannel初始化时指定的，值就是OP_READ final int interestOps = selectionKey.interestOps(); if ((interestOps &amp; readInterestOp) == 0) &#123; selectionKey.interestOps(interestOps | readInterestOp); &#125;&#125; 可以看到，和客户端是一样的，最终都是调用了AbstractNioChannel的doBeginRead方法。而readInterestOp属性值在NioServerSocketChannel就是OP_ACCEPT。所以执行到这里，ServerSocketChannel就已经拥有了接收链接的能力了。 关于 bossGroup 与 workerGroup在客户端的时候, 我们只提供了一个 EventLoopGroup 对象, 而在服务器端的初始化时, 我们设置了两个 EventLoopGroup, 一个是 bossGroup, 另一个是 workerGroup. 那么这两个EventLoopGroup 都是干什么用的呢? 其实呢, bossGroup 是用于服务端 的 accept 的, 即用于处理客户端的连接请求. 而 workerGroup, 其实就是实际上干活的啦, 它们负责客户端连接通道的 IO 操作。这也就是典型的Multiple Reactor Threads 关于 bossGroup 与 workerGroup 的关系, 我们可以用如下图来展示: 首先, 服务器端 bossGroup 不断地监听是否有客户端的连接, 当发现有一个新的客户端连接到来时, bossGroup 就会为此连接初始化各项资源, 然后从 workerGroup 中选出一个 EventLoop 绑定到此客户端连接中. 那么接下来的服务器与客户端的交互过程就全部在此分配的 EventLoop 中了. 首先在ServerBootstrap 初始化时, 调用了 b.group(bossGroup, workerGroup) 设置了两个 EventLoopGroup, 我们跟踪进去看一下: 12345678910111213/** * Set the &#123;@link EventLoopGroup&#125; for the parent (acceptor) and the child (client). These * &#123;@link EventLoopGroup&#125;&#x27;s are used to handle all the events and IO for &#123;@link ServerChannel&#125; and * &#123;@link Channel&#125;&#x27;s. */public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) &#123; super.group(parentGroup); if (this.childGroup != null) &#123; throw new IllegalStateException(&quot;childGroup set already&quot;); &#125; this.childGroup = ObjectUtil.checkNotNull(childGroup, &quot;childGroup&quot;); return this;&#125; 显然, 这个方法初始化了两个字段, 一个是 group &#x3D; parentGroup, 它是在 super.group(parentGroup) 中初始化的, 另一个是 childGroup &#x3D; childGroup. 接着我们启动程序调用了 b.bind 方法来监听一个本地端口. bind 方法会触发如下的调用链: 1AbstractBootstrap.bind -&gt; AbstractBootstrap.doBind -&gt; AbstractBootstrap.initAndRegister AbstractBootstrap.initAndRegister 是我们的老朋友了, 我们在分析客户端程序时, 和它打过很多交到了, 我们再来回顾一下这个方法吧: 1234567final ChannelFuture initAndRegister() &#123; final Channel channel = channelFactory().newChannel(); ... 省略异常判断 init(channel); ChannelFuture regFuture = group().register(channel); return regFuture;&#125; 这里 group() 方法返回的是上面我们提到的 bossGroup, 而这里的 channel 我们也已经分析过了, 它是一个是一个 NioServerSocketChannsl 实例, 因此我们可以知道, group().register(channel) 将 bossGroup 和 NioServerSocketChannsl 关联起来了。 那么 workerGroup 是在哪里与 NioSocketChannel 关联的呢?我们继续看 init(channel) 方法，该方法在ServerBootstrap中实现： 123456789101112131415161718192021222324252627282930313233343536@Overridevoid init(Channel channel) &#123; // 把ServerBootstrap#option方法设置的参数，传递给NioServerSocketChannel的DefaultServerSocketChannelConfig setChannelOptions(channel, newOptionsArray(), logger); // 把ServerBootstrap#attr方法设置的参数，传递给NioServerSocketChannel的DefaultServerSocketChannelConfig setAttributes(channel, newAttributesArray()); //通过handler方法设置的 ChannelPipeline p = channel.pipeline(); final EventLoopGroup currentChildGroup = childGroup; // 这个是通过childHandler方法设置的 final ChannelHandler currentChildHandler = childHandler; final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions = newOptionsArray(childOptions); final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs = newAttributesArray(childAttrs); p.addLast(new ChannelInitializer&lt;Channel&gt;() &#123; @Override public void initChannel(final Channel ch) &#123; //这个pipeline仍然是NioServerSocketChannel的pipeline final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); //这个handler就是通过ServerBootstrap的handler方法设置的 if (handler != null) &#123; pipeline.addLast(handler); &#125; ch.eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; //这里就是childGroup与NioSocketChannel关联起来的关键 pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); &#125; &#125;); &#125; &#125;);&#125; 从上面的代码片段中我们看到, 它为 pipeline 中添加了一个 ChannelInitializer, 而这个 ChannelInitializer 中添加了一个关键的 ServerBootstrapAcceptor 。关于 handler 的添加与初始化的过程, 我们留待下一小节中分析, 我们现在关注一下 ServerBootstrapAcceptor 类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private static class ServerBootstrapAcceptor extends ChannelInboundHandlerAdapter &#123; private final EventLoopGroup childGroup; private final ChannelHandler childHandler; private final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] childOptions; private final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] childAttrs; private final Runnable enableAutoReadTask; ServerBootstrapAcceptor( final Channel channel, EventLoopGroup childGroup, ChannelHandler childHandler, Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] childOptions, Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] childAttrs) &#123; this.childGroup = childGroup; this.childHandler = childHandler; this.childOptions = childOptions; this.childAttrs = childAttrs; ..... &#125; @Override @SuppressWarnings(&quot;unchecked&quot;) public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; final Channel child = (Channel) msg; child.pipeline().addLast(childHandler); setChannelOptions(child, childOptions, logger); setAttributes(child, childAttrs); try &#123; childGroup.register(child).addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; if (!future.isSuccess()) &#123; forceClose(child, future.cause()); &#125; &#125; &#125;); &#125; catch (Throwable t) &#123; forceClose(child, t); &#125; &#125; private static void forceClose(Channel child, Throwable t) &#123; child.unsafe().closeForcibly(); logger.warn(&quot;Failed to register an accepted channel: &#123;&#125;&quot;, child, t); &#125; ......&#125; ServerBootstrapAcceptor 中的 childGroup 是构造此对象是传入的 currentChildGroup, 即我们的 workerGroup, 而 Channel 是一个 NioSocketChannel 的实例, 因此这里的 childGroup.register 就是将 workerGroup 中的某个 EventLoop 和 NioSocketChannel 关联了. 既然这样, 那么现在的问题是, ServerBootstrapAcceptor.channelRead 方法是怎么被调用的呢? 其实当一个 client 连接到 server 时, Java 底层的 NIO ServerSocketChannel 会有一个 SelectionKey.OP_ACCEPT 就绪事件, 接着就会调用到 NioServerSocketChannel.doReadMessages: 1234567@Overrideprotected int doReadMessages(List&lt;Object&gt; buf) throws Exception &#123; SocketChannel ch = javaChannel().accept(); ... 省略异常处理 buf.add(new NioSocketChannel(this, ch)); return 1;&#125; 在 doReadMessages 中, 通过 javaChannel().accept() 获取到客户端新连接的 SocketChannel, 接着就实例化一个 NioSocketChannel, 并且传入 NioServerSocketChannel 对象(即 this), 由此可知, 我们创建的这个 NioSocketChannel 的父 Channel 就是 NioServerSocketChannel 实例 .接下来就经由 Netty 的 ChannelPipeline 机制, 将读取事件逐级发送到各个 handler 中, 于是就会触发前面我们提到的 ServerBootstrapAcceptor.channelRead 方法啦. handler 的添加过程服务器端的 handler 的添加过程和客户端的有点区别, 和 EventLoopGroup 一样, 服务器端的 handler 也有两个, 一个是通过 handler() 方法设置 handler 字段, 另一个是通过 childHandler() 设置 childHandler 字段. 通过前面的 bossGroup 和 workerGroup 的分析, 其实我们在这里可以大胆地猜测: handler 字段与 accept 过程有关, 即这个 handler 负责处理客户端的连接请求; 而 childHandler 就是负责和客户端的连接的 IO 交互.那么实际上是不是这样的呢? 来, 我们继续通过代码证明. 在 关于 bossGroup 与 workerGroup 小节中, 我们提到, ServerBootstrap 重写了 init 方法, 在这个方法中添加了 handler: 123456789101112131415161718192021222324252627282930313233343536@Overridevoid init(Channel channel) &#123; // 把ServerBootstrap#option方法设置的参数，传递给NioServerSocketChannel的DefaultServerSocketChannelConfig setChannelOptions(channel, newOptionsArray(), logger); // 把ServerBootstrap#attr方法设置的参数，传递给NioServerSocketChannel的DefaultServerSocketChannelConfig setAttributes(channel, newAttributesArray()); //通过handler方法设置的 ChannelPipeline p = channel.pipeline(); final EventLoopGroup currentChildGroup = childGroup; // 这个是通过childHandler方法设置的 final ChannelHandler currentChildHandler = childHandler; final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions = newOptionsArray(childOptions); final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs = newAttributesArray(childAttrs); p.addLast(new ChannelInitializer&lt;Channel&gt;() &#123; @Override public void initChannel(final Channel ch) &#123; //这个pipeline仍然是NioServerSocketChannel的pipeline final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); //这个handler就是通过ServerBootstrap的handler方法设置的 if (handler != null) &#123; pipeline.addLast(handler); &#125; ch.eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; //这里就是childGroup与NioSocketChannel关联起来的关键 pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); &#125; &#125;); &#125; &#125;);&#125; 上面代码的 initChannel 方法中, 首先通过 handler() 方法获取一个 handler, 如果获取的 handler 不为空,则添加到 pipeline 中. 然后接着, 添加了一个 ServerBootstrapAcceptor 实例. 那么这里 handler() 方法返回的是哪个对象呢? 其实它返回的是 handler 字段, 而这个字段就是我们在服务器端的启动代码中设置的: 123b.group(bossGroup, workerGroup) ... .handler(new LoggingHandler(LogLevel.INFO)) 那么这个时候, pipeline 中的 handler 情况如下: 根据我们原来分析客户端的经验, 我们指定, 当 channel 绑定到 eventLoop 后(在这里是 NioServerSocketChannel 绑定到 bossGroup)中时, 会调用pipeline.fireChannelRegistered 使得Registered事件从head节点开始流经各个可达节点的Handler的channelRegistered方法。这样就会触发 ChannelInitializer.initChannel 方法的调用.因此在绑定完成后, 此时的 pipeline 的内如如下: 前面我们在分析 bossGroup 和 workerGroup 时, 已经知道了在 ServerBootstrapAcceptor.channelRead 中会为新建的 Channel 设置 handler 并注册到一个 eventLoop 中, 即: 12345678@Override@SuppressWarnings(&quot;unchecked&quot;)public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; final Channel child = (Channel) msg; child.pipeline().addLast(childHandler); ... childGroup.register(child).addListener(...);&#125; 而这里的 childHandler 就是我们在服务器端启动代码中设置的 handler: 12345678910111213b.group(bossGroup, workerGroup) ... .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline p = ch.pipeline(); if (sslCtx != null) &#123; p.addLast(sslCtx.newHandler(ch.alloc())); &#125; //p.addLast(new LoggingHandler(LogLevel.INFO)); p.addLast(new EchoServerHandler()); &#125; &#125;); 后续的步骤就没有什么好说的了, 当这个客户端连接 Channel 注册后, 就会触发 ChannelInitializer.initChannel 方法的调用, 此后的客户端连接的 ChannelPipeline 状态如下: 最后我们来总结一下服务器端的 handler 与 childHandler 的区别与联系: 在服务器 NioServerSocketChannel 的 pipeline 中添加的是 handler 与 ServerBootstrapAcceptor. 当有新的客户端连接请求时, ServerBootstrapAcceptor.channelRead 中负责新建此连接的 NioSocketChannel 并添加 childHandler 到 NioSocketChannel 对应的 pipeline 中, 并将此 channel 绑定到 workerGroup 中的某个 eventLoop 中. handler 是在 accept 阶段起作用, 它处理客户端的连接请求. childHandler 是在客户端连接建立以后起作用, 它负责客户端连接的 IO 交互. 服务端接收的链接是怎么到workerGroup中的服务端链接和客户端很像，不同的是客户端用的是connect而服务端调用的是bind，而且，在NioServerSocketChannel注册到一个Eventloop后会执行ServerSocketChannel.bind操作。 接收链接也是在NioEventLoop的线程中操作的 在这里说明下为什么有一个 SelectionKey.OP_ACCEPT 就绪事件, 接着就会调用到 NioServerSocketChannel.doReadMessages: 在NioEventLoop线程中run方法中有一段代码: 1234567891011121314151617NioEventLoop.run -&gt; NioEventLoop.processSelectedKeys() -&gt; NioEventLoop.processSelectedKeysPlain(Set&lt;SelectionKey&gt; selectedKeys) -&gt; NioEventLoop.processSelectedKey(SelectionKey k, AbstractNioChannel ch) private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) &#123; final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe(); if (!k.isValid()) &#123; //省略了不关键代码 ... if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) &#123; unsafe.read(); &#125; &#125; catch (CancelledKeyException ignored) &#123; unsafe.close(unsafe.voidPromise()); &#125;&#125; 看了了吧， 123if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) &#123; unsafe.read();&#125; 在NioServerSocketChannel的端口绑定和OP_ACCEPT事件注册这一节中就讲了，ServerSocketChannel是怎么注册OP_ACCEPT事件的。 这里有一个客户端链接后，这个代码就会被触发。而NioServerSocketChannel的unsafe又是NioMessageUnsafe NioMessageUnsafe.read() 1234567891011121314151617181920@Overridepublic void read() &#123; //省略了不关键代码 .... int localRead = doReadMessages(readBuf); .... int size = readBuf.size(); for (int i = 0; i &lt; size; i ++) &#123; readPending = false; pipeline.fireChannelRead(readBuf.get(i)); &#125; readBuf.clear(); allocHandle.readComplete(); pipeline.fireChannelReadComplete(); pipeline.fireExceptionCaught(exception); if (exception != null) &#123; closed = closeOnReadError(exception); pipeline.fireExceptionCaught(exception); &#125;&#125; 所以有SelectionKey.OP_ACCEPT事件后就会执行NioServerSocketChannel.doReadMessages方法，接着会触发 pipeline.fireChannelRead(readBuf.get(i)) pipeline.fireChannelReadComplete(); 这里的pipeline是父pipeline 现在需要留意的是pipeline.fireChannelRead(readBuf.get(i));由于这时NioServerSocketChannel的pipeline的情况： 没错，pipeline.fireChannelRead就是会触发ChannelRead事件。该事件会在pipeline中的沿着InBound传输，也就是一定会执行到ServerBootstrapAcceptor的channelRead方法来为NioSocketChannel分配EventLoop 123456789101112131415161718192021public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; final Channel child = (Channel) msg; child.pipeline().addLast(childHandler); setChannelOptions(child, childOptions, logger); setAttributes(child, childAttrs); try &#123; childGroup.register(child).addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; if (!future.isSuccess()) &#123; forceClose(child, future.cause()); &#125; &#125; &#125;); &#125; catch (Throwable t) &#123; forceClose(child, t); &#125;&#125; 总结1234567891011121314151617181920212223242526private Selector selector;private ServerSocketChannel serverChannel;private volatile boolean started;/** * 构造方法 * @param port 指定要监听的端口号 */public NioServerHandle(int port) &#123; try&#123; //创建选择器 selector = Selector.open(); //打开监听通道 serverChannel = ServerSocketChannel.open(); //如果为 true，则此通道将被置于阻塞模式； // 如果为 false，则此通道将被置于非阻塞模式 serverChannel.configureBlocking(false);//开启非阻塞模式 serverChannel.socket().bind(new InetSocketAddress(port)); serverChannel.register(selector, SelectionKey.OP_ACCEPT); //标记服务器已开启 started = true; System.out.println(&quot;服务器已启动，端口号：&quot; + port); &#125;catch(IOException e)&#123; e.printStackTrace(); System.exit(1); &#125;&#125;","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"Netty 源码分析之 一 Bootstrap(客户端)","slug":"netty/netty源码/Netty 源码分析之 一 Bootstrap(客户端)","date":"2021-11-23T12:00:17.000Z","updated":"2022-03-23T09:03:58.208Z","comments":true,"path":"blog/netty/netty源码/Netty 源码分析之 一 Bootstrap(客户端)/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/netty%E6%BA%90%E7%A0%81/Netty%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8B%20%E4%B8%80%20Bootstrap(%E5%AE%A2%E6%88%B7%E7%AB%AF)/","excerpt":"","text":"基于4.1.59 注意：netty的源码阅读都是根据java 的 Nio讲解的，如果是别的比如epoll，在对应的Channel和Group类上会有差别，不过整体的流程是一样的 123456789101112131415161718EventLoopGroup group = new NioEventLoopGroup();try &#123; Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .remoteAddress(new InetSocketAddress(&quot;127.0.1.1&quot;, port)) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addFirst(new EchoClientHandler()); &#125; &#125;); b.connect().addListener((ChannelFutureListener)(future) -&gt; &#123; System.out.println(&quot;已经执行完了&quot;); &#125;).channel().closeFuture().sync();&#125; catch (Exception e) &#123; group.shutdownGracefully().sync();&#125; 从上面的客户端代码虽然简单, 但是却展示了 Netty 客户端初始化时所需的所有内容: EventLoopGroup: 不论是服务器端还是客户端, 都必须指定 EventLoopGroup. 在这个例子中, 指定了 NioEventLoopGroup, 表示一个 NIO 的EventLoopGroup. ChannelType: 指定 Channel 的类型. 因为是客户端, 因此使用了 NioSocketChannel. Handler: 设置数据的处理器. Bootstrap: 用来连接服务。封装并隐藏网络io的细节 NioSocketChannel 的初始化过程在 Netty 中, Channel 是一个 Socket 的抽象, 它为用户提供了关于 Socket 状态(是否是连接还是断开) 以及对 Socket 的读写等操作. 每当 Netty 建立了一个连接后, 都会有一个对应的 Channel 实例. NioSocketChannel 的类层次结构如下: ChannelFactory 和 Channel 类型的确定除了 TCP 协议以外, Netty 还支持很多其他的连接协议, 并且每种协议还有 NIO(异步 IO) 和 OIO(Old-IO, 即传统的阻塞 IO) 版本的区别. 不同协议不同的阻塞类型的连接都有不同的 Channel 类型与之对应下面是一些常用的 Channel 类型: NioSocketChannel, 代表异步的客户端 TCP Socket 连接. NioServerSocketChannel, 异步的服务器端 TCP Socket 连接. NioDatagramChannel, 异步的 UDP 连接 NioSctpChannel, 异步的客户端 Sctp 连接. NioSctpServerChannel, 异步的 Sctp 服务器端连接. OioSocketChannel, 同步的客户端 TCP Socket 连接. OioServerSocketChannel, 同步的服务器端 TCP Socket 连接. OioDatagramChannel, 同步的 UDP 连接 OioSctpChannel, 同步的 Sctp 服务器端连接. OioSctpServerChannel, 同步的客户端 TCP Socket 连接. 那么我们是如何设置所需要的 Channel 的类型的呢? 答案是 channel() 方法的调用.回想一下我们在客户端连接代码的初始化 Bootstrap 中, 会调用 channel() 方法, 传入 NioSocketChannel.class, 这个方法其实就是初始化了一个 ReflectiveChannelFactory: 123456789101112131415/** * 设置Channel的类型，在netty中，Channel是一个Socket抽象，它为用户提供了关于Socket的状态和读写的操作，每当Netty建立一个 * 连接后，都会有一个对应的Channel实例，而这个Channel的实例类型就是通过这个方法确定了。 * 通过对方法的跟踪 * * The &#123;@link Class&#125; which is used to create &#123;@link Channel&#125; instances from. * You either use this or &#123;@link #channelFactory(io.netty.channel.ChannelFactory)&#125; if your * &#123;@link Channel&#125; implementation has no no-args constructor. */public B channel(Class&lt;? extends C&gt; channelClass) &#123; //最后将ReflectiveChannelFactory赋值给内部的属性，channelFactory return channelFactory(new ReflectiveChannelFactory&lt;C&gt;( ObjectUtil.checkNotNull(channelClass, &quot;channelClass&quot;) ));&#125; 而 ReflectiveChannelFactory 实现了 ChannelFactory 接口, 它提供了唯一的方法, 即 newChannel. ChannelFactory, 顾名思义, 就是产生 Channel 的工厂类. 12345678910111213141516171819202122232425262728293031323334public class ReflectiveChannelFactory&lt;T extends Channel&gt; implements ChannelFactory&lt;T&gt; &#123; private final Constructor&lt;? extends T&gt; constructor; /** * 获取类的无参构造方法 * * @param clazz */ public ReflectiveChannelFactory(Class&lt;? extends T&gt; clazz) &#123; ObjectUtil.checkNotNull(clazz, &quot;clazz&quot;); try &#123; this.constructor = clazz.getConstructor(); &#125; catch (NoSuchMethodException e) &#123; throw new IllegalArgumentException(&quot;Class &quot; + StringUtil.simpleClassName(clazz) + &quot; does not have a public non-arg constructor&quot;, e); &#125; &#125; @Override public T newChannel() &#123; try &#123; return constructor.newInstance(); &#125; catch (Throwable t) &#123; throw new ChannelException(&quot;Unable to create Channel from class &quot; + constructor.getDeclaringClass(), t); &#125; &#125; @Override public String toString() &#123; return StringUtil.simpleClassName(ReflectiveChannelFactory.class) + &#x27;(&#x27; + StringUtil.simpleClassName(constructor.getDeclaringClass()) + &quot;.class)&quot;; &#125;&#125; 根据上面代码, 我们就可以确定: Bootstrap 中的 ChannelFactory 的实现是 ReflectiveChannelFactory 生成的 Channel 的具体类型是 NioSocketChannel. Channel 的实例化过程, 其实就是调用的 ChannelFactory#newChannel 方法, 而实例化的 Channel 的具体的类型又是和在初始化 Bootstrap 时传入的 channel() 方法的参数相关. 因此对于我们这个例子中的客户端的 Bootstrap 而言, 生成的的 Channel 实例就是 NioSocketChannel. Channel 实例化前面我们已经知道了如何确定一个 Channel 的类型, 并且了解到 Channel 是通过工厂方法 ChannelFactory.newChannel() 来实例化的, 那么 ChannelFactory.newChannel() 方法在哪里调用呢? 继续跟踪, 我们发现其调用链是: 1Bootstrap.connect -&gt; Bootstrap.doResolveAndConnect -&gt; AbstractBootstrap.initAndRegister 在 AbstractBootstrap.initAndRegister 中就调用了 channelFactory().newChannel() 来获取一个新的 NioSocketChannel 实例, 其源码如下: 1234567891011121314151617181920212223242526final ChannelFuture initAndRegister() &#123; Channel channel = null; try &#123; channel = channelFactory.newChannel(); init(channel); &#125; catch (Throwable t) &#123; if (channel != null) &#123; // channel can be null if newChannel crashed (eg SocketException(&quot;too many open files&quot;)) channel.unsafe().closeForcibly(); // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(channel, GlobalEventExecutor.INSTANCE).setFailure(t); &#125; // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(new FailedChannel(), GlobalEventExecutor.INSTANCE).setFailure(t); &#125; ChannelFuture regFuture = config().group().register(channel); if (regFuture.cause() != null) &#123; if (channel.isRegistered()) &#123; channel.close(); &#125; else &#123; channel.unsafe().closeForcibly(); &#125; &#125; return regFuture;&#125; 在 newChannel 中, 通过类对象的 newInstance 来获取一个新 Channel 实例, 因而会调用NioSocketChannel 的无参构造器.NioSocketChannel 默认构造器代码如下: 123456789101112131415161718private static final SelectorProvider DEFAULT_SELECTOR_PROVIDER = SelectorProvider.provider public NioSocketChannel() &#123; this(DEFAULT_SELECTOR_PROVIDER);&#125;public NioSocketChannel(SelectorProvider provider) &#123; this(newSocket(provider));&#125;public NioSocketChannel(SocketChannel socket) &#123; this(null, socket);&#125;public NioSocketChannel(Channel parent, SocketChannel socket) &#123; super(parent, socket); config = new NioSocketChannelConfig(this, socket.socket());&#125; 这里的代码比较关键, 我们看到, 在这个构造器中, 会调用 newSocket 来打开一个新的 Java NIO SocketChannel: 1234private static SocketChannel newSocket(SelectorProvider provider) &#123; ... return provider.openSocketChannel();&#125; 接着会调用父类, 即 AbstractNioByteChannel 的构造器: 12345//SelectableChannel是java NIO SocketChannel和ServerSocketChannel的共同父类//在这里，ch是SocketChannelprotected AbstractNioByteChannel(Channel parent, SelectableChannel ch) &#123; super(parent, ch, SelectionKey.OP_READ);&#125; 接着会继续调用父类 AbstractNioChannel 的构造器, 并传入了参数 readInterestOp &#x3D; SelectionKey.OP_READ,表示这Channel关注读事件（这里只是一个简单的赋值，还没有把OP_READ注册到selector中） 12345678910111213141516171819202122232425262728293031323334353637/** * Create a new instance * * @param parent the parent &#123;@link Channel&#125; by which this instance was created. May be &#123;@code null&#125; * @param ch the underlying &#123;@link SelectableChannel&#125; on which it operates * @param readInterestOp the ops to set to receive data from the &#123;@link SelectableChannel&#125; */protected AbstractNioChannel(Channel parent, SelectableChannel ch, int readInterestOp) &#123; super(parent); this.ch = ch; this.readInterestOp = readInterestOp; try &#123; ch.configureBlocking(false); &#125; catch (IOException e) &#123; try &#123; ch.close(); &#125; catch (IOException e2) &#123; logger.warn( &quot;Failed to close a partially initialized socket.&quot;, e2); &#125; throw new ChannelException(&quot;Failed to enter non-blocking mode.&quot;, e); &#125;&#125;/** * Creates a new instance. * * @param parent * the parent of this channel. &#123;@code null&#125; if there&#x27;s no parent. */protected AbstractChannel(Channel parent) &#123; this.parent = parent; id = newId(); unsafe = newUnsafe(); pipeline = newChannelPipeline();&#125; 这里有4点要关注： 每个Channel都有一个id，类型是ChannelId，从名字就可以知道，这是代表了Chanel的id 创建了一个unsafe对象，这个unsafe对象是NioSocketChannelUnsafe（在NioSocketChannel中实现了newUnsafe方法） 这个ChannelUnsafe是对处理IO事件的抽象 每个Channel都有一个pipeline，这个对象类型是DefaultChannelPipeline。这里说明每一个Channel都有一个自己的pipeline，并且在Channel创建时自动创建。 传入了关注的事件，这里是读事件，并把该事件的值赋值给了readInterestOp属性，表示这个创建的Channel关注的是网络IO操作（注意，这里只是赋值操作） 之所以还没关注OP_CONNECT，是因为启动流程还没结束，这里只是一个简单的赋值，还没有注册到一个selector中，而且还需要为Channel分配一个EventLoop。 到这里, 一个完整的 NioSocketChannel 就初始化完成了, 我们可以稍微总结一下构造一个 NioSocketChannel 所需要做的工作: 调用 NioSocketChannel.newSocket(SelectorProvider) 创建一个新的 Java NIO SocketChannel AbstractChannel(Channel parent) 中初始化 AbstractChannel 的属性: parent 属性置为 null unsafe 通过newUnsafe() 实例化一个 unsafe 对象, 它的类型是 NioSocketChannel内部中定义的类型为NioSocketChannelUnsafe 的内部类 ChannelId id &#x3D; new DefaultChannelId(); pipeline &#x3D; new DefaultChannelPipeline(this) 。 AbstractNioChannel 中的属性: SelectableChannel ch 被设置为 Java SocketChannel, 即NioSocketChannel#newSocket 返回的 Java NIO SocketChannel. readInterestOp 被设置为 SelectionKey.OP_READ。 SelectableChannel ch 被配置为非阻塞的 **ch.configureBlocking(false)**NioSocketChannel 中的属性: NioSocketChannel 中的属性: SocketChannelConfig config &#x3D; new NioSocketChannelConfig(this, socket.socket()) ，这个对应是用来保存Bootstrap#handler、Bootstrap#attr和Bootstrap#option这三个方法设置的属性 关于 unsafe 字段的初始化unsafe 特别关键, 它封装了对 Java 底层 Socket 的操作, 因此实际上是沟通 Netty 上层和 Java 底层的重要的桥梁。(在juc中，会把一些需要使用native的方法的操作（jvm和系统层）放到一个类中，这些累创建对象的属性名都喜欢加个unsafe) 那么我们就来看一下 Unsafe 接口所提供的方法吧: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110interface Unsafe &#123; /** * Return the assigned &#123;@link RecvByteBufAllocator.Handle&#125; which will be used to allocate &#123;@link ByteBuf&#125;&#x27;s when * receiving data. */ RecvByteBufAllocator.Handle recvBufAllocHandle(); /** * Return the &#123;@link SocketAddress&#125; to which is bound local or * &#123;@code null&#125; if none. */ SocketAddress localAddress(); /** * Return the &#123;@link SocketAddress&#125; to which is bound remote or * &#123;@code null&#125; if none is bound yet. */ SocketAddress remoteAddress(); /** * Register the &#123;@link Channel&#125; of the &#123;@link ChannelPromise&#125; and notify * the &#123;@link ChannelFuture&#125; once the registration was complete. */ void register(EventLoop eventLoop, ChannelPromise promise); /** * Bind the &#123;@link SocketAddress&#125; to the &#123;@link Channel&#125; of the &#123;@link ChannelPromise&#125; and notify * it once its done. */ void bind(SocketAddress localAddress, ChannelPromise promise); /** * Connect the &#123;@link Channel&#125; of the given &#123;@link ChannelFuture&#125; with the given remote &#123;@link SocketAddress&#125;. * If a specific local &#123;@link SocketAddress&#125; should be used it need to be given as argument. Otherwise just * pass &#123;@code null&#125; to it. * * The &#123;@link ChannelPromise&#125; will get notified once the connect operation was complete. */ void connect(SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise); /** * Disconnect the &#123;@link Channel&#125; of the &#123;@link ChannelFuture&#125; and notify the &#123;@link ChannelPromise&#125; once the * operation was complete. */ void disconnect(ChannelPromise promise); /** * Close the &#123;@link Channel&#125; of the &#123;@link ChannelPromise&#125; and notify the &#123;@link ChannelPromise&#125; once the * operation was complete. */ void close(ChannelPromise promise); /** * Closes the &#123;@link Channel&#125; immediately without firing any events. Probably only useful * when registration attempt failed. */ void closeForcibly(); /** * Deregister the &#123;@link Channel&#125; of the &#123;@link ChannelPromise&#125; from &#123;@link EventLoop&#125; and notify the * &#123;@link ChannelPromise&#125; once the operation was complete. */ void deregister(ChannelPromise promise); /** * Schedules a read operation that fills the inbound buffer of the first &#123;@link ChannelInboundHandler&#125; in the * &#123;@link ChannelPipeline&#125;. If there&#x27;s already a pending read operation, this method does nothing. */ void beginRead(); /** * Schedules a write operation. */ void write(Object msg, ChannelPromise promise); /** * Flush out all write operations scheduled via &#123;@link #write(Object, ChannelPromise)&#125;. */ void flush(); /** * Return a special ChannelPromise which can be reused and passed to the operations in &#123;@link Unsafe&#125;. * It will never be notified of a success or error and so is only a placeholder for operations * that take a &#123;@link ChannelPromise&#125; as argument but for which you not want to get notified. */ ChannelPromise voidPromise(); /** * Returns the &#123;@link ChannelOutboundBuffer&#125; of the &#123;@link Channel&#125; where the pending write requests are stored. */ ChannelOutboundBuffer outboundBuffer();&#125;interface Unsafe &#123; SocketAddress localAddress(); SocketAddress remoteAddress(); void register(EventLoop eventLoop, ChannelPromise promise); void bind(SocketAddress localAddress, ChannelPromise promise); void connect(SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise); void disconnect(ChannelPromise promise); void close(ChannelPromise promise); void closeForcibly(); void deregister(ChannelPromise promise); void beginRead(); void write(Object msg, ChannelPromise promise); void flush(); ChannelPromise voidPromise(); ChannelOutboundBuffer outboundBuffer();&#125; 一看便知, 这些方法其实都会对应到相关的 Java 底层的 Socket 的操作. 关于 pipeline 的初始化在实例化一个 Channel 时, 必然伴随着实例化一个 ChannelPipeline. 而我们确实在 AbstractChannel 的构造器看到了 pipeline 字段被初始化为 DefaultChannelPipeline 的实例. 那么我们就来看一下, DefaultChannelPipeline 构造器做了哪些工作吧: 1234567891011protected DefaultChannelPipeline(Channel channel) &#123; this.channel = ObjectUtil.checkNotNull(channel, &quot;channel&quot;); succeededFuture = new SucceededChannelFuture(channel, null); voidPromise = new VoidChannelPromise(channel, true); tail = new TailContext(this); head = new HeadContext(this); head.next = tail; tail.prev = head;&#125; 我们调用 DefaultChannelPipeline 的构造器, 传入了一个 channel, 而这个 channel 其实就是我们实例化的 NioSocketChannel, DefaultChannelPipeline 会将这个 NioSocketChannel 对象保存在channel 字段中. DefaultChannelPipeline 中, 还有两个特殊的字段, 即 head 和 tail, 而这两个字段是一个双向链表的头和尾. 其实在 DefaultChannelPipeline 中, 维护了一个以 AbstractChannelHandlerContext 为节点的双向链表, 这个链表是 Netty 实现 Pipeline 机制的关键。 我们都知道，ChannelPipeline提供了ChannelHandler链的容器，并定义了用于在该链上传播入站和出站事件流的API。而再看看TailContext和HeadContext 我们可以看到, 链表中 head 是一个 ChannelOutboundHandler、ChannelInboundHandler, 而 tail 则是一个 ChannelInboundHandler.这说明了pipeline的入站、出站总是会流经head。入站总是会流经tail 注意，虽然HandlerContext也实现了ChannelOutboundHandler、ChannelInboundHandler接口，但是HandlerContext是作为Handler和pipeline绑定的桥梁，只是表示事件都能流经handlerContext，是否处理还需要看Handler实现的接口。 接着看一下 HeadContext 的构造器12345HeadContext(DefaultChannelPipeline pipeline) &#123; super(pipeline, null, HEAD_NAME, HeadContext.class); unsafe = pipeline.channel().unsafe(); setAddComplete();&#125; 它调用了父类 AbstractChannelHandlerContext 的构造器. 把在创建Channel时创建的NioSocketChannelUnsafe赋值给了HeadContext，前面已经说了unsafe封装了对 Java 底层 Socket 的操作，而把它赋值给HeadContext，而且HeadContext实现了ChannelOutboundHandler和ChannelInboundHandler，想到了什么？没错，就是HeadContext作为pipeline的头结点（ChannelInboundHandler链的头结点、ChannelOutboundHandler链的最后一个结点）负责从SocketChannel中读数据，使数据能流经之后的ChannelInboundHandler；把前面的ChannelOutboundHandler处理过的数据发送给目标机器(SocketChannel.write)。注意：虽然从这些值推出了它的作用，但实际的读数据和写数据不是在HeadContext中操作的，真实的操作流程在之后会讲 &#x3D;&#x3D;执行了setAddComplete()&#x3D;&#x3D; 1234567891011121314final boolean setAddComplete() &#123; for (;;) &#123; int oldState = handlerState; if (oldState == REMOVE_COMPLETE) &#123; return false; &#125; // Ensure we never update when the handlerState is REMOVE_COMPLETE already. // oldState is usually ADD_PENDING but can also be REMOVE_COMPLETE when an EventExecutor is used that is not // exposing ordering guarantees. if (HANDLER_STATE_UPDATER.compareAndSet(this, oldState, ADD_COMPLETE)) &#123; return true; &#125; &#125;&#125; 挺简单的，就是用cas+循环（避免使用锁）把这个Handler的属性handlerState设为ADD_COMPLETE，表示添加HeadContext完成 &#x3D;&#x3D;现在回头看下父类 AbstractChannelHandlerContext 的构造器。&#x3D;&#x3D; 这个方法挺有趣的，从这个方法就可以知道为什么事件pipeling流经时，能知道下一个Handler能不能处理。下面看下AbstractChannelHandlerContext 的构造器源码： 123456789AbstractChannelHandlerContext(DefaultChannelPipeline pipeline, EventExecutor executor, String name, Class&lt;? extends ChannelHandler&gt; handlerClass) &#123; this.name = ObjectUtil.checkNotNull(name, &quot;name&quot;); this.pipeline = pipeline; this.executor = executor; this.executionMask = mask(handlerClass); // Its ordered if its driven by the EventLoop or the given Executor is an instanceof OrderedEventExecutor. ordered = executor == null || executor instanceof OrderedEventExecutor;&#125; pipeline的executionMask属性重点：mask(handlerClass)，在HeadContext中调用也就是handlerClass是HeadContext的类对象(HeadContext.class) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108static int mask(Class&lt;? extends ChannelHandler&gt; clazz) &#123; // Try to obtain the mask from the cache first. If this fails calculate it and put it in the cache for fast // lookup in the future. Map&lt;Class&lt;? extends ChannelHandler&gt;, Integer&gt; cache = MASKS.get(); Integer mask = cache.get(clazz); if (mask == null) &#123; mask = mask0(clazz); cache.put(clazz, mask); &#125; return mask;&#125;private static int mask0(Class&lt;? extends ChannelHandler&gt; handlerType) &#123; int mask = MASK_EXCEPTION_CAUGHT; try &#123; if (ChannelInboundHandler.class.isAssignableFrom(handlerType)) &#123; mask |= MASK_ALL_INBOUND; if (isSkippable(handlerType, &quot;channelRegistered&quot;, ChannelHandlerContext.class)) &#123; // 这段代码的作用就是把对应的位置设置为0 mask &amp;= ~MASK_CHANNEL_REGISTERED; &#125; if (isSkippable(handlerType, &quot;channelUnregistered&quot;, ChannelHandlerContext.class)) &#123; mask &amp;= ~MASK_CHANNEL_UNREGISTERED; &#125; if (isSkippable(handlerType, &quot;channelActive&quot;, ChannelHandlerContext.class)) &#123; mask &amp;= ~MASK_CHANNEL_ACTIVE; &#125; if (isSkippable(handlerType, &quot;channelInactive&quot;, ChannelHandlerContext.class)) &#123; mask &amp;= ~MASK_CHANNEL_INACTIVE; &#125; if (isSkippable(handlerType, &quot;channelRead&quot;, ChannelHandlerContext.class, Object.class)) &#123; mask &amp;= ~MASK_CHANNEL_READ; &#125; if (isSkippable(handlerType, &quot;channelReadComplete&quot;, ChannelHandlerContext.class)) &#123; mask &amp;= ~MASK_CHANNEL_READ_COMPLETE; &#125; if (isSkippable(handlerType, &quot;channelWritabilityChanged&quot;, ChannelHandlerContext.class)) &#123; mask &amp;= ~MASK_CHANNEL_WRITABILITY_CHANGED; &#125; if (isSkippable(handlerType, &quot;userEventTriggered&quot;, ChannelHandlerContext.class, Object.class)) &#123; mask &amp;= ~MASK_USER_EVENT_TRIGGERED; &#125; &#125; if (ChannelOutboundHandler.class.isAssignableFrom(handlerType)) &#123; mask |= MASK_ALL_OUTBOUND; if (isSkippable(handlerType, &quot;bind&quot;, ChannelHandlerContext.class, SocketAddress.class, ChannelPromise.class)) &#123; mask &amp;= ~MASK_BIND; &#125; if (isSkippable(handlerType, &quot;connect&quot;, ChannelHandlerContext.class, SocketAddress.class, SocketAddress.class, ChannelPromise.class)) &#123; mask &amp;= ~MASK_CONNECT; &#125; if (isSkippable(handlerType, &quot;disconnect&quot;, ChannelHandlerContext.class, ChannelPromise.class)) &#123; mask &amp;= ~MASK_DISCONNECT; &#125; if (isSkippable(handlerType, &quot;close&quot;, ChannelHandlerContext.class, ChannelPromise.class)) &#123; mask &amp;= ~MASK_CLOSE; &#125; if (isSkippable(handlerType, &quot;deregister&quot;, ChannelHandlerContext.class, ChannelPromise.class)) &#123; mask &amp;= ~MASK_DEREGISTER; &#125; if (isSkippable(handlerType, &quot;read&quot;, ChannelHandlerContext.class)) &#123; mask &amp;= ~MASK_READ; &#125; if (isSkippable(handlerType, &quot;write&quot;, ChannelHandlerContext.class, Object.class, ChannelPromise.class)) &#123; mask &amp;= ~MASK_WRITE; &#125; if (isSkippable(handlerType, &quot;flush&quot;, ChannelHandlerContext.class)) &#123; mask &amp;= ~MASK_FLUSH; &#125; &#125; if (isSkippable(handlerType, &quot;exceptionCaught&quot;, ChannelHandlerContext.class, Throwable.class)) &#123; mask &amp;= ~MASK_EXCEPTION_CAUGHT; &#125; &#125; catch (Exception e) &#123; // Should never reach here. PlatformDependent.throwException(e); &#125; return mask;&#125;private static boolean isSkippable( final Class&lt;?&gt; handlerType, final String methodName, final Class&lt;?&gt;... paramTypes) throws Exception &#123; return AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Boolean&gt;() &#123; @Override public Boolean run() throws Exception &#123; Method m; try &#123; m = handlerType.getMethod(methodName, paramTypes); &#125; catch (NoSuchMethodException e) &#123; if (logger.isDebugEnabled()) &#123; logger.debug( &quot;Class &#123;&#125; missing method &#123;&#125;, assume we can not skip execution&quot;, handlerType, methodName, e); &#125; return false; &#125; // 判断方法是否有Skip这个注解 return m.isAnnotationPresent(Skip.class); &#125; &#125;);&#125; 123456789101112131415161718192021222324252627// Using to mask which methods must be called for a ChannelHandler.static final int MASK_EXCEPTION_CAUGHT = 1;static final int MASK_CHANNEL_REGISTERED = 1 &lt;&lt; 1;static final int MASK_CHANNEL_UNREGISTERED = 1 &lt;&lt; 2;static final int MASK_CHANNEL_ACTIVE = 1 &lt;&lt; 3;static final int MASK_CHANNEL_INACTIVE = 1 &lt;&lt; 4;static final int MASK_CHANNEL_READ = 1 &lt;&lt; 5;static final int MASK_CHANNEL_READ_COMPLETE = 1 &lt;&lt; 6;static final int MASK_USER_EVENT_TRIGGERED = 1 &lt;&lt; 7;static final int MASK_CHANNEL_WRITABILITY_CHANGED = 1 &lt;&lt; 8;static final int MASK_BIND = 1 &lt;&lt; 9;static final int MASK_CONNECT = 1 &lt;&lt; 10;static final int MASK_DISCONNECT = 1 &lt;&lt; 11;static final int MASK_CLOSE = 1 &lt;&lt; 12;static final int MASK_DEREGISTER = 1 &lt;&lt; 13;static final int MASK_READ = 1 &lt;&lt; 14;static final int MASK_WRITE = 1 &lt;&lt; 15;static final int MASK_FLUSH = 1 &lt;&lt; 16;static final int MASK_ONLY_INBOUND = MASK_CHANNEL_REGISTERED | MASK_CHANNEL_UNREGISTERED | MASK_CHANNEL_ACTIVE | MASK_CHANNEL_INACTIVE | MASK_CHANNEL_READ | MASK_CHANNEL_READ_COMPLETE | MASK_USER_EVENT_TRIGGERED | MASK_CHANNEL_WRITABILITY_CHANGED;private static final int MASK_ALL_INBOUND = MASK_EXCEPTION_CAUGHT | MASK_ONLY_INBOUND;static final int MASK_ONLY_OUTBOUND = MASK_BIND | MASK_CONNECT | MASK_DISCONNECT | MASK_CLOSE | MASK_DEREGISTER | MASK_READ | MASK_WRITE | MASK_FLUSH;private static final int MASK_ALL_OUTBOUND = MASK_EXCEPTION_CAUGHT | MASK_ONLY_OUTBOUND; 注意：类对象的getMethod只能获取 上面代码的意思就是使用17个bit(mask)来表示对应的操作，比如mask数值如下： 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 并把这个mask值赋值给了Handler的executionMask属性。就代表了这个handler可以执行channelRegistered、channelUnregistered、channelActive、channelInactive和channelRead这些方法。 而且，默认情况下都是为1的，表示都会执行。 12mask |= MASK_ALL_INBOUND;mask |= MASK_ALL_OUTBOUND; 只有在方法上添加了@ChannelHandlerMask.Skip 注解后，才会把对应的位设置为0 接着看一下 TailContext 的构造器1234TailContext(DefaultChannelPipeline pipeline) &#123; super(pipeline, null, TAIL_NAME, TailContext.class); setAddComplete();&#125; 上面已经说过了，不过这里有一个非常不同的地方，就是TailContext不持有unsafe对象，也就是说TailContext对象不会处理真实的socket操作。 关于 EventLoopGroup 初始化回到最开始的 EchoClient.java 代码中, 我们在一开始就实例化了一个 NioEventLoopGroup 对象, 因此我们就从它的构造器中追踪一下 EventLoop 的初始化过程.首先来看一下 NioEventLoopGroup 的类继承层次: 跟踪代码，最终到父类MultithreadEventLoopGroup构造器: 123protected MultithreadEventLoopGroup(int nThreads, Executor executor, Object... args) &#123; super(nThreads == 0 ? DEFAULT_EVENT_LOOP_THREADS : nThreads, executor, args);&#125; 其中有一点有意思的地方是, 如果我们传入的线程数 nThreads 是0, 那么 Netty 会为我们设置默认的线程数 DEFAULT_EVENT_LOOP_THREADS, 而这个默认的线程数是怎么确定的呢?其实很简单, 在静态代码块中, 会首先确定 DEFAULT_EVENT_LOOP_THREADS 的值: 12345678910111213141516171819static &#123; DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt( &quot;io.netty.eventLoopThreads&quot;, NettyRuntime.availableProcessors() * 2)); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;-Dio.netty.eventLoopThreads: &#123;&#125;&quot;, DEFAULT_EVENT_LOOP_THREADS); &#125;&#125;synchronized int availableProcessors() &#123; if (this.availableProcessors == 0) &#123; final int availableProcessors = SystemPropertyUtil.getInt( &quot;io.netty.availableProcessors&quot;, Runtime.getRuntime().availableProcessors()); setAvailableProcessors(availableProcessors); &#125; return this.availableProcessors;&#125; 这两块代码的意思就是取环境变量io.netty.eventLoopThreads的值，如果没有就取核心数x2 回到MultithreadEventLoopGroup构造器中, 这个构造器会继续调用父类 MultithreadEventExecutorGroup 的构造器: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Create a new instance. * * @param nThreads the number of threads that will be used by this instance. * @param executor the Executor to use, or &#123;@code null&#125; if the default should be used. * @param chooserFactory the &#123;@link EventExecutorChooserFactory&#125; to use. * @param args arguments which will passed to each &#123;@link #newChild(Executor, Object...)&#125; call */protected MultithreadEventExecutorGroup(int nThreads, Executor executor, EventExecutorChooserFactory chooserFactory, Object... args) &#123; if (executor == null) &#123; executor = new ThreadPerTaskExecutor(newDefaultThreadFactory()); &#125; children = new EventExecutor[nThreads]; for (int i = 0; i &lt; nThreads; i ++) &#123; boolean success = false; try &#123; children[i] = newChild(executor, args); success = true; &#125; catch (Exception e) &#123; ..... &#125; finally &#123; .... 如果失败，做一些释放资源的逻辑 &#125; &#125; chooser = chooserFactory.newChooser(children); final FutureListener&lt;Object&gt; terminationListener = new FutureListener&lt;Object&gt;() &#123; @Override public void operationComplete(Future&lt;Object&gt; future) throws Exception &#123; if (terminatedChildren.incrementAndGet() == children.length) &#123; terminationFuture.setSuccess(null); &#125; &#125; &#125;; for (EventExecutor e: children) &#123; e.terminationFuture().addListener(terminationListener); &#125; Set&lt;EventExecutor&gt; childrenSet = new LinkedHashSet&lt;EventExecutor&gt;(children.length); Collections.addAll(childrenSet, children); readonlyChildren = Collections.unmodifiableSet(childrenSet);&#125; 123if (executor == null) &#123; executor = new ThreadPerTaskExecutor(newDefaultThreadFactory());&#125; 这段代码的的ThreadPerTaskExecutor很简单，它只有一个方法，就是execute方法 123public void execute(Runnable command) &#123; threadFactory.newThread(command).start();&#125; 就是用来创建并启动线程的。 1newChild(executor, args): 该方法在NioEventLoopGroup中重写了，核心就是new 一个 NioEventLoop对象 1chooserFactory.newChooser(children); chooserFactory默认情况下是DefaultEventExecutorChooserFactory 1234567891011121314private DefaultEventExecutorChooserFactory() &#123; &#125;@Overridepublic EventExecutorChooser newChooser(EventExecutor[] executors) &#123; if (isPowerOfTwo(executors.length)) &#123; return new PowerOfTwoEventExecutorChooser(executors); &#125; else &#123; return new GenericEventExecutorChooser(executors); &#125;&#125;private static boolean isPowerOfTwo(int val) &#123; return (val &amp; -val) == val;&#125; 注意isPowerOfTwo是判断val是否2的幂，至于原理看[java 数字二进制表示](..&#x2F;..&#x2F;..&#x2F;java&#x2F;java 数字二进制表示) 根据代码, 我们就很清楚 MultithreadEventExecutorGroup 中的处理逻辑了: 创建一个大小为 nThreads 的 EventExecutor 数组 调用 newChhild 方法初始化 children 数组. 根据 nThreads 的大小, 创建不同的 Chooser, 即如果 nThreads 是 2 的幂, 则使用 PowerOfTwoEventExecutorChooser, 反之使用 GenericEventExecutorChooser. 不论使用哪个 Chooser, 它们的功能都是一样的, 即从 children 数组中选出一个合适的 EventExecutor 实例. 会出现选择EventExecutorChooser，是因为取模的方式不同，对于是2的幂的数，可以使用这样取模 1&gt;idx.getAndIncrement() &amp; executors.length - 1 这种对位的操作，性能更高 根据上面的代码, 我们知道, MultithreadEventExecutorGroup 内部维护了一个 EventExecutor 数组，指向的实际类型是EventLoop。Netty 的 EventLoopGroup 的实现机制其实就建立在 MultithreadEventExecutorGroup 之上. 每当 Netty 需要一个 EventLoop 时, 会调用 next() 方法获取一个可用的 EventLoop. 上面代码的中的newChild 方法, 这个是一个抽象方法, 它的任务是实例化 EventLoop 对象. 我们跟踪一下它的代码, 可以发现, 这个方法在 NioEventLoopGroup 类中实现了, 其内容很简单: 123456789101112131415161718@Overrideprotected EventLoop newChild(Executor executor, Object... args) throws Exception &#123; EventLoopTaskQueueFactory queueFactory = args.length == 4 ? (EventLoopTaskQueueFactory) args[3] : null; return new NioEventLoop(this, executor, (SelectorProvider) args[0], ((SelectStrategyFactory) args[1]).newSelectStrategy(), (RejectedExecutionHandler) args[2], queueFactory);&#125;NioEventLoop(NioEventLoopGroup parent, Executor executor, SelectorProvider selectorProvider, SelectStrategy strategy, RejectedExecutionHandler rejectedExecutionHandler, EventLoopTaskQueueFactory taskQueueFactory, EventLoopTaskQueueFactory tailTaskQueueFactory) &#123; super(parent, executor, false, newTaskQueue(taskQueueFactory), newTaskQueue(tailTaskQueueFactory), rejectedExecutionHandler); this.provider = ObjectUtil.checkNotNull(selectorProvider, &quot;selectorProvider&quot;); this.selectStrategy = ObjectUtil.checkNotNull(strategy, &quot;selectStrategy&quot;); final SelectorTuple selectorTuple = openSelector(); this.selector = selectorTuple.selector; this.unwrappedSelector = selectorTuple.unwrappedSelector;&#125; 其实就是实例化一个 NioEventLoop 对象, 然后返回它。看下NioEventLoop的类图 他继承了SingleThreadEventLoop 最后总结一下整个 NioEventLoopGroup 的初始化过程: NioEventLoopGroup(其实是MultithreadEventExecutorGroup) 内部维护一个类型为 EventExecutor children 数组, 其大小是 nThreads, 这样就构成了一个线程池 如果我们在实例化 NioEventLoopGroup 时, 如果指定线程池大小, 则 nThreads 就是指定的值, 反之是io.netty.availableProcessors参数设置的值或者是处理器核心数 * 2 MultithreadEventExecutorGroup 中会调用 newChild 抽象方法来初始化 children 数组 抽象方法 newChild 是在 NioEventLoopGroup 中实现的, 它返回一个 NioEventLoop 实例. channel 的注册过程在前面的分析中, 我们提到, channel 会在 Bootstrap.initAndRegister 中进行初始化, 但是这个方法还会将初始化好的 Channel 注册到 EventGroup 中. 接下来我们就来分析一下 Channel 注册的过程. 回顾一下 AbstractBootstrap.initAndRegister 方法: 123456final ChannelFuture initAndRegister() &#123; // 去掉非关键代码 final Channel channel = channelFactory().newChannel(); init(channel); ChannelFuture regFuture = config().group().register(channel);&#125; 当Channel 初始化后, 会紧接着调用 group().register() 方法来注册 Channel, 我们继续跟踪的话, 会发现其调用链如下: 123456789101112131415161718AbstractBootstrap.initAndRegister -&gt; MultithreadEventLoopGroup.register: public ChannelFuture register(Channel channel) &#123; //这里的next方法就是从children数组中获取一个EventExecutor，实际上就是EventLoop return next().register(channel); &#125;--&gt;SingleThreadEventLoop.register： public ChannelFuture register(Channel channel) &#123; return register(new DefaultChannelPromise(channel, this)); &#125; public ChannelFuture register(final ChannelPromise promise) &#123; promise.channel().unsafe().register(this, promise); return promise; &#125;--&gt;AbstractUnsafe.registerAbstractUnsafe是AbstractChannel的内部类 通过跟踪调用链, 最终我们发现是调用到了 unsafe 的 register 方法, 那么接下来我们就仔细看一下 AbstractUnsafe.register 方法中到底做了什么: 1234567891011121314151617181920212223242526272829303132333435//MultithreadEventLoopGroup.register@Overridepublic ChannelFuture register(Channel channel) &#123; return next().register(channel);&#125;//SingleThreadEventLoop.registerpublic ChannelFuture register(final ChannelPromise promise) &#123; ObjectUtil.checkNotNull(promise, &quot;promise&quot;); promise.channel().unsafe().register(this, promise); return promise;&#125;//AbstractUnsafe.register @Overridepublic final void register(EventLoop eventLoop, final ChannelPromise promise) &#123; // 省略参数检测和错误处理 AbstractChannel.this.eventLoop = eventLoop; //这里会判断当前线程是不是分配给evetnLoop的线程，如果是就直接执行，如果不是，就放到eventLoop的队列中，异步执行 //这种模式的代码在源码中经常出现，详情在EventLoop中讲 if (eventLoop.inEventLoop()) &#123; register0(promise); &#125; else &#123; try &#123; eventLoop.execute(new Runnable() &#123; @Override public void run() &#123; register0(promise); &#125; &#125;); &#125; catch (Throwable t) &#123; ..... &#125; &#125;&#125; 首先, 将 eventLoop 赋值给 Channel 的 eventLoop 属性, 而我们知道这个 eventLoop 对象其实是 MultithreadEventLoopGroup.next() 方法获取的, 根据我们前面 关于 EventLoop 初始化 小节中, 我们可以确定 next() 方法返回的 eventLoop 对象是 NioEventLoop 实例。这里也解析了一个Channel被创建的时候就分配一个EventLoop。 register 方法接着调用了 register0 方法: 1234567891011121314151617181920212223242526private void register0(ChannelPromise promise) &#123; try &#123; if (!promise.setUncancellable() || !ensureOpen(promise)) &#123; return; &#125; boolean firstRegistration = neverRegistered; doRegister(); neverRegistered = false; registered = true; pipeline.invokeHandlerAddedIfNeeded(); safeSetSuccess(promise); pipeline.fireChannelRegistered(); if (isActive()) &#123; if (firstRegistration) &#123; pipeline.fireChannelActive(); &#125; else if (config().isAutoRead()) &#123; beginRead(); &#125; &#125; &#125; catch (Throwable t) &#123; closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); &#125;&#125; register0 又调用了 AbstractNioChannel.doRegister: 12345@Overrideprotected void doRegister() throws Exception &#123; // 省略不关键的代码 selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this);&#125; javaChannel() 这个方法返回的是一个 Java NIO SocketChannel对象。所以，这断代码就是java NIO 的api。 这里，我们将这个 SocketChannel 注册到与 eventLoop 关联的 selector 上了。并且把这个最后返回一个SelectionKey对象，并把该对象赋值给了NioSocketChannel中的selectionKey属性。并且这个SelectionKey对象里面的的附加对象引用了NioSocketChannel对象。 这时为NioSocketChannel分配一个EventLoop已经完成了，但整个启动流程还没结束。因为现在这个SocketChannel没有任何关注的事件，所以这个SocketChanel现在还没有实际的作用。而Netty是一个事件驱动的框架。所以在NioSocketChannel注册到一个EventLoop中时，会触发一个Registered事件。这里会在后面再分析。 我们总结一下 Channel 的注册过程: 首先在 AbstractBootstrap.initAndRegister中, 通过 group.register(channel), 调用 MultithreadEventLoopGroup.register 方法 在MultithreadEventLoopGroup.register 中, 通过 next() 获取一个 SingleThreadEventLoop, 然后调用它的 register方法。 在 SingleThreadEventLoop.register 中, 通过 channel.unsafe().register(this, promise) 来获取 channel 的 unsafe() 底层操作对象, 然后调用它的 register. 在 AbstractUnsafe.register 方法中, 调用 register0 方法注册 Channel 在 AbstractUnsafe.register0 中, 调用 AbstractNioChannel.doRegister 方法 AbstractNioChannel.doRegister 方法通过 javaChannel().register(eventLoop().selector, 0, this) 将 Channel 对应的 Java NIO SockerChannel 注册到一个 eventLoop 的 Selector 中, 并且将当前 Channel 作为 attachment（SelectionKey的附加对象，可以通过SelectionKey#attachment()方法获取这个附加对象）. 总的来说, Channel 注册过程所做的工作就是将 Channel 与对应的 EventLoop 关联, 因此这也体现了, 在 Netty 中, 每个 Channel 都会关联一个特定的 EventLoop, 并且这个 Channel 中的所有 IO 操作都是在这个 EventLoop 中执行的; 当关联好 Channel 和 EventLoop 后, 会继续调用底层的 Java NIO SocketChannel 的 register 方法, 将底层的 Java NIO SocketChannel 注册到指定的 selector 中. 通过这两步, 就完成了 Netty Channel 的注册过程。但是现在EventLoop还不能处理任何网络IO，因为SocketChannel注册到selector中时，指定的关注事件为0。 handler 的添加过程Netty 的一个强大和灵活之处就是基于 Pipeline 的自定义 handler 机制. 基于此, 我们可以像添加插件一样自由组合各种各样的 handler 来完成业务逻辑. 例如我们需要处理 HTTP 数据, 那么就可以在 pipeline 前添加一个 Http 的编解码的 Handler, 然后接着添加我们自己的业务逻辑的 handler, 这样网络上的数据流就向通过一个管道一样, 从不同的 handler 中流过并进行编解码, 最终在到达我们自定义的 handler 中. 这里不打算详细展开 Netty 的 ChannelPipeline 的实现机制(具体的细节会在后续的章节中展示), 我在这一小节中, 从简单的入手, 展示一下我们自定义的 handler 是如何以及何时添加到 ChannelPipeline 中的. 首先让我们看一下如下的代码片段: 123456789101112....handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline p = ch.pipeline(); if (sslCtx != null) &#123; p.addLast(sslCtx.newHandler(ch.alloc(), HOST, PORT)); &#125; //p.addLast(new LoggingHandler(LogLevel.INFO)); p.addLast(new EchoClientHandler()); &#125; &#125;); 这个代码片段就是实现了 handler 的添加功能. 我们看到, Bootstrap.handler 方法接收一个 ChannelHandler, 而我们传递的是一个 派生于 ChannelInitializer 的匿名类, 它正好也实现了 ChannelHandler 接口. 我们来看一下, ChannelInitializer 类内到底有什么玄机: 1234567891011121314151617181920212223242526272829303132333435363738394041//去掉了非重点代码@Sharablepublic abstract class ChannelInitializer&lt;C extends Channel&gt; extends ChannelInboundHandlerAdapter &#123; private static final InternalLogger logger = InternalLoggerFactory.getInstance(ChannelInitializer.class); private final Set&lt;ChannelHandlerContext&gt; initMap = Collections.newSetFromMap( new ConcurrentHashMap&lt;ChannelHandlerContext, Boolean&gt;()); protected abstract void initChannel(C ch) throws Exception; @Override @SuppressWarnings(&quot;unchecked&quot;) public final void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; if (initChannel(ctx)) &#123; ctx.pipeline().fireChannelRegistered(); removeState(ctx); &#125; else &#123; ctx.fireChannelRegistered(); &#125; &#125; ... private boolean initChannel(ChannelHandlerContext ctx) throws Exception &#123; if (initMap.add(ctx)) &#123; // Guard against re-entrance. try &#123; initChannel((C) ctx.channel()); &#125; catch (Throwable cause) &#123; // Explicitly call exceptionCaught(...) as we removed the handler before calling initChannel(...). // We do so to prevent multiple calls to initChannel(...). exceptionCaught(ctx, cause); &#125; finally &#123; ChannelPipeline pipeline = ctx.pipeline(); if (pipeline.context(this) != null) &#123; pipeline.remove(this); &#125; &#125; return true; &#125; return false; &#125;&#125; ChannelInitializer 是一个抽象类, 它有一个抽象的方法 initChannel, 我们正是实现了这个方法, 并在这个方法中添加的自定义的 handler 的。至此，有两个问题——ChannelInitializer对象什么时候进入Pipeline和Pipeline什么时候通过ChannelInitializer初始化Handler ChannelInitializer对象进入Pipeline前面讲了NioSocketChanel创建时会new一个DefaultChannelPipeline对象 那到底是什么时候把ChannelInitializer对象放入这个pipeline对象中的？ 还记得我们调用Bootstrap#connect后到了Channel的初始化吗: 123456final ChannelFuture initAndRegister() &#123; // 去掉非关键代码 final Channel channel = channelFactory().newChannel(); init(channel); ChannelFuture regFuture = config().group().register(channel);&#125; 重点就在init(channel); 12345678910void init(Channel channel) &#123; ChannelPipeline p = channel.pipeline(); //将通过Bootstrap#handler方法设置的ChannelHandler添加到pipelient中。 p.addLast(config.handler()); //将通过Bootstrap#option方法设置的值，传递到NioSocketChannel对象中的Config对象中。 setChannelOptions(channel, newOptionsArray(), logger); //将通过Bootstrap#attr方法设置的值，传递到NioSocketChannel对象中的Config对象中。 setAttributes(channel, newAttributesArray());&#125; 在上面的代码中config.handler()就返回我们在上面创建的ChannelInitializer对象，并把它放入了Pipeline中，执行完后Pipeline变成了这样。 Pipeline通过ChannelInitializer初始化Handler那ChannelInitializer#initChannel这个方法又是有什么时候执行的呢？ 在channel注册流程的最后，也就是AbstractUnsafe.register0方法： 1234567891011121314151617181920212223242526private void register0(ChannelPromise promise) &#123; try &#123; if (!promise.setUncancellable() || !ensureOpen(promise)) &#123; return; &#125; boolean firstRegistration = neverRegistered; doRegister(); neverRegistered = false; registered = true; pipeline.invokeHandlerAddedIfNeeded(); safeSetSuccess(promise); pipeline.fireChannelRegistered(); if (isActive()) &#123; if (firstRegistration) &#123; pipeline.fireChannelActive(); &#125; else if (config().isAutoRead()) &#123; beginRead(); &#125; &#125; &#125; catch (Throwable t) &#123; closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); &#125;&#125; 在把Channel中的SocketChannel注册到EventLoop中的Selector中后，会产生一个Registered事件，而Netty的事件都会流经pipeline的。在netty中通过执行了pipeline.fireChannel*把事件交给pipeline中的handler的对应方法处理。而Registered对应的就是fireChannelRegistered。在上边的代码中正好有这代码。 123456//DefaultChannelPipelien@Overridepublic final ChannelPipeline fireChannelRegistered() &#123; AbstractChannelHandlerContext.invokeChannelRegistered(head); return this;&#125; 关于上面代码的 head.fireXXX 的调用形式, 是 Netty 中 Pipeline 传递事件的常用方式, 我们以后会经常看到. AbstractChannelHandlerContext.invokeChannelRegistered(head)： 1234567891011121314151617181920212223242526272829/** * 这个方法的作用就是在next节点执行ChannelHandler的channelRegistered方法。 * @param next */static void invokeChannelRegistered(final AbstractChannelHandlerContext next) &#123; EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; next.invokeChannelRegistered(); &#125; else &#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; next.invokeChannelRegistered(); &#125; &#125;); &#125;&#125;private void invokeChannelRegistered() &#123; if (invokeHandler()) &#123; try &#123; ((ChannelInboundHandler) handler()).channelRegistered(this); &#125; catch (Throwable t) &#123; invokeExceptionCaught(t); &#125; &#125; else &#123; fireChannelRegistered(); &#125;&#125; HeadContext.handler(): 1234@Overridepublic ChannelHandler handler() &#123; return this;&#125; HeadContext.channelRegistered(): 123456789/** * 把事件传递给下一个可达的节点。 * @return */@Overridepublic void channelRegistered(ChannelHandlerContext ctx) &#123; invokeHandlerAddedIfNeeded(); ctx.fireChannelRegistered();&#125; AbstractChannelHandlerContext.fireChannelRegistered： 12345@Overridepublic ChannelHandlerContext fireChannelRegistered() &#123; invokeChannelRegistered(findContextInbound(MASK_CHANNEL_REGISTERED)); return this;&#125; findContextInbound的作用就是找到当前结点的下一个可达结点。 1234567891011121314/** * 找到下一个可达节点 * @param mask * @return */private AbstractChannelHandlerContext findContextInbound(int mask) &#123; AbstractChannelHandlerContext ctx = this; EventExecutor currentExecutor = executor(); do &#123; ctx = ctx.next; //这个就是使用在初始化HandlerContext时，初始化的executionMask，来判断对应的位上是否位1 &#125; while (skipContext(ctx, currentExecutor, mask, MASK_ONLY_INBOUND)); return ctx;&#125; 又重复上边的过程。 一开始执行了pipeline.fireChannelRegistered()事件从pipeline的head节点开始，调用了head节点的channelRegistered方法。然后在head节点的channelRegistered方法最后调用了head.fireChannelRegistered方法，把事件传递给了下一个可达节点。在这个启动阶段，这个结点就是我们创建的ChannelInitializer对象，接着ChannelInitializer又调用了自己的channelRegistered 12345678910111213141516171819202122232425262728public final void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; if (initChannel(ctx)) &#123; //完成pipeline的handler组装后，Registered事件重新从pipeline的头节点开始。 ctx.pipeline().fireChannelRegistered(); removeState(ctx); &#125; else &#123; ctx.fireChannelRegistered(); &#125;&#125;private boolean initChannel(ChannelHandlerContext ctx) throws Exception &#123; if (initMap.add(ctx)) &#123; // Guard against re-entrance. try &#123; initChannel((C) ctx.channel()); &#125; catch (Throwable cause) &#123; // Explicitly call exceptionCaught(...) as we removed the handler before calling initChannel(...). // We do so to prevent multiple calls to initChannel(...). exceptionCaught(ctx, cause); &#125; finally &#123; ChannelPipeline pipeline = ctx.pipeline(); if (pipeline.context(this) != null) &#123; pipeline.remove(this); &#125; &#125; return true; &#125; return false;&#125; 可以看到，调用了initChannel方法了，根据程序定义，完成了对pipeline的初始化后，接着调用pipeline的remove方法，把ChannelInitializer从当前的pipeline中移除。最后把这个Registered事件从头开始执行一次，直到到达TailContext为止。 因此当调用了这个方法后, 我们自定义的 ChannelHandler 就插入到 Pipeline 了, 此时的 Pipeline 如下图所示: 当添加了自定义的 ChannelHandler 后, 会删除 ChannelInitializer 这个 ChannelHandler, 即 “ctx.pipeline().remove(this)”, 因此最后的 Pipeline 如下: 客户端连接分析经过上面的各种分析后, 我们大致了解了 Netty 初始化时, 所做的工作。 那么接下来我们就直奔主题, 分析一下客户端是如何发起 TCP 连接的. 首先, 客户端通过调用 Bootstrap 的 connect 方法进行连接. 在 connect 中, 完成了Channle的创建、Channel的注册和pipelinet的初始化后（这些都是在AbstarctBootstrap#initAndRegister方法中完成的）, 最终调用的是 doConnect0 方法, 其实现如下: 123456789101112131415161718private static void doConnect( final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise connectPromise) &#123; // This method is invoked before channelRegistered() is triggered. Give user handlers a chance to set up // the pipeline in its channelRegistered() implementation. final Channel channel = connectPromise.channel(); channel.eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; if (localAddress == null) &#123; channel.connect(remoteAddress, connectPromise); &#125; else &#123; channel.connect(remoteAddress, localAddress, connectPromise); &#125; connectPromise.addListener(ChannelFutureListener.CLOSE_ON_FAILURE); &#125; &#125;);&#125; 在 doConnect0 中, 会在 eventloop的线程中调用 Channel 的 connect 方法, 而这个 Channel 的具体类型是什么呢? 我们在 Channel 初始化这一小节中已经分析过了, 这里 channel 的类型就是 NioSocketChannel. 进行跟踪到 channel.connect 中, 我们发现它调用的是 DefaultChannelPipeline#connect, 而, pipeline 的 connect 代码如下: 1234@Overridepublic final ChannelFuture connect(SocketAddress remoteAddress, ChannelPromise promise) &#123; return tail.connect(remoteAddress, promise);&#125; 而 tail 字段, 我们已经分析过了, 是一个 TailContext 的实例, 而 TailContext 又是AbstractChannelHandlerContext 的子类, 并且没有重写 connect 方法, 因此这里调用的其实是 AbstractChannelHandlerContext.connect, 我们看一下这个方法的实现: 1234567891011121314151617181920212223@Overridepublic ChannelFuture connect(SocketAddress remoteAddress, ChannelPromise promise) &#123; return connect(remoteAddress, null, promise);&#125;@Overridepublic ChannelFuture connect( final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) &#123; ...... final AbstractChannelHandlerContext next = findContextOutbound(MASK_CONNECT); EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; next.invokeConnect(remoteAddress, localAddress, promise); &#125; else &#123; safeExecute(executor, new Runnable() &#123; @Override public void run() &#123; next.invokeConnect(remoteAddress, localAddress, promise); &#125; &#125;, promise, null, false); &#125; return promise;&#125; 上面的代码中有一个关键的地方, 即 final AbstractChannelHandlerContext next &#x3D; findContextOutbound(MASK_CONNECT), 这个方法就是以当前的HandlerContext开始，找到前一个可达的HandlerContext。这里调用 findContextOutbound 方法, 从 DefaultChannelPipeline 内的双向链表的 tail 开始, 不断向前寻找第一个可以执行connect方法的AbstractChannelHandlerContext然后调用它的 invokeConnect 方法, 其代码如下： 1234567891011private void invokeConnect(SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise) &#123; if (invokeHandler()) &#123; try &#123; ((ChannelOutboundHandler) handler()).connect(this, remoteAddress, localAddress, promise); &#125; catch (Throwable t) &#123; notifyOutboundHandlerException(t, promise); &#125; &#125; else &#123; connect(remoteAddress, localAddress, promise); &#125;&#125; 最后找到了输出链表的尾端也就是head(HeadContext)而又因为 HeadContext 重写了 connect 方法, 因此实际上调用的是 HeadContext.connect. 我们接着跟踪到 HeadContext.connect, 其代码如下: 1234567@Overridepublic void connect( ChannelHandlerContext ctx, SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise) &#123; unsafe.connect(remoteAddress, localAddress, promise);&#125; 这个 connect 方法很简单, 仅仅调用了 unsafe 的 connect 方法. 而 unsafe 又是什么呢?回顾一下 HeadContext 的构造器, 我们发现 unsafe 是 pipeline.channel().unsafe() 返回的, 而 Channel 的 unsafe 的值, 在这个例子中, 我们已经知道了, 其实是 AbstractNioByteChannel.NioByteUnsafe 内部类. 兜兜转转了一大圈, 我们找到了创建 Socket 连接的关键代码. NioSocketChannel的unsafe的类型实际上是NioSocketChannelUnsafe，下面列出类图： 进行跟踪 NioByteUnsafe -&gt; AbstractNioUnsafe.connect: 1234567891011@Overridepublic final void connect( final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) &#123; //去掉了非关键代码 boolean wasActive = isActive(); if (doConnect(remoteAddress, localAddress)) &#123; fulfillConnectPromise(promise, wasActive); &#125; else &#123; ... &#125;&#125; AbstractNioUnsafe.connect 的实现如上代码所示, 在这个 connect 方法中, 调用了 doConnect 方法, 注意, 这个方法并不是 AbstractNioUnsafe 的方法, 而是 AbstractNioChannel 的抽象方法. （AbstractNioUnsafe是AbstractNioChannel的抽象内部类）doConnect 方法是在 NioSocketChannel 中实现的, 因此进入到NioSocketChannel.doConnect 中: 1234567891011121314151617181920@Overrideprotected boolean doConnect(SocketAddress remoteAddress, SocketAddress localAddress) throws Exception &#123; if (localAddress != null) &#123; doBind0(localAddress); &#125; boolean success = false; try &#123; boolean connected = SocketUtils.connect(javaChannel(), remoteAddress); if (!connected) &#123; //如果连接没有马上成功，则注册一个OP_CONNECT事件 selectionKey().interestOps(SelectionKey.OP_CONNECT); &#125; success = true; return connected; &#125; finally &#123; if (!success) &#123; doClose(); &#125; &#125;&#125; SocketUtils.connect: 1234567891011121314public static boolean connect(final SocketChannel socketChannel, final SocketAddress remoteAddress) throws IOException &#123; try &#123; return AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Boolean&gt;() &#123; @Override public Boolean run() throws IOException &#123; //重点代码 return socketChannel.connect(remoteAddress); &#125; &#125;); &#125; catch (PrivilegedActionException e) &#123; throw (IOException) e.getCause(); &#125;&#125; 我们终于看到的最关键的部分了, 庆祝一下! 上边的代码就是一个标准的java nio 客户端的启动流程。 最后, 上面的代码流程可以用如下时序图直观地展示: NioEventLoop就是负责处理NioSocketChannel关注的读写事件的通知线程实现类，但上边只是讲了SocketChannel连接。并没有把最重要的OP_READ注册到Selector中。 客户端连接Socket关注OP_READ事件上边只是SocketChannel执行了connect方法，但还没完，因为只是执行了socketChannel.connect而已，现在只是问题是还没告诉selector，这个socketChannel关注什么事件。在java NIO中，客户端的连接是要这样处理的 123456789private void doConnect() throws IOException &#123; if (sc.connect(socketAddress)) &#123; sc.register(selector, SelectionKey.OP_READ); System.out.println(&quot;客户端链接完成&quot;); &#125; else &#123; sc.register(selector, SelectionKey.OP_CONNECT); &#125; started = true;&#125; 在netty也不例外。从上边的doConnect有这样的一段代码 在socketChannel.connect执行了不代表TCP链接的三次握手已经成功了，这时有两种情况： 还没成功时会在关注一个SelectionKey.OP_CONNECT事件，这和java NIO编程时处理方法一样，而一个Channel的所有事件都是在EventLoop中处理的，这种时候就要在NioEventLoop线程中怎么处理链接建立成功的通知的，跟踪NioEventLoop中的run方法，得到调用链如下: 1234NioEventLoop.run -&gt; NioEventLoop.processSelectedKeys() -&gt; NioEventLoop.processSelectedKeysPlain(Set&lt;SelectionKey&gt; selectedKeys) -&gt; NioEventLoop.processSelectedKey(SelectionKey k, AbstractNioChannel ch) 看下NioEventLoop.processSelectedKey(SelectionKey k, AbstractNioChannel ch) : 12345678910111213141516171819202122private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) &#123; final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe(); //去掉了非关键代码 try &#123; //获取这个SocketChannel关注的事件， int readyOps = k.readyOps(); if ((readyOps &amp; SelectionKey.OP_CONNECT) != 0) &#123; int ops = k.interestOps(); ops &amp;= ~SelectionKey.OP_CONNECT; k.interestOps(ops); unsafe.finishConnect(); &#125; if ((readyOps &amp; SelectionKey.OP_WRITE) != 0) &#123; ch.unsafe().forceFlush(); &#125; if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) &#123; unsafe.read(); &#125; &#125; catch (CancelledKeyException ignored) &#123; unsafe.close(unsafe.voidPromise()); &#125;&#125; 可以看到SelectionKey.readyOps();返回了已经就绪的事件。由于开始时这个SocketChannel只关注OP_CONNECT，而如果OP_CONNECT事件已经就绪了，那第一个if的readyOps&amp; SelectionKey.OP_CONNECT肯定不为0的，这时先取消关注OP_CONNECT事件 123int ops = k.interestOps();ops &amp;= ~SelectionKey.OP_CONNECT;k.interestOps(ops); 然后调用了unsafe.finishConnect()，上面已经说明过了，在客户端中unsafe是NioByteUnsafe实例，看下NioByteUnsafe.finishConnect方法： 1234567891011121314151617181920212223public final void finishConnect() &#123; assert eventLoop().inEventLoop(); try &#123; boolean wasActive = isActive(); doFinishConnect(); fulfillConnectPromise(connectPromise, wasActive); &#125; catch (Throwable t) &#123; fulfillConnectPromise(connectPromise, annotateConnectException(t, requestedRemoteAddress)); &#125; finally &#123; if (connectTimeoutFuture != null) &#123; connectTimeoutFuture.cancel(false); &#125; connectPromise = null; &#125;&#125;//检查3次握手是否成功//这里的SocketChannel#isConnected需要注意，这个方法只有在手动调用SocketChannel.finishConnect方法时，才会返回true@Overridepublic boolean isActive() &#123; SocketChannel ch = javaChannel(); return ch.isOpen() &amp;&amp; ch.isConnected();&#125; 由于一开始时还没把SocketChannel里面的状态state改为ST_CONNECTED，所以wasActive&#x3D;false，执行完doFinishConnect，通过调用SocketChannel.finishConnect方法，把SocketChannel.state改为了ST_CONNECTED了，接着看fulfillConnectPromise源码： 123456789private void fulfillConnectPromise(ChannelPromise promise, boolean wasActive) &#123; // 去掉了非关键代码 boolean active = isActive(); if (!wasActive &amp;&amp; active) &#123; pipeline().fireChannelActive(); &#125;&#125; 当再次调用isActive()放回了true，所以条件满足，进入了if语句块，执行了&#96;pipeline().fireChannelActive(); 123456789101112131415161718192021222324252627282930public final ChannelPipeline fireChannelActive() &#123; AbstractChannelHandlerContext.invokeChannelActive(head); return this;&#125;static void invokeChannelActive(final AbstractChannelHandlerContext next) &#123; EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; next.invokeChannelActive(); &#125; else &#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; next.invokeChannelActive(); &#125; &#125;); &#125;&#125;private void invokeChannelActive() &#123; if (invokeHandler()) &#123; try &#123; ((ChannelInboundHandler) handler()).channelActive(this); &#125; catch (Throwable t) &#123; invokeExceptionCaught(t); &#125; &#125; else &#123; fireChannelActive(); &#125;&#125; 也就是说实际是调用了HeadContext的channelActive方法: 123456@Overridepublic void channelActive(ChannelHandlerContext ctx) &#123; ctx.fireChannelActive(); readIfIsAutoRead();&#125; 在HeadContext.channelActive中，先看下ctx.fireChannelActive(): 12345@Overridepublic ChannelHandlerContext fireChannelActive() &#123; invokeChannelActive(findContextInbound(MASK_CHANNEL_ACTIVE)); return this;&#125; 前面在Channel的注册也说过了类似的，这个方法就是找到下一个可以执行channelActive的Inbound，执行完channelActive后，然后重复上面的过程，一直把channelActive事件沿着Inbound链传输，直到最后一个结点，也就是TailContext结点。 HeadContext把事件传递下去后接着执行了readIfIsAutoRead(); 12345678910111213141516171819202122232425262728293031323334353637383940414243441、HeadContext#readIfIsAutoReadprivate void readIfIsAutoRead() &#123; if (channel.config().isAutoRead()) &#123; channel.read(); &#125;&#125;2、AbstractChannel#readpublic Channel read() &#123; pipeline.read(); return this;&#125;3、DefaultChannelPipeline#readpublic final ChannelPipeline read() &#123; tail.read(); return this;&#125;4、TailContext#readpublic ChannelHandlerContext read() &#123; final AbstractChannelHandlerContext next = findContextOutbound(MASK_READ); EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; next.invokeRead(); &#125; else &#123; Tasks tasks = next.invokeTasks; if (tasks == null) &#123; next.invokeTasks = tasks = new Tasks(next); &#125; executor.execute(tasks.invokeReadTask); &#125; return this;&#125;5、AbstractChannelHandlerContext#invokeReadprivate void invokeRead() &#123; if (invokeHandler()) &#123; try &#123; ((ChannelOutboundHandler) handler()).read(this); &#125; catch (Throwable t) &#123; invokeExceptionCaught(t); &#125; &#125; else &#123; read(); &#125;&#125; 看到这已经明白了吧，从TailContextk开始找前一个可以执行read方法的Outbound，然后调用该Outbound的read方法，直到链表头，也就是HeadContext为止。好了，看下HeadContext.read方法： 123public void read(ChannelHandlerContext ctx) &#123; unsafe.beginRead();&#125; 在客户端中unsafe是NioByteUnsafe，实际上NioByteUnsafe没有重写这个方法，最终看AbstractUnsafe.beginRead: 123456789101112131415public final void beginRead() &#123; assertEventLoop(); try &#123; doBeginRead(); &#125; catch (final Exception e) &#123; invokeLater(new Runnable() &#123; @Override public void run() &#123; pipeline.fireExceptionCaught(e); &#125; &#125;); close(voidPromise()); &#125;&#125; doBeginRead是Channel的方法，该方法需要重写，我们使用了Nio所以最终方法锁定在了AbstractNioChannel.doBeginRead： 12345678910111213141516@Overrideprotected void doBeginRead() throws Exception &#123; // Channel.read() or ChannelHandlerContext.read() was called final SelectionKey selectionKey = this.selectionKey; if (!selectionKey.isValid()) &#123; return; &#125; readPending = true; final int interestOps = selectionKey.interestOps();//readInterestOp的值是在NioSocketChannel初始化时指定的，值就是OP_READ if ((interestOps &amp; readInterestOp) == 0) &#123; selectionKey.interestOps(interestOps | readInterestOp); &#125;&#125; 好了，看到熟悉的方法了，这个方法就是告诉selector，SocketChannel要关注OP_READ事件。 当socket.connect直接链接成功后，就执行了fulfillConnectPromise方法，这个方法刚才已经说过了 在看ChannelPipeline read()方法时后点困惑，就是为什么read是流经outbound链的呢? 比如有一个有个链接事件，一开始由selector通知，然后在NioEventLoop线程中处理，接着channelActive就在从HeadContext开始，流经Inbound，此时数据流向是这样的： 当写就绪的情况也是一样： channel.read方法数据流线是这样的： channel.write方法数据流是这样的： 最后写就绪，少了一步，也是最后一部，就是写完后会取消订阅write事件，因为在NIO中，只要socket buffer没有写满就会一直触发可写事件，所以如果直接注册*OP_*WRITE而不取消会导致CPU跑满。 从这些事件处理的流程可以了解一个事实，当系统通知应用程序时，数据是经过Inbound；而当应用程序告诉系统（订阅事件）时，是经过Outbound。 而且也发现了，netty的设计是完全遵循：事件都是在pipeline中传播的，输入事件从head开始，输出事件从tail开始。 总结都是基于NioSocketChannel。 在调用了connct方法后开始初始化流程 一开始会通过ReflectiveChannelFactory#newChannel创建一个Channel对象，这个对象也就是NioSocketChannel对象。初始化这个NioSocketChannel对象时会创建一个SocketChanel和DefaultChannelPipeline对象，并把他们赋值给了NioSocketChannel里的对应属性。 接着会把调用connct前调用的handle传递的ChannelInitializer对象传入到NioSocketChannel中的pipeline中，形成了 的结构。 接着就是把EventLoopGroup中的EventLoop分配给Channel的阶段了，在这阶段会在EventLoopGroup中的一个EventLoop数组children中按照规则取到一个EventLoop，这个EventLoop也就是NioEventLoop(继承了SingleThreadEventLoop)，然后调用了NioEventLoop.register，通过AbstractUnsafe.register把Channel和EventLoop绑定，并且把Channel中的SocketChannel注册到了EventLoop的Selector中，最用NioSocketChannel中selectionKey引用指向了返回的selectionKey对象不过这里要注意一点就是注册的代码是这样的 1javaChannel().register(eventLoop().unwrappedSelector(), 0, NioSocketChannel（对象）) 在这里并不关注任何事件，而且会把NioSocketChannel作为附加对象传递给SelectionKeys。 在注册完后就会执行pipeline.fireChannelRegistered()也就是会触发Channel已经注册完成的事件（说是事件，其实就是一个同步的方法调用）而且这个事件会在pipelin的输入流的头部流入到尾部结束。还记得的我们添加的ChannelInitializer对象吗？在这时就会触发ChannelInitializer的channelRegistered()方法，开始把我们需要的handler包装成ChannelHandlerContext，并把这个ChannelHandlerContext放入到pipeline中（ChannelHandlerContext 代表了 ChannelHandler 和 ChannelPipeline 之间的关联 ）最终pipeline形成了这样的结构 注册完成了就执行channel.connect方法了，这个操作是交由NioSocketChannel绑定的NioEventloop来完成的，因为： NioEventloop线程执行了connect后的时序图： channel.connect操作完成后，当NioEventloop线程收到了链接完成通知，最终执行了pipeline().fireChannelActive()，使channelActive事件从pipeline的head开始沿着Inbound链表传播。而且在HeadContext中，还执行了channel.read，使得一个read事件从pipeline的tail开始传播，沿着Outbound链表传播，最终到达head的read方法，执行了订阅OP_READ操作。这个过程可以看到，netty的事件总是在pipeline中传播的。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"udp","slug":"netty/06netty简单实战/udp","date":"2021-11-23T12:00:16.000Z","updated":"2022-03-23T09:03:58.193Z","comments":true,"path":"blog/netty/06netty简单实战/udp/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/06netty%E7%AE%80%E5%8D%95%E5%AE%9E%E6%88%98/udp/","excerpt":"","text":"UDP 的基础知识面向连接的传输(如TCP)管理了两个网络端点之间的连接的建立，在连接的生命周期内的有序和可靠的消息传输，以及最后，连接的有序终止。相比之下，在类似于 UDP 这样的无连接协议中，并没有持久化连接这样的概念，并且每个消息(一个 UDP 数据报)都是一个单独的传输单元。 此外，UDP 也没有 TCP 的纠错机制：其中每个节点都将确认它们所接收到的包，而没有被确认的包将会被发送方重新传输。 通过类比，TCP 连接就像打电话，其中一系列的有序消息将会在两个方向上流动。相反， UDP 则类似于往邮箱中投入一叠明信片。你无法知道它们将以何种顺序到达它们的目的地，或者它们是否所有的都能够到达它们的目的地。 UDP的这些方面可能会让你感觉到严重的局限性， 但是它们也解释了为何它会比TCP快那么多：所有的握手以及消息管理机制的开销都已经被消除了。显然，UDP很适合那些能够处理或者容忍消息丢失的应用程序，但可能不适合那些处理金融交易的应用程序。 UDP 广播单播的传输模式：定义为发送消息给一个由唯一的地址所标识的单一的网络目的地。面向连接的协议和无连接协议都支持这种模式。 UDP 提供了向多个接收者发送消息的额外传输模式: 多播—传输到一个预定义的主机组; 广播—传输到网络(或者子网)上的所有主机。 受限广播地址或者零网络地址 255.255.255.255。发送到这个地址的消息都将会被定向给本地网络(0.0.0.0)上的所有主机 udp程序所有的在该 UDP 端口上监听的事件监视器都将会接收到广播消息。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133/** * 类说明：发送端 */public class UdpQuestionSide &#123; public final static String QUESTION = &quot;告诉我一句古诗&quot;; public void run(int port) throws Exception&#123; EventLoopGroup group = new NioEventLoopGroup(); try &#123; Bootstrap b = new Bootstrap(); b.group(group) /*由于我们用的是UDP协议，所以要用NioDatagramChannel来创建*/ .channel(NioDatagramChannel.class) .handler(new QuestoinHandler()); //不需要建立连接 Channel ch = b.bind(0).sync().channel(); //将UDP请求的报文以DatagramPacket打包发送给接受端 ch.writeAndFlush( new DatagramPacket(Unpooled.copiedBuffer(QUESTION, CharsetUtil.UTF_8), new InetSocketAddress(&quot;127.0.0.1&quot;, port))) .sync(); //不知道接收端能否收到报文，也不知道能否收到接收端的应答报文 // 所以等待15秒后，不再等待，关闭通信 if(!ch.closeFuture().await(15000))&#123; System.out.println(&quot;查询超时！&quot;); &#125; &#125; catch (Exception e) &#123; group.shutdownGracefully(); &#125; &#125; public static void main(String [] args) throws Exception&#123; int answerPort = 8080; new UdpQuestionSide().run(answerPort); &#125;&#125;/** * 类说明：提问端的Handler，读取服务器的应答 */public class QuestoinHandler extends SimpleChannelInboundHandler&lt;DatagramPacket&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, DatagramPacket msg) throws Exception &#123; //获得应答，DatagramPacket提供了content()方法取得报文的实际内容 String response = msg.content().toString(CharsetUtil.UTF_8); if (response.startsWith(UdpAnswerSide.ANSWER)) &#123; System.out.println(&quot;古诗来了:&quot; + response); ctx.close(); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125;/** * 类说明：应答端 */public class UdpAnswerSide &#123; public final static String ANSWER = &quot;古诗来了：&quot;; public void run(int port) throws Exception&#123; EventLoopGroup group = new NioEventLoopGroup(); try &#123; /*和tcp的不同，udp没有接受连接的说法，所以即使是接收端， 也使用Bootstrap*/ Bootstrap b = new Bootstrap(); /*由于我们用的是UDP协议，所以要用NioDatagramChannel来创建*/ b.group(group) .channel(NioDatagramChannel.class) .handler(new AnswerHandler()); //没有接受客户端连接的过程，监听本地端口即可 ChannelFuture f = b.bind(port).sync(); System.out.println(&quot;应答服务已启动.....&quot;); f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully(); &#125; &#125; public static void main(String [] args) throws Exception&#123; int port = 8080; new UdpAnswerSide().run(port); &#125;&#125;/** * 类说明：应答Handler */public class AnswerHandler extends SimpleChannelInboundHandler&lt;DatagramPacket&gt; &#123; /*应答的具体内容从常量字符串数组中取得，由nextQuote方法随机获取*/ private static final String[] DICTIONARY = &#123; &quot;只要功夫深，铁棒磨成针。&quot;, &quot;旧时王谢堂前燕,飞入寻常百姓家。&quot;, &quot;洛阳亲友如相问，一片冰心在玉壶。&quot;, &quot;一寸光阴一寸金，寸金难买寸光阴。&quot;, &quot;老骥伏枥，志在千里，烈士暮年，壮心不已&quot; &#125;; private static Random r = new Random(); private String nextQuote()&#123; return DICTIONARY[r.nextInt(DICTIONARY.length-1)]; &#125; @Override protected void channelRead0(ChannelHandlerContext ctx, DatagramPacket packet) &#123; //获得请求 String req = packet.content().toString(CharsetUtil.UTF_8); System.out.println(&quot;接收到请求：&quot;+req); if(UdpQuestionSide.QUESTION.equals(req))&#123; String answer = UdpAnswerSide.ANSWER+nextQuote(); System.out.println(&quot;接收到请求：&quot;+req); /** * 重新 new 一个DatagramPacket对象，我们通过packet.sender()来获取发送者的消息。重新发送出去！ */ ctx.writeAndFlush(new DatagramPacket(Unpooled.copiedBuffer(answer, CharsetUtil.UTF_8), packet.sender())); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); cause.printStackTrace(); &#125;&#125;","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"WebSocket","slug":"netty/06netty简单实战/WebSocket","date":"2021-11-23T12:00:15.000Z","updated":"2022-03-23T09:03:58.190Z","comments":true,"path":"blog/netty/06netty简单实战/WebSocket/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/06netty%E7%AE%80%E5%8D%95%E5%AE%9E%E6%88%98/WebSocket/","excerpt":"","text":"WebSocket是个规范，在实际的实现中有HTML5规范中的WebSocket API、WebSocket的子协议STOMP。 简单地说，WebSocket提供了“在一个单个的TCP连接上提供双向的通信—-结合WebSocket API—-它为网页和远程服务器之间的双向通信提供了一种替代HTTP轮询的方案。”。也就是说,WebSocket 在客户端和服务器之间提供了真正的双向数据交换。WebSocket现在可以用于传输任意类型的数据，很像普通的套接字。 Spring使用websocket参考ws和stomp模块 netty实现websocket要想向你的应用程序中添加对于 WebSocket 的支持，你需要将适当的客户端或者服务器WebSocket ChannelHandler 添加到ChannelPipeline中。 这个类将处理由WebSocket定义的称为帧的特殊消息类型—WebSocketFrame。WebSocketFrame可以被归类为数据帧或者控制帧。 简单的WebSocket实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283//页面public final class MakeIndexPage &#123; private static final String NEWLINE = &quot;\\r\\n&quot;; public static ByteBuf getContent(String webSocketLocation) &#123; return Unpooled.copiedBuffer( &quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;Web Socket Test&lt;/title&gt;&lt;/head&gt;&quot; + NEWLINE + &quot;&lt;body&gt;&quot; + NEWLINE + &quot;&lt;script type=\\&quot;text/javascript\\&quot;&gt;&quot; + NEWLINE + &quot;var socket;&quot; + NEWLINE + &quot;if (!window.WebSocket) &#123;&quot; + NEWLINE + &quot; window.WebSocket = window.MozWebSocket;&quot; + NEWLINE + &#x27;&#125;&#x27; + NEWLINE + &quot;if (window.WebSocket) &#123;&quot; + NEWLINE + &quot; socket = new WebSocket(\\&quot;&quot; + webSocketLocation + &quot;\\&quot;);&quot; + NEWLINE + &quot; socket.onmessage = function(event) &#123;&quot; + NEWLINE + &quot; var ta = document.getElementById(&#x27;responseText&#x27;);&quot; + NEWLINE + &quot; ta.value = ta.value + &#x27;\\\\n&#x27; + event.data&quot; + NEWLINE + &quot; &#125;;&quot; + NEWLINE + &quot; socket.onopen = function(event) &#123;&quot; + NEWLINE + &quot; var ta = document.getElementById(&#x27;responseText&#x27;);&quot; + NEWLINE + &quot; ta.value = \\&quot;Web Socket opened!\\&quot;;&quot; + NEWLINE + &quot; &#125;;&quot; + NEWLINE + &quot; socket.onclose = function(event) &#123;&quot; + NEWLINE + &quot; var ta = document.getElementById(&#x27;responseText&#x27;);&quot; + NEWLINE + &quot; ta.value = ta.value + \\&quot;Web Socket closed\\&quot;; &quot; + NEWLINE + &quot; &#125;;&quot; + NEWLINE + &quot;&#125; else &#123;&quot; + NEWLINE + &quot; alert(\\&quot;Your browser does not support Web Socket.\\&quot;);&quot; + NEWLINE + &#x27;&#125;&#x27; + NEWLINE + NEWLINE + &quot;function send(message) &#123;&quot; + NEWLINE + &quot; if (!window.WebSocket) &#123; return; &#125;&quot; + NEWLINE + &quot; if (socket.readyState == WebSocket.OPEN) &#123;&quot; + NEWLINE + &quot; socket.send(message);&quot; + NEWLINE + &quot; &#125; else &#123;&quot; + NEWLINE + &quot; alert(\\&quot;The socket is not open.\\&quot;);&quot; + NEWLINE + &quot; &#125;&quot; + NEWLINE + &#x27;&#125;&#x27; + NEWLINE + &quot;&lt;/script&gt;&quot; + NEWLINE + &quot;&lt;form onsubmit=\\&quot;return false;\\&quot;&gt;&quot; + NEWLINE + &quot;&lt;input type=\\&quot;text\\&quot; name=\\&quot;message\\&quot; &quot; + &quot;value=\\&quot;Hello, World!\\&quot;/&gt;&quot; + &quot;&lt;input type=\\&quot;button\\&quot; value=\\&quot;Send Web Socket Data\\&quot;&quot; + NEWLINE + &quot; onclick=\\&quot;send(this.form.message.value)\\&quot; /&gt;&quot; + NEWLINE + &quot;&lt;h3&gt;Output&lt;/h3&gt;&quot; + NEWLINE + &quot;&lt;textarea id=\\&quot;responseText\\&quot; &quot; + &quot;style=\\&quot;width:500px;height:300px;\\&quot;&gt;&lt;/textarea&gt;&quot; + NEWLINE + &quot;&lt;/form&gt;&quot; + NEWLINE + &quot;&lt;/body&gt;&quot; + NEWLINE + &quot;&lt;/html&gt;&quot; + NEWLINE, CharsetUtil.US_ASCII); &#125;&#125;/** * 类说明：对websocket的数据进行处理 */public class ProcesssWsFrameHandler extends SimpleChannelInboundHandler&lt;WebSocketFrame&gt; &#123; private final ChannelGroup group; public ProcesssWsFrameHandler(ChannelGroup group) &#123; this.group = group; &#125; private static final Logger logger = LoggerFactory.getLogger(ProcesssWsFrameHandler.class); @Override protected void channelRead0(ChannelHandlerContext ctx, WebSocketFrame frame) throws Exception &#123; //判断是否为文本帧，目前只处理文本帧 if (frame instanceof TextWebSocketFrame) &#123; // Send the uppercase string back. String request = &quot;Server响应:&quot; + ((TextWebSocketFrame) frame).text(); logger.info(&quot;&#123;&#125; received &#123;&#125;&quot;, ctx.channel(), request);// ctx.channel().writeAndFlush(new TextWebSocketFrame(request.toUpperCase(Locale.CHINA))); /*群发实现，Mark增加，一对一道理一样*/ group.writeAndFlush(new TextWebSocketFrame(&quot;Client &quot; + ctx.channel() + &quot; say:&quot;+request.toUpperCase(Locale.CHINA))); &#125; else &#123; String message = &quot;unsupported frame type: &quot; + frame.getClass().getName(); throw new UnsupportedOperationException(message); &#125; &#125; /*重写 userEventTriggered()方法以处理自定义事件*/ @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; /*检测事件，如果是握手成功事件，做点业务处理*/ if (evt instanceof WebSocketServerProtocolHandler.HandshakeComplete) &#123; //通知所有已经连接的 WebSocket 客户端新的客户端已经连接上了 group.writeAndFlush(new TextWebSocketFrame(&quot;Client &quot; + ctx.channel() + &quot; joined&quot;)); //将新的 WebSocket Channel 添加到 ChannelGroup 中， // 以便它可以接收到所有的消息 group.add(ctx.channel()); &#125; else if (evt instanceof IdleStateEvent) &#123; //检查链接是否关闭了 ctx.writeAndFlush(HEARTBEAT_SEQUENCE.duplicate()) //发送心跳信息，客户端必须可以支持心跳信息，不然就当作发送失败处理 .addListener((future -&gt; &#123; //发送失败时关闭链接 ChannelFuture f = (ChannelFuture)future; if (!f.isSuccess()) &#123; f.channel().close(); logger.info(&quot;&#123;&#125; remove success&quot;, f.channel()); &#125; &#125;)); //失败时关闭 &#125; else &#123; super.userEventTriggered(ctx, evt); &#125; &#125;&#125;/** * 类说明：对http请求，将index的页面返回给前端 */public class ProcessWsIndexPageHandler extends SimpleChannelInboundHandler&lt;FullHttpRequest&gt; &#123; private final String websocketPath; public ProcessWsIndexPageHandler(String websocketPath) &#123; this.websocketPath = websocketPath; &#125; @Override protected void channelRead0(ChannelHandlerContext ctx, FullHttpRequest req) throws Exception &#123; // 处理错误或者无法解析的http请求 if (!req.decoderResult().isSuccess()) &#123; sendHttpResponse(ctx, req, new DefaultFullHttpResponse(HTTP_1_1, BAD_REQUEST)); return; &#125; //只允许Get请求 if (req.method() != GET) &#123; sendHttpResponse(ctx, req, new DefaultFullHttpResponse(HTTP_1_1, FORBIDDEN)); return; &#125; // 发送index页面的内容 if (&quot;/&quot;.equals(req.uri()) || &quot;/index.html&quot;.equals(req.uri())) &#123; //生成WebSocket的访问地址，写入index页面中 String webSocketLocation = getWebSocketLocation(ctx.pipeline(), req, websocketPath); System.out.println(&quot;WebSocketLocation:[&quot;+webSocketLocation+&quot;]&quot;); //生成index页面的具体内容,并送往浏览器 ByteBuf content = MakeIndexPage.getContent(webSocketLocation); FullHttpResponse res = new DefaultFullHttpResponse(HTTP_1_1, OK, content); res.headers().set(HttpHeaderNames.CONTENT_TYPE, &quot;text/html; charset=UTF-8&quot;); HttpUtil.setContentLength(res, content.readableBytes()); sendHttpResponse(ctx, req, res); &#125; else &#123; sendHttpResponse(ctx, req, new DefaultFullHttpResponse(HTTP_1_1, NOT_FOUND)); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125; /*发送应答*/ private static void sendHttpResponse(ChannelHandlerContext ctx, FullHttpRequest req, FullHttpResponse res) &#123; // 错误的请求进行处理 （code&lt;&gt;200). if (res.status().code() != 200) &#123; ByteBuf buf = Unpooled.copiedBuffer(res.status().toString(), CharsetUtil.UTF_8); res.content().writeBytes(buf); buf.release(); HttpUtil.setContentLength(res, res.content().readableBytes()); &#125; // 发送应答. ChannelFuture f = ctx.channel().writeAndFlush(res); //对于不是长连接或者错误的请求直接关闭连接 if (!HttpUtil.isKeepAlive(req) || res.status().code() != 200) &#123; f.addListener(ChannelFutureListener.CLOSE); &#125; &#125; /*根据用户的访问，告诉用户的浏览器，WebSocket的访问地址*/ private static String getWebSocketLocation(ChannelPipeline cp, HttpRequest req, String path) &#123; String protocol = &quot;ws&quot;; if (cp.get(SslHandler.class) != null) &#123; protocol = &quot;wss&quot;; &#125; return protocol + &quot;://&quot; + req.headers().get(HttpHeaderNames.HOST) + path; &#125;&#125;/** * 类说明：增加handler */public class WebSocketServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; private final ChannelGroup group; /*websocket访问路径*/ private static final String WEBSOCKET_PATH = &quot;/websocket&quot;; private final SslContext sslCtx; public WebSocketServerInitializer(SslContext sslCtx,ChannelGroup group) &#123; this.sslCtx = sslCtx; this.group = group; &#125; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); if (sslCtx != null) &#123; pipeline.addLast(sslCtx.newHandler(ch.alloc())); &#125; //超时检查 pipeline.addLast(new IdleStateHandler(0, 0, 20)); /*增加对http的支持*/ pipeline.addLast(new HttpServerCodec()); pipeline.addLast(new HttpObjectAggregator(65536)); /*Netty提供，支持WebSocket应答数据压缩传输*/ pipeline.addLast(new WebSocketServerCompressionHandler()); /*Netty提供，对整个websocket的通信进行了初始化(发现http报文中有升级为websocket的请求) ，包括握手，以及以后的一些通信控制*/ pipeline.addLast(new WebSocketServerProtocolHandler(WEBSOCKET_PATH,null, true)); /*浏览器访问时展示index页面*/ pipeline.addLast(new ProcessWsIndexPageHandler(WEBSOCKET_PATH)); /*对websocket的数据进行处理*/ pipeline.addLast(new ProcesssWsFrameHandler(group)); &#125;&#125;public final class WebSocketServer &#123; /*创建 DefaultChannelGroup，用来保存所 有已经连接的 WebSocket Channel，群发和一对一功能可以用上*/ private final static ChannelGroup channelGroup = new DefaultChannelGroup(ImmediateEventExecutor.INSTANCE); static final boolean SSL = false;//是否启用ssl /*通过ssl访问端口为8443，否则为8080*/ static final int PORT = Integer.parseInt(System.getProperty(&quot;port&quot;, SSL? &quot;8443&quot; : &quot;8080&quot;)); public static void main(String[] args) throws Exception &#123; /*SSL配置*/ final SslContext sslCtx; if (SSL) &#123; SelfSignedCertificate ssc = new SelfSignedCertificate(); sslCtx = SslContextBuilder.forServer(ssc.certificate(), ssc.privateKey()).build(); &#125; else &#123; sslCtx = null; &#125; EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new WebSocketServerInitializer(sslCtx, channelGroup)); Channel ch = b.bind(PORT).sync().channel(); System.out.println(&quot;打开浏览器访问： &quot; + (SSL? &quot;https&quot; : &quot;http&quot;) + &quot;://127.0.0.1:&quot; + PORT + &#x27;/&#x27;); ch.closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178public class WebSocketClientHandler extends SimpleChannelInboundHandler&lt;Object&gt; &#123; //负责和服务器进行握手 private final WebSocketClientHandshaker handshaker; //握手的结果 private ChannelPromise handshakeFuture; public WebSocketClientHandler(WebSocketClientHandshaker handshaker) &#123; this.handshaker = handshaker; &#125; public ChannelFuture handshakeFuture() &#123; return handshakeFuture; &#125; //当前Handler被添加到ChannelPipeline时， // new出握手的结果的实例，以备将来使用 @Override public void handlerAdded(ChannelHandlerContext ctx) &#123; handshakeFuture = ctx.newPromise(); &#125; //通道建立，进行握手 @Override public void channelActive(ChannelHandlerContext ctx) &#123; handshaker.handshake(ctx.channel()); &#125; //通道关闭 @Override public void channelInactive(ChannelHandlerContext ctx) &#123; System.out.println(&quot;WebSocket Client disconnected!&quot;); &#125; //读取数据 @Override public void channelRead0(ChannelHandlerContext ctx, Object msg) throws Exception &#123; Channel ch = ctx.channel(); //握手未完成，完成握手 if (!handshaker.isHandshakeComplete()) &#123; try &#123; handshaker.finishHandshake(ch, (FullHttpResponse) msg); System.out.println(&quot;WebSocket Client connected!&quot;); handshakeFuture.setSuccess(); &#125; catch (WebSocketHandshakeException e) &#123; System.out.println(&quot;WebSocket Client failed to connect&quot;); handshakeFuture.setFailure(e); &#125; return; &#125; //握手已经完成，升级为了websocket，不应该再收到http报文 if (msg instanceof FullHttpResponse) &#123; FullHttpResponse response = (FullHttpResponse) msg; throw new IllegalStateException( &quot;Unexpected FullHttpResponse (getStatus=&quot; + response.status() + &quot;, content=&quot; + response.content().toString(CharsetUtil.UTF_8) + &#x27;)&#x27;); &#125; //处理websocket报文 WebSocketFrame frame = (WebSocketFrame) msg; if (frame instanceof TextWebSocketFrame) &#123; TextWebSocketFrame textFrame = (TextWebSocketFrame) frame; System.out.println(&quot;WebSocket Client received message: &quot; + textFrame.text()); &#125; else if (frame instanceof PongWebSocketFrame) &#123; System.out.println(&quot;WebSocket Client received pong&quot;); &#125; else if (frame instanceof CloseWebSocketFrame) &#123; System.out.println(&quot;WebSocket Client received closing&quot;); ch.close(); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); if (!handshakeFuture.isDone()) &#123; handshakeFuture.setFailure(cause); &#125; ctx.close(); &#125;&#125;/** 这是WebSocket客户端的示例。 要运行此示例，需要兼容的WebSocket服务器。 因此，可以通过运行cn.enjoyedu.server.WebSocketServer来启动WebSocket服务器， */public final class WebSocketClient &#123; static final String URL = System.getProperty(&quot;url&quot;, &quot;ws://127.0.0.1:8080/websocket&quot;); static final String SURL = System.getProperty(&quot;url&quot;, &quot;wss://127.0.0.1:8443/websocket&quot;); public static void main(String[] args) throws Exception &#123; URI uri = new URI(URL); String scheme = uri.getScheme() == null? &quot;ws&quot; : uri.getScheme(); final String host = uri.getHost() == null? &quot;127.0.0.1&quot; : uri.getHost(); final int port = uri.getPort(); if (!&quot;ws&quot;.equalsIgnoreCase(scheme) &amp;&amp; !&quot;wss&quot;.equalsIgnoreCase(scheme)) &#123; System.err.println(&quot;Only WS(S) is supported.&quot;); return; &#125; final boolean ssl = &quot;wss&quot;.equalsIgnoreCase(scheme); final SslContext sslCtx; if (ssl) &#123; sslCtx = SslContextBuilder.forClient().trustManager(InsecureTrustManagerFactory.INSTANCE).build(); &#125; else &#123; sslCtx = null; &#125; EventLoopGroup group = new NioEventLoopGroup(); try &#123; // Connect with V13 (RFC 6455 aka HyBi-17). You can change it to V08 or V00. // If you change it to V00, ping is not supported and remember to change // HttpResponseDecoder to WebSocketHttpResponseDecoder in the pipeline. final WebSocketClientHandler handler = new WebSocketClientHandler( WebSocketClientHandshakerFactory .newHandshaker( uri, WebSocketVersion.V13, null, true, new DefaultHttpHeaders())); Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) &#123; ChannelPipeline p = ch.pipeline(); if (sslCtx != null) &#123; p.addLast(sslCtx.newHandler(ch.alloc(), host, port)); &#125; p.addLast( //http协议为握手必须 new HttpClientCodec(), new HttpObjectAggregator(8192), //支持WebSocket数据压缩 WebSocketClientCompressionHandler.INSTANCE, handler); &#125; &#125;); //连接服务器 Channel ch = b.connect(uri.getHost(), port).sync().channel(); //等待握手完成 handler.handshakeFuture().sync(); BufferedReader console = new BufferedReader( new InputStreamReader(System.in)); while (true) &#123; String msg = console.readLine(); if (msg == null) &#123; break; &#125; else if (&quot;bye&quot;.equals(msg.toLowerCase())) &#123; ch.writeAndFlush(new CloseWebSocketFrame()); ch.closeFuture().sync(); break; &#125; else if (&quot;ping&quot;.equals(msg.toLowerCase())) &#123; WebSocketFrame frame = new PingWebSocketFrame( Unpooled.wrappedBuffer(new byte[] &#123; 8, 1, 8, 1 &#125;)); ch.writeAndFlush(frame); &#125; else &#123; WebSocketFrame frame = new TextWebSocketFrame(msg); ch.writeAndFlush(frame); &#125; &#125; &#125; finally &#123; group.shutdownGracefully(); &#125; &#125;&#125; 可以参考cn.enjoyedu.nettyws中的代码","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"05序列化问题","slug":"netty/开箱即用的ChannelHandler/05序列化问题","date":"2021-11-23T12:00:14.000Z","updated":"2022-03-23T09:03:58.188Z","comments":true,"path":"blog/netty/开箱即用的ChannelHandler/05序列化问题/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/%E5%BC%80%E7%AE%B1%E5%8D%B3%E7%94%A8%E7%9A%84ChannelHandler/05%E5%BA%8F%E5%88%97%E5%8C%96%E9%97%AE%E9%A2%98/","excerpt":"","text":"Java序列化的缺点: 无法跨语言 序列化后的码流太大 序列化性能太低 第三方的MessagePack实战cn.enjoyedu.nettybasic.serializable.msgpack 使用 JBoss Marshalling 进行序列化 通过 Protocol Buffers 序列化Protocol Buffers使用代码看cn.enjoyedu.nettybasic.serializable.protobuf","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"04写大型数据","slug":"netty/开箱即用的ChannelHandler/04写大型数据","date":"2021-11-23T12:00:13.000Z","updated":"2022-03-23T09:03:58.183Z","comments":true,"path":"blog/netty/开箱即用的ChannelHandler/04写大型数据/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/%E5%BC%80%E7%AE%B1%E5%8D%B3%E7%94%A8%E7%9A%84ChannelHandler/04%E5%86%99%E5%A4%A7%E5%9E%8B%E6%95%B0%E6%8D%AE/","excerpt":"","text":"因为网络饱和的可能性，如何在异步框架中高效地写大块的数据是一个特殊的问题。由于写操作是非阻塞的，所以即使没有写出所有的数据，写操作也会在完成时返回并通知ChannelFuture。当这种情况发生时，如果仍然不停地写入，就有内存耗尽的风险。所以在写大型数据时，需要准备好处理到远程节点的连接是慢速连接的情况，这种情况会导致内存释放的延迟。 NIO的零拷贝特性消除了将文件的内容从文件系统移动到网络栈的复制过程。所有的这一切都发生在 Netty 的核心中，所以应用程序所有需要做的就是使用一个 FileRegion 接口的实现，其在 Netty 的 API 文档中的定义是:“通过支持零拷贝的文件传输的 Channel 来发送的文件区域。”这个示例只适用于文件内容的直接传输，不包括应用程序对数据的任何处理。需要将数据从文件系统复制到用户内存中时，可以使用 ChunkedWriteHandler，它支持异步写大型数据流,而又不会导致大量的内存消耗。 关键是 interface ChunkedInput，其中类型参数 B 是 readChunk()方法返回的类型。Netty 预置了该接口的4个实现，每个都代表了一个将由 ChunkedWriteHandler 处理的不定长度的数据流。当 Channel 的状态变为活动的时，WriteStreamHandler 将会逐块地把来自文件中的数据作为 ChunkedStream 写入。数据在传输之前将会由 SslHandler 加密。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"03基于分隔符的协议和基于长度的协议","slug":"netty/开箱即用的ChannelHandler/03基于分隔符的协议和基于长度的协议","date":"2021-11-23T12:00:12.000Z","updated":"2022-03-23T09:03:58.179Z","comments":true,"path":"blog/netty/开箱即用的ChannelHandler/03基于分隔符的协议和基于长度的协议/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/%E5%BC%80%E7%AE%B1%E5%8D%B3%E7%94%A8%E7%9A%84ChannelHandler/03%E5%9F%BA%E4%BA%8E%E5%88%86%E9%9A%94%E7%AC%A6%E7%9A%84%E5%8D%8F%E8%AE%AE%E5%92%8C%E5%9F%BA%E4%BA%8E%E9%95%BF%E5%BA%A6%E7%9A%84%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"这两种协议是为了解决解决粘包和半包问题 基于分隔符的协议下面以LineBasedFrameDecoder说明，下图展示了当帧由行尾序列\\r\\n(回车符+换行符)分隔时是如何被处理的。 如果你正在使用除了行尾符之外的分隔符分隔的帧，那么你可以以类似的方式使用DelimiterBasedFrameDecoder，只需要将特定的分隔符序列指定到其构造函数即可。 基于长度的协议基于长度的协议通过将它的长度编码到帧的头部来定义帧（对于固定帧大小的协议来说，不需要将帧长度编码到头部），而不是使用特殊的分隔符来标记它的结束。 FixedLengthFrameDecoder—固定帧大小 LengthFieldBasedFrameDecoder—帧大小不是固定值如果遇到被编码到消息头部的帧大小不是固定值的协议。为了处理这种变长帧，你可以使用 LengthFieldBasedFrameDecoder，它将从头部字段确定帧长，然后从数据流中提取指定的字节数。LengthFieldBasedFrame的构造函数参数说明： maxFrameLength：表示的是包的最大长度； lengthFieldOffset：指的是长度域的偏移量，表示跳过指定个数字节之后的才是长度域； lengthFieldLength：记录该帧数据长度的字段，也就是长度域本身的长度； lengthAdjustment：长度的一个修正值，可正可负（要添加到长度字段值的补偿值）； initialBytesToStrip：从数据帧中跳过的字节数，表示得到一个完整的数据包之后，忽略多少字节，开始读取实际我要的数据 failFast：如果为true，则一旦解码器注意到帧的长度超过maxFrameLength，就抛出一个TooLongFrameException，而为false表示在读取超过maxFrameLength的整个帧后，才会抛出TooLongFrameException。默认情况下设置为true，建议不要修改，否则可能会造成内存溢出。 情况1:数据包大小: 14B &#x3D; 长度域2B + “HELLO, WORLD”（单词HELLO+一个逗号+一个空格+单词WORLD）长度域的值为12B(0x000c)。希望解码后保持一样，根据上面的公式,参数应该为： lengthFieldOffset &#x3D; 0 lengthFieldLength &#x3D; 2 lengthAdjustment 无需调整 initialBytesToStrip &#x3D; 0 （解码过程中，没有丢弃任何数据） 情况2:数据包大小: 14B &#x3D; 长度域2B + “HELLO, WORLD”长度域的值为12B(0x000c)。解码后，希望丢弃长度域2B字段，所以，只要initialBytesToStrip &#x3D; 2即可。 lengthFieldOffset &#x3D; 0 lengthFieldLength &#x3D; 2 lengthAdjustment 无需调整 initialBytesToStrip &#x3D; 2 （解码过程中，丢弃2个字节的数据） 情况3:数据包大小: 14B &#x3D; 长度域2B + “HELLO, WORLD”。长度域的值为14(0x000E)长度域的值为14(0x000E)，包含了长度域本身的长度。希望解码后保持一样，根据上面的公式，参数应该为： lengthFieldOffset &#x3D; 0 lengthFieldLength &#x3D; 2 lengthAdjustment &#x3D; -2 因为长度域为14，而报文内容为12，为了防止读取报文超出报文本体，和将长度字段一起读取进来，需要告诉netty，实际读取的报文长度比长度域中的要少2（12-14&#x3D;-2） initialBytesToStrip &#x3D; 0（解码过程中，没有丢弃任何数据） 情况4:在长度域前添加2个字节的Header。长度域的值(0x00000C) &#x3D; 12。总数据包长度: 17&#x3D;Header(2B) + 长度域(3B) + “HELLO, WORLD” 长度域的值为12B(0x000c)。编码解码后，长度保持一致，所以initialBytesToStrip &#x3D; 0。参数应该为: lengthFieldOffset &#x3D; 2 lengthFieldLength &#x3D; 3 lengthAdjustment &#x3D; 0 initialBytesToStrip &#x3D; 0（解码过程中，没有丢弃任何数据） 情况5:Header与长度域的位置换了。总数据包长度: 17&#x3D;长度域(3B) + Header(2B) + “HELLO, WORLD”长度域的值为12B(0x000c)。编码解码后，长度保持一致，所以initialBytesToStrip &#x3D; 0。参数应该为: lengthFieldOffset &#x3D; 0 lengthFieldLength &#x3D; 3 lengthAdjustment &#x3D; 2 因为长度域为12，而报文内容为12，但是我们需要把Header的值一起读取进来，需要告诉netty，实际读取的报文内容长度比长度域中的要多2（12+2&#x3D;14） initialBytesToStrip &#x3D; 0（解码过程中，没有丢弃任何数据） 情况6:带有两个header。HDR1 丢弃，长度域丢弃，只剩下第二个header和有效包体，这种协议中，一般HDR1可以表示magicNumber，表示应用只接受以该magicNumber开头的二进制数据，rpc里面用的比较多。总数据包长度: 16&#x3D;HDR1(1B)+长度域(2B) +HDR2(1B) + “HELLO, WORLD”(12B)长度域的值为12B(0x000c)，参数应该为: lengthFieldOffset &#x3D; 1 lengthFieldLength &#x3D; 2 lengthAdjustment &#x3D; 1 因为长度域为12，而报文内容为12，但是我们需要把HDR2的值一起读取进来，需要告诉netty，实际读取的报文内容长度比长度域中的要多1（12+1&#x3D;13） initialBytesToStrip &#x3D; 3（丢弃了HDR1和长度字段 ） 情况7:带有两个header，HDR1 丢弃，长度域丢弃，只剩下第二个header和有效包体。总数据包长度: 16&#x3D;HDR1(1B)+长度域(2B) +HDR2(1B) + “HELLO, WORLD”(12B)长度域的值为16B(0x0010)，长度为2，HDR1的长度为1，HDR2的长度为1，包体的长度为12，1+1+2+12&#x3D;16。参数应该为: lengthFieldOffset &#x3D; 1 lengthFieldLength &#x3D; 2 lengthAdjustment &#x3D; -3 因为长度域为16，需要告诉netty，实际读取的报文内容长度比长度域中的要少3（13-16&#x3D; -3） initialBytesToStrip &#x3D; 3（丢弃了HDR1和长度字段 ） 总结：lengthFieldOffset、lengthFieldLength和initialBytesToStrip这3个参数很好理解，lengthFieldOffset和lengthFieldLength共同确定的长度域的位置。initialBytesToStrip表示的是解码前和解码后的差值，表示丢弃多少数据。 lengthAdjustment这个参数进过上面的7种情况分析，lengthAdjustment的取之为：解码后的参数-长度域表示的值。例如情况7中，解码后的长度为13，长度域表示的值为16，所以lengthAdjustment&#x3D;13-16。lengthAdjustment这个值所表示的是对长度域的修正值，具体来说就是实际读取的长度可能于长度域的值不同，在这种情况下需要用lengthAdjustment来告诉netty长度域的值要修正多少。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"02空闲的连接和超时","slug":"netty/开箱即用的ChannelHandler/02空闲的连接和超时","date":"2021-11-23T12:00:11.000Z","updated":"2022-03-23T09:03:58.171Z","comments":true,"path":"blog/netty/开箱即用的ChannelHandler/02空闲的连接和超时/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/%E5%BC%80%E7%AE%B1%E5%8D%B3%E7%94%A8%E7%9A%84ChannelHandler/02%E7%A9%BA%E9%97%B2%E7%9A%84%E8%BF%9E%E6%8E%A5%E5%92%8C%E8%B6%85%E6%97%B6/","excerpt":"","text":"检测空闲连接以及超时对于及时释放资源来说是至关重要的。由于这是一项常见的任务，Netty特地为它提供了几个ChannelHandler实现。 1234567891011121314151617181920212223242526272829303132/** * 发送心跳 */public class IdleStateHandlerInitializer extends ChannelInitializer&lt;Channel&gt; &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast( //(1) IdleStateHandler 将在被触发时发送一个IdleStateEvent 事件 new IdleStateHandler(0, 0, 60, TimeUnit.SECONDS)); //将一个 HeartbeatHandler 添加到ChannelPipeline中 pipeline.addLast(new HeartbeatHandler()); &#125; //实现 userEventTriggered() 方法以发送心跳消息 public static final class HeartbeatHandler extends ChannelInboundHandlerAdapter &#123; //发送到远程节点的心跳消息 private static final ByteBuf HEARTBEAT_SEQUENCE = Unpooled.unreleasableBuffer( Unpooled.copiedBuffer(&quot;HEARTBEAT&quot;, CharsetUtil.ISO_8859_1)); @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; //(2) 发送心跳消息，并在发送失败时关闭该连接 if (evt instanceof IdleStateEvent) &#123; ctx.writeAndFlush(HEARTBEAT_SEQUENCE.duplicate()) .addListener(ChannelFutureListener.CLOSE_ON_FAILURE); &#125; else &#123; //不是 IdleStateEvent 事件，所以将它传递给下一个 ChannelInboundHandler super.userEventTriggered(ctx, evt); &#125; &#125; &#125;&#125; 这个示例演示了如何使用 IdleStateHandler 来测试远程节点是否仍然还活着，并且在它失活时通过关闭连接来释放资源。如果连接超过 60 秒没有接收或者发送任何的数据,那么IdleStateHandler会使用一个IdleStateEvent 事件来调用fireUserEventTriggered()方法。HeartbeatHandler 实现了 userEventTriggered()方法，如果这个方法检测到 IdleStateEvent 事件，它将会发送心跳消息，并且添加一个将在发送操作失败时关闭该连接的ChannelFutureListener。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"01构建基Netty的HTTP或HTTPS应用程序","slug":"netty/开箱即用的ChannelHandler/01构建基Netty的HTTP或HTTPS应用程序","date":"2021-11-23T12:00:10.000Z","updated":"2022-03-23T09:03:58.169Z","comments":true,"path":"blog/netty/开箱即用的ChannelHandler/01构建基Netty的HTTP或HTTPS应用程序/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/%E5%BC%80%E7%AE%B1%E5%8D%B3%E7%94%A8%E7%9A%84ChannelHandler/01%E6%9E%84%E5%BB%BA%E5%9F%BANetty%E7%9A%84HTTP%E6%88%96HTTPS%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/","excerpt":"","text":"通过SSL&#x2F;TLS保护Netty应用程序（HTTPS）SSL 和 TLS 这样的安全协议，它们层叠在其他协议之上，用以实现数据安全。我们在访 问安全网站时遇到过这些协议，但是它们也可用于其他不是基于 HTTP 的应用程序，如安全 SMTP(SMTPS)邮件服务器甚至是关系型数据库系统。 为了支持 SSL&#x2F;TLS，Java 提供了 javax.net.ssl 包，它的 SSLContext 和 SSLEngine 类使得 实现解密和加密相当简单直接。Netty 通过一个名为 SslHandler 的 ChannelHandler 实现利用 了这个 API，其中 SslHandler 在内部使用 SSLEngine 来完成实际的工作。 在大多数情况下，SslHandler 将是 ChannelPipeline 中的第一个 ChannelHandler。 Netty 的 OpenSSL&#x2F;SSLEngine 实现 Netty 还提供了使用 OpenSSL 工具包(www.openssl.org)的 SSLEngine 实现。这个 OpenSslEngine 类提供了比 JDK 提供的 SSLEngine 实现更好的性能。如果OpenSSL库可用, 可以将 Netty 应用程序 (客户端和服务器) 配置为默认使用OpenSslEngine。如果不可用,Netty 将会回退到 JDK 实现。 SslHandler的方法: setHandshakeTimeout (long,TimeUnit) setHandshakeTimeoutMillis (long) getHandshakeTimeoutMillis() 设置和获取超时时间,超时之后,握手ChannelFuture将会被通知失败 setCloseNotifyTimeout (long,TimeUnit) setCloseNotifyTimeoutMillis (long) getCloseNotifyTimeoutMillis() 设置和获取超时时间,超时之后,将会触发一个关闭通知并关闭连接。 这也将会导致通知该ChannelFuture失败 handshakeFuture() 返回一个在握手完成后将会得到通知的ChannelFuture。如果握手先前已经执行过了, 则返回一个包含了先前的握手结果的ChannelFuture close() close(ChannelPromise) close(ChannelHandlerContext,ChannelPromise) 发送close_notify以请求关闭并销毁底层的SslEngine 构建基于 Netty 的 HTTP&#x2F;HTTPS 应用程序HTTP 解码器、编码器和编解码器 HTTP 解码器和编码器： HttpRequestEncoder 将HttpRequest、HttpContent和LastHttpContent消息编码为字节 HttpResponseEncoder 将HttpResponse、HttpContent和LastHttpContent消息编码为字节 HttpRequestDecoder 将字节解码为HttpRequest、HttpContent和LastHttpContent消息 HttpResponseDecoder 将字节解码为HttpResponse、HttpContent和LastHttpContent消息 netty还提供了HttpClientCodec和HttpServerCodec，是对上边的简化 对于客户端： 12345# 发请求和接受响应，所以需要对请求编码，对响应解码pipeline.addLast(&quot;decoder&quot;, new HttpResponseDecoder());pipeline.addLast(&quot;encoder&quot;, new HttpRequestEncoder());简化后：pipeline.addLast(&quot;codec&quot;, new HttpClientCodec()); 对于服务端： 12345# 接受请求和发送响应，所以需要对请求解码，对响应编码pipeline.addLast(&quot;decoder&quot;, new HttpRequestDecoder());pipeline.addLast(&quot;encoder&quot;, new HttpResponseEncoder());简化后：pipeline.addLast(&quot;codec&quot;, new HttpServerCodec()); 聚合 HTTP 消息——HttpObjectAggregator由于 HTTP 的请求和响应可能由许多部分组成,因此你需要聚合它们以形成完整的消息。为了消除这项繁琐的任务,Netty 提供了一个聚合器,它可以将多个消息部分合并为 FullHttpRequest 或者 FullHttpResponse 消息。通过这样的方式,你将总是看到完整的消息内容。 HTTP 压缩——HttpContentCompressor当使用 HTTP 时, 建议开启压缩功能以尽可能多地减小传输数据的大小。 Netty 为压缩和解压缩提供了 ChannelHandler 实现, 它们同时支持 gzip 和 deflate 编码。 客户端可以通过提供以下头部信息来指示服务器它所支持的压缩格式: 1234GET /encrypted-area HTTP/1.1Host: www.example.comAccept-Encoding: gzip, deflate#然而,需要注意的是,服务器没有义务压缩它所发送的数据。 12345pipeline.addLast(&quot;encoder&quot;,new HttpResponseEncoder());pipeline.addLast(&quot;decoder&quot;,new HttpRequestDecoder());pipeline.addLast(&quot;aggregator&quot;, new HttpObjectAggregator(10*1024*1024));pipeline.addLast(&quot;compressor&quot;,new HttpContentCompressor());pipeline.addLast(&quot;handler&quot;, new BusiHandler());// 服务端业务逻辑 自己的简单的http服务器代码","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"解决粘包和半包问题","slug":"netty/解决粘包和半包问题","date":"2021-11-23T12:00:09.000Z","updated":"2022-03-23T09:03:58.163Z","comments":true,"path":"blog/netty/解决粘包和半包问题/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/%E8%A7%A3%E5%86%B3%E7%B2%98%E5%8C%85%E5%92%8C%E5%8D%8A%E5%8C%85%E9%97%AE%E9%A2%98/","excerpt":"","text":"该代码存在两个问题第一个client在调用writeAndFlush后才把数据发送到server第二个client把第一次把数据发到server时，有部分数据不完整，因为缓存区满了，放不下，只能等到下一次发送。 这两个问题就是典型的TCP粘包&#x2F;半包问题。 什么是TCP粘包半包？假设客户端分别发送了两个数据包D1和D2给服务端，由于服务端一次读取到的字节数是不确定的，故可能存在以下4种情况。 服务端分两次读取到了两个独立的数据包，分别是D1和D2，没有粘包和拆包； 服务端一次接收到了两个数据包，D1和D2粘合在一起，被称为TCP粘包； 服务端分两次读取到了两个数据包，第一次读取到了完整的D1包和D2包的部分内容，第二次读取到了D2包的剩余内容，这被称为TCP拆包； 服务端分两次读取到了两个数据包，第一次读取到了D1包的部分内容D1_1，第二次读取到了D1包的剩余内容D1_2和D2包的整包。 如果此时服务端TCP接收滑窗非常小，而数据包D1和D2比较大，很有可能会发生第五种可能，即服务端分多次才能将D1和D2包接收完全，期间发生多次拆包。 TCP粘包&#x2F;半包发生的原因由于TCP协议本身的机制（面向连接的可靠地协议-三次握手机制）客户端与服务器会维持一个连接（Channel），数据在连接不断开的情况下，可以持续不断地将多个数据包发往服务器，但是如果发送的网络数据包太小，那么他本身会启用Nagle算法（可配置是否启用）对较小的数据包进行合并（基于此，TCP的网络延迟要UDP的高些）然后再发送（超时或者包大小足够）。那么这样的话，服务器在接收到消息（数据流）的时候就无法区分哪些数据包是客户端自己分开发送的，这样产生了粘包；服务器在接收到数据库后，放到缓冲区中，如果消息没有被及时从缓存区取走，下次在取数据的时候可能就会出现一次取出多个数据包的情况，造成粘包现象 注意UDP：本身作为无连接的不可靠的传输协议（适合频繁发送较小的数据包），他不会对数据包进行合并发送（也就没有Nagle算法之说了），他直接是一端发送什么数据，直接就发出去了，既然他不会对数据合并，每一个数据包都是完整的（数据+UDP头+IP头等等发一次数据封装一次）也就没有粘包一说了。 分包产生的原因就简单的多：可能是IP分片传输导致的，也可能是传输过程中丢失部分包导致出现的半包，还有可能就是一个包可能被分成了两次传输，在取数据的时候，先取到了一部分（还可能与接收的缓冲区大小有关系），总之就是一个数据包被分成了多次接收。更具体的原因有三个，分别如下: 应用程序写入数据的字节大小大于套接字发送缓冲区的大小 进行MSS大小的TCP分段。MSS是最大报文段长度的缩写。MSS是TCP报文段中的数据字段的最大长度。数据字段加上TCP首部才等于整个的TCP报文段。所以MSS并不是TCP报文段的最大长度，而是：MSS&#x3D;TCP报文段长度-TCP首部长度 以太网的payload大于MTU进行IP分片。MTU指：一种通信协议的某一层上面所能通过的最大数据包大小。如果IP层有一个数据包要传，而且数据的长度比链路层的MTU大，那么IP层就会进行分片，把数据包分成托干片，让每一片都不超过MTU。注意，IP分片可以发生在原始发送端主机上，也可以发生在中间路由器上。 解决粘包半包问题由于底层的TCP无法理解上层的业务数据，所以在底层是无法保证数据包不被拆分和重组的，这个问题只能通过上层的应用协议栈设计来解决，根据业界的主流协议的解决方案，可以归纳如下。 在包尾增加分割符，比如回车换行符进行分割，例如FTP协议； 消息定长，例如每个报文的大小为固定长度200字节，如果不够，空位补空格； 将消息分为消息头和消息体，消息头中包含表示消息总长度（或者消息体长度）的字段，通常设计思路为消息头的第一个字段使用int32来表示消息的总长度，LengthFieldBasedFrameDecoder。 在netty提供了对应的handler实现：第一种：LineBasedFrameDecoder和DelimiterBasedFrameDecoder参见cn.enjoyedu.nettybasic.splicing.linebase和cn.enjoyedu.nettybasic.splicing.delimiter下的代码 第二种：FixedLengthFrameDecoder参见cn.enjoyedu.nettybasic.splicing.fixed下的代码 第三种：LengthFieldBasedFrameDecoder参见cn.enjoyedu.nettybasic.serializable.msgpack 参见cn.enjoyedu.nettybasic.splicing.demo下的代码","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"01-07-编解码器","slug":"netty/01-07-编解码器","date":"2021-11-23T12:00:08.000Z","updated":"2022-03-23T09:03:58.162Z","comments":true,"path":"blog/netty/01-07-编解码器/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/01-07-%E7%BC%96%E8%A7%A3%E7%A0%81%E5%99%A8/","excerpt":"","text":"当你通过Netty发送或者接收一个消息的时候，就将会发生一次数据转换。入站消息会被解码；也就是说，从字节转换为另一种格式，通常是一个Java对象。如果是出站消息，则会发生相反方向的转换：它将从它的当前格式被编码为字节。这两种方向的转换的原因很简单：网络数据总是一系列的字节。 对应于特定的需要，Netty为编码器和解码器提供了不同类型的抽象类。这些基类的名称将类似于ByteToMessageDecoder、MessageToByteEncoder或MessageToMessageEncoder。对于特殊的类型，比如Protocol Buffers提供了ProtobufEncoder和ProtobufDecoder这样的名称。 所有由Netty提供的编码器&#x2F;解码器适配器类都实现了ChannelOutboundHandler或者ChannelInboundHandler接口。 解决粘包&#x2F;半包问题参见cn.enjoyedu.nettybasic.splicing.demo下的代码该代码存在两个问题第一个client在调用writeAndFlush后才把数据发送到server第二个client把第一次把数据发到server时，有部分数据不完整，因为缓存区满了，放不下，只能等到下一次发送。 这两个问题就是典型的TCP粘包&#x2F;半包问题。 什么是TCP粘包半包？假设客户端分别发送了两个数据包D1和D2给服务端，由于服务端一次读取到的字节数是不确定的，故可能存在以下4种情况。 服务端分两次读取到了两个独立的数据包，分别是D1和D2，没有粘包和拆包； 服务端一次接收到了两个数据包，D1和D2粘合在一起，被称为TCP粘包； 服务端分两次读取到了两个数据包，第一次读取到了完整的D1包和D2包的部分内容，第二次读取到了D2包的剩余内容，这被称为TCP拆包； 服务端分两次读取到了两个数据包，第一次读取到了D1包的部分内容D1_1，第二次读取到了D1包的剩余内容D1_2和D2包的整包。 如果此时服务端TCP接收滑窗非常小，而数据包D1和D2比较大，很有可能会发生第五种可能，即服务端分多次才能将D1和D2包接收完全，期间发生多次拆包。 TCP粘包&#x2F;半包发生的原因由于TCP协议本身的机制（面向连接的可靠地协议-三次握手机制）客户端与服务器会维持一个连接（Channel），数据在连接不断开的情况下，可以持续不断地将多个数据包发往服务器，但是如果发送的网络数据包太小，那么他本身会启用Nagle算法（可配置是否启用）对较小的数据包进行合并（基于此，TCP的网络延迟要UDP的高些）然后再发送（超时或者包大小足够）。那么这样的话，服务器在接收到消息（数据流）的时候就无法区分哪些数据包是客户端自己分开发送的，这样产生了粘包；服务器在接收到数据库后，放到缓冲区中，如果消息没有被及时从缓存区取走，下次在取数据的时候可能就会出现一次取出多个数据包的情况，造成粘包现象 注意UDP：本身作为无连接的不可靠的传输协议（适合频繁发送较小的数据包），他不会对数据包进行合并发送（也就没有Nagle算法之说了），他直接是一端发送什么数据，直接就发出去了，既然他不会对数据合并，每一个数据包都是完整的（数据+UDP头+IP头等等发一次数据封装一次）也就没有粘包一说了。 分包产生的原因就简单的多：可能是IP分片传输导致的，也可能是传输过程中丢失部分包导致出现的半包，还有可能就是一个包可能被分成了两次传输，在取数据的时候，先取到了一部分（还可能与接收的缓冲区大小有关系），总之就是一个数据包被分成了多次接收。更具体的原因有三个，分别如下: 应用程序写入数据的字节大小大于套接字发送缓冲区的大小 进行MSS大小的TCP分段。MSS是最大报文段长度的缩写。MSS是TCP报文段中的数据字段的最大长度。数据字段加上TCP首部才等于整个的TCP报文段。所以MSS并不是TCP报文段的最大长度，而是：MSS&#x3D;TCP报文段长度-TCP首部长度 以太网的payload大于MTU进行IP分片。MTU指：一种通信协议的某一层上面所能通过的最大数据包大小。如果IP层有一个数据包要传，而且数据的长度比链路层的MTU大，那么IP层就会进行分片，把数据包分成托干片，让每一片都不超过MTU。注意，IP分片可以发生在原始发送端主机上，也可以发生在中间路由器上。 解决粘包半包问题由于底层的TCP无法理解上层的业务数据，所以在底层是无法保证数据包不被拆分和重组的，这个问题只能通过上层的应用协议栈设计来解决，根据业界的主流协议的解决方案，可以归纳如下。 在包尾增加分割符，比如回车换行符进行分割，例如FTP协议； 消息定长，例如每个报文的大小为固定长度200字节，如果不够，空位补空格； 将消息分为消息头和消息体，消息头中包含表示消息总长度（或者消息体长度）的字段，通常设计思路为消息头的第一个字段使用int32来表示消息的总长度，LengthFieldBasedFrameDecoder。 在netty提供了对应的handler实现：第一种：LineBasedFrameDecoder和DelimiterBasedFrameDecoder参见cn.enjoyedu.nettybasic.splicing.linebase和cn.enjoyedu.nettybasic.splicing.delimiter下的代码 第二种：FixedLengthFrameDecoder参见cn.enjoyedu.nettybasic.splicing.fixed下的代码 第三种：LengthFieldBasedFrameDecoder参见cn.enjoyedu.nettybasic.serializable.msgpack 解码器解码器是负责将入站数据从一种格式转换到另一种格式的，所以Netty的解码器实现了 ChannelInboundHandler。 将字节解码为消息–ByteToMessageDecoder和ReplayingDecoder。 将一种消息类型解码为另一种–MessageToMessageDecoder。 抽象类 ByteToMessageDecoder—-将字节解码为消息由于你不可能知道远程节点是否会一次性地发送一个完整的消息，所以这个类会对入站数据进行缓冲，直到它准备好处理。 decode(ChannelHandlerContext ctx,ByteBuf in,List&lt;Object&gt; out)这是你必须实现的唯一抽象方法。decode()方法被调用时将会传入一个包含了传入数据的ByteBuf，以及一个用来添加解码消息的List。对这个方法的调用将会重复进行，直到确定没有新的元素被添加到该List，或者该ByteBuf 中没有更多可读取的字节时为止。然后，如果该List不为空，那么它的内容将会被传递给ChannelPipeline 中的下一个ChannelInboundHandler。 抽象类 ReplayingDecoder—ByteToMessageDecoder的扩展ReplayingDecoder扩展了ByteToMessageDecoder类。它通过使用一个自定义的ByteBuf实现，ReplayingDecoderByteBuf，使得我们不必调用readableBytes()方法。 抽象类 MessageToMessageDecoder—将一种消息类型解码为另一种在两个消息格式之间进行转换（例如，从String-&gt;Integer）decode(ChannelHandlerContext ctx,I msg,List&lt;Object&gt; out)对于每个需要被解码为另一种格式的入站消息来说，该方法都将会被调用。解码消息随后会被传递给ChannelPipeline中的下一个ChannelInboundHandler MessageToMessageDecoder，T代表源数据的类型 TooLongFrameException由于Netty 是一个异步框架，所以需要在字节可以解码之前在内存中缓冲它们。因此，不能让解码器缓冲大量的数据以至于耗尽可用的内存。为了解除这个常见的顾虑，Netty 提供了TooLongFrameException 类，其将由解码器在帧超出指定的大小限制时抛出。 为了避免这种情况，你可以设置一个最大字节数的阈值，如果超出该阈值，则会导致抛出一个TooLongFrameException（随后会被ChannelHandler.exceptionCaught()方法捕获）。然后，如何处理该异常则完全取决于该解码器的用户。某些协议（如HTTP）可能允许你返回一个特殊的响应。而在其他的情况下，唯一的选择可能就是关闭对应的连接。 编码器解码器的功能正好相反。Netty 提供了一组类，用于帮助你编写具有以下功能的编码器： 将消息编码为字节；MessageToByteEncoder 将消息编码为消息：MessageToMessageEncoder，T代表源数据的类型 抽象类 MessageToByteEncoder—将消息编码为字节encode()方法是你需要实现的唯一抽象方法。它被调用时将会传入要被该类编码为ByteBuf的（类型为I的）出站消息。该ByteBuf随后将会被转发给ChannelPipeline中的下一个ChannelOutboundHandler. 抽象类 MessageToMessageEncoder—将消息编码为消息这是你需要实现的唯一方法。每个通过write()方法写入的消息都将会被传递给encode()方法，以编码为一个或者多个出站消息。随后，这些出站消息将会被转发给ChannelPipeline中的下一个ChannelOutboundHandler 编解码器类虽然我们一直将解码器和编码器作为单独的实体讨论，但是你有时将会发现在同一个类中管理入站和出站数据和消息的转换是很有用的。Netty的抽象编解码器类正好用于这个目的,因为它们每个都将捆绑一个解码器&#x2F;编码器对，以处理我们一直在学习的这两种类型的操作。这些类同时实现了ChannelInboundHandler 和ChannelOutboundHandler 接口。 抽象类 ByteToMessageCodec它结合了ByteToMessageDecoder以及它的逆向——MessageToByteEncoder。 抽象类 MessageToMessageCodecMessageToMessageCodec 是一个参数化的类,定义如下:public abstract class MessageToMessageCodec&lt;INBOUND_IN,OUTBOUND_IN&gt; CombinedChannelDuplexHandler 类CombinedChannelDuplexHandler声明为: 12public class CombinedChannelDuplexHandler&lt;I extends ChannelInboundHandler, O extends ChannelOutboundHandler&gt; extends ChannelDuplexHandler 这个类充当了ChannelInboundHandler和ChannelOutboundHandler(该类的类型参数 I 和 O)的容器。通过提供分别继承了解码器类和编码器类的类型,我们可以实现一个编解码器,而又不必直接扩展抽象的编解码器类。初始化代码","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"01-06-单元测试","slug":"netty/01-06-单元测试","date":"2021-11-23T12:00:07.000Z","updated":"2022-03-23T09:03:58.154Z","comments":true,"path":"blog/netty/01-06-单元测试/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/01-06-%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/","excerpt":"","text":"Netty提供了它所谓的Embedded传输，用于测试ChannelHandler。这个传输是一种特殊的Channel实现–EmbeddedChannel–的功能，这个实现提供了通过ChannelPipeline传播事件的简便方法。 这个想法是直截了当的:将入站数据或者出站数据写入到EmbeddedChannel中，然后检查是否有任何东西到达了ChannelPipeline的尾端。以这种方式，你便可以确定消息是否已经被编码或者被解码过了，以及是否触发了任何的ChannelHandler动作。 在每种情况下，消息都将会传递过ChannelPipeline，并且被相关的ChannelInboundHandler 或者ChannelOutboundHandler 处理。 测试入站消息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class FixedLengthFrameDecoder extends ByteToMessageDecoder &#123; private final int frameLength; //指定要生成的帧的长度 public FixedLengthFrameDecoder(int frameLength) &#123; if (frameLength &lt;= 0) &#123; throw new IllegalArgumentException( &quot;frameLength must be a positive integer: &quot; + frameLength); &#125; this.frameLength = frameLength; &#125; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; //检查是否有足够的字节可以被读取，以生成下一个帧 while (in.readableBytes() &gt;= frameLength) &#123; //从 ByteBuf 中读取一个新帧 ByteBuf buf = in.readBytes(frameLength); //将该帧添加到已被解码的消息列表中 out.add(buf); &#125; &#125;&#125;public class FixedLengthFrameDecoderTest &#123; @Test public void testFramesDecoded() &#123; //创建一个 ByteBuf，并存储 9 字节 ByteBuf buf = Unpooled.buffer(); for (int i = 0; i &lt; 9; i++) &#123; buf.writeByte(i); &#125; ByteBuf input = buf.duplicate(); //创建一个EmbeddedChannel，并添加一个FixedLengthFrameDecoder， // 其将以 3 字节的帧长度被测试 EmbeddedChannel channel = new EmbeddedChannel(new FixedLengthFrameDecoder(3)); //返回 false，因为没有一个完整的可供读取的帧 assertFalse(channel.writeInbound(input.readBytes(1))); //还是返回 false，因为没有一个完整的可供读取的帧 assertFalse(channel.writeInbound(input.readBytes(1))); //返回 true，因为已经有了可供读取的帧 assertTrue(channel.writeInbound(input.readBytes(1))); //将剩余的数据写入，准备readInbound测试 assertTrue(channel.writeInbound(input.readBytes(6))); //标记 Channel 为已完成状态 assertTrue(channel.finish()); // read messages //读取所生成的消息，并且验证是否有 3 帧（切片），其中每帧（切片）都为 3 字节 ByteBuf read = (ByteBuf) channel.readInbound(); //和源进行比对 assertEquals(buf.readSlice(3), read); read.release(); read = (ByteBuf) channel.readInbound(); assertEquals(buf.readSlice(3), read); read.release(); read = (ByteBuf) channel.readInbound(); assertEquals(buf.readSlice(3), read); read.release(); assertNull(channel.readInbound()); buf.release(); &#125;&#125; 测试出站消息12345678910111213141516171819202122232425262728293031323334353637383940//扩展 MessageToMessageEncoder 以将一个消息编码为另外一种格式public class AbsIntegerEncoder extends MessageToMessageEncoder&lt;ByteBuf&gt; &#123; @Override protected void encode(ChannelHandlerContext channelHandlerContext, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; //检查是否有足够的字节用来编码,int为4个字节 while (in.readableBytes() &gt;= 4) &#123; //从输入的 ByteBuf中读取下一个整数，并且计算其绝对值 int value = Math.abs(in.readInt()); //将该整数写入到编码消息的 List 中 out.add(value); &#125; &#125;&#125;public class AbsIntegerEncoderTest &#123; @Test public void testEncoded() &#123; //(1) 创建一个 ByteBuf，并且写入 9 个负整数 ByteBuf buf = Unpooled.buffer(); for (int i = 1; i &lt; 10; i++) &#123; buf.writeInt(i * -1); &#125; //(2) 创建一个EmbeddedChannel，并安装一个要测试的 AbsIntegerEncoder EmbeddedChannel channel = new EmbeddedChannel(new AbsIntegerEncoder()); //(3) 写入 ByteBuf，并断言调用 readOutbound()方法将会产生数据 assertTrue(channel.writeOutbound(buf)); //(4) 将该 Channel 标记为已完成状态 assertTrue(channel.finish()); // read bytes //(5) 读取所产生的消息，并断言它们包含了对应的绝对值 for (int i = 1; i &lt; 10; i++) &#123; int x = channel.readOutbound(); assertEquals(i, x); &#125; assertNull(channel.readOutbound()); &#125;&#125; 测试异常处理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758//扩展 ByteToMessageDecoder以将入站字节解码为消息public class FrameChunkDecoder extends ByteToMessageDecoder &#123; private final int maxFrameSize; //指定将要产生的帧的最大允许大小 public FrameChunkDecoder(int maxFrameSize) &#123; this.maxFrameSize = maxFrameSize; &#125; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; int readableBytes = in.readableBytes(); if (readableBytes &gt; maxFrameSize) &#123; //如果该帧超出允许的大小，则丢弃它并抛出一个 TooLongFrameException…… in.clear(); throw new TooLongFrameException(); &#125; //……否则，从 ByteBuf 中读取一个新的帧 ByteBuf buf = in.readBytes(readableBytes); //将该帧添加到解码 读取一个新的帧消息的 List 中 out.add(buf); &#125;&#125;/** * 类说明：只允许最大的帧的字节大小为设定的值,测试异常处理 */public class FrameChunkDecoderTest &#123; @Test public void testFramesDecoded() &#123; //创建一个 ByteBuf，并向它写入 9 字节 ByteBuf buf = Unpooled.buffer(); for (int i = 0; i &lt; 9; i++) &#123; buf.writeByte(i); &#125; ByteBuf input = buf.duplicate(); //创建一个 EmbeddedChannel，并向其安装允许一个帧最大为3字节的 // FrameChunkDecoder EmbeddedChannel channel = new EmbeddedChannel(new FrameChunkDecoder(3)); //向它写入 2 字节，并断言它们将会产生一个新帧 assertTrue(channel.writeInbound(input.readBytes(2))); try &#123; //写入一个 4 字节大小的帧，并捕获预期的TooLongFrameException channel.writeInbound(input.readBytes(4)); &#125; catch (TooLongFrameException e) &#123; e.printStackTrace(); &#125; //写入剩余的2字节，并断言将会产生一个有效帧 assertTrue(channel.writeInbound(input.readBytes(3))); //将该 Channel 标记为已完成状态 assertTrue(channel.finish()); &#125;&#125;","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"01-05-Bootstrap和ServerBootstrap","slug":"netty/01-05-Bootstrap和ServerBootstrap","date":"2021-11-23T12:00:06.000Z","updated":"2022-03-23T09:03:58.151Z","comments":true,"path":"blog/netty/01-05-Bootstrap和ServerBootstrap/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/01-05-Bootstrap%E5%92%8CServerBootstrap/","excerpt":"","text":"引导类的层次结构包括一个抽象的父类和两个具体的引导子类, 这两个分别作为客户端和服务器的引导类。 ServerBootstrap作为服务器引导类，致力于使用一个父 Channel 来接受来自客户端的连接,并创建子 Channel 以用于它们之间的通信 Bootstrap作为客户端引导类，只需要一个单独的、没有父 Channel 的 Channel 来用于所有的网络交互。（这也适用于无连接的传输协议,如 UDP,因为它们并不是每个连接都需要一个单独的 Channel。） BootstrapBootstrap 类被用于客户端或者使用了无连接协议的应用程序中。 ServerBootstrap 类 childHandler()、childAttr()和 childOption()等方法。ServerChannel 的实现负责创建子 Channel,这些子 Channel 代表了已被接受的连接。因此，负责引导 ServerChannel 的 ServerBootstrap 提供了这些方法,以简化将设置应用到已被接受的子 Channel 的 ChannelConfig 的任务。 从 Channel 引导客户端假设你的服务器正在处理一个客户端的请求, 这个请求需要它充当第三方系统的客户端。 当一个应用程序(如一个代理服务器)必须要和组织现有的系统(如 Web 服务或者数据库)集成时,就可能发生这种情况。在这种情况下,将需要从已经被接受的子 Channel 中引导一个客户端 Channel。 可以通过将已被接受的子Channel 的 EventLoop 传递给新建的 Bootstrap 的 group()方法来共享该 EventLoop。因为分配给 EventLoop 的所有 Channel 都使用同一个线程,所以这避免了额外的线程创建,以及前面所提到的相关的上下文切换。如图： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950ServerBootstrap bs2 = new ServerBootstrap();bs2.channel(EpollServerSocketChannel.class);bs2.childOption(EpollChannelOption.EPOLL_MODE, EpollMode.LEVEL_TRIGGERED);bs2.group(group).childHandler(new ChannelInboundHandlerAdapter() &#123; @Override public void channelActive(final ChannelHandlerContext ctx) throws Exception &#123; ctx.channel().config().setAutoRead(false); Bootstrap bs = new Bootstrap(); bs.option(EpollChannelOption.EPOLL_MODE, EpollMode.LEVEL_TRIGGERED); bs.channel(EpollSocketChannel.class); bs.group(ctx.channel().eventLoop()).handler(new ChannelInboundHandlerAdapter() &#123; @Override public void channelActive(ChannelHandlerContext context) throws Exception &#123; final EpollSocketChannel ch = (EpollSocketChannel) ctx.channel(); final EpollSocketChannel ch2 = (EpollSocketChannel) context.channel(); ch.spliceTo(ch2, Integer.MAX_VALUE).addListener((ChannelFutureListener) future -&gt; &#123; if (!future.isSuccess()) &#123; future.channel().close(); &#125; &#125;); ch2.spliceTo(ch, SPLICE_LEN).addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; if (!future.isSuccess()) &#123; future.channel().close(); &#125; else &#123; ch2.spliceTo(ch, SPLICE_LEN).addListener(this); &#125; &#125; &#125;); ctx.channel().config().setAutoRead(true); &#125; @Override public void channelInactive(ChannelHandlerContext context) throws Exception &#123; context.close(); &#125; &#125;); bs.connect(sc.localAddress()).addListener((ChannelFutureListener) future -&gt; &#123; if (!future.isSuccess()) &#123; ctx.close(); &#125; else &#123; future.channel().closeFuture().addListener((ChannelFutureListener) future1 -&gt; ctx.close()); &#125; &#125;); &#125;&#125;);Channel pc = bs2.bind(NetUtil.LOCALHOST, 0).syncUninterruptibly().channel(); 尽可能地重用 EventLoop,以减少线程创建所带来的开销。 在引导过程中添加多个 ChannelHandler可以使用ChannelInitializer这个类提供了一种将多个 ChannelHandler 添加到一个 ChannelPipeline 中的简便方法。一旦 Channel 被注册到了它的 EventLoop 之后,就会调用你的ChannelInitializer#initChannel()版本。在该方法返回之后,ChannelInitializer 的实例将会从 ChannelPipeline 中移除它自己。 使用 Netty 的 ChannelOption 和属性在每个 Channel 创建时都手动配置它可能会变得相当乏味。幸运的是,你不必这样做。相反,你可以使用 option()方法来将 ChannelOption 应用到引导。你所提供的值将会被自动应用到引导所创建的所有 Channel。可用的 ChannelOption 包括了底层连接的详细信息,如keep-alive 或者超时属性以及缓冲区设置。 123#设置 ChannelOption, 其将在 connect()或者 bind()方法被调用时 被设置到已经创建的 Channel 上bootstrap.option(ChannelOption.SO_KEEPALIVE, true)bootstrap.option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 5000) 其中有: ChannelOption.SO_BACKLOG ChannelOption.SO_BACKLOG 对应的是 tcp&#x2F;ip 协议 listen 函数中的 backlog 参数，函数 listen(int socketfd,int backlog)用来初始化服务端可连接队列，服务端处理客户端连接请求是顺序处理的，所以同一时间只能处理一个客户端连接，多 个客户端来的时候，服务端将不能处理的客户端连接请求放在队列中等待处理，backlog 参 数指定了队列的大小 ChannelOption.SO_REUSEADDR ChanneOption.SO_REUSEADDR对应于套接字选项中的SO_REUSEADDR，这个参数表示允许重复使用本地地址和端口，比如，某个服务器进程占用了 TCP 的 80 端口进行监听，此时再次监听该端口就会返回 错误，使用该参数就可以解决问题，该参数允许共用该端口，这个在服务器程序中比较常使 用，比如某个进程非正常退出，该程序占用的端口可能要被占用一段时间才能允许其他进程 使用，而且程序死掉以后，内核一需要一定的时间才能够释放此端口，不设置 SO_REUSEADDR 就无法正常使用该端口。 ChannelOption.SO_KEEPALIVE Channeloption.SO_KEEPALIVE 参数对应于套接字选项中的 SO_KEEPALIVE，该参数用于设 置 TCP 连接，当设置该选项以后，连接会测试链接的状态，这个选项用于可能长时间没有数 据交流的连接。当设置该选项以后，如果在两小时内没有数据的通信时，TCP 会自动发送一 个活动探测数据报文。 ChannelOption.SO_SNDBUF 和 ChannelOption.SO_RCVBUF ChannelOption.SO_SNDBUF 参数对应于套接字选项中的 SO_SNDBUF， ChannelOption.SO_RCVBUF 参数对应于套接字选项中的 SO_RCVBUF 这两个参数用于操作接 收缓冲区和发送缓冲区的大小，接收缓冲区用于保存网络协议站内收到的数据，直到应用程 序读取成功，发送缓冲区用于保存发送数据，直到发送成功。 ChannelOption.SO_LINGER ChannelOption.SO_LINGER 参数对应于套接字选项中的 SO_LINGER,Linux 内核默认的处理 方式是当用户调用 close()方法的时候，函数返回，在可能的情况下，尽量发送数据，不 一定保证会发生剩余的数据，造成了数据的不确定性，使用 SO_LINGER 可以阻塞 close()的调 用时间，直到数据完全发送 ChannelOption.TCP_NODELAY ChannelOption.TCP_NODELAY 参数对应于套接字选项中的 TCP_NODELAY,该参数的使用 与 Nagle 算法有关，Nagle 算法是将小的数据包组装为更大的帧然后进行发送，而不是输入 一次发送一次,因此在数据包不足的时候会等待其他数据的到了，组装成大的数据包进行发 送，虽然该方式有效提高网络的有效负载，但是却造成了延时，而该参数的作用就是禁止使 用 Nagle 算法，使用于小数据即时传输，于 TCP_NODELAY 相对应的是 TCP_CORK，该选项是 需要等到发送的数据量最大的时候，一次性发送数据，适用于文件传输。 关闭最重要的是,你需要关闭 EventLoopGroup,它将处理任何挂起的事件和任务,并且随后释放所有活动的线程。这就是调用 EventLoopGroup.shutdownGracefully()方法的作用。这个方法调用将会返回一个 Future, 这个 Future 将在关闭完成时接收到通知。 需要注意的是, shutdownGracefully()方法也是一个异步的操作, 所以你需要阻塞等待直到它完成, 或者向所返回的 Future 注册一个监听器以在关闭完成时获得通知。 1234EventLoopGroup group = new NioEventLoopGroup();.....Future&lt;?&gt; future = group.shutdownGracefully();future.syncUninterruptibly();","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"01-04-netty线程模型","slug":"netty/01-04-netty线程模型","date":"2021-11-23T12:00:05.000Z","updated":"2022-03-23T09:03:58.137Z","comments":true,"path":"blog/netty/01-04-netty线程模型/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/01-04-netty%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"EventLoop 接口Netty 的 EventLoop 是协同设计的一部分,它采用了两个基本的 API：并发和网络编程。 io.netty.util.concurrent 包构建在 JDK 的 java.util.concurrent 包上,用来提供线程执行器。 io.netty.channel 包中的类,为了与 Channel 的事件进行交互, 扩展了这些接口类。 在这个模型中,一个 EventLoop 将由一个永远都不会改变的 Thread 驱动,同时任务(Runnable 或者 Callable)可以直接提交给 EventLoop ,以立即执行或者调度执行。根据配置和可用核心的不同,可能会创建多个 EventLoop 实例用以优化资源的使用,并且单个EventLoop 可能会被指派用于服务多个 Channel。 使用 EventLoop 调度任务ScheduledExecutorService 的实现具有局限性,例如,事实上作为线程池管理的一部分,将会有额外的线程创建。如果有大量任务被紧凑地调度,那么这将成为一个瓶颈。Netty 通过 Channel 的 EventLoop 实现任务调度解决了这一问题。 1234ChannelHandlerContext ctxctx.channel().eventLoop().schedule(() -&gt; &#123; System.out.println(&quot;60 seconds later&quot;);&#125;, 60, TimeUnit.MINUTES); 经过 60 秒之后, Runnable 实例将由分配给 Channel 的 EventLoop 执行。 如果要调度任务以每隔 60 秒执行一次,请使用 scheduleAtFixedRate()方法 线程管理在netty中，一个 EventLoop 将由一个永远都不会改变的 Thread 驱动，而且，如果(当前)调用线程正是支撑 EventLoop 的线程,那么所提交的代码块将会被(直接) 执行。 否则, EventLoop 将调度该任务以便稍后执行, 并将它放入到内部队列中。 当 EventLoop 下次处理它的事件时,它会执行队列中的那些任务&#x2F;事件。 永远不要将一个长时间运行的任务放入到执行队列中, 因为它将阻塞需要在同一线程上执行的任何其他任务。 如果必须要进行阻塞调用或者执行长时间运行的任务，建议使用一个专门的EventExecutor。ChannelPipeline 有一些接受一个EventExecutorGroup 的add()方法。如果一个事件被传递给一个自定义的EventExecutorGroup，它将被包含在这个 EventExecutorGroup 中的某个 EventExecutor 所处理,从而被从该Channel 本身的 EventLoop 中移除。对于这种用例,Netty 提供了一个叫 DefaultEventExecutorGroup 的默认实现。 EventLoop线程的分配服务于 Channel 的 I&#x2F;O 和事件的 EventLoop 包含在 EventLoopGroup 中 异步传输实现只使用了少量的 EventLoop(以及和它们相关联的 Thread) , 而且在当前的线程模型中,它们可能会被多个 Channel 所共享。这使得可以通过尽可能少量的 Thread 来支撑大量的 Channel,而不是每个 Channel 分配一个 Thread。 EventLoopGroup 负责为每个新创建的 Channel 分配一个 EventLoop。在当前实现中, 使用顺序循环(round-robin)的方式进行分配以获取一个均衡的分布,并且相同的 EventLoop 可能会被分配给多个 Channel。 一旦一个 Channel 被分配给一个 EventLoop,它将在它的整个生命周期中都使用这个EventLoop(以及相关联的 Thread) 。也就意味着，每个事件的处理都是线程安全的。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"01-03-ByteBuf","slug":"netty/01-03-ByteBuf","date":"2021-11-23T12:00:04.000Z","updated":"2022-03-23T09:03:58.132Z","comments":true,"path":"blog/netty/01-03-ByteBuf/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/01-03-ByteBuf/","excerpt":"","text":"Netty 的数据处理 API 通过两个组件暴露——abstract class ByteBuf 和 interface ByteBufHolder。 ByteBuf API 的优点: 它可以被用户自定义的缓冲区类型扩展; 通过内置的复合缓冲区类型实现了透明的零拷贝; 容量可以按需增长(类似于 JDK 的 StringBuilder) ; 在读和写这两种模式之间切换不需要调用 ByteBuffer 的 flip()方法; 读和写使用了不同的索引; 支持方法的链式调用; 支持引用计数; 支持池化。 ByteBuf维护了两个不同的索引:一个用于读取，一个用于写入。当你从ByteBuf读取时，它的readerIndex将会被递增已经被读取的字节数。同样地，当你写入ByteBuf时，它的writerIndex也会被递增。 名称以 read 或者 write 开头的 ByteBuf 方法，将会 推进其对应的索引，而名称以 set 或者 get 开头的操作则不会。 分配ByteBufAllocator 接口Netty 通过 interface ByteBufAllocator 分配我们所描述过的任意类型的 ByteBuf 实例。 名称 描述 buffer() 返回一个基于堆或者直接内存存储的 ByteBuf heapBuffer() 返回一个基于堆内存存储的 ByteBuf directBuffer() 返回一个基于直接内存存储的 ByteBuf compositeBuffer() 返回一个可以通过添加最大到指定数目的基于堆的或者直接内存存储的缓冲区来扩展的 CompositeByteBuf ioBuffer() 返回一个用于套接字的 I&#x2F;O 操作的 ByteBuf，当所运行的环境具有 sun.misc.Unsafe 支持时，返回基于直接内存存储的 ByteBuf， 否则返回基于堆内存存储的 ByteBuf;当指定使用 PreferHeapByteBufAllocator 时，则只会返回基于堆内存存储的 ByteBuf。 可以通过 Channel(每个都可以有一个不同的 ByteBufAllocator 实例)或者绑定到ChannelHandler 的 ChannelHandlerContext 获取一个到 ByteBufAllocator 的引用。 Netty提供了两种ByteBufAllocator的实现:PooledByteBufAllocator和UnpooledByteBufAllocator。 前者池化了ByteBuf的实例以提高性能并最大限度地减少内存碎片。 此实现使用了一种称为jemalloc的已被大量现代操作系统所采用的高效方法来分配内存。后者的实现不池化ByteBuf实例,并且在每次它被调用时都会返回一个新的实例。 Unpooled 缓冲区可能某些情况下,你未能获取一个到ByteBufAllocator 的引用。对于这种情况,Netty 提供了一个简单的称为 Unpooled 的工具类,它提供了静态的辅助方法来创建未池化的 ByteBuf 实例。 名称 描述 directBuffer() directBuffer(int initialCapacity) directBuffer(int initialCapacity, int maxCapacity) 返回一个未池化的基于直接内存存储的ByteBuf buffer()buffer(int initialCapacity)buffer(int initialCapacity, int maxCapacity) 返回一个未池化的基于堆内存存储的ByteBuf wrappedBuffer() 返回一个包装了给定数据的ByteBuf copiedBuffer() 返回一个复制了给定数据的ByteBuf Unpooled 类还使得 ByteBuf 同样可用于那些并不需要 Netty 的其他组件的非网络项目 字节级操作get()和 set()操作，从给定的索引开始，并且保持索引不变；read()和 write()操作，从给定的索引开始，并且会根据已经访问过的字节数对索引 进行调整。 更多的操作： isReadable() 如果至少有一个字节可供读取，则返回 true isWritable() 如果至少有一个字节可被写入，则返回 true readableBytes() 返回可被读取的字节数 writableBytes() 返回可被写入的字节数 capacity() 返回 ByteBuf 可容纳的字节数。在此之后，它会尝试再次扩展直到达到 maxCapacity() maxCapacity() 返回 ByteBuf 可以容纳的最大字节数 hasArray() 如果 ByteBuf 由一个字节数组支撑，则返回 true array() 如果 ByteBuf 由一个字节数组支撑则返回该数组;否则，它将抛出一个 UnsupportedOperationException 异常 随机访问索引如同在普通的 Java 字节数组中一样,ByteBuf 的索引是从零开始的:第一个字节的索引是0,最后一个字节的索引总是 capacity() - 1‘ 需要注意的是,使用那些需要一个索引值参数的方法(的其中)之一来访问数据既不会改变readerIndex 也不会改变writerIndex。 如果有需要, 也可以通过调用readerIndex(index) 或者 writerIndex(index)来手动移动这两者。 顺序访问索引虽然 ByteBuf 同时具有读索引和写索引,但是 JDK 的 ByteBuffer 却只有一个索引,这也就是为什么必须调用 flip()方法来在读模式和写模式之间进行切换的原因。下图 展示了ByteBuf 是如何被它的两个索引划分成 3 个区域的。 可丢弃字节可丢弃字节的分段包含了已经被读过的字节。通过调用 discardReadBytes()方法, 可以丢弃它们并回收空间。 这个分段的初始大小为 0, 存储在 readerIndex 中, 会随着 read 操作的执行而增加(get*操作不会移动 readerIndex) 。 可读字节任何名称以 read 或者 skip 开头的操作都将检索或者跳过位于当前readerIndex 的数据,并且将它增加已读字节数。 如果被调用的方法需要一个 ByteBuf 参数作为写入的目标,并且没有指定目标索引参数, 那么该目标缓冲区的 writerIndex 也将被增加,例如: 1readBytes(ByteBuf dest); 如果尝试在缓冲区的可读字节数已经耗尽时从中读取数据,那么将会引发一个 IndexOutOfBoundsException。 可写字节可写字节分段是指一个拥有未定义内容的、写入就绪的内存区域。新分配的缓冲区的writerIndex 的默认值为 0。任何名称以 write 开头的操作都将从当前的 writerIndex 处开始写数据,并将它增加已经写入的字节数。如果写操作的目标也是 ByteBuf,并且没有指定源索引的值,则源缓冲区的 readerIndex 也同样会被增加相同的大小。这个调用如下所示: 1writeBytes(ByteBuf dest); 如果尝试往目标写入超过目标容量的数据,将会引发一个IndexOutOfBoundException 可以用writeableBytes()来判断是否可写 索引管理可以通过调用markReaderIndex()、markWriterIndex()、resetWriterIndex() 和 resetReaderIndex()来标记和重置 ByteBuf 的 readerIndex 和 writerIndex。这些和InputStream 上的调用类似,只是没有readlimit 参数来指定标记什么时候失效。 也可以通过调用readerIndex(int)或者writerIndex(int)来将索引移动到指定位置。 试图将任何一个索引设置到一个无效的位置都将导致一个IndexOutOfBoundsException。 可以通过调用 clear()方法来将 readerIndex 和 writerIndex 都设置为 0。注意,这并不会清除内存中的内容 调用 clear()比调用 discardReadBytes()轻量得多,因为它将只是重置索引而不会复制任何的内存。 派生缓冲区派生缓冲区为 ByteBuf 提供了以专门的方式来呈现其内容的视图。这类视图是通过以下方法被创建的: duplicate(); slice(); slice(int, int);‘ Unpooled.unmodifiableBuffer(…); order(ByteOrder); readSlice(int)。 每个这些方法都将返回一个新的 ByteBuf 实例,它具有自己的读索引、写索引和标记索引。其内部存储和 JDK 的 ByteBuffer 一样也是共享的。这使得派生缓冲区的创建成本是很低廉的,但是这也意味着,如果你修改了它的内容,也同时修改了其对应的源实例,所以要小心。 ByteBuf 复制 如果需要一个现有缓冲区的真实副本,请使用copy()或者copy(int, int)方法。不同于派生缓冲区,由这个调用所返回的ByteBuf拥有独立的数据副本。 ByteBuf 的使用模式堆缓冲区最常用的 ByteBuf 模式是将数据存储在 JVM 的堆空间中。这种模式被称为支撑数组(backing array) ,它能在没有使用池化的情况下提供快速的分配和释放。这种方式,非常适合于需要对数据进行处理的情况。 直接缓冲区接缓冲区的内容将驻留在常规的会被垃圾回收的堆之外。 ”这也就解释了为何直接缓冲区对于网络数据传输是理想的选择。如果你的数据包含在一个在堆上分配的缓冲区中,那么事实上,在通过套接字发送它之前,JVM将会在内部把你的缓冲区复制到一个直接缓冲区中。 直接缓冲区的主要缺点是,相对于基于堆的缓冲区,它们的分配和释放都较为昂贵。 复合缓冲区CompositeByteBuf为多个 ByteBuf 提供一个聚合视图。它提供了一个将多个缓冲区表示为单个合并缓冲区的虚拟表示。 CompositeByteBuf中的ByteBuf实例可能同时包含直接内存分配和非直接内存分配。如果其中只有一个实例,那么对 CompositeByteBuf 上的 hasArray()方法的调用将返回该组件上的hasArray()方法的值;否则它将返回false。 比如HTTP 协议， 分为消息头和消息体，这两部分可能由应用程序的不同模块产生，各有各的 ByteBuf，将会 在消息被发送的时候组装为一个 ByteBuf，此时可以将这两个 ByteBuf 聚合为一个 CompositeByteBuf，然后使用统一和通用的 ByteBuf API 来操作。 ByteBufHolder 接口我们经常发现,除了实际的数据负载之外,我们还需要存储各种属性值。HTTP 响应便是一个很好的例子,除了表示为字节的内容,还包括状态码、cookie等。 为了处理这种常见的用例,Netty 提供了 ByteBufHolder。ByteBufHolder 也为 Netty 的高级特性提供了支持,如缓冲区池化,其中可以从池中借用ByteBuf,并且在需要时自动释放。 ByteBufUtil 类ByteBufUtil 提供了用于操作 ByteBuf 的静态的辅助方法。因为这个 API 是通用的,并且和池化无关,所以这些方法已然在分配类的外部实现。 这些静态方法中最有价值的可能就是 hexdump()方法,它以十六进制的表示形式打印ByteBuf 的内容。这在各种情况下都很有用,例如,出于调试的目的记录 ByteBuf 的内容。十六进制的表示通常会提供一个比字节值的直接表示形式更加有用的日志条目, 此外, 十六进制的版本还可以很容易地转换回实际的字节表示。 另一个有用的方法是 boolean equals(ByteBuf, ByteBuf),它被用来判断两个 ByteBuf 实例的相等性。 引用计数ByteBuf 和 ByteBufHolder 引入了引用计数技术,它们都实现了 interface ReferenceCounted。 引用计数背后的想法并不是特别的复杂;它主要涉及跟踪到某个特定对象的活动引用的数量。 一个 ReferenceCounted 实现的实例将通常以活动的引用计数为 1 作为开始。 只要引用计数大于 0, 就能保证对象不会被释放。 当活动引用的数量减少到 0 时,该实例就会被释放 试图访问一个已经被释放的引用计数的对象,将会导致一个 IllegalReferenceCountException。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"01-02-Channel、ChannelHandler、ChannelPipeline和ChannelHandlerContext","slug":"netty/01-02-Channel、ChannelHandler、ChannelPipeline和ChannelHandlerContext","date":"2021-11-23T12:00:03.000Z","updated":"2022-03-23T09:03:58.124Z","comments":true,"path":"blog/netty/01-02-Channel、ChannelHandler、ChannelPipeline和ChannelHandlerContext/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/01-02-Channel%E3%80%81ChannelHandler%E3%80%81ChannelPipeline%E5%92%8CChannelHandlerContext/","excerpt":"","text":"","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"01-01-netty基础","slug":"netty/01-01-netty基础","date":"2021-11-23T12:00:02.000Z","updated":"2022-03-23T09:03:58.123Z","comments":true,"path":"blog/netty/01-01-netty基础/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/01-01-netty%E5%9F%BA%E7%A1%80/","excerpt":"","text":"异步和事件驱动一个既是异步的又是事件驱动的系统会表现出一种特殊的、 对我们来说极具价值的行为:&#x3D;&#x3D;它可以以任意的顺序响应在任意的时间点产生的事件。&#x3D;&#x3D; 这种能力对于实现最高级别的可伸缩性至关重要,定义为: “&#x3D;&#x3D;一种系统、网络或者进程在需要处理的工作不断增长时,可以通过某种可行的方式或者扩大它的处理能力来适应这种增长的能力。&#x3D;&#x3D; ” Netty 的核心组件核心组件： Channel 回调 Fututer 事件和ChannelHandler 这些构建块代表了不同类型的构造:资源、逻辑以及通知。你的应用程序将使用它们来访问网络以及流经网络的数据。 ChannelChannel 是 Java NIO 的一个基本构造。它是数据流通的通道，它是双通的。 回调回到就是一个方法，一个指向被提供给另一个方法的方法引用。这样使得前者可以在适当的时机调用后者。是在操作完成后通知相关方最常见的方式之一 Netty在内部使用了回调来处理事件；当一个回调被触发时，相同的事件可以被一个interface——ChannelHandler的实现来处理。 Fututer（ChannelFuture）Future 提供了另一种在操作完成时通知应用程序的方式。Fututer可以看作是一个异步操作结果的占位符；它可以在未来的某个时刻完成，并提供对其结果的访问。 JDK提供的Fututer，但它的功能很简单，只能手动的检查操作是否完成，或者一直堵塞直到异步操作完成。而Netty提供了自己的实现ChannelFuture。它与JDK的Fututer最大的区别就是，ChannelFuture提供监听器，也就是可以注册一个或者多个ChannelFutureListener实例，该监听器会在异步操作完成时被调用。由ChannelFutureListener提供的通知机制消除了手动检查对应的操作是否完成的必要。 每个 Netty 的出站 I&#x2F;O 操作都将返回一个 ChannelFuture;也就是说,它们都不会阻塞。正如我们前面所提到过的一样,Netty 完全是异步和事件驱动的。 1ChannelFuture future = channel.connect(new InetSocketAddress(&quot;192.168.0.1&quot;, 25)) 这里, connect()方法将会直接返回,而不会阻塞,该调用将会在后台完成。这究竟什么时候会发生则取决于若干的因素, 但这个关注点已经从代码中抽象出来了。 因为线程不用阻塞以等待对应的操作完成,所以它可以同时做其他的工作,从而更加有效地利用资源。 123456789101112ChannelFuture future = channel.connect(new InetSocketAddress(&quot;192.168.0.1&quot;, 25))future.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; if (!future.isSuccess()) &#123; Throwable cause = future.cause(); cause.printStackTrace(); &#125; else &#123; ..... &#125; &#125;&#125;); 上述代码注册一个新的 ChannelFutureListener 到对 connect()方法的调用所返回的 ChannelFuture 上，这样，就能在连接完成时收到通知，并且能知道连接是否成功还是失败。 通过上述介绍，完全可以ChannelFuture的ChannelFutureListener看过是回调的一种方式，回调和Future 是相互补充的机制; 它们相互结合, 构成了 Netty 本身的关键构件块之一。 事件和 ChannelHandlerNetty 使用不同的事件来通知我们状态的改变或者是操作的状态。这使得我们能够基于已经发生的事件来触发适当的动作。这些操作可以是 逻辑处理 记录日志 数据转换 …… Netty 是一个网络编程框架, 所以事件是按照它们与入站或出站数据流的相关性进行分类的。可能由入站数据或者相关的状态更改而触发的事件包括: 连接已被激活或者连接失活 数据读取 用户事件 错误事件 出站事件是未来将会触发的某个动作的操作结果,这些动作包括: 打开或者关闭到远程节点的连接; 将数据写到或者冲刷到套接字。 每个事件都可以被分发给 ChannelHandler 类中的某个用户实现的方法。这是一个很好的将事件驱动范式直接转换为应用程序构件块的例子。下图展示了一个事件是如何被一个这样的ChannelHandler 链处理的。 Netty 的 ChannelHandler 为处理器提供了基本的抽象，ChannelHandler 的实例都类似于一种为了响应特定事件而被执行的回调。 总结Netty的异步编程模型是建立在Future和回调的概念之上的, 而将事件派发到ChannelHandler 的方法则发生在更深的层次上。 结合在一起, 这些元素就提供了一个处理环境, 使你的应用程序逻辑可以独立于任何网络操作相关的顾虑而独立地演变。 Netty 通过触发事件将 Selector 从应用程序中抽象出来,消除了所有本来将需要手动编写的派发代码。在内部,将会为每个 Channel分配一个 EventLoop,用以处理所有事件。 EventLoop 本身只由一个线程驱动,其处理了一个 Channel 的所有 I&#x2F;O 事件,并且在该EventLoop 的整个生命周期内都不会改变。这个简单而强大的设计消除了你可能有的在ChannelHandler实现中需要进行同步的任何顾虑,因此,你可以专注于提供正确的逻辑,用来在有感兴趣的数据要处理的时候执行。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"01-00-第一个netty程序","slug":"netty/01-00-第一个netty程序","date":"2021-11-23T12:00:01.000Z","updated":"2022-03-23T09:03:58.122Z","comments":true,"path":"blog/netty/01-00-第一个netty程序/","link":"","permalink":"http://sv.pointcut.cc/blog/netty/01-00-%E7%AC%AC%E4%B8%80%E4%B8%AAnetty%E7%A8%8B%E5%BA%8F/","excerpt":"","text":"client: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class EchoClientHandle extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; /*客户端读到数据以后，就会执行*/ @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception &#123; System.out.println(&quot;client acccept:&quot;+msg.toString(CharsetUtil.UTF_8)); &#125; /*连接建立以后*/ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ctx.writeAndFlush(Unpooled.copiedBuffer( &quot;Hello Netty&quot;,CharsetUtil.UTF_8)); //ctx.fireChannelActive(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125; @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; super.userEventTriggered(ctx, evt); &#125;&#125;public class EchoClient &#123; private final int port; private final String host; public EchoClient(int port, String host) &#123; this.port = port; this.host = host; &#125; public void start() throws InterruptedException &#123; /*线程组*/ EventLoopGroup group = new NioEventLoopGroup(); try&#123; /*客户端启动必备*/ Bootstrap b = new Bootstrap(); b.group(group)/*把线程组传入*/ /*指定使用NIO进行网络传输*/ .channel(NioSocketChannel.class) .remoteAddress(new InetSocketAddress(host,port)) .handler(new EchoClientHandle()); /*连接到远程节点，阻塞直到连接完成*/ ChannelFuture f = b.connect().sync(); /*阻塞程序，直到Channel发生了关闭*/ f.channel().closeFuture().sync(); &#125;finally &#123; group.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; new EchoClient(9999,&quot;127.0.0.1&quot;).start(); &#125;&#125; server: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283@ChannelHandler.Sharable/*不加这个注解那么在增加到childHandler时就必须new出来*/public class EchoServerHandler extends ChannelInboundHandlerAdapter &#123; /** * 客户端读到数据以后，就会执行 * 只有把所有数据接收完，能转换成Object的时候才会调用 * @param ctx * @param msg * @throws Exception */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf in = (ByteBuf)msg; System.out.println(&quot;Server accept&quot;+in.toString(CharsetUtil.UTF_8)); ctx.write(in); &#125; /** * 服务端读取完成网络数据后的处理 * 从网络缓冲区读完数据就会执行一次 * @param ctx * @throws Exception */ @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.writeAndFlush(Unpooled.EMPTY_BUFFER) .addListener(ChannelFutureListener.CLOSE); &#125; /*** 发生异常后的处理*/ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125;public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public static void main(String[] args) throws InterruptedException &#123; int port = 9999; EchoServer echoServer = new EchoServer(port); System.out.println(&quot;服务器即将启动&quot;); echoServer.start(); System.out.println(&quot;服务器关闭&quot;); &#125; public void start() throws InterruptedException &#123; final EchoServerHandler serverHandler = new EchoServerHandler(); /*线程组*/ EventLoopGroup group = new NioEventLoopGroup(); try &#123; /*服务端启动必须*/ ServerBootstrap b = new ServerBootstrap(); b.group(group)/*将线程组传入*/ .channel(NioServerSocketChannel.class)/*指定使用NIO进行网络传输*/ .localAddress(new InetSocketAddress(port))/*指定服务器监听端口*/ /*服务端每接收到一个连接请求，就会新启一个socket通信，也就是channel， 所以下面这段代码的作用就是为这个子channel增加handle*/ .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; protected void initChannel(SocketChannel ch) throws Exception &#123; /*添加到该子channel的pipeline的尾部*/ ch.pipeline().addLast(serverHandler); &#125; &#125;); ChannelFuture f = b.bind().sync();/*异步绑定到服务器，sync()会阻塞直到完成*/ f.channel().closeFuture().sync();/*阻塞直到服务器的channel关闭*/ &#125; finally &#123; group.shutdownGracefully().sync();/*优雅关闭线程组*/ &#125; &#125;&#125;","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"}]},{"title":"网络编程里通用常识","slug":"java_nio/网络编程里通用常识","date":"2021-11-22T12:00:07.000Z","updated":"2022-03-23T09:03:58.121Z","comments":true,"path":"blog/java_nio/网络编程里通用常识/","link":"","permalink":"http://sv.pointcut.cc/blog/java_nio/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E9%87%8C%E9%80%9A%E7%94%A8%E5%B8%B8%E8%AF%86/","excerpt":"","text":"在通信编程里提供服务的叫服务端，连接服务端使用服务的叫客户端。在开发过程中， 如果类的名字有 Server 或者 ServerSocket 的，表示这个类是给服务端容纳网络服务用的，如 果类的名字只有 Socket 的，那么表示这是负责具体的网络读写的。那么对于服务端来说 ServerSocket 就只是个场所，具体和客户端沟通的还是一个一个的 socket，所以在通信编程 里，ServerSocket 并不负责具体的网络读写，ServerSocket 就只是负责接收客户端连接后， 新启一个 socket 来和客户端进行沟通。这一点对所有模式的通信编程都是适用的。 在通信编程里，我们关注的其实也就是三个事情： 连接(客户端连接服务器，服务器等 待和接收连接)、 读网络数据、 写网络数据， 所有模式的通信编程都是围绕着这三件事情进 行的。服务端提供 IP 和监听端口，客户端通过连接操作想服务端监听的地址发起连接请求， 通过三次握手连接，如果连接成功建立，双方就可以通过套接字进行通信。 InetAddressInetAddress是Java 对 IP 地址（ IPv4 和 IPv6）的抽象。 InetAddress 类没有公共构造函数。实际上，InetAddress 有一些静态工厂方法，可以连 接到 DNS 服务器来解析主机名。最常用的是 InetAddress.getByName()。 123456InetAddress.getByName(&quot;www.baidu.com&quot;)#输出www.baidu.com/14.215.177.39Arrays.toString(InetAddress.getAllByName(&quot;www.baidu.com&quot;))#输出[www.baidu.com/14.215.177.39, www.baidu.com/14.215.177.38] NetworklnterfaceNe tworkInterface 对象表示物理硬件和虚拟地址网络接口，所以不能任意构造。该类有一些静态工厂方法可以返回与某个网络接口关联的 NetworkInterface 对象。可以通过 IP 地址、名字或枚举来请求一个 NetworkInterface。 1234NetworkInterface networkInterface = NetworkInterface.getByInetAddress(InetAddress.getByName(&quot;127.0.0.1&quot;));##输出name:lo0 (lo0) 12345Enumeration&lt;NetworkInterface&gt; networkInterfaces = NetworkInterface.getNetworkInterfaces();for (Iterator&lt;NetworkInterface&gt; it = networkInterfaces.asIterator(); it.hasNext(); ) &#123; NetworkInterface networkInterface1 = it.next(); System.out.println(networkInterface1);&#125;","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"javaNIO","slug":"javaNIO","permalink":"http://sv.pointcut.cc/tags/javaNIO/"}]},{"title":"javaNIO","slug":"java_nio/javaNIO","date":"2021-11-22T12:00:06.000Z","updated":"2022-03-23T09:03:58.120Z","comments":true,"path":"blog/java_nio/javaNIO/","link":"","permalink":"http://sv.pointcut.cc/blog/java_nio/javaNIO/","excerpt":"","text":"一、概述NIO主要有三大核心部分：Channel(通道)，Buffer(缓冲区), Selector。传统IO基于字节流和字符流进行操作，而NIO基于Channel和Buffer(缓冲区)进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。Selector(选择区)用于监听多个通道的事件（比如：连接打开，数据到达）。因此，单个线程可以监听多个数据通道。 NIO和传统IO（一下简称IO）之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。 Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。但是，还需要检查是否该缓冲区中包含所有您需要处理的数据。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的数据。 IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取。而不是保持线程阻塞，所以直至数据变得可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）。 Channel首先说一下Channel，国内大多翻译成“通道”。Channel和IO中的Stream(流)是差不多一个等级的。只不过Stream是单向的，譬如：InputStream, OutputStream.而Channel是双向的，既可以用来进行读操作，又可以用来进行写操作。 NIO中的Channel的主要实现有： FileChannel DatagramChannel SocketChannel ServerSocketChannel 这里看名字就可以猜出个所以然来：分别可以对应文件IO、UDP和TCP（Server和Client）。后面演示的案例基本上就是围绕这4个类型的Channel进行陈述的。 BufferNIO中的关键Buffer实现有：ByteBuffer, CharBuffer, DoubleBuffer, FloatBuffer, IntBuffer, LongBuffer, ShortBuffer，分别对应基本数据类型: byte, char, double, float, int, long, short。当然NIO中还有MappedByteBuffer, HeapByteBuffer, DirectByteBuffer等这里先不进行陈述。 SelectorSelector运行单线程处理多个Channel，如果你的应用打开了多个通道，但每个连接的流量都很低，使用Selector就会很方便。例如在一个聊天服务器中。要使用Selector, 得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新的连接进来、数据接收等。 二、FileChannel传统IO vs NIO首先，案例1是采用FileInputStream读取文件内容的： 12345678910111213141516171819202122232425public static void method2()&#123; InputStream in = null; try&#123; in = new BufferedInputStream(new FileInputStream(&quot;src/nomal_io.txt&quot;)); byte [] buf = new byte[1024]; int bytesRead = in.read(buf); while(bytesRead != -1) &#123; for(int i=0;i&lt;bytesRead;i++) System.out.print((char)buf[i]); bytesRead = in.read(buf); &#125; &#125;catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; try&#123; if(in != null)&#123; in.close(); &#125; &#125;catch (IOException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 输出结果：（略） 案例是对应的NIO（这里通过RandomAccessFile进行操作，当然也可以通过FileInputStream.getChannel()进行操作）： 123456789101112131415161718192021222324252627282930 public static void method1()&#123; RandomAccessFile aFile = null; try&#123; aFile = new RandomAccessFile(&quot;src/nio.txt&quot;,&quot;rw&quot;); FileChannel fileChannel = aFile.getChannel(); ByteBuffer buf = ByteBuffer.allocate(1024); int bytesRead = fileChannel.read(buf); System.out.println(bytesRead); while(bytesRead != -1) &#123; buf.flip(); while(buf.hasRemaining()) &#123; System.out.print((char)buf.get()); &#125; buf.compact(); bytesRead = fileChannel.read(buf); &#125; &#125;catch (IOException e)&#123; e.printStackTrace(); &#125;finally&#123; try&#123; if(aFile != null)&#123; aFile.close(); &#125; &#125;catch (IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125; 输出结果：（略）通过仔细对比案例1和案例2，应该能看出个大概，最起码能发现NIO的实现方式比叫复杂。有了一个大概的印象可以进入下一步了。 Buffer的使用从案例2中可以总结出使用Buffer一般遵循下面几个步骤： 分配空间（ByteBuffer buf &#x3D; ByteBuffer.allocate(1024); 还有一种allocateDirector后面再陈述） 写入数据到Buffer(int bytesRead &#x3D; fileChannel.read(buf);) 调用filp()方法（ buf.flip();） 从Buffer中读取数据（System.out.print((char)buf.get());） 调用clear()方法或者compact()方法 Buffer顾名思义：缓冲区，实际上是一个容器，一个连续数组。Channel提供从文件、网络读取数据的渠道，但是读写的数据都必须经过Buffer。如下图： 向Buffer中写数据： 从Channel写到Buffer (fileChannel.read(buf)) 通过Buffer的put()方法 （buf.put(…)） 从Buffer中读取数据： 从Buffer读取到Channel (channel.write(buf)) 使用get()方法从Buffer中读取数据 （buf.get()） 可以把Buffer简单地理解为一组基本数据类型的元素列表，它通过几个变量来保存这个数据的当前位置状态：capacity, position, limit, mark： 索引说明capacity缓冲区数组的总长度position下一个要操作的数据元素的位置limit缓冲区数组中不可操作的下一个元素的位置：limit&lt;=capacitymark用于记录当前position的前一个位置或者默认是-1 无图无真相，举例：我们通过ByteBuffer.allocate(11)方法创建了一个11个byte的数组的缓冲区，初始状态如上图，position的位置为0，capacity和limit默认都是数组长度。当我们写入5个字节时，变化如下图： 这时我们需要将缓冲区中的5个字节数据写入Channel的通信信道，所以我们调用ByteBuffer.flip()方法，变化如下图所示(position设回0，并将limit设成之前的position的值)： 这时底层操作系统就可以从缓冲区中正确读取这个5个字节数据并发送出去了。在下一次写数据之前我们再调用clear()方法，缓冲区的索引位置又回到了初始位置。 调用clear()方法：position将被设回0，limit设置成capacity，换句话说，Buffer被清空了，其实Buffer中的数据并未被清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先写些数据，那么使用compact()方法。compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。 通过调用Buffer.mark()方法，可以标记Buffer中的一个特定的position，之后可以通过调用Buffer.reset()方法恢复到这个position。Buffer.rewind()方法将position设回0，所以你可以重读Buffer中的所有数据。limit保持不变，仍然表示能从Buffer中读取多少个元素。 三、SocketChannel说完了FileChannel和Buffer, 大家应该对Buffer的用法比较了解了，这里使用SocketChannel来继续探讨NIO。NIO的强大功能部分来自于Channel的非阻塞特性，套接字的某些操作可能会无限期地阻塞。例如，对accept()方法的调用可能会因为等待一个客户端连接而阻塞；对read()方法的调用可能会因为没有数据可读而阻塞，直到连接的另一端传来新的数据。总的来说，创建&#x2F;接收连接或读写数据等I&#x2F;O调用，都可能无限期地阻塞等待，直到底层的网络实现发生了什么。慢速的，有损耗的网络，或仅仅是简单的网络故障都可能导致任意时间的延迟。然而不幸的是，在调用一个方法之前无法知道其是否阻塞。NIO的channel抽象的一个重要特征就是可以通过配置它的阻塞行为，以实现非阻塞式的信道。 1channel.configureBlocking(false) 在非阻塞式信道上调用一个方法总是会立即返回。这种调用的返回值指示了所请求的操作完成的程度。例如，在一个非阻塞式ServerSocketChannel上调用accept()方法，如果有连接请求来了，则返回客户端SocketChannel，否则返回null。 这里先举一个TCP应用案例，客户端采用NIO实现，而服务端依旧使用BIO实现。客户端代码（案例3）： 123456789101112131415161718192021222324252627282930313233343536373839 public static void client()&#123; ByteBuffer buffer = ByteBuffer.allocate(1024); SocketChannel socketChannel = null; try &#123; socketChannel = SocketChannel.open(); socketChannel.configureBlocking(false); socketChannel.connect(new InetSocketAddress(&quot;10.10.195.115&quot;,8080)); if(socketChannel.finishConnect()) &#123; int i=0; while(true) &#123; TimeUnit.SECONDS.sleep(1); String info = &quot;I&#x27;m &quot;+i+++&quot;-th information from client&quot;; buffer.clear(); buffer.put(info.getBytes()); buffer.flip(); while(buffer.hasRemaining())&#123; System.out.println(buffer); socketChannel.write(buffer); &#125; &#125; &#125; &#125; catch (IOException | InterruptedException e) &#123; e.printStackTrace(); &#125; finally&#123; try&#123; if(socketChannel!=null)&#123; socketChannel.close(); &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125; 服务端代码（案例4）： 12345678910111213141516171819202122232425262728293031323334353637 public static void server()&#123; ServerSocket serverSocket = null; InputStream in = null; try &#123; serverSocket = new ServerSocket(8080); int recvMsgSize = 0; byte[] recvBuf = new byte[1024]; while(true)&#123; Socket clntSocket = serverSocket.accept(); SocketAddress clientAddress = clntSocket.getRemoteSocketAddress(); System.out.println(&quot;Handling client at &quot;+clientAddress); in = clntSocket.getInputStream(); while((recvMsgSize=in.read(recvBuf))!=-1)&#123; byte[] temp = new byte[recvMsgSize]; System.arraycopy(recvBuf, 0, temp, 0, recvMsgSize); System.out.println(new String(temp)); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally&#123; try&#123; if(serverSocket!=null)&#123; serverSocket.close(); &#125; if(in!=null)&#123; in.close(); &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125; 输出结果：（略） 根据案例分析，总结一下SocketChannel的用法。打开SocketChannel： 12socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress(&quot;10.10.195.115&quot;,8080)); 关闭： 1socketChannel.close(); 读取数据： 12345678String info = &quot;I&#x27;m &quot;+i+++&quot;-th information from client&quot;; buffer.clear(); buffer.put(info.getBytes()); buffer.flip(); while(buffer.hasRemaining())&#123; System.out.println(buffer); socketChannel.write(buffer); &#125; 注意SocketChannel.write()方法的调用是在一个while循环中的。write()方法无法保证能写多少字节到SocketChannel。所以，我们重复调用write()直到Buffer没有要写的字节为止。 非阻塞模式下,read()方法在尚未读取到任何数据时可能就返回了。所以需要关注它的int返回值，它会告诉你读取了多少字节。 四、TCP服务端的NIO写法Selector类可以用于避免使用阻塞式客户端中很浪费资源的“忙等”方法。例如，考虑一个IM服务器。像QQ或者旺旺这样的，可能有几万甚至几千万个客户端同时连接到了服务器，但在任何时刻都只是非常少量的消息。 需要读取和分发。这就需要一种方法阻塞等待，直到至少有一个信道可以进行I&#x2F;O操作，并指出是哪个信道。NIO的选择器就实现了这样的功能。一个Selector实例可以同时检查一组信道的I&#x2F;O状态。用专业术语来说，选择器就是一个多路开关选择器，因为一个选择器能够管理多个信道上的I&#x2F;O操作。然而如果用传统的方式来处理这么多客户端，使用的方法是循环地一个一个地去检查所有的客户端是否有I&#x2F;O操作，如果当前客户端有I&#x2F;O操作，则可能把当前客户端扔给一个线程池去处理，如果没有I&#x2F;O操作则进行下一个轮询，当所有的客户端都轮询过了又接着从头开始轮询；这种方法是非常笨而且也非常浪费资源，因为大部分客户端是没有I&#x2F;O操作，我们也要去检查；而Selector就不一样了，它在内部可以同时管理多个I&#x2F;O，当一个信道有I&#x2F;O操作的时候，他会通知Selector，Selector就是记住这个信道有I&#x2F;O操作，并且知道是何种I&#x2F;O操作，是读呢？是写呢？还是接受新的连接；所以如果使用Selector，它返回的结果只有两种结果，一种是0，即在你调用的时刻没有任何客户端需要I&#x2F;O操作，另一种结果是一组需要I&#x2F;O操作的客户端，这时你就根本不需要再检查了，因为它返回给你的肯定是你想要的。这样一种通知的方式比那种主动轮询的方式要高效得多！ 要使用选择器（Selector），需要创建一个Selector实例（使用静态工厂方法open()）并将其注册（register）到想要监控的信道上（注意，这要通过channel的方法实现，而不是使用selector的方法）。最后，调用选择器的select()方法。该方法会阻塞等待，直到有一个或更多的信道准备好了I&#x2F;O操作或等待超时。select()方法将返回可进行I&#x2F;O操作的信道数量。现在，在一个单独的线程中，通过调用select()方法就能检查多个信道是否准备好进行I&#x2F;O操作。如果经过一段时间后仍然没有信道准备好，select()方法就会返回0，并允许程序继续执行其他任务。 下面将上面的TCP服务端代码改写成NIO的方式（案例5）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class ServerConnect&#123; private static final int BUF_SIZE=1024; private static final int PORT = 8080; private static final int TIMEOUT = 3000; public static void main(String[] args) &#123; selector(); &#125; public static void handleAccept(SelectionKey key) throws IOException&#123; ServerSocketChannel ssChannel = (ServerSocketChannel)key.channel(); SocketChannel sc = ssChannel.accept(); sc.configureBlocking(false); sc.register(key.selector(), SelectionKey.OP_READ,ByteBuffer.allocateDirect(BUF_SIZE)); &#125; public static void handleRead(SelectionKey key) throws IOException&#123; SocketChannel sc = (SocketChannel)key.channel(); ByteBuffer buf = (ByteBuffer)key.attachment(); long bytesRead = sc.read(buf); while(bytesRead&gt;0)&#123; buf.flip(); while(buf.hasRemaining())&#123; System.out.print((char)buf.get()); &#125; System.out.println(); buf.clear(); bytesRead = sc.read(buf); &#125; if(bytesRead == -1)&#123; sc.close(); &#125; &#125; public static void handleWrite(SelectionKey key) throws IOException&#123; ByteBuffer buf = (ByteBuffer)key.attachment(); buf.flip(); SocketChannel sc = (SocketChannel) key.channel(); while(buf.hasRemaining())&#123; sc.write(buf); &#125; buf.compact(); &#125; public static void selector() &#123; Selector selector = null; ServerSocketChannel ssc = null; try&#123; selector = Selector.open(); ssc= ServerSocketChannel.open(); ssc.configureBlocking(false); ssc.socket().bind(new InetSocketAddress(PORT)); ssc.register(selector, SelectionKey.OP_ACCEPT); while(true)&#123; if(selector.select(TIMEOUT) == 0)&#123; System.out.println(&quot;==&quot;); continue; &#125; Iterator&lt;SelectionKey&gt; iter = selector.selectedKeys().iterator(); while(iter.hasNext())&#123; SelectionKey key = iter.next(); if(key.isAcceptable())&#123; handleAccept(key); &#125; if(key.isReadable())&#123; handleRead(key); &#125; if(key.isWritable() &amp;&amp; key.isValid())&#123; handleWrite(key); &#125; if(key.isConnectable())&#123; System.out.println(&quot;isConnectable = true&quot;); &#125; iter.remove(); &#125; &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125;finally&#123; try&#123; if(selector!=null)&#123; selector.close(); &#125; if(ssc!=null)&#123; ssc.close(); &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 下面来慢慢讲解这段代码。 ServerSocketChannel打开ServerSocketChannel： 1ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); 关闭ServerSocketChannel： 1serverSocketChannel.close(); 监听新进来的连接： 123while(true)&#123; SocketChannel socketChannel = serverSocketChannel.accept();&#125; ServerSocketChannel可以设置成非阻塞模式。在非阻塞模式下，accept() 方法会立刻返回，如果还没有新进来的连接,返回的将是null。 因此，需要检查返回的SocketChannel是否是null.如： 1234567891011 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.socket().bind(new InetSocketAddress(9999)); serverSocketChannel.configureBlocking(false); while (true) &#123; SocketChannel socketChannel = serverSocketChannel.accept(); if (socketChannel != null) &#123; // do something with socketChannel... &#125; &#125; SelectorSelector的创建：Selector selector &#x3D; Selector.open(); 为了将Channel和Selector配合使用，必须将Channel注册到Selector上，通过SelectableChannel.register()方法来实现，沿用案例5中的部分代码： 1234ssc= ServerSocketChannel.open(); ssc.socket().bind(new InetSocketAddress(PORT)); ssc.configureBlocking(false); ssc.register(selector, SelectionKey.OP_ACCEPT); 与Selector一起使用时，Channel必须处于非阻塞模式下。这意味着不能将FileChannel与Selector一起使用，因为FileChannel不能切换到非阻塞模式。而套接字通道都可以。 注意register()方法的第二个参数。这是一个“interest集合”，意思是在通过Selector监听Channel时对什么事件感兴趣。可以监听四种不同类型的事件： 11. Connect. Accept. Read. Write 通道触发了一个事件意思是该事件已经就绪。所以，某个channel成功连接到另一个服务器称为“连接就绪”。一个server socket channel准备好接收新进入的连接称为“接收就绪”。一个有数据可读的通道可以说是“读就绪”。等待写数据的通道可以说是“写就绪”。 这四种事件用SelectionKey的四个常量来表示： 11. SelectionKey.OP_CONNECT2. SelectionKey.OP_ACCEPT3. SelectionKey.OP_READ4. SelectionKey.OP_WRITE SelectionKey当向Selector注册Channel时，register()方法会返回一个SelectionKey对象。这个对象包含了一些你感兴趣的属性： interest集合 ready集合 Channel Selector 附加的对象（可选） interest集合：就像向Selector注册通道一节中所描述的，interest集合是你所选择的感兴趣的事件集合。可以通过SelectionKey读写interest集合。 ready 集合是通道已经准备就绪的操作的集合。在一次选择(Selection)之后，你会首先访问这个ready set。Selection将在下一小节进行解释。可以这样访问ready集合： 1int readySet = selectionKey.readyOps(); 可以用像检测interest集合那样的方法，来检测channel中什么事件或操作已经就绪。但是，也可以使用以下四个方法，它们都会返回一个布尔类型： 1selectionKey.isAcceptable();selectionKey.isConnectable();selectionKey.isReadable();selectionKey.isWritable(); 从SelectionKey访问Channel和Selector很简单。如下： 1Channel channel = selectionKey.channel();Selector selector = selectionKey.selector(); 可以将一个对象或者更多信息附着到SelectionKey上，这样就能方便的识别某个给定的通道。例如，可以附加 与通道一起使用的Buffer，或是包含聚集数据的某个对象。使用方法如下： 1selectionKey.attach(theObject);Object attachedObj = selectionKey.attachment(); 还可以在用register()方法向Selector注册Channel的时候附加对象。如： 1SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); 通过Selector选择通道一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道。换句话说，如果你对“读就绪”的通道感兴趣，select()方法会返回读事件已经就绪的那些通道。 下面是select()方法： int select() int select(long timeout) int selectNow() select()阻塞到至少有一个通道在你注册的事件上就绪了。select(long timeout)和select()一样，除了最长会阻塞timeout毫秒(参数)。selectNow()不会阻塞，不管什么通道就绪都立刻返回（译者注：此方法执行非阻塞的选择操作。如果自从前一次选择操作后，没有通道变成可选择的，则此方法直接返回零。）。 select()方法返回的int值表示有多少通道已经就绪。亦即，自上次调用select()方法后有多少通道变成就绪状态。如果调用select()方法，因为有一个通道变成就绪状态，返回了1，若再次调用select()方法，如果另一个通道就绪了，它会再次返回1。如果对第一个就绪的channel没有做任何操作，现在就有两个就绪的通道，但在每次select()方法调用之间，只有一个通道就绪了。 一旦调用了select()方法，并且返回值表明有一个或更多个通道就绪了，然后可以通过调用selector的selectedKeys()方法，访问“已选择键集（selected key set）”中的就绪通道。如下所示： 1Set selectedKeys = selector.selectedKeys(); 当向Selector注册Channel时，Channel.register()方法会返回一个SelectionKey 对象。这个对象代表了注册到该Selector的通道。 注意每次迭代末尾的keyIterator.remove()调用。Selector不会自己从已选择键集中移除SelectionKey实例。必须在处理完通道时自己移除。下次该通道变成就绪时，Selector会再次将其放入已选择键集中。 SelectionKey.channel()方法返回的通道需要转型成你要处理的类型，如ServerSocketChannel或SocketChannel等。 一个完整的使用Selector和ServerSocketChannel的案例可以参考案例5的selector()方法。 五、内存映射文件JAVA处理大文件，一般用BufferedReader,BufferedInputStream这类带缓冲的IO类，不过如果文件超大的话，更快的方式是采用MappedByteBuffer。 MappedByteBuffer是NIO引入的文件内存映射方案，读写性能极高。NIO最主要的就是实现了对异步操作的支持。其中一种通过把一个套接字通道(SocketChannel)注册到一个选择器(Selector)中,不时调用后者的选择(select)方法就能返回满足的选择键(SelectionKey),键中包含了SOCKET事件信息。这就是select模型。 SocketChannel的读写是通过一个类叫ByteBuffer来操作的.这个类本身的设计是不错的,比直接操作byte[]方便多了. ByteBuffer有两种模式:直接&#x2F;间接.间接模式最典型(也只有这么一种)的就是HeapByteBuffer,即操作堆内存 (byte[]).但是内存毕竟有限,如果我要发送一个1G的文件怎么办?不可能真的去分配1G的内存.这时就必须使用”直接”模式,即 MappedByteBuffer,文件映射. 先中断一下,谈谈操作系统的内存管理.一般操作系统的内存分两部分:物理内存;虚拟内存.虚拟内存一般使用的是页面映像文件,即硬盘中的某个(某些)特殊的文件.操作系统负责页面文件内容的读写,这个过程叫”页面中断&#x2F;切换”. MappedByteBuffer也是类似的,你可以把整个文件(不管文件有多大)看成是一个ByteBuffer.MappedByteBuffer 只是一种特殊的ByteBuffer，即是ByteBuffer的子类。 MappedByteBuffer 将文件直接映射到内存（这里的内存指的是虚拟内存，并不是物理内存）。通常，可以映射整个文件，如果文件比较大的话可以分段进行映射，只要指定文件的那个部分就可以。 概念FileChannel提供了map方法来把文件影射为内存映像文件： MappedByteBuffer map(int mode,long position,long size); 可以把文件的从position开始的size大小的区域映射为内存映像文件，mode指出了 可访问该内存映像文件的方式： READ_ONLY,（只读）： 试图修改得到的缓冲区将导致抛出 ReadOnlyBufferException.(MapMode.READ_ONLY) READ_WRITE（读&#x2F;写）： 对得到的缓冲区的更改最终将传播到文件；该更改对映射到同一文件的其他程序不一定是可见的。 (MapMode.READ_WRITE) PRIVATE（专用）： 对得到的缓冲区的更改不会传播到文件，并且该更改对映射到同一文件的其他程序也不是可见的；相反，会创建缓冲区已修改部分的专用副本。 (MapMode.PRIVATE) MappedByteBuffer是ByteBuffer的子类，其扩充了三个方法： force()：缓冲区是READ_WRITE模式下，此方法对缓冲区内容的修改强行写入文件； load()：将缓冲区的内容载入内存，并返回该缓冲区的引用； isLoaded()：如果缓冲区的内容在物理内存中，则返回真，否则返回假； 案例对比这里通过采用ByteBuffer和MappedByteBuffer分别读取大小约为5M的文件”src&#x2F;1.ppt”来比较两者之间的区别，method3()是采用MappedByteBuffer读取的，method4()对应的是ByteBuffer。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 public static void method4()&#123; RandomAccessFile aFile = null; FileChannel fc = null; try&#123; aFile = new RandomAccessFile(&quot;src/1.ppt&quot;,&quot;rw&quot;); fc = aFile.getChannel(); long timeBegin = System.currentTimeMillis(); ByteBuffer buff = ByteBuffer.allocate((int) aFile.length()); buff.clear(); fc.read(buff); //System.out.println((char)buff.get((int)(aFile.length()/2-1))); //System.out.println((char)buff.get((int)(aFile.length()/2))); //System.out.println((char)buff.get((int)(aFile.length()/2)+1)); long timeEnd = System.currentTimeMillis(); System.out.println(&quot;Read time: &quot;+(timeEnd-timeBegin)+&quot;ms&quot;); &#125;catch(IOException e)&#123; e.printStackTrace(); &#125;finally&#123; try&#123; if(aFile!=null)&#123; aFile.close(); &#125; if(fc!=null)&#123; fc.close(); &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125; public static void method3()&#123; RandomAccessFile aFile = null; FileChannel fc = null; try&#123; aFile = new RandomAccessFile(&quot;src/1.ppt&quot;,&quot;rw&quot;); fc = aFile.getChannel(); long timeBegin = System.currentTimeMillis(); MappedByteBuffer mbb = fc.map(FileChannel.MapMode.READ_ONLY, 0, aFile.length()); // System.out.println((char)mbb.get((int)(aFile.length()/2-1))); // System.out.println((char)mbb.get((int)(aFile.length()/2))); //System.out.println((char)mbb.get((int)(aFile.length()/2)+1)); long timeEnd = System.currentTimeMillis(); System.out.println(&quot;Read time: &quot;+(timeEnd-timeBegin)+&quot;ms&quot;); &#125;catch(IOException e)&#123; e.printStackTrace(); &#125;finally&#123; try&#123; if(aFile!=null)&#123; aFile.close(); &#125; if(fc!=null)&#123; fc.close(); &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125; 通过在入口函数main()中运行： 1 method3(); System.out.println(&quot;=============&quot;); method4(); 输出结果（运行在普通PC机上）： 1Read time: 2ms=============Read time: 12ms 通过输出结果可以看出彼此的差别，一个例子也许是偶然，那么下面把5M大小的文件替换为200M的文件，输出结果： 1Read time: 1ms=============Read time: 407ms 可以看到差距拉大。 注：MappedByteBuffer有资源释放的问题：被MappedByteBuffer打开的文件只有在垃圾收集时才会被关闭，而这个点是不确定的。在Javadoc中这里描述：A mapped byte buffer and the file mapping that it represents remian valid until the buffer itself is garbage-collected。详细可以翻阅参考资料5和6. 六、其余功能介绍看完以上陈述，详细大家对NIO有了一定的了解，下面主要通过几个案例，来说明NIO的其余功能，下面代码量偏多，功能性讲述偏少。 Scatter&#x2F;Gatter分散（scatter）从Channel中读取是指在读操作时将读取的数据写入多个buffer中。因此，Channel将从Channel中读取的数据“分散（scatter）”到多个Buffer中。 聚集（gather）写入Channel是指在写操作时将多个buffer的数据写入同一个Channel，因此，Channel 将多个Buffer中的数据“聚集（gather）”后发送到Channel。 scatter &#x2F; gather经常用于需要将传输的数据分开处理的场合，例如传输一个由消息头和消息体组成的消息，你可能会将消息体和消息头分散到不同的buffer中，这样你可以方便的处理消息头和消息体。 案例： 12345678910111213141516171819202122232425262728293031323334import java.io.File;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.io.OutputStream;import java.nio.ByteBuffer;import java.nio.channels.Channel;import java.nio.channels.FileChannel;public class ScattingAndGather&#123; public static void main(String args[])&#123; gather(); &#125; public static void gather() &#123; ByteBuffer header = ByteBuffer.allocate(10); ByteBuffer body = ByteBuffer.allocate(10); byte [] b1 = &#123;&#x27;0&#x27;, &#x27;1&#x27;&#125;; byte [] b2 = &#123;&#x27;2&#x27;, &#x27;3&#x27;&#125;; header.put(b1); body.put(b2); ByteBuffer [] buffs = &#123;header, body&#125;; try &#123; FileOutputStream os = new FileOutputStream(&quot;src/scattingAndGather.txt&quot;); FileChannel channel = os.getChannel(); channel.write(buffs); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; transferFrom &amp; transferToFileChannel的transferFrom()方法可以将数据从源通道传输到FileChannel中。 1234567891011121314151617181920212223242526272829303132 public static void method1()&#123; RandomAccessFile fromFile = null; RandomAccessFile toFile = null; try &#123; fromFile = new RandomAccessFile(&quot;src/fromFile.xml&quot;,&quot;rw&quot;); FileChannel fromChannel = fromFile.getChannel(); toFile = new RandomAccessFile(&quot;src/toFile.txt&quot;,&quot;rw&quot;); FileChannel toChannel = toFile.getChannel(); long position = 0; long count = fromChannel.size(); System.out.println(count); toChannel.transferFrom(fromChannel, position, count); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally&#123; try&#123; if(fromFile != null)&#123; fromFile.close(); &#125; if(toFile != null)&#123; toFile.close(); &#125; &#125; catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125; 方法的输入参数position表示从position处开始向目标文件写入数据，count表示最多传输的字节数。如果源通道的剩余空间小于 count 个字节，则所传输的字节数要小于请求的字节数。此外要注意，在SoketChannel的实现中，SocketChannel只会传输此刻准备好的数据（可能不足count字节）。因此，SocketChannel可能不会将请求的所有数据(count个字节)全部传输到FileChannel中。 transferTo()方法将数据从FileChannel传输到其他的channel中。 123456789101112131415161718192021222324252627282930313233 public static void method2() &#123; RandomAccessFile fromFile = null; RandomAccessFile toFile = null; try &#123; fromFile = new RandomAccessFile(&quot;src/fromFile.txt&quot;,&quot;rw&quot;); FileChannel fromChannel = fromFile.getChannel(); toFile = new RandomAccessFile(&quot;src/toFile.txt&quot;,&quot;rw&quot;); FileChannel toChannel = toFile.getChannel(); long position = 0; long count = fromChannel.size(); System.out.println(count); fromChannel.transferTo(position, count,toChannel); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally&#123; try&#123; if(fromFile != null)&#123; fromFile.close(); &#125; if(toFile != null)&#123; toFile.close(); &#125; &#125; catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125; 上面所说的关于SocketChannel的问题在transferTo()方法中同样存在。SocketChannel会一直传输数据直到目标buffer被填满。 PipeJava NIO 管道是2个线程之间的单向数据连接。Pipe有一个source通道和一个sink通道。数据会被写到sink通道，从source通道读取。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 public static void method1()&#123; Pipe pipe = null; ExecutorService exec = Executors.newFixedThreadPool(2); try&#123; pipe = Pipe.open(); final Pipe pipeTemp = pipe; exec.submit(new Callable&lt;Object&gt;()&#123; @Override public Object call() throws Exception &#123; Pipe.SinkChannel sinkChannel = pipeTemp.sink();//向通道中写数据 while(true)&#123; TimeUnit.SECONDS.sleep(1); String newData = &quot;Pipe Test At Time &quot;+System.currentTimeMillis(); ByteBuffer buf = ByteBuffer.allocate(1024); buf.clear(); buf.put(newData.getBytes()); buf.flip(); while(buf.hasRemaining())&#123; System.out.println(buf); sinkChannel.write(buf); &#125; &#125; &#125; &#125;); exec.submit(new Callable&lt;Object&gt;()&#123; @Override public Object call() throws Exception &#123; Pipe.SourceChannel sourceChannel = pipeTemp.source();//向通道中读数据 while(true)&#123; TimeUnit.SECONDS.sleep(1); ByteBuffer buf = ByteBuffer.allocate(1024); buf.clear(); int bytesRead = sourceChannel.read(buf); System.out.println(&quot;bytesRead=&quot;+bytesRead); while(bytesRead &gt;0 )&#123; buf.flip(); byte b[] = new byte[bytesRead]; int i=0; while(buf.hasRemaining())&#123; b[i]=buf.get(); System.out.printf(&quot;%X&quot;,b[i]); i++; &#125; String s = new String(b); System.out.println(&quot;=================||&quot;+s); bytesRead = sourceChannel.read(buf); &#125; &#125; &#125; &#125;); &#125;catch(IOException e)&#123; e.printStackTrace(); &#125;finally&#123; exec.shutdown(); &#125; &#125; DatagramChannelJava NIO中的DatagramChannel是一个能收发UDP包的通道。因为UDP是无连接的网络协议，所以不能像其它通道那样读取和写入。它发送和接收的是数据包。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 public static void reveive()&#123; DatagramChannel channel = null; try&#123; channel = DatagramChannel.open(); channel.socket().bind(new InetSocketAddress(8888)); ByteBuffer buf = ByteBuffer.allocate(1024); buf.clear(); channel.receive(buf); buf.flip(); while(buf.hasRemaining())&#123; System.out.print((char)buf.get()); &#125; System.out.println(); &#125;catch(IOException e)&#123; e.printStackTrace(); &#125;finally&#123; try&#123; if(channel!=null)&#123; channel.close(); &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125; public static void send()&#123; DatagramChannel channel = null; try&#123; channel = DatagramChannel.open(); String info = &quot;I&#x27;m the Sender!&quot;; ByteBuffer buf = ByteBuffer.allocate(1024); buf.clear(); buf.put(info.getBytes()); buf.flip(); int bytesSent = channel.send(buf, new InetSocketAddress(&quot;10.10.195.115&quot;,8888)); System.out.println(bytesSent); &#125;catch(IOException e)&#123; e.printStackTrace(); &#125;finally&#123; try&#123; if(channel!=null)&#123; channel.close(); &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125;","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"javaNIO","slug":"javaNIO","permalink":"http://sv.pointcut.cc/tags/javaNIO/"}]},{"title":"Scalable IO in Java——多Reactor的代码实现","slug":"java_nio/Scalable IO in Java——多Reactor的代码实现","date":"2021-11-22T12:00:05.000Z","updated":"2022-03-23T09:03:58.118Z","comments":true,"path":"blog/java_nio/Scalable IO in Java——多Reactor的代码实现/","link":"","permalink":"http://sv.pointcut.cc/blog/java_nio/Scalable%20IO%20in%20Java%E2%80%94%E2%80%94%E5%A4%9AReactor%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"Java高伸缩性IO处理在Doug Lea大神的经典NIO框架文章[《Scalable IO in Java》](.&#x2F;Scalable IO in Java.pdf)中，具体阐述了如何把Reactor模式和Java NIO整合起来，一步步理论结合Java代码实践去构建了一套高伸缩性的网络处理框架雏形，从当今的流行NIO框架（Netty）中无不看到其本质均与该文章所述架构不谋而合（或者也可以说其实是借鉴并以现代化的方式实现了Doug Lea的思想吧），这里总结[《Scalable IO in Java》](.&#x2F;Scalable IO in Java.pdf)中的要点并记录下自己实现多Reactor的过程中遇到的坑 网络服务的基本结构当今网络上的各种基于TCP&#x2F;IP的应用服务，其对1次请求的处理过程的本质流程结构均为 从底层IO读取字节请求 把读取后的字节请求进行解码成为自己的业务请求对象 把解码后的业务请求对象进行业务处理 把处理后的响应编码为底层IO可写入的字节响应 利用底层IO返回（发出）编码后的字节响应 整体的流程如上述5步所示，但具体每步骤所使用到的一些技术手段不一样：例如解码协议是自定义的还是使用业界流行的？是文本协议还是二进制协议？处理过程就结合具体业务进行处理等 一般典型的网络服务设计如下图所示： 可见其对每一个请求都新产生一个线程来进行处理，缺点就是线程的创建是消耗不小的系统资源的，且最关键的是如果并发访问突然激增到一定程度，那响应就会大打折扣，甚至由于系统资源不足导致系统崩溃。。。 这里给出自己的Java实现代码如下，比较简单，就是处理每个请求都new一个Thread 所以现在tomcat等框架在客户端都是用线程池来处理请求，不让线程无限的创建。但这种也存在问题，当请求数操作了超过线程池的核心线程数后就会出现等待甚至超时的情况。 Server12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class Server implements Runnable&#123; private final int port; private ServerSocket serverSocket; public Server(int port)&#123; this.port = port; try &#123; this.serverSocket = new ServerSocket(port); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void run()&#123; try &#123; while (!Thread.interrupted()) &#123; new Thread(new Handler(serverSocket.accept())).start(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public int getPort() &#123; return port; &#125; public static void main(String[] args) &#123; new Thread(new Server(9001)).start(); &#125;&#125;改进后：public class ServerPool &#123; private static ExecutorService executorService = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors()); public static void main(String[] args) throws IOException &#123; //服务端启动必备 ServerSocket serverSocket = new ServerSocket(); //表示服务端在哪个端口上监听 serverSocket.bind(new InetSocketAddress(10001)); System.out.println(&quot;Start Server ....&quot;); try&#123; while(true)&#123; executorService.execute(new Handler(serverSocket.accept())); &#125; &#125;finally &#123; serverSocket.close(); &#125; &#125; Handle123456789101112131415161718192021222324public class Handler implements Runnable&#123; private final Socket clientSocket; public Handler(Socket clientSocket)&#123; this.clientSocket = clientSocket; &#125; @Override public void run() &#123; int readSize; byte[] readBuf = new byte[BUF_SIZE]; try &#123; InputStream in = clientSocket.getInputStream(); OutputStream out = clientSocket.getOutputStream(); while ((readSize = in.read(readBuf)) != -1) &#123; out.write(readBuf, 0, readSize); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 高伸缩性系统的目标基于“每请求每线程”的缺点比较明显且不可接受（严重时系统崩溃），Doug Lea大神提出了构建高伸缩性系统的目标： 在激增请求的负载下至少优雅退化吧，即可以相应慢点，但别崩溃（无响应）呀 然后自动的增加处理所需资源（例如CPU、内存、磁盘、带宽）来渐进改善上一步中响应缓慢的问题 然后对于系统整体的可用性和性能也提出了一些目标： 低延时，其实就是尽量高响应 能够满足最大峰值的处理请求，即在访问量突增时不至于宕机 可调控的服务处理，例如请求较多时可多加入一些服务处理线程 最后总结了针对设计高伸缩性系统的一个至理名言： 分而治之通常都是构建任何高伸缩性系统的最佳解决方案！ NIO框架的分而治之和事件驱动设计针对NIO框架的分而治之是把处理过程拆分封装成小的任务—每个任务可以单独无阻塞的进行业务处理；每个任务在其可以立即执行处理的时候就立即执行，即把原来IO阻塞部分交由NIO框架去管理处理，真正的任务只无差别&#x2F;幂等的处理真正的业务；NIO框架则把IO事件当做触发器去回调相关的任务去执行。 值得庆幸的在java.nio包中有对实现上述的NIO框架处理机制的支持： 非阻塞（Non-blocking）的读取和写入 分发（dispatch）IO事件到与其对应的任务并执行处理 这一切看起来都很类似Swing&#x2F;AWT事件驱动设计： 而实际的Swing&#x2F;AWT事件驱动本质上是多生产者&#x2F;单一消费者模式，即有多个产生事件的地方（各种交互GUI），但是处理事件却只在一个地方（AWT&#x2F;Event线程从事件队列获取事件一个个处理）。 Reactor模式和NIO同Swing&#x2F;AWT事件驱动设计类似，Reactor模式也是多生产者&#x2F;单一消费者模式，多个IO（读 &#x2F;写）事件，但是处理IO事件却只在单一的EventLoop（事件循环）线程中分发给对应的任务处理器处理。基本的Reactor模式（单线程版）如下所示： 下面来看下基于NIO的Reactor模式和Swing&#x2F;AWT事件驱动设计的相似对比 类名称 作用 对应Swing&#x2F;AWT Reactor反应器 EventLoop及时响应相对于的读&#x2F;写IO事件 AWT中的单例事件分发线程 分发到对应Handler处理器上进行业务处理 Handlers处理器 处理非阻塞读&#x2F;写IO事件所对应的业务逻辑 AWT中的ActionListeners处理器 事件绑定和处理 管理IO读&#x2F;写事件到对应处理器的绑定 AWT中的addActionListener绑定 然后再看下Java NIO中对实现Reactor提供了哪些支持 类名称 作用 Channels 连接支持非阻塞IO的读&#x2F;写的通道 例如磁盘文件、网络Socket等都有对应的非阻塞IO的通道类 Buffers Channels通道直接用来进行读&#x2F;写操作的类数组对象 Selectors 能知道哪些Channels通道集合存在IO事件 SelectionKeys 提供IO事件状态信息和IO事件绑定功能的类 Reactor模式的多线程设计单线程版Reactor模式是最基本的实现，其核心就是单线程Reactor的EventLoop在不断处理被Selector检测到的IO事件，但缺点也显而易见： 随着客户端的连接数目的增加，如果业务的处理也需要消耗不小时间的话，那仅仅单次的EventLoop循环都会消耗不少时间才能进入下一次循环，导致IO事件阻塞在Selector里不能被及时轮询处理到 而且随着多核CPU的爆发，当拥有多核机器时，应当适当利用多线程能力来分担本来是单线程的Rector，以去应对更多的客户端连接，否则依旧是单线程Rector的话，岂不是浪费了多核这个潮流强项了？ Worker Threads针对第1条缺点引入了Worker Threads（工人线程，消费线程，即有一群工人老早就做好准备处理即将到来任务了）——线程池；理由是Reactor的EventLoop轮询应当快速响应IO触发事件，而不应当消耗在本应该是任务处理器处理的业务上： 从上图可以看到其实就是在单线程Reactor的基础上把非IO相关的业务处理部分（decode、computer和encode）拆出来封装成为一个单独的任务（Runnable&#x2F;命令模式），如此一来在线程池中就能立即进行计算处理了 Multiple Reactor Threads针对第2条Multiple Reactor Threads，即多个Reactor线程；理由是随着客户端连接越来越多，单个Reactor线程处理IO能力会达到饱和状态，在多核机器上看到的现象是只有一个核心利用率较高，其他核心是闲置的，所以应当适当利用多核优势，扩展成匹配CPU核数的多个Reactor，达到分担IO负载的目的： 如上图所示，多Reactor根据职责划分为1个mainReactor和多个subReactors，mainReactor主要负责接收客户端连接，因为TCP初始需要经历3次握手才能确认连接，这个连接过程的消耗在客户端较多时其开销是不小的，单独使用mainReactor处理保证了其他已经连接上的客户端在subReactors中不受其影响，从而快速响应处理业务，以此分摊负载并提高系统整体系能 代码实现[《Scalable IO in Java》](file:&#x2F;&#x2F;&#x2F;.&#x2F;Scalable IO in Java.pdf)文章中也已经给出示例代码了，基本的Reactor模式的实现直接照搬代码，自己再写点NIO的读&#x2F;写部分以及process部分即可，所以这里主要把如何实现多Reactor&#x2F;Selector以及所遇到的坑说一下 多Reactor&#x2F;多SelectorReactor的实现依赖于NIO的Selector，是Selector去轮询Channel的，所以其实在单线程版Reactor中Reactor有一个Selector，同理既然是多Reactor，那么还是每个Reactor都有自己的Selector和EventLoop轮询。 区别在于：mainReactor的Selector感兴趣的是ACCEPT操作，而subReactors感兴趣的先是READ然后才是WRITE，然后WRITE完毕后感兴趣的是READ然后再是WRITE。。。如此反复，必须要先READ是为了避免多线程中IO重叠问题，所以需要在代码中区分Reactor是不是mainReactor。 Reactor1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public abstract class Reactor extends Thread&#123; protected final int port; protected final ServerSocketChannel serverChannel; protected final boolean isMainReactor; protected final boolean useMultipleReactors; protected final long timeout; protected Selector selector; public Reactor(int port, ServerSocketChannel serverChannel, boolean isMainReactor, boolean useMultipleReactors, long timeout)&#123; this.port = port; this.serverChannel = serverChannel; this.isMainReactor = isMainReactor; this.useMultipleReactors = useMultipleReactors; this.timeout = timeout; &#125; @Override public void run()&#123; try &#123; init(); while(!Thread.interrupted())&#123; //不可以使用阻塞的select方式，否则accept后subReactor的selector在register的时候会一直阻塞 //但是修改为带有超时的select或者selectNow后，subReactor的selector在register就不会阻塞了 //最终选择了带有超时的select是因为使用selectNow的无限循环会导致CPU飙高特别快 //并且如果使用阻塞的select方式，还需要知道在哪里调用wakeup，否则会一直阻塞，使用非阻塞方式就不需要wakeup了 //selector.select(); //if(selector.selectNow() &gt; 0)&#123; if(selector.select(timeout) &gt; 0)&#123; log(selector+&quot; isMainReactor=&quot;+isMainReactor+&quot; select...&quot;); Iterator&lt;SelectionKey&gt; keyIt = selector.selectedKeys().iterator(); while(keyIt.hasNext())&#123; SelectionKey key = keyIt.next(); dispatch(key); keyIt.remove(); &#125; &#125; &#125; log(getClass().getSimpleName()+&quot; end on &quot;+port+&quot; ...&quot;+&quot;\\n&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private void init() throws IOException&#123; selector = Selector.open(); log(selector+&quot; isMainReactor=&quot;+isMainReactor); if(isMainReactor)&#123; //serverChannel = ServerSocketChannel.open(); serverChannel.socket().bind(new InetSocketAddress(port)); serverChannel.configureBlocking(false); SelectionKey key = serverChannel.register(selector, SelectionKey.OP_ACCEPT); key.attach(newAcceptor(selector)); log(getClass().getSimpleName()+&quot; start on &quot;+port+&quot; ...&quot;+&quot;\\n&quot;); &#125;else&#123; &#125; //如果使用阻塞的select方式，且开启下面的代码的话，相当于开启了多个reactor池，而不是mainReactor和subReactor的关系了 //SelectionKey key = serverChannel.register(selector, SelectionKey.OP_ACCEPT); //key.attach(newAcceptor(selector, serverChannel)); &#125; public abstract Acceptor newAcceptor(Selector selector); /** * 事件和事件处理器的绑定 * &lt;ul&gt; * &lt;li&gt;管理IO读/写事件到事件处理器的一一对应的绑定&lt;/li&gt; * &lt;/ul&gt; */ private void dispatch(SelectionKey key)&#123; Runnable r = (Runnable)key.attachment(); if(r != null)&#123; r.run(); &#125; &#125;&#125; Reactor中的init方法里的isMainReactor字段即是用来判断是否该Reactor是否为mainReactor的，如果是mainReactor的话，则注册感兴趣的为ACCEPT事件，并且添加Acceptor附件 然后run方法里面的while循环即是EventLoop轮询了，需要注意的是这里有坑：别使用阻塞的select方法，因为该方法会导致accept后subReactor的selector在register的时候会一直阻塞；也别使用非阻塞的selecNow方法，因为selectNow在无限循环下即使没有IO事件，也会使CPU飙到100%；所以最终选择使用带有超时的select(timeout)方法 Acceptor12345678910111213141516171819202122232425262728293031323334353637public abstract class Acceptor extends Thread &#123; protected final Selector selector; protected final ServerSocketChannel serverChannel; protected final boolean useMultipleReactors; public Acceptor(Selector selector, ServerSocketChannel serverChannel, boolean useMultipleReactors)&#123; this.selector = selector; this.serverChannel = serverChannel; this.useMultipleReactors = useMultipleReactors; &#125; @Override public void run() &#123; log(selector+&quot; accept...&quot;); try &#123; SocketChannel clientChannel = serverChannel.accept(); if(clientChannel != null)&#123; log(selector+&quot; clientChannel not null...&quot;); //如果使用阻塞的select方式，且目的是开启了多个reactor池，而不是mainReactor和subReactor的关系的话， //则下面就不是nextSubSelector().selector，而是改为传递当前实例的selector对象即可 handle(useMultipleReactors ? nextSubReactor().selector : selector, clientChannel); &#125;else&#123; log(selector+&quot; clientChannel is null...&quot;); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 在每个具体的Handler下调用run方法是为了令其从connecting状态变为reading状态， * 和原pdf版本下的做法是一样的，只不过原pdf版本是在构造函数直接修改设置了感兴趣为read事件 */ public abstract void handle(Selector selector, SocketChannel clientSocket);&#125; Acceptor是被mainReactor当做ACCPET的附属对象，所以当有连接接收过来了，就使用handle方法处理，handle方法的Selector参数即可传递subReactors的Selector，然后先对READ感兴趣即可。 Handler123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203public abstract class Handler extends Thread &#123; private enum State&#123; CONNECTING(0), READING(SelectionKey.OP_READ), PROCESSING(2), WRITING(SelectionKey.OP_WRITE); private final int opBit; private State(int operateBit)&#123; opBit = operateBit; &#125; &#125; private State state; protected final SocketChannel clientChannel; protected final SelectionKey key; protected final ByteBuffer readBuf; protected final StringBuilder readData = new StringBuilder(); protected ByteBuffer writeBuf; public Handler(Selector selector, SocketChannel clientChannel)&#123; this.state = State.CONNECTING; SelectionKey key = null; try &#123; clientChannel.configureBlocking(false); //这里在使用subSelector的时候会阻塞，为什么？是因为使用了阻塞的select方法，非阻塞的才可以 //但如果使用reactor池的话，那是因为需要serverChannel注册selector的accept事件！？必须对应上才可以通过，否则阻塞 key = clientChannel.register(selector, this.state.opBit); key.attach(this); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; this.clientChannel = clientChannel; this.key = key; this.readBuf = ByteBuffer.allocate(byteBufferSize()); log(selector+&quot; connect success...&quot;); &#125; @Override public void run() &#123; switch (state) &#123; case CONNECTING: connect(); break; case READING: readAndProcess(); break; case WRITING: write(); break; default: err(&quot;\\nUnsupported State: &quot;+state+&quot; ! overlap processing with IO...&quot;); &#125; &#125; private void connect() &#123; interestOps(State.READING); &#125; /** * But harder to overlap processing with IO&lt;br/&gt; * Best when can first read all input a buffer&lt;br/&gt; * &lt;br&gt; * That why we used synchronized on read method!&lt;br/&gt; * Just to protected read buffer And handler state...&lt;br/&gt; * &lt;br&gt; * 其实就是害怕重叠IO和工作线程处理不一致：例如Reactor单线程读某个key的IO完毕后立马开启工作线程的处理， * 紧接着Reactor单线程处理第二个IO key的时候发现还是之前的那个key的读IO事件，但是之前同一个key的处理还未完成， * 不等待之前的处理完成的话，就会出现多个线程同时访问修改Handler里面数据的情况，导致出错， * 但是最好先把数据都全部读入buffer中就可以规避了！？ * * &lt;p&gt;此处的synchronized同步是为了防止state状态以及读写buffer在多线程访问中出现读脏数据， * Debug调试的时候同时访问一个SelectionKey有2个线程： * &lt;br&gt;1、Reactor单线程 * &lt;br&gt;2、读数据完毕后多线程处理的话，线程池里面执行processAndHandOff的线程 * &lt;br&gt; * 不能单一使用volatile或者原子变量的原因是因为该方法为复合操作（check and act） */ private synchronized void readAndProcess()&#123; doRead(); doProcess(); &#125; private void doRead()&#123; int readSize; try &#123; while((readSize = clientChannel.read(readBuf)) &gt; 0)&#123; readData.append(new String(Arrays.copyOfRange(readBuf.array(), 0, readSize))); readBuf.clear(); &#125; if(readSize == -1)&#123; disconnect(); return; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); disconnect(); &#125; log(&quot;readed from client:&quot;+readData+&quot;, &quot;+readData.length()); &#125; private void doProcess()&#123; if(readIsComplete())&#123; state = State.PROCESSING; processAndInterestWrite(); &#125; &#125; /** * 处理过程可能是比较耗时的，所以可考虑将其交由线程池处理，处理完毕后才注册感兴趣的write事件&lt;p&gt; * 然而正是由于交由线程池处理所以可能造成重叠IO的多线程处理的状态问题，最好能一次性全部读入buffer，否则考虑同步状态处理问题 */ private void processAndInterestWrite()&#123; Processor processor = new Processor(); if(useThreadPool)&#123; execute(processor); &#125;else&#123; processor.run(); &#125; &#125; private final class Processor implements Runnable&#123; @Override public void run() &#123; processAndHandOff(); &#125; &#125; private synchronized void processAndHandOff()&#123; if(process())&#123; interestOps(State.WRITING); &#125; &#125; //TODO 修改为复用output，即当output容量不足的时候就反复write，而不是每次都使用wrap来new一个新的 public boolean process()&#123; log(&quot;process readData=&quot;+readData.toString()); if(isQuit())&#123; disconnect(); return false; &#125; writeBuf = ByteBuffer.wrap(readData.toString().getBytes()); readData.delete(0, readData.length()); return true; &#125; private void write()&#123; try &#123; do&#123; clientChannel.write(writeBuf); &#125;while(!writeIsComplete()); &#125; catch (IOException e) &#123; e.printStackTrace(); disconnect(); &#125; String writeData = new String(Arrays.copyOf(writeBuf.array(), writeBuf.array().length)); log(&quot;writed to client:&quot;+writeData+&quot;, &quot;+writeData.length()); interestOps(State.READING); &#125; /** * 事件和事件处理器的绑定 * &lt;ul&gt; * &lt;li&gt;类似AWT中的addActionListener添加监听器/观察者&lt;/li&gt; * &lt;/ul&gt; * 不需要重置key的附件（key.attach）是因为key一直绑定使用的是当前this实例， * 在Reactor dispatch的时候如果是接受（accept）该附件就是Acceptor实例， * 否则就是绑定到该key的同一个Handler实例 */ private void interestOps(State state)&#123; this.state = state; key.interestOps(state.opBit); &#125; public boolean isQuit()&#123; return false; &#125; private void disconnect()&#123; try &#123; clientChannel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; log(&quot;\\nclient Address=【&quot;+clientAddress(clientChannel)+&quot;】 had already closed!!! &quot;); &#125; private static SocketAddress clientAddress(SocketChannel clientChannel)&#123; return clientChannel.socket().getRemoteSocketAddress(); &#125; public abstract int byteBufferSize(); public abstract boolean readIsComplete(); public abstract boolean writeIsComplete();&#125; 具体做的事情就像前面说的，先对READ感兴趣，然后是状态机的判断和处理，注意的地方使用了synchronized同步避免IO重叠并起到了保护状态机的作用，注释上也已经做出描述了。 其中有些方法是abstract是因为想自己写一个类NIO框架，达到根据应用场景的不同可以自行实现所需要的方法，当前仅仅写了个Echo（回显）和Enter（回车作为结束符）显示消息的例子，具体代码已经放到本人GitHub上：scalableIO 参考 nio框架中的多个Selector结构 Java NIO之多个Selector的实现","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"javaNIO","slug":"javaNIO","permalink":"http://sv.pointcut.cc/tags/javaNIO/"}]},{"title":"Reactor模式和观察者模式","slug":"java_nio/Reactor模式和观察者模式","date":"2021-11-22T12:00:04.000Z","updated":"2022-03-23T09:03:58.110Z","comments":true,"path":"blog/java_nio/Reactor模式和观察者模式/","link":"","permalink":"http://sv.pointcut.cc/blog/java_nio/Reactor%E6%A8%A1%E5%BC%8F%E5%92%8C%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"观察者模式：也可以称为为 发布-订阅 模式，主要适用于多个对象依赖某一个对象的状态并，当某对象状态发生改变时，要通知其他依赖对象做出更新。是一种一对多的关系。当然，如果依赖的对象只有一个时，也是一种特殊的一对一关系。通常，观察者模式适用于消息事件处理，监听者监听到事件时通知事件处理者对事件进行处理（这一点上面有点像是回调，容易与反应器模式和前摄器模式的回调搞混淆）。 Reactor模式：reactor模式，即反应器模式，是一种高效的异步IO模式，特征是回调，当IO完成时，回调对应的函数进行处理。这种模式并非是真正的异步，而是运用了异步的思想，当IO事件触发时，通知应用程序作出IO处理。模式本身并不调用系统的异步IO函数。 reactor模式与观察者模式有点像。不过，观察者模式与单个事件源关联，而反应器模式则与多个事件源关联 。当一个主体发生改变时，所有依属体都得到通知。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"javaNIO","slug":"javaNIO","permalink":"http://sv.pointcut.cc/tags/javaNIO/"}]},{"title":"03原生JDK网络编程NIO","slug":"java_nio/03原生JDK网络编程NIO ","date":"2021-11-22T12:00:03.000Z","updated":"2022-03-23T09:03:58.110Z","comments":true,"path":"blog/java_nio/03原生JDK网络编程NIO /","link":"","permalink":"http://sv.pointcut.cc/blog/java_nio/03%E5%8E%9F%E7%94%9FJDK%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BNIO%20/","excerpt":"","text":"这了解一些必要的概念就好了，java的原生NIO有太多东西都要自己实现了，涉及到网络编程的都用netty吧。 和BIO的主要区别面向流与面向缓冲Java NIO和IO之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。 Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。 Java NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。但是，还需要检查是否该缓冲区中包含所有需要处理的数据。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的数据。bio: nio： 阻塞与非阻塞IOJava IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 Java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取。而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）。 java NIO概念Java NIO的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。 NIO主要有三个核心部分组成：buffer缓冲区、Channel管道、Selector选择器 Selector Selector的英文含义是“选择器”，也可以称为为“轮询代理器”、“事件订阅器”、“channel容器管理机”。 Java NIO 的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用 一个选择器(Selectors)，然后使用一个单独的线程来操作这个选择器，进而“选择”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个 单独的线程很容易来管理多个通道。 应用程序将向Selector对象注册需要它关注的Channel，以及具体的某一个Channel会对哪些IO事件感兴趣。Selector中也会维护一个“已经注册的Channel”的容器。 Channel（管道&#x2F;通道）通道，被建立的一个应用程序和操作系统交互事件、传递内容的渠道（注意是连接到操作系统）。那么既然是和操作系统进行内容的传递，说明应用程序可以通过通道读取数据，也可以通过通道向操作系统写数据，而且可以同时进行读写。 所有被Selector（选择器）注册的通道，只能是继承了SelectableChannel类的子类。 ServerSocketChannel：应用服务器程序的监听通道。只有通过这个通道，应用程序才能向操作系统注册支持“多路复用IO”的端口监听。同时支持UDP协议和TCP协议。 ScoketChannel：TCP Socket套接字的监听通道，一个Socket套接字对应了一个客户端IP：端口到服务器IP：端口的通信连接。 通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入。 有很多种channel，分为两种类型：文件通道、套接字通道常用的有： FileChannel：用于读取、写入、映射和操作文件的通道 DatagramChannel:读写UDP通信的数据，对应DatagramSocket类 SocketChannel:读写TCP通信的数据，对应Socket类 ServerSocketChannel:监听新的TCP连接，并且会创建一个可读写的SocketChannel，对应ServerSocket类（服务器） ScatteringByteChannel和GatheringByteChannel：分散聚集通道，由操作系统完成 WritableByteChannel和ReadableByteChannel：接口提供读写API 常用操作： 实例化：通道可以使用流的getChannel()方法创建，JDK1.7 中的NIO2针对各个通道提供了一个静态的方法open()，JDK1.7 中的NIO2的Files工具类的newByteChannel() isOpen()：Channel自带的方法，告诉这个通道是否打开 close: Channel自带的方法，关闭通道 read() : Channel大部分子类拥有的方法，从通道读取数据到缓冲区，不同的参数有不同的作用，FileChannel有四种read方法 write():Channel大部分子类拥有的方法，从缓冲区写入数据到通道，FileChannel有四种write方法 缓冲区与通道：分散、聚集前面知道了通道类似与流，缓冲区暂时保存数据那么程序与实体间数据交流就是通过缓冲区与通道的**分散读取、聚集写入 分散读取：将数据从通道中读取到多个缓冲区（read方法） 聚集写入：将多个缓冲区的数据写入到单个通道中（write方法） 有两个专门的接口就是实现聚集写入与分散读出：ScatteringByteChannel和GatheringByteChannel FileChannel实现了这两个接口 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package com.company.ScatterGather;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.Channel;import java.nio.channels.FileChannel;import java.nio.channels.GatheringByteChannel;import java.nio.channels.ScatteringByteChannel;public class ScatterGatherIO &#123; //聚集写入 public static void Gather(String data) throws FileNotFoundException &#123; //创建两个ByteBuffer存数据 ByteBuffer byteBuffer1=ByteBuffer.allocate(20); ByteBuffer byteBuffer2=ByteBuffer.allocate(400); //把整数放入byteBuffer1 byteBuffer1.asIntBuffer().put(1024); //把输入的String变量放入byteBuffer2 byteBuffer2.asCharBuffer().put(data); //GatheringByteChannel接口允许委托操作系统完成任务 //CreatChanner使用文件写入流 GatheringByteChannel gatherChannel=CreatChanner(&quot;TestOut.txt&quot;,true); //聚集写入通道 try &#123; //write只允许一个ByteBuffer gatherChannel.write(new ByteBuffer[]&#123;byteBuffer1,byteBuffer2&#125;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; //分散写出 public static void Scatter() throws FileNotFoundException &#123; //创建两个ByteBuffer存数据 ByteBuffer byteBuffer1=ByteBuffer.allocate(20); ByteBuffer byteBuffer2=ByteBuffer.allocate(400); //读取文件通道 ScatteringByteChannel scatterChannel=CreatChanner(&quot;TestOut.txt&quot;,false); try &#123; scatterChannel.read(new ByteBuffer[]&#123;byteBuffer1,byteBuffer2&#125;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; //buffer位置置0 byteBuffer1.rewind(); byteBuffer2.rewind(); System.out.println(byteBuffer1.asIntBuffer().get()); System.out.println(byteBuffer2.asCharBuffer().toString()); &#125; //输入文件地址和输入方向，决定通道方向 public static FileChannel CreatChanner(String fileUrl,boolean out) throws FileNotFoundException &#123; FileChannel fileChannel=null; if (out)&#123; fileChannel=new FileOutputStream(fileUrl).getChannel(); &#125; else fileChannel=new FileInputStream(fileUrl).getChannel(); return fileChannel; &#125; public static void main(String[] args) throws FileNotFoundException &#123; String data=&quot;hello,welcome to ScatterGatherIO&quot;; Gather(data); Scatter(); &#125;&#125; buffer缓冲区Buffer 重要概念 SelectionKeySelectionKey是一个抽象类，表示selectableChannel在Selector中注册的标识.每个Channel向Selector注册时，都将会创建一个selectionKey。SelectionKey将Channel与Selector建立了关系，并维护了channel事件。 可以通过cancel方法取消键，取消的键不会立即从selector中移除，而是添加到cancelledKeys中，在下一次select操作时移除它.所以在调用某个key时，需要使用isValid进行校验. 在向Selector对象注册感兴趣的事件时，JAVA NIO共定义了四种：OP_READ、OP_WRITE、OP_CONNECT、OP_ACCEPT（定义在SelectionKey中），分别对应读、写、请求连接、接受连接等网络Socket操作。 ServerSocketChannel和SocketChannel可以注册自己感兴趣的操作类型，当对应操作类型的就绪条件满足时OS会通知channel，下表描述各种Channel允许注册的操作类型，Y表示允许注册，N表示不允许注册，其中服务器SocketChannel指由服务器ServerSocketChannel.accept()返回的对象。我们可以看看每个操作类型的就绪条件： Bufferjava NIO是面向缓冲的。Buffer用于和NIO通道进行交互。数据是从通道读入缓冲区，从缓冲区写入到通道中的。以写为例，应用程序都是将数据写入缓冲，再通过通道把缓冲的数据发送出去，读也是一样，数据总是先从通道读到缓冲，应用程序再读缓冲的数据。 缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存（ 其实就是数组）。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。 重要属性capacity作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。 position当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1. 当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0. 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。 limit在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。 当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position） Buffer的分配要想获得一个Buffer对象首先要进行分配。 每一个Buffer类都有allocate方法(可以在堆上分配，也可以在直接内存上分配)。比如分配48字节capacity的ByteBuffer的例子:ByteBuffer buf = ByteBuffer.allocate(48); wrap方法：把一个byte数组或byte数组的一部分包装成ByteBuffer：ByteBuffer wrap(byte [] array) ByteBuffer wrap(byte [] array， int offset， int length) Buffer的直接内存HeapByteBuffer与DirectByteBuffer，在原理上，前者可以看出分配的buffer是在heap区域的，其实真正flush到远程的时候会先拷贝到直接内存，再做下一步操作；在NIO的框架下，很多框架会采用DirectByteBuffer来操作，这样分配的内存不再是在java heap上，可以得到非常快速的网络交互，在大量的网络交互下，一般速度会比HeapByteBuffer要快速好几倍（这也是NIO的零拷贝） NIO可以使用Native 函数库直接分配堆外内存，然后通过一个存储在Java 堆里面的DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在Java 堆和Native 堆中来回复制数据。 堆外内存的优点堆外内存，其实就是不受JVM控制的内存。相比于堆内内存有几个优势： 减少了垃圾回收的工作，因为垃圾回收会暂停其他的工作（可能使用多线程或者时间片的方式，根本感觉不到） 加快了复制的速度。因为堆内在flush到远程时，会先复制到直接内存（非堆内存），然后在发送；而堆外内存相当于省略掉了这个工作。 堆外内存的缺点 堆外内存难以控制，如果内存泄漏，那么很难排查 堆外内存相对来说，不适合存储很复杂的对象。一般简单的对象或者扁平化的比较适合。 直接内存（堆外内存）与堆内存比较直接内存申请空间耗费更高的性能，当频繁申请到一定量时尤为明显（这可以通过池话技术解决，这也是netty相对于java nio的优势） 直接内存IO读写的性能要差于普通的堆内存，在多次读写操作的情况下差异明显 12345678910111213141516171819202122232425262728293031323334353637public static void operateCompare()&#123; int time = 100000000; ByteBuffer buffer = ByteBuffer.allocate(2*time); long st = System.currentTimeMillis(); for (int i = 0; i &lt; time; i++) &#123; // putChar(char value) 用来写入 char 值的相对 put 方法 buffer.putChar(&#x27;a&#x27;); &#125; buffer.flip(); for (int i = 0; i &lt; time; i++) &#123; buffer.getChar(); &#125; long et = System.currentTimeMillis(); System.out.println(&quot;在进行&quot;+time+&quot;次读写操作时，非直接内存读写耗时：&quot; + (et-st) +&quot;ms&quot;); ByteBuffer buffer_d = ByteBuffer.allocateDirect(2*time); long st_direct = System.currentTimeMillis(); for (int i = 0; i &lt; time; i++) &#123; // putChar(char value) 用来写入 char 值的相对 put 方法 buffer_d.putChar(&#x27;a&#x27;); &#125; buffer_d.flip(); for (int i = 0; i &lt; time; i++) &#123; buffer_d.getChar(); &#125; long et_direct = System.currentTimeMillis(); System.out.println(&quot;在进行&quot;+time+&quot;次读写操作时，直接内存读写耗时:&quot; + (et_direct - st_direct) +&quot;ms&quot;);&#125;结果：在进行100000000次读写操作时，非直接内存读写耗时：50ms在进行100000000次读写操作时，直接内存读写耗时:254ms Buffer方法总结123456789101112131415161718192021222324252627282930//分配ByteBuffer buffer = ByteBuffer.allocate(32);//写数据buffer.put((byte) &#x27;a&#x27;);//切换到读模式buffer.flip();/** clear()方法会清空整个缓冲区* position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。*/buffer.clear();/** compact()方法只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面* 调用此方法后，变回写模式*/buffer.compact();/** rewind()将position设回0，所以你可以重读Buffer中的所有数据。limit保持不变*/buffer.rewind();/** mark()与reset()方法* 通过调用Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用Buffer.reset()方法恢复到这个position*/buffer.mark(); // 记录当前position的值buffer.reset(); //set position back to mark. 可以使用equals()和compareTo()方法两个Buffer。equals()当满足下列条件时，表示两个Buffer相等： 有相同的类型（byte、char、int等）。 Buffer中剩余的byte、char等的个数相等。 Buffer中所有剩余的byte、char等都相同。如你所见，equals只是比较Buffer的一部分，不是每一个在它里面的元素都比较。实际上，它只比较Buffer中的剩余元素。 compareTo()方法比较两个Buffer的剩余元素(byte、char等)， 如果满足下列条件，则认为一个Buffer“小于”另一个Buffer： 第一个不相等的元素小于另一个Buffer中对应的元素 。 所有元素都相等，但第一个Buffer比另一个先耗尽(第一个Buffer的元素个数比另一个少)。 java NIO的模型 java NIO demo简单的例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277public class NioClientHandle implements Runnable&#123; private String host; private int port; private volatile boolean started; private Selector selector; private SocketChannel socketChannel; public NioClientHandle(String ip， int port) &#123; this.host = ip; this.port = port; try &#123; /*创建选择器*/ this.selector = Selector.open(); /*打开监听通道*/ socketChannel = SocketChannel.open(); /*如果为 true，则此通道将被置于阻塞模式； * 如果为 false，则此通道将被置于非阻塞模式 * 缺省为true*/ socketChannel.configureBlocking(false); started = true; &#125; catch (IOException e) &#123; e.printStackTrace(); System.exit(-1); &#125; &#125; public void stop() throws IOException &#123; started = false; selector.close(); //让堵塞的线程继续执行 &#125; @Override public void run() &#123; //连接服务器 try &#123; doConnect(); &#125; catch (IOException e) &#123; e.printStackTrace(); System.exit(-1); &#125; /*循环遍历selector*/ while(started)&#123; try &#123; /*阻塞方法，当至少一个注册的事件发生的时候就会继续*/ selector.select(); /*获取当前有哪些事件可以使用*/ if (!selector.isOpen()) &#123; break; &#125; Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); /*转换为迭代器*/ Iterator&lt;SelectionKey&gt; it = keys.iterator(); SelectionKey key = null; while(it.hasNext())&#123; key = it.next(); /*我们必须首先将处理过的 SelectionKey 从选定的键集合中删除。 如果我们没有删除处理过的键，那么它仍然会在事件集合中以一个激活 的键出现，这会导致我们尝试再次处理它。*/ it.remove(); try &#123; handleInput(key); &#125; catch (Exception e) &#123; if(key!=null)&#123; key.cancel(); if(key.channel()!=null)&#123; key.channel().close(); &#125; &#125; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); System.exit(-1); &#125; &#125; &#125; /*具体的事件处理方法*/ private void handleInput(SelectionKey key) throws IOException &#123; if(key.isValid())&#123; /*获得关心当前事件的channel*/ SocketChannel sc =(SocketChannel)key.channel(); /*处理连接就绪事件 * 但是三次握手未必就成功了，所以需要等待握手完成和判断握手是否成功*/ if(key.isConnectable())&#123; /*finishConnect的主要作用就是确认通道连接已建立， 方便后续IO操作（读写）不会因连接没建立而 导致NotYetConnectedException异常。*/ if(sc.finishConnect())&#123; /*连接既然已经建立，当然就需要注册读事件， 写事件一般是不需要注册的。*/ socketChannel.register(selector，SelectionKey.OP_READ); &#125;else System.exit(-1); &#125; /*处理读事件，也就是当前有数据可读*/ if(key.isReadable())&#123; /*创建ByteBuffer，并开辟一个1k的缓冲区*/ ByteBuffer buffer = ByteBuffer.allocate(1024); /*将通道的数据读取到缓冲区，read方法返回读取到的字节数*/ int readBytes = sc.read(buffer); if(readBytes&gt;0)&#123; buffer.flip(); byte[] bytes = new byte[buffer.remaining()]; buffer.get(bytes); String result = new String(bytes，&quot;UTF-8&quot;); System.out.println(&quot;客户端收到消息：&quot;+result); &#125; /*链路已经关闭，释放资源*/ else if(readBytes&lt;0)&#123; key.cancel(); sc.close(); &#125; &#125; &#125; &#125; /*进行连接*/ private void doConnect() throws IOException &#123; /*如果此通道处于非阻塞模式，则调用此方法将启动非阻塞连接操作。 如果连接马上建立成功，则此方法返回true。 否则，此方法返回false， 因此我们必须关注连接就绪事件， 并通过调用finishConnect方法完成连接操作。*/ if(socketChannel.connect(new InetSocketAddress(host，port)))&#123; /*连接成功，关注读事件*/ socketChannel.register(selector，SelectionKey.OP_READ); &#125; else&#123; socketChannel.register(selector，SelectionKey.OP_CONNECT); &#125; &#125; /*写数据对外暴露的API*/ public void sendMsg(String msg) throws IOException &#123; doWrite(socketChannel，msg); &#125; private void doWrite(SocketChannel sc，String request) throws IOException &#123; byte[] bytes = request.getBytes(); ByteBuffer writeBuffer = ByteBuffer.allocate(bytes.length); writeBuffer.put(bytes); writeBuffer.flip(); sc.write(writeBuffer); System.out.println(&quot;fefe&quot;); &#125;&#125;public class NioServerHandle implements Runnable&#123; private Selector selector; private ServerSocketChannel serverChannel; private volatile boolean started; /** * 构造方法 * @param port 指定要监听的端口号 */ public NioServerHandle(int port) &#123; try&#123; //创建选择器 selector = Selector.open(); //打开监听通道 serverChannel = ServerSocketChannel.open(); //如果为 true，则此通道将被置于阻塞模式； // 如果为 false，则此通道将被置于非阻塞模式 serverChannel.configureBlocking(false);//开启非阻塞模式 serverChannel.socket().bind(new InetSocketAddress(port)); serverChannel.register(selector，SelectionKey.OP_ACCEPT); //标记服务器已开启 started = true; System.out.println(&quot;服务器已启动，端口号：&quot; + port); &#125;catch(IOException e)&#123; e.printStackTrace(); System.exit(1); &#125; &#125; public void stop()&#123; started = false; &#125; @Override public void run() &#123; //循环遍历selector while(started)&#123; try&#123; //阻塞，只有当至少一个注册的事件发生的时候才会继续. selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; it = keys.iterator(); SelectionKey key = null; while(it.hasNext())&#123; key = it.next(); it.remove(); try&#123; handleInput(key); &#125; catch(Exception e)&#123; if(key != null)&#123; key.cancel(); if(key.channel() != null)&#123; key.channel().close(); &#125; &#125; &#125; &#125; &#125;catch(Throwable t)&#123; t.printStackTrace(); &#125; &#125; //selector关闭后会自动释放里面管理的资源 if(selector != null) try&#123; selector.close(); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; private void handleInput(SelectionKey key) throws IOException&#123; if(key.isValid())&#123; //处理新接入的请求消息 if(key.isAcceptable())&#123; ServerSocketChannel ssc = (ServerSocketChannel)key.channel(); SocketChannel sc = ssc.accept(); System.out.println(&quot;=======建立连接===&quot;); sc.configureBlocking(false); sc.register(selector，SelectionKey.OP_READ); &#125; //读消息 if(key.isReadable())&#123; System.out.println(&quot;======socket channel 数据准备完成，&quot; + &quot;可以去读==读取=======&quot;); SocketChannel sc = (SocketChannel) key.channel(); //创建ByteBuffer，并开辟一个1M的缓冲区 ByteBuffer buffer = ByteBuffer.allocate(1024); //读取请求码流，返回读取到的字节数 int readBytes = sc.read(buffer); //读取到字节，对字节进行编解码 if(readBytes&gt;0)&#123; //将缓冲区当前的limit设置为position，position=0， // 用于后续对缓冲区的读取操作 buffer.flip(); //根据缓冲区可读字节数创建字节数组 byte[] bytes = new byte[buffer.remaining()]; //将缓冲区可读字节数组复制到新建的数组中 buffer.get(bytes); String message = new String(bytes，&quot;UTF-8&quot;); System.out.println(&quot;服务器收到消息：&quot; + message); //处理数据 String result = response(message) ; //发送应答消息 doWrite(sc，result); &#125; //链路已经关闭，释放资源 else if(readBytes&lt;0)&#123; key.cancel(); sc.close(); &#125; &#125; &#125; &#125; //发送应答消息 private void doWrite(SocketChannel channel，String response) throws IOException &#123; //将消息编码为字节数组 byte[] bytes = response.getBytes(); //根据数组容量创建ByteBuffer ByteBuffer writeBuffer = ByteBuffer.allocate(bytes.length); //将字节数组复制到缓冲区 writeBuffer.put(bytes); //flip操作 writeBuffer.flip(); //发送缓冲区的字节数组 channel.write(writeBuffer); &#125;&#125; client 与 多个client 聊天例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439public abstract class SocketNio3 implements Runnable &#123; final static String EMPTY_STR = &quot;&quot;; volatile boolean started; boolean running = true; void stop() &#123; started = false; &#125; public abstract void sendMsg(String msg) throws IOException; public void start() throws IOException &#123; new Thread(this).start(); Scanner scanner = new Scanner(System.in); while (running) &#123; sendMsg(scanner.next()); &#125; &#125; abstract void handleIO(SelectionKey key) throws IOException; void doWrite(SocketChannel sc， Serializable msg) throws IOException &#123; ObjectOutputStream outputStream = null; try &#123; ByteArrayOutputStream bytes = new ByteArrayOutputStream(); outputStream = new ObjectOutputStream(bytes); outputStream.writeObject(msg); byte[] bs = bytes.toByteArray(); ByteBuffer buffer = ByteBuffer.allocate(bs.length); buffer.put(bs); buffer.flip(); sc.write(buffer); &#125; finally &#123; if (outputStream != null) &#123; outputStream.close(); &#125; &#125; &#125; public void close(Selector selector) &#123; if (selector != null) &#123; try &#123; selector.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private void doSelect0(Selector selector) throws IOException &#123; while (started) &#123; //堵塞 selector.select(); //需要处理IO的链接 if (!selector.isOpen()) &#123; break; &#125; Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); SelectionKey key; while (iterator.hasNext()) &#123; key = iterator.next(); //防止下一次又轮训到 iterator.remove(); try &#123; handleIO(key); &#125; catch (Exception e) &#123; key.cancel(); if (key.channel() != null) &#123; key.channel().close(); &#125; &#125; &#125; &#125; if (selector != null) &#123; selector.close(); &#125; running = false; &#125; void doSelect(Selector selector) &#123; try &#123; doSelect0(selector); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; close(selector); &#125; static class ClientHandler extends SocketNio3 &#123; Selector selector; SocketChannel socketChannel; final InetSocketAddress inetSocketAddress; final String name; List&lt;String&gt; recipients; //重发的消息 Message msg; public ClientHandler(String url， int port， String name) throws IOException &#123; inetSocketAddress = new InetSocketAddress(url， port); selector = Selector.open(); this.started = true; this.name = name; &#125; public ClientHandler(String url， int port， String name， Message msg) throws IOException &#123; this(url， port， name); this.msg = msg; &#125; private boolean doConnect() throws IOException &#123; socketChannel = SocketChannel.open(); socketChannel.configureBlocking(false); if (socketChannel.connect(inetSocketAddress)) &#123; socketChannel.register(selector， SelectionKey.OP_READ); sendMsg(&quot;@user&quot;); System.out.println(&quot;客户端链接完成&quot;); return true; &#125; else &#123; socketChannel.register(selector， SelectionKey.OP_CONNECT); return false; &#125; &#125; @Override public void sendMsg(String msg) throws IOException &#123; if (msg.startsWith(&quot;@name:&quot;)) &#123; recipients = Arrays.asList(msg.replace(&quot;@name:&quot;， &quot;&quot;).split(&quot;，&quot;)); System.out.println(&quot;设置接收者完成&quot;); &#125; else if (&quot;@user&quot;.equals(msg)) &#123; doWrite(socketChannel， new ReMessage(name， null)); &#125; else &#123; if (recipients == null || recipients.isEmpty()) &#123; System.out.println(&quot;先选这发送给谁，格式如@name:userName，userName&quot;); &#125; else &#123; doWrite0(new Message(recipients， msg， name)); &#125; &#125; &#125; private void doWrite0(Message msg) throws IOException &#123; if (!socketChannel.isConnected()) &#123; stop(); running = false; this.msg = msg; &#125; else &#123; doWrite(socketChannel， msg); &#125; &#125; @Override public void run() &#123; try &#123; doConnect(); &#125; catch (Exception e) &#123; e.printStackTrace(); stop(); &#125; doSelect(selector); &#125; public Message getMsg() &#123; return msg; &#125; @Override void handleIO(SelectionKey key) throws IOException &#123; if (key.isValid()) &#123; SocketChannel sc = (SocketChannel)key.channel(); if (key.isConnectable()) &#123; if (sc.finishConnect()) &#123; sc.register(selector， SelectionKey.OP_READ); if (msg != null) &#123; doWrite(sc， new ReMessage(name， msg)); recipients = msg.recipients; System.out.println(&quot;设置接收者完成&quot;); msg = null; &#125; else &#123; sendMsg(&quot;@user&quot;); System.out.println(&quot;客户端链接完成&quot;); &#125; &#125; &#125; if (key.isReadable()) &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); int isReadable = sc.read(buffer); if (isReadable &gt; 0) &#123; buffer.flip(); //读模式 byte[] bytes = new byte[buffer.remaining()]; buffer.get(bytes); parse(bytes); &#125; else if (isReadable &lt; 0) &#123; key.cancel(); sc.close(); &#125; &#125; &#125; &#125; void parse(byte[] bytes) throws IOException &#123; ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes); ObjectInputStream inputStream = null; try &#123; inputStream = new ObjectInputStream(byteArrayInputStream); Message message = (Message) inputStream.readObject(); System.out.println(name + &quot; &quot; + message.timestamp + &quot;接收到 &quot; + message.user + &quot;的消息：&quot;+ message.msg); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (inputStream != null) &#123; inputStream.close(); &#125; &#125; &#125; &#125; static class Client &#123; private static ClientHandler handler; private String url; private int port; String name; public Client(String url， int port， String name) throws IOException &#123; this.url = url; this.port = port; this.name = name; handler = new ClientHandler(url， port， name); &#125; public void start() throws IOException &#123; handler.start(); System.out.println(&quot;重启&quot;); handler = new ClientHandler(url， port， name， handler.getMsg()); start(); &#125; public static void main(String[] args) throws IOException &#123; Client client = new Client(&quot;127.0.0.1&quot;，12345， &quot;xyz&quot;); client.start(); &#125; &#125; static class Client1 extends Client &#123; public Client1(String url， int port， String name) throws IOException &#123; super(url， port， name); &#125; public static void main(String[] args) throws IOException &#123; Client1 client1 = new Client1(&quot;127.0.0.1&quot;，12345， &quot;ljj&quot;); client1.start(); &#125; &#125; static class Server extends SocketNio3 &#123; final Selector selector; final ServerSocketChannel serverSocketChannel; final Map&lt;String， SocketChannel&gt; socketChannelMap; final Map&lt;SocketChannel， String&gt; nameMap; public Server(int port) throws IOException &#123; selector = Selector.open(); serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.socket().bind(new InetSocketAddress(port)); serverSocketChannel.register(selector， SelectionKey.OP_ACCEPT); this.started = true; socketChannelMap = new ConcurrentHashMap&lt;&gt;(); nameMap = new ConcurrentHashMap&lt;&gt;(); System.out.println(&quot;服务器已启动，端口号：&quot; + port); &#125; @Override public void sendMsg(String msg) throws IOException &#123; if (msg == null || &quot;&quot;.equals(msg)) &#123; &#125; else if (&quot;@close&quot;.equals(msg)) &#123; selector.close(); socketChannelMap.clear(); nameMap.clear(); stop(); &#125; else if (&quot;@show-connect&quot;.equals(msg)) &#123; System.out.println(&quot;已经链接的有:&quot; + Arrays.toString(socketChannelMap.keySet().toArray())); &#125; else if (msg.startsWith(&quot;@close:&quot;)) &#123; String name = msg.replace(&quot;@close:&quot;，&quot;&quot;); SocketChannel sc = socketChannelMap.get(name); if (sc == null) &#123; System.out.println(name + &quot;还没链接服务器&quot;); &#125; else &#123; sc.close(); socketChannelMap.remove(name); nameMap.remove(sc); &#125; &#125; &#125; @Override void handleIO(SelectionKey key) throws IOException &#123; if (key.isValid()) &#123; if (key.isAcceptable()) &#123; ServerSocketChannel ssc = (ServerSocketChannel)key.channel(); SocketChannel socketChannel = ssc.accept(); socketChannel.configureBlocking(false); socketChannel.register(selector， SelectionKey.OP_READ); &#125; else if (key.isReadable()) &#123; SocketChannel socketChannel = (SocketChannel)key.channel(); if (!socketChannel.isConnected()) &#123; key.cancel(); socketChannel.close(); String name = nameMap.remove(socketChannel); if (name != null) &#123; socketChannelMap.remove(name); &#125; &#125; else &#123; //2k ByteBuffer buffer = ByteBuffer.allocate(2 * 1024); int readBytes = socketChannel.read(buffer); if (readBytes &gt; 0) &#123; buffer.flip(); byte[] bytes = new byte[buffer.remaining()]; buffer.get(bytes); handleBytes(socketChannel， bytes); &#125; else if (readBytes &lt; 0) &#123; //链接关闭时触发 key.cancel(); socketChannel.close(); socketChannelMap.remove(nameMap.remove(socketChannel)); &#125; &#125; &#125; &#125; &#125; private void handleBytes(SocketChannel socketChannel， byte[] bytes) &#123; ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes); try (ObjectInputStream inputStream = new ObjectInputStream(byteArrayInputStream)) &#123; Object obj = inputStream.readObject(); if (obj instanceof Message) &#123; sendToUser((Message) obj); &#125; else if (obj instanceof ReMessage) &#123; ReMessage message = (ReMessage) obj; bindUser(socketChannel， message.user); sendToUser(message.message); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; private void bindUser(SocketChannel socketChannel， String user) &#123; socketChannelMap.put(user， socketChannel); nameMap.put(socketChannel， user); System.out.println(user + &quot;链接到服务器&quot;); &#125; private void sendToUser(Message message) &#123; if (message != null) &#123; List&lt;String&gt; recipients = message.recipients; message.recipients = null; if (recipients != null &amp;&amp; !recipients.isEmpty()) &#123; recipients.forEach(recipient -&gt; &#123; if (!recipient.equals(message.user)) &#123; SocketChannel sc = socketChannelMap.get(recipient); if (sc != null) &#123; if (sc.isConnected()) &#123; try &#123; doWrite(sc， message); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; socketChannelMap.remove(recipient); nameMap.remove(sc); &#125; &#125; &#125; &#125;); &#125; &#125; &#125; @Override public void run() &#123; doSelect(selector); &#125; public static void main(String[] args) throws IOException &#123; Server server = new Server(12345); server.start(); &#125; &#125;&#125;public class Message implements Serializable &#123; private static final long serialVersionUID = -5013727463902301586L; List&lt;String&gt; recipients; String msg; String user; long timestamp = System.currentTimeMillis(); public Message(List&lt;String&gt; recipients， String msg， String user) &#123; this.recipients = recipients; this.msg = msg; this.user = user; &#125; @Override public String toString() &#123; return &quot;Message&#123;&quot; + &quot;msg=&#x27;&quot; + msg + &#x27;\\&#x27;&#x27; + &quot;， user=&#x27;&quot; + user + &#x27;\\&#x27;&#x27; + &quot;， timestamp=&quot; + timestamp + &#x27;&#125;&#x27;; &#125;&#125;public class ReMessage implements Serializable &#123; private static final long serialVersionUID = 7379677726256980320L; String user; Message message; public ReMessage(String user， Message message) &#123; this.user = user; this.message = message; &#125;&#125; java NIO 说明12345selector = Selector.open();serverSocketChannel = ServerSocketChannel.open();serverSocketChannel.configureBlocking(false);serverSocketChannel.socket().bind(new InetSocketAddress(port));serverSocketChannel.register(selector， SelectionKey.OP_ACCEPT); Selector 对象是通过调用静态工厂方法 open()来实例化的，如下: 1Selector Selector=Selector.open(); 要实现 Selector 管理 Channel，需要将 channel 注册到相应的 Selector 上，如下: 12channel.configureBlocking(false);SelectionKey key= channel.register(selector,SelectionKey,OP_READ); 通过调用通道的 register()方法会将它注册到一个选择器上。与 Selector 一起使用时， Channel 必须处于非阻塞模式下，否则将抛出 IllegalBlockingModeException 异常，这意 味着不能将 FileChannel 与 Selector 一起使用，因为 FileChannel 不能切换到非阻塞模式， 而套接字通道都可以。另外通道一旦被注册，将不能再回到阻塞状态，此时若调用通道的 configureBlocking(true)将抛出 BlockingModeException 异常。 register()方法的第二个参数是“interest 集合”，表示选择器所关心的通道操作， 它实际上是一个表示选择器在检查通道就绪状态时需要关心的操作的比特掩码。比如一个选 择器对通道的 read 和 write 操作感兴趣，那么选择器在检查该通道时，只会检查通道的 read 和 write 操作是否已经处在就绪状态。 如果 Selector 对通道的多操作类型感兴趣，可以用“位或”操作符来实现: 1int interestSet=SelectionKey.OP_READ|SelectionKey.OP_WRITE; 同时 一个 Channel 仅仅可以被注册到一个 Selector 一次, 如果将 Channel 注册 到 Selector 多次, 那么其实就是相当于更新 SelectionKey 的 interest set。 通过 SelectionKey 可以判断 Selector 是否对 Channel 的某种事件感兴趣，比如: 12int interestSet = selectionKey.interestOps();boolean isInterestedInAccept = (interestSet &amp; SelectionKey.OP_ACCEPT) == SelectionKey.OP_ACCEPT; 通过 SelctionKey 对象的 readyOps()来获取相关通道已经就绪的操作。它是 interest 集合的子集，并且表示了 interest 集合中从上次调用 select()以后已经就绪的那些操作。 JAVA 中定义几个方法用来检查这些操作是否就绪，比如 selectionKey.isAcceptable(); 同时，通过SelectionKey可以取出这个SelectionKey所关联的Selector和Channel。 如果我们要取消关联关系，怎么办?SelectionKey 对象的 cancel()方法来取消特定的 注册关系。 在实际的应用中，我们还可以为 SelectionKey 绑定附加对象，在需要的时候取出。 123SelectionKey key=channel.register(selector,SelectionKey.OP_READ,theObject);或selectionKey.attach(theObject); 取出这个附加对象，通过: 1Object attachedObj = key.attachment(); 在实际运行中，我们通过 Selector 的 select()方法可以选择已经准备就绪的通道(这些通道包含你感兴趣的的事件)。 下面是 Selector 几个重载的 select()方法: select():阻塞到至少有一个通道在你注册的事件上就绪了 select(long timeout):和 select()一样，但最长阻塞事件为 timeout 毫秒。 selectNow():非阻塞，立刻返回。 select()方法返回的 int 值表示有多少通道已经就绪,是自上次调用 select()方法后有 多少通道变成就绪状态。 一旦调用 select()方法，并且返回值不为 0 时，则可以通过调用 Selector 的 selectedKeys()方法来访问已选择键集合。 1Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); 这个时候，循环遍历 selectedKeys 集中的每个键，并检测各个键所对应的通道的就绪 事件，再通过 SelectionKey 关联的 Selector 和 Channel 进行实际的业务处理。 注意每次迭代末尾的 keyIterator.remove()调用。Selector 不会自己从已选择键集中 移除 SelectionKey 实例。必须在处理完通道时自己移除，否则的话，下次该通道变成就绪 时，Selector 会再次将其放入已选择键集中。 原生JDK网络编程 - NIO之Reactor模式反应”即“倒置”，“控制逆转”，具体事件处理程序不调用反应器，而向反应器注册一个事件处理器，表示自己对某些事件感兴趣，具体事件处理程序通过事件处理器对某个指定的事件发生做出反应；这种控制逆转又称为“好莱坞法则”（不要调用我，让我来调用你） 单线程Reactor模式流程： 服务器端的Reactor是一个线程对象，该线程会启动事件循环，并使用Selector(选择器)来实现IO的多路复用。注册一个Acceptor事件处理器到Reactor中，Acceptor事件处理器所关注的事件是ACCEPT事件，这样Reactor会监听客户端向服务器端发起的连接请求事件(ACCEPT事件)。 客户端向服务器端发起一个连接请求，Reactor监听到了该ACCEPT事件的发生并将该ACCEPT事件派发给相应的Acceptor处理器来进行处理。Acceptor处理器通过accept()方法得到与这个客户端对应的连接(SocketChannel)，然后将该连接所关注的READ事件以及对应的READ事件处理器注册到Reactor中，这样一来Reactor就会监听该连接的READ事件了。 当Reactor监听到有读或者写事件发生时，将相关的事件派发给对应的处理器进行处理。比如，读处理器会通过SocketChannel的read()方法读取数据，此时read()操作可以直接读取到数据，而不会堵塞与等待可读的数据到来。 每当处理完所有就绪的感兴趣的I&#x2F;O事件后，Reactor线程会再次执行select()阻塞等待新的事件就绪并将其分派给对应处理器进行处理。 注意，Reactor的单线程模式的单线程主要是针对于I&#x2F;O操作而言，也就是所有的I&#x2F;O的accept()、read()、write()以及connect()操作都在一个线程上完成的。 上面的例子就是典型的单线程Reactor模式。 目前的单线程Reactor模式中，不仅I&#x2F;O操作在该Reactor线程上，连非I&#x2F;O的业务操作也在该线程上进行处理了，这可能会大大延迟I&#x2F;O请求的响应。所以我们应该将非I&#x2F;O的业务逻辑操作从Reactor线程上卸载，以此来加速Reactor线程对I&#x2F;O请求的响应。 单线程Reactor，工作者线程池与单线程Reactor模式不同的是，添加了一个工作者线程池，并将非I&#x2F;O操作从Reactor线程中移出转交给工作者线程池来执行。这样能够提高Reactor线程的I&#x2F;O响应，不至于因为一些耗时的业务逻辑而延迟对后面I&#x2F;O请求的处理。 使用线程池的优势: 通过重用现有的线程而不是创建新线程，可以在处理多个请求时分摊在线程创建和 销毁过程产生的巨大开销。 另一个额外的好处是，当请求到达时，工作线程通常已经存在，因此不会由于等待 创建线程而延迟任务的执行，从而提高了响应性。 通过适当调整线程池的大小，可以创建足够多的线程以便使处理器保持忙碌状态。 同时还可以防止过多线程相互竞争资源而使应用程序耗尽内存或失败。 改进的版本中，所以的I&#x2F;O操作依旧由一个Reactor来完成，包括I&#x2F;O的accept()、read()、write()以及connect()操作。 对于一些小容量应用场景，可以使用单线程模型。但是对于高负载、大并发或大数据量 的应用场景却不合适，主要原因如下: 一个 NIO 线程同时处理成百上千的链路，性能上无法支撑，即便 NIO 线程的 CPU 负 荷达到 100%，也无法满足海量消息的读取和发送; 当 NIO 线程负载过重之后，处理速度将变慢，这会导致大量客户端连接超时，超时 之后往往会进行重发，这更加重了 NIO 线程的负载，最终会导致大量消息积压和处理超时， 成为系统的性能瓶颈; 多Reactor线程模式（用java NIO实现下）Reactor线程池中的每一Reactor线程都会有自己的Selector、线程和分发的事件循环逻辑。 mainReactor可以只有一个，但subReactor一般会有多个。mainReactor线程主要负责接收客户端的连接请求，然后将接收到的SocketChannel传递给subReactor，由subReactor来完成和客户端的通信。 流程： 注册一个Acceptor事件处理器到mainReactor中，Acceptor事件处理器所关注的事件是ACCEPT事件，这样mainReactor会监听客户端向服务器端发起的连接请求事件(ACCEPT事件)。启动mainReactor的事件循环。 客户端向服务器端发起一个连接请求，mainReactor监听到了该ACCEPT事件并将该ACCEPT事件派发给Acceptor处理器来进行处理。Acceptor处理器通过accept()方法得到与这个客户端对应的连接(SocketChannel)，然后将这个SocketChannel传递给subReactor线程池。 subReactor线程池分配一个subReactor线程给这个SocketChannel，即，将SocketChannel关注的READ事件以及对应的READ事件处理器注册到subReactor线程中。当然你也注册WRITE事件以及WRITE事件处理器到subReactor线程中以完成I&#x2F;O写操作。Reactor线程池中的每一Reactor线程都会有自己的Selector、线程和事件循环逻辑。 当有I&#x2F;O事件就绪时，相关的subReactor就将事件派发给响应的处理器处理。注意，这里subReactor线程只负责完成I&#x2F;O的read()操作，在读取到数据后将业务逻辑的处理放入到线程池中完成，若完成业务逻辑后需要返回数据给客户端，则相关的I&#x2F;O的write操作还是会被提交回subReactor线程来完成。 注意，所以的I&#x2F;O操作(包括，I&#x2F;O的accept()、read()、write()以及connect()操作)依旧还是在Reactor线程(mainReactor线程 或 subReactor线程)中完成的。Thread Pool(线程池)仅用来处理非I&#x2F;O操作的逻辑。 多Reactor线程模式将“接受客户端的连接请求”和“与该客户端的通信”分在了两个Reactor线程来完成。mainReactor完成接收客户端连接请求的操作，它不负责与客户端的通信，而是将建立好的连接转交给subReactor线程来完成与客户端的通信，这样一来就不会因为read()数据量太大而导致后面的客户端连接请求得不到即时处理的情况。并且多Reactor线程模式在海量的客户端并发请求的情况下，还可以通过实现subReactor线程池来将海量的连接分发给多个subReactor线程，在多核的操作系统中这能大大提升应用的负载和吞吐量。 Netty服务端就使用了“多Reactor线程模式” 1234567891011121314151617181920212223public class SelectorDispatcher &#123; private SelectorProvider selectorProvider = SelectorProvider.provider(); volatile boolean flag; private Selector[] selectors; public SelectorDispatcher() throws IOException &#123; selectors = new Selector[2]; for (int i=0; i&lt;2; i++) &#123; selectors[i] = selectorProvider.openSelector(); &#125; &#125; public Selector getAcceptSelector() &#123; return selectors[0]; &#125; public Selector getReadSelector() &#123; return selectors[1]; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990public abstract class AbstractSocketNio &#123; final static String EMPTY_STR = &quot;&quot;; volatile boolean started; boolean running = true; void stop() &#123; started = false; &#125; public abstract void sendMsg(String msg) throws IOException; abstract void handleIO(SelectionKey key) throws IOException; public void start() throws IOException &#123; new Thread(this).start(); Scanner scanner = new Scanner(System.in); while (running) &#123; sendMsg(scanner.next()); &#125; &#125; void doWrite(SocketChannel sc, Serializable msg) throws IOException &#123; ObjectOutputStream outputStream = null; try &#123; ByteArrayOutputStream bytes = new ByteArrayOutputStream(); outputStream = new ObjectOutputStream(bytes); outputStream.writeObject(msg); byte[] bs = bytes.toByteArray(); ByteBuffer buffer = ByteBuffer.allocate(bs.length); buffer.put(bs); buffer.flip(); sc.write(buffer); &#125; finally &#123; if (outputStream != null) &#123; outputStream.close(); &#125; &#125; &#125; public void close(Selector selector) &#123; if (selector != null) &#123; try &#123; selector.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private void doSelect0(Selector selector) throws IOException &#123; while (started) &#123; //堵塞 selector.select(); //需要处理IO的链接 if (!selector.isOpen()) &#123; break; &#125; Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); SelectionKey key; while (iterator.hasNext()) &#123; key = iterator.next(); //防止下一次又轮训到 iterator.remove(); try &#123; handleIO(key); &#125; catch (Exception e) &#123; key.cancel(); if (key.channel() != null) &#123; key.channel().close(); &#125; &#125; &#125; &#125; if (selector != null) &#123; selector.close(); &#125; running = false; &#125; void doSelect(Selector selector) &#123; try &#123; doSelect0(selector); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; close(selector); &#125;&#125; AcceptSocketChannelHandle: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081static class AcceptSocketChannelHandle extends AbstractSocketNio &#123; private SelectorDispatcher dispatcher; final Selector selector; //只负责接受链接，不做其他IO操作 final ServerSocketChannel serverSocketChannel; public AcceptSocketChannelHandle(int port, SelectorDispatcher dispatcher) throws IOException &#123; this.dispatcher = dispatcher; //获取Accept Selector selector = dispatcher.getAcceptSelector(); serverSocketChannel = SelectorProvider.provider().openServerSocketChannel(); serverSocketChannel.configureBlocking(false); serverSocketChannel.socket().bind(new InetSocketAddress(port)); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); this.started = true; System.out.println(&quot;服务器已启动，端口号：&quot; + port); &#125; @Override public void start() throws IOException &#123; new Thread(this).start(); &#125; @Override public void sendMsg(String msg) throws IOException &#123; &#125; @Override void handleIO(SelectionKey key) throws IOException &#123; if (key.isValid() &amp;&amp; key.isAcceptable()) &#123; ServerSocketChannel ssc = (ServerSocketChannel)key.channel(); SocketChannel socketChannel = ssc.accept(); socketChannel.configureBlocking(false); //将接收的 SocketChannel 注册到另一个selector，并告诉该selector，它关注读事件 socketChannel.register(dispatcher.getReadSelector(), SelectionKey.OP_READ); dispatcher.flag = true; &#125; &#125; @Override public void run() &#123; try &#123; while (started) &#123; //堵塞 selector.select(); //需要处理IO的链接 if (!selector.isOpen()) &#123; break; &#125; Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); SelectionKey key; while (iterator.hasNext()) &#123; key = iterator.next(); //防止下一次又轮训到 iterator.remove(); try &#123; handleIO(key); &#125; catch (Exception e) &#123; key.cancel(); if (key.channel() != null) &#123; key.channel().close(); &#125; &#125; &#125; &#125; if (selector != null) &#123; selector.close(); &#125; running = false; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; close(selector); &#125; public void stop0() throws IOException &#123; super.stop(); serverSocketChannel.close(); &#125;&#125; ServerHandler 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160public class ServerHandle extends AbstractSocketNio &#123; final Selector selector; final Map&lt;String, SocketChannel&gt; socketChannelMap; final Map&lt;SocketChannel, String&gt; nameMap; SelectorDispatcher dispatcher; public ServerHandle(SelectorDispatcher dispatcher) throws IOException &#123; this.dispatcher = dispatcher; //获取读写 Selector，现在是只用一个selector，也就是一个线程来处理读写事件，之后可以创建多个selector，用多个线程来处理 //在accept 的线程中可以通过某个策略（轮询）来分配到不同的现场 selector = dispatcher.getReadSelector(); this.started = true; socketChannelMap = new ConcurrentHashMap&lt;&gt;(); nameMap = new ConcurrentHashMap&lt;&gt;(); &#125; @Override public void sendMsg(String msg) throws IOException &#123; if (msg == null || &quot;&quot;.equals(msg)) &#123; &#125; else if (&quot;@close&quot;.equals(msg)) &#123; stop(); selector.close(); socketChannelMap.clear(); nameMap.clear(); &#125; else if (&quot;@show-connect&quot;.equals(msg)) &#123; System.out.println(&quot;已经链接的有:&quot; + Arrays.toString(socketChannelMap.keySet().toArray())); &#125; else if (msg.startsWith(&quot;@close:&quot;)) &#123; String name = msg.replace(&quot;@close:&quot;,&quot;&quot;); SocketChannel sc = socketChannelMap.get(name); if (sc == null) &#123; System.out.println(name + &quot;还没链接服务器&quot;); &#125; else &#123; sc.close(); socketChannelMap.remove(name); nameMap.remove(sc); &#125; &#125; &#125; @Override void handleIO(SelectionKey key) throws IOException &#123; if (key.isValid()) &#123; if (key.isReadable()) &#123; SocketChannel socketChannel = (SocketChannel)key.channel(); if (!socketChannel.isConnected()) &#123; key.cancel(); socketChannel.close(); String name = nameMap.remove(socketChannel); if (name != null) &#123; socketChannelMap.remove(name); &#125; &#125; else &#123; //2k ByteBuffer buffer = ByteBuffer.allocate(2 * 1024); int readBytes = socketChannel.read(buffer); if (readBytes &gt; 0) &#123; buffer.flip(); byte[] bytes = new byte[buffer.remaining()]; buffer.get(bytes); handleBytes(socketChannel, bytes); &#125; else if (readBytes &lt; 0) &#123; //链接关闭时触发 key.cancel(); socketChannel.close(); socketChannelMap.remove(nameMap.remove(socketChannel)); &#125; &#125; &#125; &#125; &#125; private void handleBytes(SocketChannel socketChannel, byte[] bytes) &#123; ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes); try (ObjectInputStream inputStream = new ObjectInputStream(byteArrayInputStream)) &#123; Object obj = inputStream.readObject(); if (obj instanceof Message) &#123; sendToUser((Message) obj); &#125; else if (obj instanceof ReMessage) &#123; ReMessage message = (ReMessage) obj; bindUser(socketChannel, message.user); sendToUser(message.message); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; private void bindUser(SocketChannel socketChannel, String user) &#123; socketChannelMap.put(user, socketChannel); nameMap.put(socketChannel, user); System.out.println(user + &quot;链接到服务器&quot;); &#125; private void sendToUser(Message message) &#123; if (message != null) &#123; List&lt;String&gt; recipients = message.recipients; message.recipients = null; if (recipients != null &amp;&amp; !recipients.isEmpty()) &#123; recipients.forEach(recipient -&gt; &#123; if (&quot;@server&quot;.equals(recipient)) &#123; System.out.println(message.toString()); &#125; else if (!recipient.equals(message.user)) &#123; SocketChannel sc = socketChannelMap.get(recipient); if (sc != null) &#123; if (sc.isConnected()) &#123; try &#123; doWrite(sc, message); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; socketChannelMap.remove(recipient); nameMap.remove(sc); &#125; &#125; &#125; &#125;); &#125; &#125; &#125; @Override public void run() &#123; try &#123; while (started) &#123; //堵塞 selector.select(10000); //需要处理IO的链接 if (!selector.isOpen()) &#123; break; &#125; Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); SelectionKey key; while (iterator.hasNext()) &#123; key = iterator.next(); //防止下一次又轮训到 iterator.remove(); try &#123; handleIO(key); &#125; catch (Exception e) &#123; key.cancel(); if (key.channel() != null) &#123; key.channel().close(); &#125; &#125; &#125; &#125; if (selector != null) &#123; selector.close(); &#125; running = false; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; close(selector); &#125;&#125; main 12345678910111213141516171819202122public class Server &#123; private final AcceptSocketChannelHandle acceptHandler; private final ServerHandle handler; public Server(int port) throws IOException &#123; SelectorDispatcher sd = new SelectorDispatcher(); this.handler = new ServerHandle(sd); this.acceptHandler = new AcceptSocketChannelHandle(port, sd); &#125; public void start() throws IOException &#123; acceptHandler.start(); handler.start(); acceptHandler.stop0(); &#125; public static void main(String[] args) throws IOException &#123; Server server = new Server(12345); server.start(); &#125;&#125;","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"javaNIO","slug":"javaNIO","permalink":"http://sv.pointcut.cc/tags/javaNIO/"}]},{"title":"02原生JDK网络编程BIO","slug":"java_nio/02原生JDK网络编程BIO ","date":"2021-11-22T12:00:02.000Z","updated":"2022-03-23T09:03:58.082Z","comments":true,"path":"blog/java_nio/02原生JDK网络编程BIO /","link":"","permalink":"http://sv.pointcut.cc/blog/java_nio/02%E5%8E%9F%E7%94%9FJDK%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BBIO%20/","excerpt":"","text":"传统的同步阻塞模型开发中，ServerSocket负责绑定IP地址，启动监听端口；Socket负责发起连接操作。连接成功后，双方通过输入和输出流进行同步阻塞式通信。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public class ServerPool &#123; private static ExecutorService executorService = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors()); public static void main(String[] args) throws IOException &#123; //服务端启动必备 ServerSocket serverSocket = new ServerSocket(); //表示服务端在哪个端口上监听 serverSocket.bind(new InetSocketAddress(10001)); System.out.println(&quot;Start Server ....&quot;); try&#123; while(true)&#123; executorService.execute(new ServerTask(serverSocket.accept())); &#125; &#125;finally &#123; serverSocket.close(); &#125; &#125; //每个和客户端的通信都会打包成一个任务，交个一个线程来执行 private static class ServerTask implements Runnable&#123; private Socket socket = null; public ServerTask(Socket socket)&#123; this.socket = socket; &#125; @Override public void run() &#123; //实例化与客户端通信的输入输出流 try(ObjectInputStream inputStream = new ObjectInputStream(socket.getInputStream()); ObjectOutputStream outputStream = new ObjectOutputStream(socket.getOutputStream()))&#123; //接收客户端的输出，也就是服务器的输入 String userName = inputStream.readUTF(); System.out.println(&quot;Accept client message:&quot;+userName); //服务器的输出，也就是客户端的输入 outputStream.writeUTF(&quot;Hello,&quot;+userName); outputStream.flush(); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally &#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125;public class Client &#123; public static void main(String[] args) throws IOException &#123; //客户端启动必备 Socket socket = null; //实例化与服务端通信的输入输出流 ObjectOutputStream output = null; ObjectInputStream input = null; //服务器的通信地址 InetSocketAddress addr = new InetSocketAddress(&quot;127.0.0.1&quot;,12345); try&#123; socket = new Socket(); /*连接服务器*/ socket.connect(addr); output = new ObjectOutputStream(socket.getOutputStream()); input = new ObjectInputStream(socket.getInputStream()); /*向服务器输出请求*/ output.writeUTF(&quot;Mark&quot;); output.flush(); //接收服务器的输出 System.out.println(input.readUTF()); &#125;finally&#123; if (socket!=null) socket.close(); if (output!=null) output.close(); if (input!=null) input.close(); &#125; &#125;&#125; 传统BIO通信模型：采用BIO通信模型的服务端，通常由一个独立的Acceptor线程负责监听客户端的连接，它接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理，处理完成后，通过输出流返回应答给客户端，线程销毁。即典型的一请求一应答模型。 该模型最大的问题就是缺乏弹性伸缩能力，当客户端并发访问量增加后，服务端的线程个数和客户端并发访问数呈1:1的正比关系，Java中的线程也是比较宝贵的系统资源，线程数量快速膨胀后，系统的性能将急剧下降，随着访问量的继续增大，系统最终就死掉了。 为了改进这种一连接一线程的模型，我们可以使用线程池来管理这些线程，实现1个或多个线程处理N个客户端的模型（但是底层还是使用的同步阻塞I&#x2F;O），通常被称为“伪异步I&#x2F;O模型“。 但是，正因为限制了线程数量，如果发生读取数据较慢时（比如数据量大、网络传输慢等），大量并发的情况下，其他接入的消息，只能一直等待，这就是最大的弊端。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"javaNIO","slug":"javaNIO","permalink":"http://sv.pointcut.cc/tags/javaNIO/"}]},{"title":"01五种IO模型","slug":"java_nio/01五种IO模型","date":"2021-11-22T12:00:01.000Z","updated":"2022-03-23T09:03:58.081Z","comments":true,"path":"blog/java_nio/01五种IO模型/","link":"","permalink":"http://sv.pointcut.cc/blog/java_nio/01%E4%BA%94%E7%A7%8DIO%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"5个I&#x2F;O模型的比较不同I&#x2F;O模型的区别，其实主要在等待数据和数据复制这两个时间段不同，图形中已经表示得很清楚了。 阻塞I&#x2F;O模型应用程序调用一个IO函数，导致应用程序阻塞，等待数据准备好。 如果数据没有准备好，一直等待….数据准备好了，从内核拷贝到用户空间,IO函数返回成功指示。 典型应用：阻塞socket、Java BIO 非阻塞IO模型我们把一个SOCKET接口设置为非阻塞就是告诉内核，当所请求的I&#x2F;O操作无法完成时，不要将进程睡眠，而是返回一个错误。这样我们的I&#x2F;O操作函数将不断的测试数据是否已经准备好，如果没有准备好，继续测试，直到数据准备好为止。在这个不断测试的过程中，会大量的占用CPU的时间。上述模型绝不被推荐。 典型应用：socket是非阻塞的方式（设置为NONBLOCK） 信号驱动IO首先我们允许套接口进行信号驱动I&#x2F;O,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I&#x2F;O操作函数处理数据。 特点：回调机制，实现、开发应用难度大； 异步IO模型当一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者的输入输出操作 典型应用：JAVA7 AIO IO复用模型简介：主要是select和epoll两个系统调用；对一个IO端口，两次调用，两次返回，比阻塞IO并没有什么优越性；关键是能实现同时对多个IO端口进行监听； 多个的进程的IO可以注册到一个复用器（select）上，然后用一个进程调用该select， select会监听所有注册进来的IO；如果select没有监听的IO在内核缓冲区都没有可读数据，select调用进程会被阻塞；而当任一IO在内核缓冲区中有可数据时，select调用就会返回； 而后select调用进程可以自己或通知另外的进程（注册进程）来再次发起读取IO，读取内核中准备好的数据。 I&#x2F;O复用模型会用到select、poll、epoll函数，这几个函数也会使进程阻塞，但是和阻塞I&#x2F;O所不同的的，这两个函数可以同时阻塞多个I&#x2F;O操作。而且可以同时对多个读操作，多个写操作的I&#x2F;O函数进行检测，直到有数据可读或可写时，才真正调用I&#x2F;O操作函数。 当用户进程调用了select，那么整个进程会被block；而同时，kernel会“监视”所有select负责的socket；当任何一个socket中的数据准备好了，select就会返回。这个时候，用户进程再调用read操作，将数据从kernel拷贝到用户进程。 这个图和blocking IO的图其实并没有太大的不同，事实上还更差一些。因为这里需要使用两个系统调用(select和recvfrom)，而blocking IO只调用了一个系统调用(recvfrom)。但是，用select的优势在于它可以同时处理多个connection。（多说一句：所以，如果处理的连接数不是很高的话，使用select&#x2F;epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select&#x2F;epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） 典型应用：**select、poll、epoll三种方案，nginx都可以选择使用这三个方案;**Java NIO;","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"javaNIO","slug":"javaNIO","permalink":"http://sv.pointcut.cc/tags/javaNIO/"}]},{"title":"03HTTP","slug":"network_programming/基础/03HTTP","date":"2021-11-20T12:00:06.000Z","updated":"2022-03-23T09:03:58.074Z","comments":true,"path":"blog/network_programming/基础/03HTTP/","link":"","permalink":"http://sv.pointcut.cc/blog/network_programming/%E5%9F%BA%E7%A1%80/03HTTP/","excerpt":"","text":"HTTP协议是Hyper Text Transfer Protocol（超文本传输协议）的缩写,是用于从万维网（WWW:World Wide Web ）服务器传输超文本到本地浏览器的传送协议。 HTTP协议Content-Type指示响应的内容，这里是text&#x2F;html表示HTML网页。请注意，浏览器就是依靠Content-Type来判断响应的内容是网页还是图片，是视频还是音乐。 HTTP使用统一资源标识符（Uniform Resource Identifiers, URI）来传输数据和建立连接。URL是一种特殊类型的URI，包含了用于查找某个资源的足够的信息。URL，全称是UniformResourceLocator, 中文叫统一资源定位符,是互联网上用来标识某一处资源的地址。 URI 和 URL 的区别URI 是个纯粹的句法结构，用于指定标识 Web 资源的字符串的各个不同部分。URL 是 URI 的一个特例，它包含了定位 Web 资源的足够信息。其他 URI，比如 1mailto:cay@horstman.com 则不属于定位符，因为根据该标识符无法定位任何资源。 URI 是统一资源标识符，而 URL 是统一资源定位符。因此，笼统地说，每个 URL 都 是 URI，但不一定每个 URI 都是 URL。这是因为 URI 还包括一个子类，即统一资源名称 (URN)，它命名资源但不指定如何定位资源。上面的 mailto 就是一个 URN 的示例。 URL 是 uniform resource locator，统一资源定位器，它是一种具体的 URI，即 URL 可以用 来标识一个资源，而且还指明了如何 locate （定位）这个资源。 一个完整的URL包括以下几部分：http://www.enjoyedu.com:8080/news/index.asp?boardID=5&amp;ID=24618&amp;page=1#name 协议部分（http）、域名部分（www.enjoyedu.com）、端口部分（8080）、虚拟目录部分（/news/）、文件名部分（index.asp）、参数部分（?开始）、锚部分（#name） HTTP请求的传输过程首先作为发送端的客户端在应用层（HTTP 协议）发出一个想看某个 Web 页面的 HTTP 请求。 接着，为了传输方便，在传输层（TCP 协议）把从应用层处收到的数据（HTTP 请求报文）进行分割，并在各个报文上打上标记序号及端口号后转发给网络层。 在网络层（IP 协议），增加作为通信目的地的 MAC 地址后转发给链路层。这样一来，发往网络的通信请求就准备齐全了。 接收端的服务器在链路层接收到数据，按序往上层发送，一直到应用层。当传输到应用层，才能算真正接收到由客户端发送过来的 HTTP请求。 一次完整http请求的过程 首先进行DNS域名解析（本地浏览器缓存、操作系统缓存或者DNS服务器） 如果浏览器自身的缓存里面没有找到，那么浏览器会搜索系统自身的 DNS 缓存 如果还没有找到，那么尝试从 hosts 文件里面去找 在前面三个过程都没获取到的情况下，就去域名服务器去查找， 三次握手建立 TCP 连接 在经过DNS域名解析后，确定目的ip。在 HTTP 工作开始之前，客户端首先要通过网络与服务器建立连接，HTTP 连接是通过 TCP 来完成的。HTTP 是比 TCP 更高层次的应用层协议，根据规则，只有低层协议建立之 后，才能进行高层协议的连接，因此，首先要建立 TCP 连接，一般 TCP 连接的端口号是 80; 客户端发起 HTTP 请求 服务器响应 HTTP 请求 客户端解析 html 代码，并请求 html 代码中的资源 客户端渲染展示内容 关闭 TCP 连接 一般情况下，一旦服务器向客户端返回了请求数据，它就要关闭 TCP 连接，然后如果 客户端或者服务器在其头信息加入了这行代码 Connection:keep-alive ，TCP 连接在发送后 将仍然保持打开状态，于是，客户端可以继续通过相同的连接发送请求，也就是说前面的 3到 6，可以反复进行。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带 宽。 HTTP 协议报文结构请求报文结构 首部（通用首部、请求首部和Cookie等） 响应报文结构","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"02TCP-IP","slug":"network_programming/基础/02TCP-IP","date":"2021-11-20T12:00:05.000Z","updated":"2022-03-23T09:03:58.069Z","comments":true,"path":"blog/network_programming/基础/02TCP-IP/","link":"","permalink":"http://sv.pointcut.cc/blog/network_programming/%E5%9F%BA%E7%A1%80/02TCP-IP/","excerpt":"","text":"TCP是面向连接的通信协议，通过三次握手建立连接，通讯完成时要拆除连接，由于TCP是面向连接的所以只能用于端到端的通讯。 TCP提供的是一种可靠的数据流服务，采用“带重传的肯定确认”技术来实现传输的可靠性。TCP还采用一种称为“滑动窗口”的方式进行流量控制，所谓窗口实际表示接收能力，用以限制发送方的发送速度。 如果IP数据包中有已经封好的TCP数据包，那么IP将把它们向‘上’传送到TCP层。TCP将包排序并进行错误检查，同时实现虚电路间的连接。TCP数据包中包括序号和确认，所以未按照顺序收到的包可以被排序，而损坏的包可以被重传。 TCP将它的信息送到更高层的应用程序，例如Telnet的服务程序和客户程序。应用程序轮流将信息送回TCP层，TCP层便将它们向下传送到IP层，设备驱动程序和物理介质，最后到接收方。 TCP三次握手在socket编程中，客户端通过connect完成。 第一次握手：客户端将标志位SYN置为1，随机产生一个值seq&#x3D;J，并将该数据包发送给服务器端，客户端进入SYN_SENT状态，等待服务器端确认。 第二次握手：服务器端收到数据包后由标志位SYN&#x3D;1知道客户端请求建立连接，服务器端将标志位SYN和ACK都置为1，ack&#x3D;J+1，随机产生一个值seq&#x3D;K，并将该数据包发送给客户端以确认连接请求，服务器端进入SYN_RCVD状态。 第三次握手：客户端收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack&#x3D;K+1，并将该数据包发送给服务器端，服务器端检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，客户端和服务器端进入ESTABLISHED状态，完成三次握手，随后客户端与服务器端之间可以开始传输数据了。 为什么 TCP 握手需要三次?为了实现可靠数据传输， TCP 协议的通信双方，都必须维护一个序列号， 以标识发送出去的数据包中，哪些是已经被对方收到的。 举例说明:发送方在发送数据包(假设大小为 10 byte)时， 同时送上一个序号( 假设 为 500)，那么接收方收到这个数据包以后， 就可以回复一个确认号(510 &#x3D; 500 + 10) 告 诉发送方 “我已经收到了你的数据包， 你可以发送下一个数据包， 序号从 511 开始” 三次握手的过程即是通信双方相互告知序列号起始值，并确认对方已经收到了序列号 起始值的必经步骤，也是最少的步骤。 TCP的三次握手的漏洞（装逼用）SYN洪泛攻击三次握手中有一个第二次握手，服务端向客户端应道请求，应答请求是需要客户端IP的，服务端是需要知道客户端IP的，攻击者就伪造这个IP，往服务器端狂发送第一次握手的内容，当然第一次握手中的客户端IP地址是伪造的，从而服务端忙于进行第二次握手但是第二次握手当然没有结果，所以导致服务器端被拖累，死机。 面对这种攻击，有以下的解决方案，最好的方案是防火墙。 TCP四次挥手（分手）在socket编程中，这一过程由客户端或服务端任一方执行close来触发。 由于 TCP 连接是全双工的，因此，每个方向都必须要单独进行关闭，这一原则是当甲方 完成数据发送任务后，发送一个 FIN 给乙方来终止这一方向的连接，乙方收到一个 FIN 只是 意味着不会再收到甲方数据了，但是乙方依然可以给甲方发送数据，直到这乙方也发送了 FIN 给甲方。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭。 某个应用进程首先调用 close，我们称该端执行主动关闭(active close)。该端的 TCP于是发送一个 FIN 分节，表示数据发送完毕，应用进程进入 FIN-WAIT-1(终止等待 1)状态。 接收到这个 FIN 的对端执行被动关闭(passive close)，发出确认报文。这个 FIN 由 TCP 确认。因为 FIN 的接收意味着接收端应用进程在相应连接上再无额外数据可接收。接收端进 入了 CLOSE-WAIT(关闭等待)状态，这时候处于半关闭状态，即主动关闭端已经没有数据 要发送了，但是被动关闭端若发送数据，主动关闭端依然要接受。这个状态还要持续一段时 间，也就是整个 CLOSE-WAIT 状态持续的时间。主动关闭端收到确认报文后进入 FIN-WAIT-2 (终止等待 2)状态。 一段时间后，被动关闭的应用进程将调用 close 关闭它的套接字。这导致它的 TCP 也 发送一个 FIN。 接收这个最终 FIN 的原发送端 TCP(即执行主动关闭的那一端)确认这个 FIN 发出一个确认ACK报文，并进入了TIME-WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2*MSL（最长报文段寿命&#x2F;最长分节生命期 max segement lifetime，MSL 是任何 IP 数据报能够在因特网中存活的最长时间，任何 TCP 实现都必须为 MSL 选择一个值。）的时间后，当主动关闭端撤 销相应的 TCB 后，才进入 CLOSED 状态。 被动关闭端只要收到了客户端发出的确认，立即进入 CLOSED 状态。同样，撤销 TCB 后，就结束了这次的 TCP 连接。可以看到，被动关闭端结束 TCP 连接的时间要比主动关闭 端早一些。 TCP的通讯原理Socket套接字Socket 的原意是“插座”，在计算机通信领域，socket 被翻译为“套接字”，它是计算机之间进行通信的一种约定或一种方式。 区分不同应用程序进程间的网络通信和连接，主要有3个参数：通信的目的IP地址、使用的传输层协议(TCP或UDP)和使用的端口号。通过将这3个参数结合起来，与一个“插座”Socket绑定，应用层就可以和传输层通过套接字接口，区分来自不同应用程序进程或网络连接的通信，实现数据传输的并发服务。 套接字对是一个定义该连接的两个端点的四元组：本地IP地址、本地TCP端口号、外地IP地址、外地ＴＣＰ端口号。套接字对唯一标识一个网络上的每个TCP连接。 TCP缓冲区每个TCP的Socket的内核中都有一个发送缓冲区和一个接收缓冲区。write()并不立即向网络中传输数据，而是先将数据写入缓冲区中，再由TCP协议将数据从缓冲区发送到目标机器。一旦将数据写入到缓冲区，函数就可以成功返回，不管它们有没有到达目标机器，也不管它们何时被发送到网络，这些都是TCP协议负责的事情。 TCP协议独立于 write()函数，数据有可能刚被写入缓冲区就发送到网络，也可能在缓冲区中不断积压，多次写入的数据被一次性发送到网络，这取决于当时的网络情况、当前线程是否空闲等诸多因素，不由程序员控制。 read()也是如此，也从输入缓冲区中读取数据，而不是直接从网络中读取。 总得来说，I&#x2F;O缓冲区在每个TCP套接字中单独存在；I&#x2F;O缓冲区在创建套接字时自动生成； TCP 的可靠性在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个已收到消息的通知。这个消息叫做确认应答（ACK）。当发送端将数据发出之后会等待对端的确认应答。如果有确认应答，说明数据已经成功到达对端。反之，则数据丢失的可能性很大。 在一定时间内没有等待到确认应答，发送端就可以认为数据已经丢失，并进行重发。由此，即使产生了丢包，仍然能够保证数据能够到达对端，实现可靠传输。 TCP中的滑动窗口发送方和接收方都会维护一个数据帧的序列，这个序列被称作窗口。发送方的窗口大小由接收方确认，目的是控制发送速度，以免接收方的缓存不够大导致溢出，同时控制流量也可以避免网络拥塞。 首先是第一次发送数据这个时候的窗口大小是根据链路带宽的大小来决定的。我们假设这个时候窗口的大小是3。这个时候接受方收到数据以后会对数据进行确认告诉发送方我下次希望手到的是数据是多少。这里我们看到接收方发送的ACK&#x3D;3(这是发送方发送序列2的回答确认，下一次接收方期望接收到的是3序列信号)。这个时候发送方收到这个数据以后就知道我第一次发送的3个数据对方只收到了2个。就知道第3个数据对方没有收到。下次在发送的时候就从第3个数据开始发。此时窗口大小变成了2 。 滑动窗口协议，是TCP使用的一种流量控制方法。该协议允许发送方在停止并等待确认前可以连续发送多个分组。由于发送方不必每发一个分组就停下来等待确认，因此该协议可以加速数据的传输。 只有在接收窗口向前滑动时（与此同时也发送了确认），发送窗口才有可能向前滑动。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"01网络协议","slug":"network_programming/基础/01网络协议","date":"2021-11-20T12:00:04.000Z","updated":"2022-03-23T09:03:58.063Z","comments":true,"path":"blog/network_programming/基础/01网络协议/","link":"","permalink":"http://sv.pointcut.cc/blog/network_programming/%E5%9F%BA%E7%A1%80/01%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"计算机网络体系结构OSI七层模型开放系统互连参考模型 (Open System Interconnect 简称OSI）。 OSI采用了分层的结构化技术，共分七层，物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。 TCP&#x2F;IP模型我们实际使用的TCP&#x2F;IP模型，共分4层，链路层、网络层、传输层、应用层 两个模型之间的对应关系 TCP&#x2F;IP协议族TCP&#x2F;IP协议是Transmission Control Protocol&#x2F;Internet Protocol的简写，中译名为传输控制协议&#x2F;因特网互联协议，是Internet最基本的协议、Internet国际互联网络的基础，由网络层的IP协议和传输层的TCP协议组成。协议采用了4层的层级结构。然而在很多情况下，它是利用 IP 进行通信时所必须用到的协议群的统称。也就是说，它其实是个协议家族，由很多个协议组成，并且是在不同的层， 是互联网的基础通信架构。 TCP&#x2F;IP 网络传输中的数据每个分层中，都会对所发送的数据附加一个首部，在这个首部中包含了该层必要的信息， 如发送的目标地址以及协议相关信息。通常，为协议提供的信息为包首部，所要发送的内容 为数据。在下一层的角度看，从上一层收到的包全部都被认为是本层的数据。网络中传输的数据包由两部分组成:一部分是协议所要用到的首部，另一部分是上一层 传过来的数据。首部的结构由协议的具体规范详细定义。在数据包的首部，明确标明了协议 应该如何读取数据。反过来说，看到首部，也就能够了解该协议必要的信息以及所要处理的 数据。 应用程序处 首先应用程序会进行编码处理，这些编码相当于 OSI 的表示层功能; 编码转化后，邮件不一定马上被发送出去，这种何时建立通信连接何时发送数据的管理功能， 相当于 OSI 的会话层功能。 TCP 模块的处理 TCP 根据应用的指示，负责建立连接、发送数据以及断开连接。TCP 提供将应用层发来的 数据顺利发送至对端的可靠传输。为了实现这一功能，需要在应用层数据的前端附加一个 TCP 首部。 IP 模块的处理 IP 将 TCP 传过来的 TCP 首部和 TCP 数据合起来当做自己的数据，并在 TCP 首部的前端 加上自己的 IP 首部。IP 包生成后，参考路由控制表决定接受此 IP 包的路由或主机。 网络接口(以太网驱动)的处理 从 IP 传过来的 IP 包对于以太网来说就是数据。给这些数据附加上以太网首部并进行发送 处理，生成的以太网数据包将通过物理层传输给接收端。 网络接口(以太网驱动)的处理 主机收到以太网包后，首先从以太网包首部找到 MAC 地址判断是否为发送给自己的包，若 不是则丢弃数据。 如果是发送给自己的包，则从以太网包首部中的类型确定数据类型，再传给相应的模块，如 IP、ARP 等。这里的例子则是 IP 。 IP 模块的处理 IP 模块接收到 数据后也做类似的处理。从包首部中判断此 IP 地址是否与自己的 IP 地址 匹配，如果匹配则根据首部的协议类型将数据发送给对应的模块，如 TCP、UDP。 另外吗，对于有路由器的情况，接收端地址往往不是自己的地址，此时，需要借助路由控制 表，在调查应该送往的主机或路由器之后再进行转发数据。 TCP 模块的处理 在 TCP 模块中，首先会计算一下校验和，判断数据是否被破坏。然后检查是否在按照序号 接收数据。最后检查端口号，确定具体的应用程序。数据被完整地接收以后，会传给由端口 号识别的应用程序。 应用程序的处理接收端应用程序会直接接收发送端发送的数据。通过解析数据，展示相应的内容。 TCP和UDP在上述表格中，网际协议IP是TCP&#x2F;IP中非常重要的协议。负责对数据加上IP地址（有发送它的主机的地址（源地址）和接收它的主机的地址（目的地址））和其他的数据以确定传输的目标。 而TCP和UDP都是传输层的协议，传输层主要为两台主机上的应用程序提供端到端的通信。 但是TCP和UDP最不同的地方是： TCP提供了一种可靠的数据传输服务， TCP是面向连接的，也就是说，利用TCP通信的两台主机首先要经历一个建立连接的过程，等到连接建立后才开始传输数据，而且传输过程中采用“带重传的肯定确认”技术来实现传输的可靠性。 TCP还采用一种称为“滑动窗口”的方式进行流量控制，发送完成后还会关闭连接。 所以TCP要比UDP可靠的多。 UDP（User Datagram Protocol的简称， 中文名是用户数据报协议）是把数据直接发出去，而不管对方是不是在接收，也不管对方是否能接收的了，也不需要接收方确认，属于不可靠的传输，可能会出现丢包现象，实际应用中要求程序员编程验证。 我们一些常见的网络应用基本上都是基于 TCP 和 UDP 的，这两个协议又会使用网络层 的 IP 协议。但是我们完全可以绕过传输层的 TCP 和 UDP，直接使用 IP，比如 Linux 中 LVS， 甚至直接访问链路层，比如 tcpdump 程序就是直接和链路层进行通信的。 地址和端口号我们常听说 MAC 地址和 IP 地址。MAC地址就是在媒体接入层上使用的地址，也叫物理地址、硬件地址或链路地址，由网络设备制造商生产时写在硬件内部。MAC地址与网络无关，也即无论将带有这个地址的硬件（如网卡、集线器、路由器等）接入到网络的何处，都有相同的MAC地址，它由厂商写在网卡的BIOS里。 IP 地址后者用来识别 TCP&#x2F;IP 网络中互连的主机和路由器。IP地址基于逻辑，比较灵活，不受硬件限制，也容易记忆。 端口号用来识别同一台计算机中进行通信的不同应用程序。因此，它也被称为程序地址。 端口号与协议 端口号由其使用的传输层协议决定。因此，不同的传输层协议可以使用相同的端口号。 此外，那些知名端口号与传输层协议并无关系。只要端口一致都将分配同一种应用程序进行处理。","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"零拷贝与MMAP","slug":"network_programming/零拷贝与MMAP","date":"2021-11-20T12:00:03.000Z","updated":"2022-03-23T09:03:58.057Z","comments":true,"path":"blog/network_programming/零拷贝与MMAP/","link":"","permalink":"http://sv.pointcut.cc/blog/network_programming/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E4%B8%8EMMAP/","excerpt":"","text":"零拷贝(英语: Zero-copy) 技术是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。 零拷贝技术可以减少数据拷贝和共享总线操作的次数，消除传输数据在存储器之间不必要的中间拷贝次数，从而有效地提高数据传输效率 零拷贝技术减少了用户进程地址空间和内核地址空间之间因为上:下文切换而带来的开销 可以看出没有说不需要拷贝，只是说减少冗余[不必要]的拷贝。 下面这些组件、框架中均使用了零拷贝技术：Kafka、Netty、Rocketmq、Nginx、Apache、javaNIO Linux 的 I&#x2F;O 机制与 DMA在早期计算机中，用户进程需要读取磁盘数据，需要 CPU 中断和 CPU 参与，因此效率比较低，发起 IO 请求，每次的 IO 中断，都带来 CPU 的上下文切换。因此出现了——DMA。 DMA(Direct Memory Access，直接内存存取) 是所有现代电脑的重要特色，它允许不同 速度的硬件装置来沟通，而不需要依赖于 CPU 的大量中断负载。 DMA 控制器，接管了数据读写请求，减少 CPU 的负担。这样一来，CPU 能高效工作了。 现代硬盘基本都支持 DMA。 实际因此 IO 读取，涉及两个过程: DMA 等待数据准备好，把磁盘数据读取到操作系统内核缓冲区; 用户进程，将内核缓冲区的数据 copy 到用户空间。 这两个过程，都是阻塞的。 传统数据传送机制比如：读取文件，再用socket发送出去，实际经过四次copy。 伪码实现如下： 12buffer = File.read() Socket.send(buffer) 第一次：将磁盘文件，读取到操作系统内核缓冲区； 第二次：将内核缓冲区的数据，copy到应用程序的buffer； 第三步：将application应用程序buffer中的数据，copy到socket网络发送缓冲区(属于操作系统内核的缓冲区)； 第四次：将socket buffer的数据，copy到网卡，由网卡进行网络传输。 分析上述的过程，虽然引入DMA来接管CPU的中断请求，但四次copy是存在“不必要的拷贝”的。实际上并不需要第二个和第三个数据副本。应用程序除了缓存数据并将其传输回套接字缓冲区之外什么都不做。相反，数据可以直接从读缓冲区传输到套接字缓冲区。 显然，第二次和第三次数据copy 其实在这种场景下没有什么帮助反而带来开销(DMA拷贝速度一般比CPU拷贝速度快一个数量级)，这也正是零拷贝出现的背景和意义。打个比喻：200M的数据，读取文件，再用socket发送出去，实际经过四次copy（2次cpu拷贝每次100ms ，2次DMS拷贝每次10ms）传统网络传输的话：合计耗时将有220ms 同时，read和send都属于系统调用，每次调用都牵涉到两次上下文切换： 总结下，传统的数据传送所消耗的成本：4次拷贝，4次上下文切换。4次拷贝，其中两次是DMA copy，两次是CPU copy。 Linux 支持的(常见)零拷贝mmap内存映射硬盘上文件的位置和应用程序缓冲区(application buffers)进行映射（建立一种一一对应关系），由于mmap()将文件直接映射到用户空间，所以实际文件读取时根据这个映射关系，直接将文件从硬盘拷贝到用户空间，只进行了一次数据拷贝，不再有文件内容从硬盘拷贝到内核空间的一个缓冲区。 mmap内存映射将会经历：3次拷贝: 1次cpu copy，2次DMA copy； 打个比喻：200M的数据，读取文件，再用socket发送出去，如果是使用MMAP实际经过三次copy（1次cpu拷贝每次100ms ，2次DMS拷贝每次10ms）合计只需要120ms从数据拷贝的角度上来看，就比传统的网络传输，性能提升了近一倍。 以及4次上下文切换 mmap()是在 &lt;sys&#x2F;mman.h&gt; 中定义的一个函数，此函数的作用是创建一个新的 虚拟内存 区域，并将指定的对象映射到此区域。 mmap 其实就是通过 内存映射 的机制来进行文件操作。 在java中可以使用的使用 12345678910111213141516171819202122232425262728293031323334353637383940414243import sun.misc.Cleaner;import sun.nio.ch.DirectBuffer;import java.io.File;import java.io.RandomAccessFile;import java.nio.MappedByteBuffer;import java.nio.channels.FileChannel;import java.nio.charset.StandardCharsets;public class MmapCopy &#123; public static void main(String[] args) throws Exception &#123; //需要映射的文件 File file = new File(&quot;/Volumes/vm/logs/mmap_text.log&quot;); if (!file.exists()) &#123; file.createNewFile(); &#125; //映射文件的fileChannel对象，用来操作文件 FileChannel fileChannel = new RandomAccessFile(file, &quot;rw&quot;).getChannel(); //fileChannel定义了map方法，MMAP的映射，1K MappedByteBuffer mmap = fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, 1024); //向mmap写入数据 mmap.put(&quot;xyz&quot;.getBytes(StandardCharsets.UTF_8)); //刷新写入的数据到磁盘 mmap.flip(); byte[] bytes = new byte[3]; //从mmap中读取数据 mmap.get(bytes, 0, 3); System.out.println(new String(bytes)); //解除MMAP// unmap(mmap); fileChannel.close(); &#125; private static void unmap(MappedByteBuffer mappedByteBuffer) &#123; if (mappedByteBuffer != null) &#123; Cleaner cl = ((DirectBuffer)mappedByteBuffer).cleaner(); if (cl != null) &#123; cl.clean(); &#125; &#125; &#125;&#125; sendfile当调用 sendfile()时，DMA 将磁盘数据复制到 kernel buffer，然后将内核中的 kernel buffer 直接拷贝到 socket buffer;但是数据并未被真正复制到 socket 关联的缓冲区内。取而代之的 是，只有记录数据位置和长度的描述符被加入到 socket 缓冲区中。DMA 模块将数据直接从 内核缓冲区传递给协议引擎，从而消除了遗留的最后一次复制。 一旦数据全都拷贝到 socket buffer，sendfile()系统调用将会 return、代表数据转化的完 成。socket buffer 里的数据就能在网络传输了。 sendfile 会经历:3 次拷贝，1 次 CPU copy 2 次 DMA copy; 以及 2 次上下文切换 简单归纳上述的过程： sendfile系统调用利用DMA引擎将文件数据拷贝到内核缓冲区，之后数据被拷贝到内核socket缓冲区中 DMA引擎将数据从内核socket缓冲区拷贝到协议引擎中 这里没有用户态和内核态之间的切换，也没有内核缓冲区和用户缓冲区之间的拷贝，大大提升了传输性能。 sendfile() 系统调用不需要将数据拷贝或者映射到应用程序地址空间中去，所以 sendfile() 只是适用于应用程序地址空间不需要对所访问数据进行处理的情况。因为 sendfile 传输的数据没有越过用户应用程序 &#x2F; 操作系统内核的边界线，所以 sendfile () 也极大地减少了存储管理的开销。 在java中可以使用的使用。 Java NIO 中提供的 FileChannel 拥有 transferTo 和 transferFrom 两个方法，可直接把 FileChannel 中的数据拷贝到另外一个 Channel，或者直接把另外一个 Channel 中的数据拷 贝到 FileChannel。该接口常被用于高效的网络 &#x2F; 文件的数据传输和大文件拷贝。在操作系 统支持的情况下，通过该方法传输数据并不需要将源数据从内核态拷贝到用户态，再从用户 态拷贝到目标通道的内核态，同时也避免了两次用户态和内核态间的上下文切换，也即使用 了“零拷贝”，所以其性能一般高于 Java IO 中提供的方法 123456789101112131415161718192021222324import java.io.FileInputStream;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.channels.FileChannel;import java.nio.channels.SocketChannel;public class SendFileCopy &#123; public static void main(String[] args) throws IOException &#123; //socket套接字 SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress(&quot;localhost&quot;, 8089)); socketChannel.configureBlocking(true); //文件, fileChannel 文件读写、映射和操作的通道 FileChannel fileChannel = new FileInputStream(&quot;文件&quot;).getChannel(); long startTime = System.currentTimeMillis(); long transferCount = fileChannel.transferTo(0, fileChannel.size(), socketChannel); long endTime = System.currentTimeMillis(); System.out.println(&quot;发送的总字节数:&quot; + transferCount + &quot; 耗时:&quot; + (endTime - startTime)); fileChannel.close(); socketChannel.close(); &#125;&#125; splice数据从磁盘读取到 OS 内核缓冲区后，在内核缓冲区直接可将其转成内核空间其他数据buffer，而不需要拷贝到用户空间。 如下图所示，从磁盘读取到内核 buffer 后，在内核空间直接与 socket buffer 建立 pipe 管道。 和 sendfile()不同的是，splice()不需要硬件支持。 sendfile 是将磁盘数据加载到 kernel buffer 后，需要一次 CPU copy，拷贝到 socket buffer。而 splice 是更进一步，连这个 CPU copy 也不需要了，直接 将两个内核空间的 buffer 进行 pipe。 splice 会经历 2 次拷贝: 0 次 cpu copy 2 次 DMA copy; 以及 2 次上下文切换","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"javaNIO结构图","slug":"network_programming/javaNIO结构图","date":"2021-11-20T12:00:02.000Z","updated":"2022-03-23T09:03:58.050Z","comments":true,"path":"blog/network_programming/javaNIO结构图/","link":"","permalink":"http://sv.pointcut.cc/blog/network_programming/javaNIO%E7%BB%93%E6%9E%84%E5%9B%BE/","excerpt":"","text":"","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"IO多路复用","slug":"network_programming/IO多路复用","date":"2021-11-20T12:00:01.000Z","updated":"2022-03-23T09:03:58.044Z","comments":true,"path":"blog/network_programming/IO多路复用/","link":"","permalink":"http://sv.pointcut.cc/blog/network_programming/IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/","excerpt":"","text":"从流中读取数据或者写入数据到流中，可能存在这样的情况：读取数据时，流中还没有数据；写入数据时，流中数据已经满了，没有空间写入了。典型的例子为客户端要从socket流中读入数据，但是服务器还没有把数据准备好。此时有两种处理办法： 阻塞，等待数据准备好了，再读取出来返回； 非阻塞，通过轮询的方式，查询是否有数据可以读取，直到把数据读取返回。 I&#x2F;O同步、异步、阻塞、非阻塞在IO操作过程中，可能会涉及到同步（synchronous）、异步（asynchronous）、阻塞（blocking）、非阻塞（non-blocking）、IO多路复用(IO multiplexing)等概念。他们之间的区别是什么呢？以网络IO为例，在IO操作过程会涉及到两个对象： 一个是调用这个IO的process (or thread)； 另外一个是一个就是系统内核(kernel)。 在一个IO操作过程中，以read为例，会涉及到两个过程： 等待数据准备好(Waiting for the data to be ready)； 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process) 阻塞VS非阻塞阻塞IO：在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样：blocking IO的特点就是在IO执行等待数据（socket阻塞线程）和拷贝数据（将数据从kernel中拷贝到用户内存）两个阶段都被block了。 这给网络编程带来了一个很大的问题，如在调recv(1024)的同时，线程将被阻塞，在此期间，线程将无法执行任何运算或响应任何的网络请求。 一个简单的解决方案是：在服务器端使用多线程（或多进程）。多线程（或多进程）的目的是让每个连接都拥有独立的线程（或进程），这样任何一个连接的阻塞都不会影响其他的连接。但该方案有个问题，开启多进程或都线程的方式，在遇到要同时响应成百上千路的连接请求，则无论多线程还是多进程都会严重占据系统资源，降低系统对外界响应效率，而且线程与进程本身也更容易进入假死状态。 改进方案：很多程序员可能会考虑使用“线程池”或“连接池”。“线程池”旨在减少创建和销毁线程的频率，其维持一定合理数量的线程，并让空闲的线程重新承担新的执行任务。“连接池”维持连接的缓存池，尽量重用已有的连接、减少创建和关闭连接的频率。这两种技术都可以很好的降低系统开销，都被广泛应用很多大型系统，如websphere、tomcat和各种数据库等。 改进后方案其实也存在着问题：“线程池”和“连接池”技术也只是在一定程度上缓解了频繁调用IO接口带来的资源占用。而且，所谓“池”始终有其上限，当请求大大超过上限时，“池”构成的系统对外界的响应并不比没有池的时候效果好多少。所以使用“池”必须考虑其面临的响应规模，并根据响应规模调整“池”的大小。 对应上例中的所面临的可能同时出现的上千甚至上万次的客户端请求，“线程池”或“连接池”或许可以缓解部分压力，但是不能解决所有问题。总之，多线程模型可以方便高效的解决小规模的服务请求，但面对大规模的服务请求。 总结阻塞I&#x2F;O有一个比较明显的缺点是在I&#x2F;O阻塞模式下，一个线程只能处理一个流的I&#x2F;O事件。如果想要同时处理多个流，需要多个进程或者多个线程，但是这种方式效率不高。而且在请求多时会有大量的线程创建和销毁，资源占用高。就算使用了线程池等池化技术，也只是控制了线程的数量和线程创建销毁的资源消耗，应付不了非常高的并发情况，也存在资源占用问题。所以这种multi-threading + blocking IO的方式只是局部解决了问题。 非阻塞IOLinux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子：从图可以看出，虽然一个socket不阻塞了，但是在数据准备好后，数据复制是需要阻塞的。这种方式有个致命的缺点：线程在不断的轮询内核，询问数据是否准备好，如果数据长时间都没准备好，这种方式在白白的浪费cpu资源，而且这种轮询方式会使吞吐量降低。 总结这种方式典型的忙轮询：过不停的把所有的流从头到尾轮询一遍，查询是否有流已经准备就绪，然后又从头开始。如果所有流都没有准备就绪，那么只会白白浪费CPU时间。所以了解即可 同步VS异步同步IO：同步IO操作将导致请求的进程一直被blocked，直到IO操作完成。从这个层次来，阻塞IO、非阻塞IO操作、IO多路复用都是同步IO。 异步IO：异步IO操作不会导致请求的进程被blocked。当发出IO操作请求，直接返回，等待IO操作完成后，再通知调用进程。 多路复用IO（事件驱动）I&#x2F;O多路复用是通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。 多路复用IO也是阻塞IO，不过阻塞的方式不同。多路复用IO模型中，对于每一个socket，一般都设置成为non-blocking，阻塞是通过select&#x2F;poll&#x2F;epoll完成。select&#x2F;epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理是select&#x2F;epoll这个函数会不断轮询所负责的IO操作，当某个IO操作有数据到达时，就通知用户进程。然后由用户进程去操作IO。当用户进程调用了select，那么整个进程会被block；而同时，kernel会“监视”所有select负责的socket；当任何一个socket中的数据准备好了，select就会返回。这个时候，用户进程再调用read操作，将数据从kernel拷贝到用户进程。 这个图和blocking IO的图其实并没有太大的不同，事实上还更差一些。因为这里需要使用两个系统调用(select和recvfrom)，而blocking IO只调用了一个系统调用(recvfrom)。但是，用select的优势在于它可以同时处理多个connection。 所以多路复用IO模型在并发少时性能不一定比阻塞IO模型高，因为涉及了两次系统调用。但它的优势在于处理高并发的情况。 总结 如果处理的连接数不是很高的话，使用select&#x2F;epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select&#x2F;epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。 在多路复用模型中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 I&#x2F;O多路复用技术的最大优势是系统开销小，系统不必创建进程&#x2F;线程，也不必维护这些进程&#x2F;线程，从而大大减小了系统的开销。 前面说过，IO复用通过select&#x2F;epoll会不断轮询所负责的IO操作，但这两个也有区别 select方式select是无差别的轮询，我们的程序就会阻塞在select处。我们通过select那里只是知道了有I&#x2F;O事件准备好了，但不知道具体是哪几个流（可能有一个，也可能有多个），所以需要无差别的轮询所有的流，找出已经准备就绪的流。可以看到，使用select时，我们需要O(n)的时间复杂度来处理流，处理的流越多，消耗的时间也就越多。 epoll方式epoll是最小轮询，即通过epoll方式来观察多个流，epoll只会把发生了I&#x2F;O事件的流通知我们，我们对这些流的操作都是有意义的，时间复杂度降低到O(K)，其中k为产生I&#x2F;O事件的流个数。 注意：这两种方式还有别的不同，这里只是列举了一点。在netty如果系统支持epoll，推荐epoll","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"Java与协程","slug":"jvm/05java内存模型/Java与协程","date":"2021-11-19T12:00:48.000Z","updated":"2022-03-23T09:03:58.034Z","comments":true,"path":"blog/jvm/05java内存模型/Java与协程/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/05java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/Java%E4%B8%8E%E5%8D%8F%E7%A8%8B/","excerpt":"","text":"Java目前的并发编程机制就与上述架构趋势产生了一些矛盾，1:1的内核线程模型是如今Java虚拟机线程实现的主流选择，但是这种映射到操作系统上的线程天然的缺陷是切换、调度成本高昂，系统能容纳的线程数量也很有限。以前处理一个请求可以允许花费很长时间在单体应用中，具有这种线程切换的成本也是无伤大雅的，但现在在每个请求本身的执行时间变得很短、数量变得很多的前提下， 用户线程切换的开销甚至可能会接近用于计算本身的开销，这就会造成严重的浪费。 协程协程，英文Coroutines，是一种基于线程之上，但又比线程更加轻量级的存在，这种由程序员自己写程序来管理的轻量级线程叫做『用户空间线程』，具有对内核来说不可见的特性。 因为是自主开辟的异步任务，所以很多人也更喜欢叫它们纤程（Fiber），或者绿色线程（GreenThread）。正如一个进程可以拥有多个线程一样，一个线程也可以拥有多个协程。 而协程的目的就是当出现长时间的I&#x2F;O操作时，通过让出目前的协程调度，执行下一个任务的方式，来消除ContextSwitch上的开销。 协程的特点优点： 线程的切换由操作系统负责调度，协程由用户自己进行调度，因此减少了上下文切换，提高了效率。 线程的默认Stack大小是1M，而协程更轻量，接近1K。因此可以在相同的内存中开启更多的协程。 由于在同一个线程上，因此可以避免竞争关系而使用锁。 适用于被阻塞的，且需要大量并发的场景。但不适用于大量计算的多线程，遇到此种情况，更好使用线程去解决。 协程当然也有它的局限,需要在应用层面实现的内容(调用栈、调度器这些)特别多 Java的解决方案Loom项目能正在为Java引入纤程(Fiber)。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"03jvm对锁的优化","slug":"jvm/05java内存模型/03jvm对锁的优化","date":"2021-11-19T12:00:47.000Z","updated":"2022-03-23T09:03:58.033Z","comments":true,"path":"blog/jvm/05java内存模型/03jvm对锁的优化/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/05java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/03jvm%E5%AF%B9%E9%94%81%E7%9A%84%E4%BC%98%E5%8C%96/","excerpt":"","text":"高效并发是从JDK 5升级到JDK 6后一项重要的改进项，HotSpot虚拟机开发团队在这个版本上花费了大量的资源去实现各种锁优化技术，如适应性自旋（Adaptive Spinning）、锁消除（Lock Elimination）、锁膨胀（Lock Coarsening）、轻量级锁（Lightweight Locking）、偏向锁（Biased Locking）等，这些技术都是为了在线程之间更高效地共享数据及解决竞争问题，从而提高程序的执行效率。 线程安全定义：当多个线程同时访问一个对象时,如果不用考虑这些线程在运行时环境下的调度和交替执行,也不需要进行额外的同步,或者在调用方进行任何其他的协调操作,调用这个对象的行为都可以获得正确的结果,那就称这个对象是线程安全的。 线程安全的实现方法互斥同步在Java里面，最基本的互斥同步手段就是synchronized关键字，这是一种块结构(Block Structured)的同步语法。synchronized关键字经过Javac编译之后，会在同步块的前后分别形成monitorenter和monitorexit这两个字节码指令。这两个字节码指令都需要一个reference类型的参数来指明要锁定和解锁的对象。如果Java源码中的synchronized明确指定了对象参数，那就以这个对象的引用作为reference；如果没有明确指定，那将根据synchronized修饰的方法类型(如实例方法或类方法)，来决定是取代码所在的对象实例还是取类型对应的Class对象来作为线程要持有的锁。 在执行monitorenter指令时，首先要去尝试获取对象的锁。如果这个对象没被锁定，或者当前线程已经持有了那个对象的锁，就把锁的计数器的值增加一，而在执行monitorexit指令时会将锁计数器的值减一。一旦计数器的值为零，锁随即就被释放了。如果获取对象锁失败，那当前线程就应当被阻塞等待，直到请求锁定的对象被持有它的线程释放为止。 非阻塞同步互斥同步面临的主要问题是进行线程阻塞和唤醒所带来的性能开销，因此这种同步也被称为阻塞同步(Blocking Synchronization)。从解决问题的方式上看，互斥同步属于一种悲观的并发策略，其总是认为只要不去做正确的同步措施(例如加锁)，那就肯定会出现问题，无论共享的数据是否真的会出现竞争，它都会进行加锁(这里讨论的是概念模型，实际上虚拟机会优化掉很大一部分不必要的加锁)，这将会导致用户态到核心态转换、维护锁计数器和检查是否有被阻塞的线程需要被唤醒等开销。 基于冲突检测的乐观并发策略，通俗地说就是不管风险，先进行操作，如果没有其他线程争用共享数据，那操作就直接成功了;如果共享的数据的确被争用，产生了冲突，那再进行其他的补偿措施，最常用的补偿措施是不断地重试，直到出现没有竞争的共享数据为止。这种乐观并发策略的实现不再需要把线程阻塞挂起，因此这种同步操作被称为非阻塞同步(Non-Blocking Synchronization)，使用这种措施的代码也常被称为无锁(Lock-Free) 编程。CAS就是典型的非阻塞同步。 CAS实现原子操作的三大问题： ABA问题：如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA问题的解决思路就是使用版本号，JDK的Atomic包里提供了一个类AtomicStampedReference来解决ABA问题。 循环时间长开销大 只能保证一个共享变量的原子操作，Java 1.5开始， JDK提供AtomicReference类来保证引用对象之间的原子性 无同步方案Java语言中，如果一个变量要被多线程访问，可以使用volatile关键字将它声明为“易变的”； 如果一个变量只要被某个线程独享，我们可以通过java.lang.ThreadLocal类来实现线程本地存储的功能。每一个线程的Thread对象中都有一个ThreadLocalMap对象，这个对象存储了一组以ThreadLocal.threadLocalHashCode为键，以本地线程变量为值的K-V值对，ThreadLocal对象就是当前线程的ThreadLocalMap的访问入口，每一个ThreadLocal对象都包含了一个独一无二的threadLocalHashCode值，使用这个值就可以在线程K-V值对中找回对应的本地线程变量。 自旋锁与自适应自旋互斥同步对性能最大的影响是阻塞的实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给Java虚拟机的并发性能带来了很大的压力。同时，虚拟机的开发团队也注意到在许多应用上，共享数据的锁定状态只会持续很短的一段时间，为了这段时间去挂起和恢复线程并不值得。 让后面请求锁的那个线程“稍等一会”，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁。为了让线程等待，我们只须让线程执行一个忙循环（自旋），这项技术就是所谓的自旋锁。 自旋锁在JDK 1.4.2中就已经引入，只不过默认是关闭的，可以使用-XX:+UseSpinning参数来开启，在JDK 6中就已经改为默认开启了。自旋等待不能代替阻塞，且先不说对处理器数量的要求，自旋等待本身虽然避免了线程切换的开销，但它是要占用处理器时间的，所以如果锁被占用的时间很短，自旋等待的效果就会非常好，反之如果锁被占用的时间很长，那么自旋的线程只会白白消耗处理器资源，而不会做任何有价值的工作，这就会带来性能的浪费。因此自旋等待的时间必须有一定的限度，如果自旋超过了限定的次数仍然没有成功获得锁，就应当使用传统的方式去挂起线程。自旋次数的默认值是十次，用户也可以使用参数-XX:PreBlockSpin来自行更改。 在JDK 6中对自旋锁的优化，引入了自适应的自旋。自适应意味着自旋的时间不再是固定的了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定的。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，进而允许自旋等待持续相对更长的时间，比如持续100次忙循环。另一方面，如果对于某个锁，自旋很少成功获得过锁，那在以后要获取这个锁时将有可能直接省略掉自旋过程，以避免浪费处理器资源。 锁消除锁消除是指虚拟机即时编译器在运行时，通过jvm的逃逸分析，判断到一段代码中，在堆上的所有数据都不会逃逸出去被其他线程访问到，那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须再进行。 由于String是一个不可变的类，对字符串的连接操作总是通过生成新的String对象来进行的，因此Javac编译器会对String连接做自动优化。在JDK 5之前，字符串加法会转化为StringBuffer对象的连续append()操作，在JDK 5及以后的版本中，会转化为StringBuilder对象的连续append()操作。 也就是说代码清单13-6 123public String concatString(String s1, String s2, String s3) &#123; return s1 + s2 + s3;&#125; 可能变成清单13-7: 1234567public String concatString(String s1, String s2, String s3) &#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString();&#125; 锁粗化原则上，我们在编写代码的时候，总是推荐将同步块的作用范围限制得尽量小——只在共享数据的实际作用域中才进行同步，这样是为了使得需要同步的操作数量尽可能变少，即使存在锁竞争，等待锁的线程也能尽可能快地拿到锁。 大多数情况下，上面的原则都是正确的，但是如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体之中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗。 如果虚拟机探测到有这样一串零碎的操作都对同一个对象加锁，将会把加锁同步的范围扩展（粗化）到整个操作序列的外部，以代码清单13-7为例，就是扩展到第一个append()操作之前直至最后一个append()操作之后，这样只需要加锁一次就可以了。 轻量级锁对象头布局轻量级锁并不是用来代替重量级锁的，它设计的初衷是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。 HotSpot虚拟机的对象头（Object Header）分为两部分 用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄（Generational GC Age）等，这部分数据的长度在32位和64位的Java虚拟机中分别会占用32个或64个比特，官方称它为“Mark Word”。这部分是实现轻量级锁和偏向锁的关键。 用于存储指向方法区对象类型数据的指针，如果是数组对象，还会有一个额外的部分用于存储数组长度。 由于对象头信息是与对象自身定义的数据无关的额外存储成本，考虑到Java虚拟机的空间使用效率，Mark Word被设计成一个非固定的动态数据结构，以便在极小的空间内存储尽量多的信息。它会根据对象的状态复用自己的存储空间。 例如在32位的HotSpot虚拟机中，对象未被锁定的状态下，Mark Word的32个比特空间里的25个比特将用于存储对象哈希码，4个比特用于存储对象分代年龄，2个比特用于存储锁标志位，还有1个比特固定为0（这表示未进入偏向模式）。对象除了未被锁定的正常状态外，还有轻量级锁定、重量级锁定、GC标记、可偏向等几种不同状态，这些状态下对象头的存储内容如表13-1所示。 轻量级锁的加锁过程在代码即将进入同步块的时候，如果此同步对象没有被锁定（锁标志位为“01”状态），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝（官方为这份拷贝加了一个Displaced前缀，即Displaced Mark Word），这时候线程堆栈与对象头的状态如图13-3所示。 然后，虚拟机将使用CAS操作尝试把对象的Mark Word更新为指向Lock Record的指针。如果这个更新动作成功了，即代表该线程拥有了这个对象的锁，并且对象Mark Word的锁标志位（Mark Word的最后两个比特）将转变为“00”，表示此对象处于轻量级锁定状态。这时候线程堆栈与对象头的状态如图13-4所示。 如果这个更新操作失败了，那就意味着至少存在一条线程与当前线程竞争获取该对象的锁。虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是，说明当前线程已经拥有了这个对象的锁，那直接进入同步块继续执行就可以了，否则就说明这个锁对象已经被其他线程抢占了。如果出现两条以上的线程争用同一个锁的情况，那轻量级锁就不再有效，必须要膨胀为重量级锁，锁标志的状态值变为“10”，此时Mark Word中存储的就是指向重量级锁（互斥量）的指针，后面等待锁的线程也必须进入阻塞状态。 轻量级锁的解锁过程如果对象的Mark Word仍然指向线程的锁记录，那就用CAS操作把对象当前的Mark Word和线程中复制的Displaced Mark Word替换回来。假如能够成功替换，那整个同步过程就顺利完成了；如果替换失败，则说明有其他线程尝试过获取该锁，就要在释放锁的同时，唤醒被挂起的线程。 轻量级锁能提升程序同步性能的依据是“对于绝大部分的锁，在整个同步周期内都是不存在竞争的”这一经验法则。如果没有竞争，轻量级锁便通过CAS操作成功避免了使用互斥量的开销；但如果确实存在锁竞争，除了互斥量的本身开销外，还额外发生了CAS操作的开销。因此在有竞争的情况下，轻量级锁反而会比传统的重量级锁更慢。 轻量级锁的膨胀流程 偏向锁偏向锁也是JDK 6中引入的一项锁优化措施，它的目的是消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能。 偏向锁中的“偏”，就是偏心的“偏”、偏袒的“偏”。它的意思是这个锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁一直没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步。 假设当前虚拟机启用了偏向锁（启用参数-XX:+UseBiased Locking，这是自JDK 6起HotSpot虚拟机的默认值），那么当锁对象第一次被线程获取的时候，虚拟机将会把对象头中的标志位设置为“01”、把偏向模式设置为“1”，表示进入偏向模式。同时使用CAS操作把获取到这个锁的线程的ID记录在对象的Mark Word之中。如果CAS操作成功，持有偏向锁的线程以后每次进入这个锁相关的同步块时，虚拟机都可以不再进行任何同步操作。 一旦出现另外一个线程去尝试获取这个锁的情况，偏向模式就马上宣告结束。根据锁对象目前是否处于被锁定的状态决定是否撤销偏向（偏向模式设置为“0”），撤销后标志位恢复到未锁定（标志位为“01”）或轻量级锁定（标志位为“00”）的状态 注意： 有一个问题：当对象进入偏向状态的时候，Mark Word大部分的空间（23个比特）都用于存储持有锁的线程ID了，这部分空间占用了原有存储对象哈希码的位置，那原来对象的哈希码怎么办呢？ 在Java语言里面一个对象如果计算过哈希码，就应该一直保持该值不变。而作为绝大多数对象哈希码来源的Object::hashCode()方法，返回的是对象的一致性哈希码（Identity Hash Code），这个值是能强制保证不变的，它通过在对象头中存储计算结果来保证第一次计算之后，再次调用该方法取到的哈希码值永远不会再发生改变。因此，当一个对象已经计算过一致性哈希码后，它就再也无法进入偏向锁状态了；而当一个对象当前正处于偏向锁状态，又收到需要计算其一致性哈希码请求时，它的偏向状态会被立即撤销，并且锁会膨胀为重量级锁。在重量级锁的实现中，对象头指向了重量级锁的位置，代表重量级锁的ObjectMonitor类里有字段可以记录非加锁状态（标志位为“01”）下的Mark Word，其中自然可以存储原来的哈希码。 偏向锁可以提高带有同步但无竞争的程序性能，但它同样是一个带有效益权衡（Trade Off）性质的优化，也就是说它并非总是对程序运行有利。如果程序中大多数的锁都总是被多个不同的线程访问，那偏向模式就是多余的。在具体问题具体分析的前提下，有时候使用参数-XX:-UseBiasedLocking来禁止偏向锁优化反而可以提升性能。 偏向锁的获取和撤销流程 锁的优缺点对比","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"02Java与线程","slug":"jvm/05java内存模型/02Java与线程","date":"2021-11-19T12:00:46.000Z","updated":"2022-03-23T09:03:58.020Z","comments":true,"path":"blog/jvm/05java内存模型/02Java与线程/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/05java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/02Java%E4%B8%8E%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"线程是比进程更轻量级的调度执行单位，线程的引入，可以把一个进程的资源分配和执行调度分开，各个线程既可以共享进程资源(内存地址、文件I&#x2F;O等)，又可以独立调度。目前线程是Java里面进行处理器资源调度的最基本单位，不过如果日后Loom项目能成功为Java引入纤程(Fiber)的话，可能就会改变这一点。 线程实现实现线程主要有三种方式：使用内核线程实现(1：1实现)，使用用户线程实现(1：N实现)， 使用用户线程加轻量级进程混合实现(N：M实现)。 内核线程实现(1：1实现)内核线程(Kernel-Level Thread，KLT)就是直接由操作系统内核(Kernel，下称内核)支持的线程，这种线程由内核来完成线程切换，内核通过操纵调度器(Scheduler)对线程进行调度，并负责将线程的任务映射到各个处理器上。每个内核线程可以视为内核的一个分身，这样操作系统就有能力同时处理多件事情。，这样操作系统就有能力同时处理多件事情，支持多线程的内核就称为多线程内核（Multi-Threads Kernel）。 程序一般不会直接使用内核线程，而是使用内核线程的一种高级接口——轻量级进程(Light Weight Process，LWP)，轻量级进程就是我们通常意义上所讲的线程，由于每个轻量级进程都由一个内核线程支持，因此只有先支持内核线程，才能有轻量级进程。这种轻量级进程与内核线程之间1：1 的关系称为一对一的线程模型，如图： P为进程；LWP为轻量级进程，也就是线程；KLT为内核线程。 由于内核线程的支持，每个轻量级进程都成为一个独立的调度单元，即使其中某一个轻量级进程在系统调用中被阻塞了，也不会影响整个进程继续工作。轻量级进程也具有它的局限性：首先，由于是基于内核线程实现的，所以各种线程操作，如创建、析构及同步，都需要进行系统调用。而系统调用的代价相对较高，需要在用户态(User Mode)和内核态(Kernel Mode)中来回切换（上下文切换）。其次，每个轻量级进程都需要有一个内核线程的支持，因此轻量级进程要消耗一定的内核资源(如内核线程的栈空间)，因此一个系统支持轻量级进程的数量是有限的。 用户线程实现（1：N实现）协程使用的方式用户线程指的是完全建立在用户空间的线程库上，系统内核不能感知到用户线程的存在及如何实现的。用户线程的建立、同步、销毁和调度完全在用户态中完成，不需要内核的帮助。如果程序实现得当，这种线程不需要切换到内核态，因此操作可以是非常快速且低消耗的，也能够支持规模更大的线程数量，这种进程与用户线程之间1：N的关系称为一对多的线程模型。 用户线程的优势在于不需要系统内核支援,劣势也在于没有系统内核的支援，实现起来非常复杂。 Golang、Erlang等，就是使得用户线程。 混合实现线程除了依赖内核线程实现和完全由用户程序自己实现之外，还有一种将内核线程与用户线程一起使用的实现方式，被称为N：M实现。在这种混合实现下，既存在用户线程，也存在轻量级进程。用户线程还是完全建立在用户空间中，因此用户线程的创建、切换、析构等操作依然廉价，并且可以支持大规模的用户线程并发。而操作系统支持的轻量级进程则作为用户线程和内核线程之间的桥梁， 这样可以使用内核提供的线程调度功能及处理器映射，并且用户线程的系统调用要通过轻量级进程来完成，这大大降低了整个进程被完全阻塞的风险。在这种混合模式中，用户线程与轻量级进程的数量比是不定的，是N：M的关系，如图所示，这种就是多对多的线程模型。 Java线程的实现java使用的是1：1的线程模型。 Java线程调度线程调度是指系统为线程分配处理器使用权的过程，调度主要方式有两种，分别是协同式(Cooperative Threads-Scheduling)线程调度和抢占式(Preemptive Threads-Scheduling)线程调度。 协同式调度使用协同式调度的多线程系统，线程的执行时间由线程本身来控制，线程把自己的工作执行完了之后，要主动通知系统切换到另外一个线程上去。协同式多线程的最大好处是实现简单，而且由于线程要把自己的事情干完后才会进行线程切换，切换操作对线程自己是可知的，所以一般没有什么线程同步的问题。 缺点：线程执行时间不可控制，甚至如果一个线程的代码编写有问题，一直不告知系统进行线程切换，那么程序就会一直阻塞在那里。 抢占式调度使用抢占式调度的多线程系统，那么每个线程将由系统来分配执行时间，线程的切换不由线程本身来决定。譬如在Java中，有Thread::yield()方法可以主动让出执行时间，但是如果想要主动获取执行时间，线程本身是没有什么办法的。在这种实现线程调度的方式下，线程的执行时间是系统可控的，也不会有一个线程导致整个进程甚至整个系统阻塞的问题。Java使用的线程调度方式就是抢占式调度。 虽然说Java线程调度是系统自动完成的，但是我们仍然可以“建议”操作系统给某些线程多分配一点执行时间，另外的一些线程则可以少分配一点——这项操作是通过设置线程优先级来完成的。Java语言一共设置了10个级别的线程优先级（Thread.MIN_PRIORITY至Thread.MAX_PRIORITY）。在两个线程同时处于Ready状态时，优先级越高的线程越容易被系统选择执行。 不过，线程优先级并不是一项稳定的调节手段，很显然因为主流虚拟机上的Java线程是被映射到系统的原生线程上来实现的，所以线程调度最终还是由操作系统说了算。 状态转换 新建(New)：创建后尚未启动的线程处于这种状态。 运行(Runnable)：包括操作系统线程状态中的Running和Ready，也就是处于此状态的线程有可能正在执行，也有可能正在等待着操作系统为它分配执行时间。 无限期等待(Waiting)：处于这种状态的线程不会被分配处理器执行时间，它们要等待被其他线程显式唤醒。以下方法会让线程陷入无限期的等待状态： 没有设置Timeout参数的Object::wait()方法; 没有设置Timeout参数的Thread::join()方法; LockSupport::park()方法。 限期等待(Timed Waiting)：处于这种状态的线程也不会被分配处理器执行时间，不过无须等待被其他线程显式唤醒，在一定时间之后它们会由系统自动唤醒。以下方法会让线程进入限期等待状态： Thread::sleep()方法; 设置了Timeout参数的Object::wait()方法; 设置了Timeout参数的Thread::join()方法; LockSupport::parkNanos()方法; LockSupport::parkUntil()方法。 阻塞(Blocked)：线程被阻塞了，“阻塞状态”与“等待状态”的区别是“阻塞状态”在等待着获取到一个排它锁，这个事件将在另外一个线程放弃这个锁的时候发生;而“等待状态”则是在等待一段时间，或者唤醒动作的发生。在程序等待进入同步区域的时候，线程将进入这种状态。 结束(Terminated)：已终止线程的线程状态,线程已经结束执行。 上下文切换即使是单核处理器也支持多线程执行代码,CPU通过给每个线程分配CPU时间片来实现这个机制。时间片是CPU分配给各个线程的时间,因为时间片非常短,所以CPU通过不停地切换线程执行,让我们感觉多个线程是同时执行的,时间片一般是几十毫秒(ms)。 CPU通过时间片分配算法来循环执行任务,当前任务执行一个时间片后会切换到下一个任务。但是,在切换前会保存上一个任务的状态,以便下次切换回这个任务时,可以再加载这个任务的状态。所以任务从保存到再加载的过程就是一次上下文切换。 这就像我们同时读两本书,当我们在读一本英文的技术书时,发现某个单词不认识,于是便打开中英文字典,但是在放下英文技术书之前,大脑必须先记住这本书读到了多少页的第多少行,等查完单词之后,能够继续读这本书。这样的切换是会影响读书效率的,同样上下文切换也会影响多线程的执行速度。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"01Java内存模型与线程","slug":"jvm/05java内存模型/01Java内存模型与线程","date":"2021-11-19T12:00:45.000Z","updated":"2022-03-23T09:03:58.015Z","comments":true,"path":"blog/jvm/05java内存模型/01Java内存模型与线程/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/05java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/01Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"《Java虚拟机规范》中曾试图定义一种“Java内存模型”（Java Memory Model，JMM）来屏蔽各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。在此之前，主流程序语言（如C和C++等）直接使用物理硬件和操作系统的内存模型。因此，由于不同平台上内存模型的差异，有可能导致程序在一套平台上并发完全正常，而在另外一套平台上并发访问却经常出错，所以在某些场景下必须针对不同的平台来编写程序。 JMMJava内存模型的主要目的是定义程序中各种变量的访问规则，即关注在虚拟机中把变量值存储到内存和从内存中取出变量值这样的底层细节。在Java中，所有实例域、静态域和数组元素都存储在堆内存中，堆内存在线程之间共享(本章用“共享变量”这个术语代指实例域，静态域和数组元素)。局部变量(Local Variables)，方法定义参数(Java语言规范称之为Formal Method Parameters)和异常处理器参数(Exception Handler Parameters)不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。为了获得更好的执行效能，Java内存模型并没有限制执行引擎使用处理器的特定寄存器或缓存来和主内存进行交互，也没有限制即时编译器是否要进行调整代码执行顺序这类优化措施。 Java内存模型规定了所有的变量都存储在主内存（Main Memory）中（此处的主内存与介绍物理硬件时提到的主内存名字一样，两者也可以类比，但物理上它仅是虚拟机内存的一部分）。每条线程还有自己的工作内存（Working Memory，可与前面讲的处理器高速缓存类比），线程的工作内存中保存了被该线程使用的变量的主内存副本，线程对变量的所有操作(读取、赋值等)都必须在工作内存中进行，而不能直接读写主内存中的数据。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成，线程、主内存、工作内存三者的交互关系如图： 这里所讲的主内存、工作内存与第2章所讲的Java内存区域中的Java堆、栈、方法区等并不是同一个层次的对内存的划分，这两者基本上是没有任何关系的。 主内存直接对应于物理硬件的内存，而为了获取更好的运行速度，虚拟机（或者是硬件、操作系统本身的优化措施）可能会让工作内存优先存储于寄存器和高速缓存中，因为程序运行时主要访问的是工作内存。 JMM属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 重排序在执行程序时，为了提高性能，只要重排序两个操作的执行顺序,程序的执行结果就会被改变。编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时,会遵守数据依赖性,编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作, 不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 Within-Thread As-If-Serial Semantics ——线程内表现为串行的语义Within-Thread As-If-Serial Semantics：不管怎么重排序(编译器和处理器为了提高并行度)，单线程)程序的执行结果不能被改变。 为了遵守as-if-serial语义,编译器和处理器不会对存在数据依赖关系的操作做重排序,因为这种重排序会改变执行结果。但是,如果操作之间不存在数据依赖关系,这些操作就可能被编译器和处理器重排序。 happens-before（先行发生于）下面是Java内存模型下一些“天然的”先行发生关系，这些先行发生关系无须任何同步器协助就已经存在，可以在编码中直接使用。如果两个操作之间的关系不在此列，并且无法从下列规则推导出来，则它们就没有顺序性保障，虚拟机可以对它们随意地进行重排序。 这个原则非常重要，它是判断数据是否存在竞争，线程是否安全的非常有用的手段。依赖这个原则，我们可以通过几条简单规则一揽子解决并发环境下两个操作之间是否可能存在冲突的所有问题，而不需要陷入Java内存模型苦涩难懂的定义之中。 程序次序规则（Program Order Rule）：在一个线程内，按照控制流顺序，书写在前面的操作先行发生于书写在后面的操作。注意，这里说的是控制流顺序而不是程序代码顺序，因为要考虑分支、循环等结构。 管程锁定规则（Monitor Lock Rule）：一个unlock操作先行发生于后面对同一个锁的lock操作。这里必须强调的是“同一个锁”，而“后面”是指时间上的先后。 volatile变量规则（Volatile Variable Rule）：对一个volatile变量的写操作先行发生于后面对这个变量的读操作，这里的“后面”同样是指时间上的先后。 对final变量的写，happen-before于final域对象的读，happen-before于后续对final变量的读。 线程启动规则（Thread Start Rule）：Thread对象的start()方法先行发生于此线程的每一个动作。 线程终止规则（Thread Termination Rule）：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread::join()方法是否结束、Thread::isAlive()的返回值等手段检测线程是否已经终止执行。 join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。 线程中断规则（Thread Interruption Rule）：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread::interrupted()方法检测到是否有中断发生。 对象终结规则(Finalizer Rule)：一个对象的初始化完成(构造函数执行结束)happens-before于它的finalize()方法的开始。 传递性（Transitivity）：如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论。 间先后顺序与先行发生原则之间基本没有因果关系，所以我们衡量并发安全问题的时候不要受时间顺序的干扰，一切必须以先行发生原则为准。 volatile型变量的特殊规则关键字volatile可以说是Java虚拟机提供的最轻量级的同步机制 当一个变量被定义成volatile之后，它将具备两项特性： 第一项是保证此变量对所有线程的可见性，这里的“可见性”是指当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。而普通变量并不能做到这一点，普通变量的值在线程间传递时均需要通过主内存来完成。比如，线程A修改一个普通变量的值，然后向主内存进行回写，另外一条线程B在线程A回写完成了之后再对主内存进行读取操作，新变量值才会对线程B可见。 由于volatile变量只能保证可见性，在不符合以下两条规则的运算场景中，我们仍然要通过加锁（使用synchronized、java.util.concurrent中的锁或原子类）来保证原子性： 运算结果并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值。 变量不需要与其他的状态变量共同参与不变约束。 使用volatile变量的第二个语义是禁止指令重排序优化，普通的变量仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致。因为在同一个线程的方法执行过程中无法感知到这点，这就是Java内存模型中描述的所谓“线程内表现为串行的语义”（Within-Thread As-If-Serial Semantics）。 1234567891011121314151617Map configOptions;char[] configText; // 此变量必须定义为volatile// volatile boolean initialized = false;// 假设以下代码在线程A中执行// 模拟读取配置信息,当读取完成后// 将initialized设置为true,通知其他线程配置可用configOptions = new HashMap();configText = readConfigFile(fileName);processConfigOptions(configText, configOptions);initialized = true;// 假设以下代码在线程B中执行// 等待initialized为true,代表线程A已经把配置信息初始化完成while (!initialized) &#123; sleep();&#125;// 使用线程A中初始化好的配置信息// doSomethingWithConfig(); 如果定义initialized变量时没有使用volatile修饰，就可能会由于指令重排序的优化，导致位于线程A中最后一条代码“initialized&#x3D;true”被提前执行（这里虽然使用Java作为伪代码，但所指的重排序优化是机器级的优化操作，提前执行是指这条语句对应的汇编代码被提前执行），这样在线程B中使用配置信息的代码就可能出现错误，而volatile关键字则可以避免此类情况的发生。 假设线程A执行writer()方法之后，线程B执行reader()方法。根据happens-before规则，这个过程建立的happens-before关系可以分为3类： 根据程序次序规则，1 happens-before 2;3 happens-before 4。 根据volatile规则，2 happens-before 3。 根据happens-before的传递性规则，1 happens-before 4。形成的happens-before关系如图： 这里A线程写一个volatile变量后，B线程读同一个volatile变量。A线程在写volatile变量之前所有可见的共享变量，在B线程读同一个volatile变量后，将立即变得对B线程可见。 volatile写的内存语义如下当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存。 volatile读的内存语义如下当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 volatile内存语义的实现 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 针对long和double型变量的特殊规则允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行，这就是所谓的“long和double的非原子性协定” 在实际开发中，除非该数据有明确可知的线程竞争，否则我们在编写代码时一般不需要因为“long和double的非原子性协定”刻意把用到的long和double变量专门声明为volatile。 原子性、可见性与有序性Java内存模型是围绕着在并发过程中如何处理原子性、可见性和有序性这三个特征来建立的 原子性(Atomicity)Java内存模型还提供了lock和unlock操作来满足这种需求，尽管虚拟机未把lock和unlock操作直接开放给用户使用，但是却提供了更高层次的字节码指令monitorenter和monitorexit来隐式地使用这两个操作。这两个字节码指令反映到Java代码中就是同步块——synchronized关键字，因此在synchronized块之间的操作也具备原子性。 可见性(Visibility)可见性就是指当一个线程修改了共享变量的值时,其他线程能够立即得知这个修改。 可以用volatile、synchronized和final实现可见性 有序性(Ordering)Java程序中天然的有序性可以总结为一句话：如果在本线程内观察，所有的操作都是有序的;如果在一个线程中观察另一个线程， 所有的操作都是无序的。前半句是指“线程内似表现为串行的语义”(Within-Thread As-If-Serial Semantics)，后半句是指“指令重排序”现象和“工作内存与主内存同步延迟”现象。 Java语言提供了volatile和synchronized两个关键字来保证线程之间操作的有序性，volatile关键字本身就包含了禁止指令重排序的语义，而synchronized则是由“一个变量在同一个时刻只允许一条线程对其进行lock操作”这条规则获得的，这个规则决定了持有同一个锁的两个同步块只能串行地进入。 锁的内存语义从内存语义的角度来说，volatile的写-读与锁的释放-获取有相同的内存效果：volatile写和锁的释放有相同的内存语义;volatile读与锁的获取有相同的内存语义。 假设线程A执行writer()方法，随后线程B执行reader()方法。根据happens-before规则，这个 过程包含的happens-before关系可以分为3类。 根据程序次序规则，1 happens-before 2,2 happens-before 3;4 happens-before 5,5 happens- before 6。 根据监视器锁规则，3 happens-before 4。 根据happens-before的传递性，2 happens-before 5。 线程A在释放锁之前所有可见的共享变量，在线程B获取同一个锁之后，将立刻变得对B线程可见。 final域的内存语义对于final域，编译器和处理器要遵守两个重排序规则。 在构造函数内对一个final域的写入，与随后把这个被构造对象赋值给一个引用变量，这两个操作之间不能重排序。： 写final域的重排序规则禁止把final域的写重排序到构造函数之外 写final域的重排序规则可以确保：在对象引用为任意线程可见之前,对象的final域已经被正确初始化过了,而普通域不具有这个保障。 初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。 读final域的重排序规则可以确保：在读一个对象的final域之前,一定会先读包含这个final 域的对象的引用。 final域为一个引用类型，在构造函数内对一个final引用的对象的成员域 的写入，与随后在构造函数外把这个被构造对象赋值给一个引用变量，这两个操作之间不能重排序 在构造函数返回前,被构造对象的引用不能为其他线程所见,因为此时的final域可能还没有被初始化。在构造函数返回后,任意线程都将保证能看到final域正确初始化之后的值。 双重检查锁定与延迟初始化 在上类中,假设A线程执行代码4的同时,B线程执行代码7。此时,线程A可能会看到instance引用的对象还没有完成初始化 因为，前面的双重检查锁定示例代码的第7行(instance&#x3D;new Singleton();)创建了一个对象。这一行代码可以分解为如下的3行伪代码。 memory &#x3D; allocate(); &#x2F;&#x2F; 1：分配对象的内存空间 ctorInstance(memory); &#x2F;&#x2F; 2：初始化对象 instance &#x3D; memory; &#x2F;&#x2F; 3：设置instance指向刚分配的内存地址 上面3行伪代码中的2和3之间,可能会被重排序，2和3之间重排序之后的执行时序如下。 在知晓了问题发生的根源之后,我们可以想出两个办法来实现线程安全的延迟初始化。 不允许2和3重排序。 允许2和3重排序,但不允许其他线程“看到”这个重排序。 基于volatile的解决方案 当声明对象的引用为volatile后，3行伪代码中的2和3之间的重排序,在多线程环境中将会被禁止。 基于类初始化的解决方案在类的加载，JVM都会去获取一个锁，这个锁可以同步多个线程对同一个累的初始化。 基于这个特性,可以实现另一种线程安全的延迟初始化方案(这个方案被称之为Initialization On Demand Holder idiom)。 这是允许2和3重排序,但不允许其他线程“看到”这个重排序。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"虚拟机字节码指令表","slug":"jvm/04方法调用/虚拟机字节码指令表","date":"2021-11-19T12:00:44.000Z","updated":"2022-03-23T09:03:57.996Z","comments":true,"path":"blog/jvm/04方法调用/虚拟机字节码指令表/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/04%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8/%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%97%E8%8A%82%E7%A0%81%E6%8C%87%E4%BB%A4%E8%A1%A8/","excerpt":"","text":"","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"方法调用","slug":"jvm/04方法调用/方法调用","date":"2021-11-19T12:00:43.000Z","updated":"2022-03-23T09:03:57.967Z","comments":true,"path":"blog/jvm/04方法调用/方法调用/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/04%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8/%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8/","excerpt":"","text":"java的方法调用并不等同于方法中的代码被执行，方法调用阶段唯一的任务就是确定被调用方法的版本(即调用哪一个方法)，Class文件的编译过程中不包含传统程序语言编译的连接步骤，一切方法调用在Class文件里面存储的都只是符号引用，而不是方法在实际运行时内存布局中的入口地址(也就是之前说的直接引用)。 解析（类加载中的解析阶段）所有方法调用的目标方法在Class文件里面都是一个常量池中的符号引用，在类加载的解析阶段，会将其中的一部分符号引用转化为直接引用，这种解析能够成立的前提是：方法在程序真正运行之前就有一个可确定的调用版本，并且这个方法的调用版本在运行期是不可改变的。换句话说，调用目标在程序代码写好、编译器进行编译那一刻就已经确定下来。这类方法的调用被称为解析(Resolution)。 在Java语言中符合“编译期可知，运行期不可变”这个要求的方法，主要有静态方法和私有方法两大类，前者与类型直接关联，后者在外部不可被访问，这两种方法各自的特点决定了它们都不可能通过继承或别的方式重写出其他版本，因此它们都适合在类加载阶段进行解析。 调用不同类型的方法，字节码指令集里设计了不同的指令。在Java虚拟机支持以下5条方法调用字节码指令，分别是： invokestatic。用于调用静态方法。 invokespecial。用于调用实例构造器&lt;init&gt;()方法、私有方法和父类中的方法。 invokevirtual。用于调用所有的虚方法。 invokeinterface。用于调用接口方法，会在运行时再确定一个实现该接口的对象。 invokedynamic。先在运行时动态解析出调用点限定符所引用的方法，然后再执行该方法。前面4条调用指令，分派逻辑都固化在Java虚拟机内部，而invokedynamic指令的分派逻辑是由用户设定的引导方法来决定的 只要能被invokestatic和invokespecial指令调用的方法，都可以在解析阶段中确定唯一的调用版本，Java语言里符合这个条件的方法共有静态方法、私有方法、实例构造器、父类方法4种，再加上被final修饰的方法（尽管它使用invokevirtual指令调用），这5种方法调用会在类加载的时候就可以把符号引用解析为该方法的直接引用。这些方法统称为“非虚方法”（Non-Virtual Method），与之相反，其他方法就被称为“虚方法”（Virtual Method）。 1234567891011121314151617public class TestTT &#123; public void test(String a) &#123; byte[] placeholder = new byte[64 * 1024 * 1024]; System.gc(); &#125; public static void sayHello() &#123; System.out.println(&quot;hello world&quot;); &#125; public static void main(String[] args) &#123; TestTT.sayHello();// TestTT t = new TestTT();// t.test(&quot;2&quot;); &#125;&#125; class结构： 这个方法调用在编译期间就明确以常量池项的形式固化在字节码指令的参数之中了。而test方法仍然只是一个符号应用。 如果改下代码: 12345678910111213141516171819202122public class TestTT &#123; public void test(String a) &#123; test0(); byte[] placeholder = new byte[64 * 1024 * 1024]; System.gc(); &#125; private void test0() &#123; &#125; public static void sayHello() &#123; System.out.println(&quot;hello world&quot;); &#125; public static void main(String[] args) &#123; TestTT.sayHello();// TestTT t = new TestTT();// t.test(&quot;2&quot;); &#125;&#125; test0这个私有方法已经变为直接引用了 分派(“重载”，“重写”实现)要了解虚方法我们必须了解以下基础: Java 是一门面向对象的程序语言，因为 Java 具备面向对象的 3 个基本特征:继承、封装和多态。 分派调用过程将会揭示多态性特征的一些最基本的体现，如“重载”和“重写”在 Java 虚拟机之中是如何实现的 静态分派 invokestatic和invokespecial 下面代码 12345678910111213141516171819202122232425262728293031323334public static class StaticDispatch &#123; static abstract class Human &#123; &#125; static class Man extends Human &#123; &#125; static class Woman extends Human &#123; &#125; public void sayHello(Human guy) &#123; System.out.println(&quot;hello,guy!&quot;); &#125; public void sayHello(Man guy) &#123; System.out.println(&quot;hello,gentleman!&quot;); &#125; public void sayHello(Woman guy) &#123; System.out.println(&quot;hello,lady!&quot;); &#125; public static void main(String[] args) &#123; Human man = new Man(); Human woman = new Woman(); StaticDispatch sr = new StaticDispatch(); sr.sayHello(man); sr.sayHello(woman); &#125;&#125;结果：hello,guy!hello,guy! 1Human man = new Man(); 我们把上面代码中的“Human”称为变量的“静态类型”(Static Type)，或者叫“外观类型”(Apparent Type)，后面的“Man”则被称为变量的“实际类型”(Actual Type)或者叫“运行时类型”(Runtime Type)。静态类型和实际类型在程序中都可能会发生变化，区别是静态类型的变化仅仅在使用时发生，变量本身的静态类型不会被改变，并且最终的静态类型是在编译期可知的;而实际类型变化的结果在运行期才可确定，编译器在编译程序的时候并不知道一个对象的实际类型是什么。比如： 对象human的实际类型是可变的，编译期间它完全是个“薛定谔的人”，到底是Man还是Woman，必须等到程序运行到这行的时候才能确定。而human的静态类型是Human，也可以在使用时(如sayHello()方法中的强制转型)临时改变这个类型，但这个改变仍是在编译期是可知的，两次sayHello() 方法的调用，在编译期完全可以明确转型的是Man还是Woman。 虚拟机(或者准确地说是编译器)在重载时是通过参数的静态类型而不是实际类型作为判定依据的。由于静态类型在编译期可知，所以在编译阶段，Javac编译器就根据参数的静态类型决定了会使用哪个重载版本，因此选择了sayHello(Human)作为调用目标。 所有依赖静态类型来决定方法执行版本的分派动作，都称为静态分派。静态分派的最典型应用表现就是方法重载。静态分派发生在编译阶段，因此确定静态分派的动作实际上不是由虚拟机来执行的。 需要注意Javac编译器虽然能确定出方法的重载版本，但在很多情况下这个重载版本并不是“唯一”的，往往只能确定一个“相对更合适的”版本。典型的代码入下： 1234567891011121314151617181920212223242526272829303132333435public static class Overload &#123; public static void sayHello(Object arg) &#123; System.out.println(&quot;hello Object&quot;); &#125; public static void sayHello(int arg) &#123; System.out.println(&quot;hello int&quot;); &#125; public static void sayHello(long arg) &#123; System.out.println(&quot;hello long&quot;); &#125; public static void sayHello(Character arg) &#123; System.out.println(&quot;hello Character&quot;); &#125; public static void sayHello(char arg) &#123; System.out.println(&quot;hello char&quot;); &#125; public static void sayHello(char... arg) &#123; System.out.println(&quot;hello char ...&quot;); &#125; public static void sayHello(Serializable arg) &#123; System.out.println(&quot;hello Serializable&quot;); &#125; public static void main(String[] args) &#123; sayHello(&#x27;a&#x27;); &#125; &#125; 上面的代码运行后会输出：hello char这很好理解，’a’是一个char类型的数据，自然会寻找参数类型为char的重载方法，如果注释掉sayHello(char arg)方法，那输出会变为：hello int这时发生了一次自动类型转换，’a’除了可以代表一个字符串，还可以代表数字97（字符’a’的Unicode数值为十进制数字97），因此参数类型为int的重载也是合适的。我们继续注释掉sayHello(int arg)方法，那输出会变为：hello long这时发生了两次自动类型转换，’a’转型为整数97之后，进一步转型为长整数97L，匹配了参数类型为long的重载。笔者在代码中没有写其他的类型如float、double等的重载，不过实际上自动转型还能继续发生多次，按照char&gt;int&gt;long&gt;float&gt;double的顺序转型进行匹配，但不会匹配到byte和short类型的重载，因为char到byte或short的转型是不安全的。我们继续注释掉sayHello(long arg)方法，那输出会变为：hello Character这时发生了一次自动装箱，’a’被包装为它的封装类型java.lang.Character，所以匹配到了参数类型为Character的重载，继续注释掉sayHello(Character arg)方法，那输出会变为：hello Serializable这个输出可能会让人摸不着头脑，一个字符或数字与序列化有什么关系？出现hello Serializable，是因为java.lang.Serializable是java.lang.Character类实现的一个接口，当自动装箱之后发现还是找不到装箱类，但是找到了装箱类所实现的接口类型，所以紧接着又发生一次自动转型。char可以转型成int，但是Character是绝对不会转型为Integer的，它只能安全地转型为它实现的接口或父类。Character还实现了另外一个接口java.lang.Comparable，如果同时出现两个参数分别为Serializable和Comparable的重载方法，那它们在此时的优先级是一样的。编译器无法确定要自动转型为哪种类型，会提示“类型模糊”（Type Ambiguous），并拒绝编译。程序必须在调用时显式地指定字面量的静态类型，如：sayHello((Comparable)’a’)，才能编译通过。 动态分派 invokevirtual和invokeinterface 下面代码 123456789101112131415161718192021222324252627282930313233public static class DynamicDispatch &#123; static abstract class Human &#123; protected abstract void sayHello(); &#125; static class Man extends Human &#123; @Override protected void sayHello() &#123; System.out.println(&quot;man say hello&quot;); &#125; &#125; static class Woman extends Human &#123; @Override protected void sayHello() &#123; System.out.println(&quot;woman say hello&quot;); &#125; &#125; public static void main(String[] args) &#123; Human man = new Man(); Human woman = new Woman(); man.sayHello(); woman.sayHello(); man = new Woman(); man.sayHello(); &#125;&#125;结果：man say hello woman say hello woman say hello 显然这里选择调用的方法版本是不可能再根据静态类型来决定的，因为静态类型同样都是Human 的两个变量man和woman在调用sayHello()方法时产生了不同的行为，甚至变量man在两次调用中还执行了两个不同的方法。导致这个现象的原因很明显，是因为这两个变量的实际类型不同。我们看javap输出的字节码： 0~15行的字节码是准备动作，作用是建立man和woman的内存空间、调用Man和Woman类型的构造方法，讲这两个实例的引用存放到第一和第二个局部变量表的变量槽中。对应的源码是 12Human man = new Man();Human woman = new Woman(); 接下来的16~21行是关键部分，16和20行的aload指令分别把刚刚创建的两个对象的引用压到栈顶，这两个对象是将要执行的sayHello()方法的所有者，称为接收者(Receiver)。17和21调用了invokevirtual指令，而invokevirtual指令运行时解析过程大致分为以下几步: 找到操作数栈顶的第一个元素所指向的对象的实际类型，记作C。 如果在类型C中找到与常量中的描述符和简单名称都相符的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用，查找过程结束；不通过则返回java.lang.IllegalAccessError异常。 否则，按照继承关系从下往上依次对C的各个父类进行第二步的搜索和验证过程。 如果始终没有找到合适的方法，则抛出java.lang.AbstractMethodError异常。 正是因为invokevirtual指令执行的第一步就是在运行期确定接收者的实际类型，所以两次调用中的invokevirtual指令并不是把常量池中方法的符号引用解析到直接引用上就结束了，还会根据方法接收者的实际类型来选择方法版本，这个过程就是Java语言中方法重写的本质。我们把这种在运行期根据实际类型确定方法执行版本的分派过程称为动态分派。 字段永远不参与多态，哪个类的方法访问某个名字的字段时，该名字指的就是这个类能看到的那个字段。当子类声明了与父类同名的字段时，虽然在子类的内存中两个字段都会存在，但是子类的字段会遮蔽父类的同名字段。 1234567891011121314151617181920212223242526272829public class FieldHasNoPolymorphic &#123; static class Father &#123; public int money = 1; public Father() &#123; money = 2; showMeTheMoney(); &#125; public void showMeTheMoney() &#123; System.out.println(&quot;I am Father, i have $&quot; + money); &#125; &#125; static class Son extends Father &#123; public int money = 3; public Son() &#123; money = 4; showMeTheMoney(); &#125; public void showMeTheMoney() &#123; System.out.println(&quot;I am Son, i have $&quot; + money); &#125; &#125; public static void main(String[]args)&#123; Father gay = new Son(); System.out.println(&quot;This gay has $&quot; + gay.money); &#125;&#125; 运行后输出结果为：I am Son, i have $0I am Son, i have ​$4This gay has ​$2 输出两句都是“I am Son”，这是因为Son类在创建的时候，首先隐式调用了Father的构造函数，而Father构造函数中对showMeTheMoney()的调用是一次虚方法调用，实际执行的版本是Son::showMeTheMoney()方法，所以输出的是“I am Son”，这点经过前面的分析相信读者是没有疑问的了。而这时候虽然父类的money字段已经被初始化成2了，但Son::showMeTheMoney()方法中访问的却是子类的money字段，这时候结果自然还是0，因为它要到子类的构造函数执行时才会被初始化。main()的最后一句通过静态类型访问到了父类中的money，输出了2。 虚拟机动态分派的实现动态分派会执行非常频繁的动作，JVM 运行时会频繁的、反复的去搜索元数据，所以 JVM 使用了一种优化手段，这个就是在方法区中建立一个虚方法表。 使用虚方法表索引来替代元数据查找以提高性能。 在实现上，最常用的手段就是为类在方法区中建立一个虚方法表。虚方法表中存放着各个方法的实际入口地址。如果某个方法在子类中没有被重写，那 子类的虚方法表里面的地址入口和父类相同方法的地址入口是一致的，都指向父类的实现入口。如果子类中重写了这个方法，子类方法表中的地址将会 替换为指向子类实现版本的入口地址。PPT 图中，Son 重写了来自 Father 的全部方法，因此 Son 的方法表没有指向 Father 类型数据的箭头。但是 Son 和 Father 都没有重写来自 Object 的方法，所以它们的方法表中所有从 Object 继承来的方法都指向了 Object 的数据类型。 接口调用invokeinterface 和 invokevirtual 指令类似，不过作用于接口类; 0～7是先创建对象，然后进入到槽1，8把对象压入栈顶，9从栈顶拿到对象，并在对象的类型的常量池中找到与描述符和简单名称都相符的方法 方法句柄(MethodHandle)举个例子，如果我们要实现一个带谓词（谓词就是由外部传入的排序时比较大小的动作）的排序函数，在C&#x2F;C++中的常用做法是把谓词定义为函数，用函数指针来把谓词传递到排序方法，像这样： 1void sort(int list[], const int size, int (*compare)(int, int)) 但在Java语言中做不到这一点，没有办法单独把一个函数作为参数进行传递。 不过，在拥有方法句柄之后，Java语言也可以拥有类似于函数指针或者委托的方法别名这样的工具了。 MethodHandle 是什么?简单的说就是方法句柄，通过这个句柄可以调用相应的方法。 用 MethodHandle 调用方法的流程为: 创建 MethodType,获取指定方法的签名(出参和入参) 在 Lookup 中查找 MethodType 的方法句柄 MethodHandle 传入方法参数通过 MethodHandle 调用方法 12345678910111213141516171819202122232425262728293031323334353637public class MethodHandleDemo &#123; static class Bike &#123; String sound() &#123; return &quot;ding ding ding&quot;; &#125; &#125; static class Animal &#123; String sound() &#123; return &quot;wow wow wow&quot;; &#125; &#125; static class Man extends Animal &#123; @Override String sound() &#123; return &quot;ha ha ha&quot;; &#125; &#125; MethodHandle getSoundMh(Object o) throws Throwable &#123; //方法句柄--工厂方法Factory MethodHandles.Lookup lookup = MethodHandles.lookup(); //方法类型表示接受的参数和返回类型（第一个参数是返回参数） MethodType methodType = MethodType.methodType(String.class); //拿到具体的MethodHandle(findVirtual相当于字节码) MethodHandle methodHandle = lookup.findVirtual(o.getClass(), &quot;sound&quot;, methodType); return methodHandle; &#125; public static void main(String[] args) throws Throwable &#123; MethodHandle methodHandle = new MethodHandleDemo().getSoundMh(new Bike());//每次送入的实例不一样 System.out.println((String) methodHandle.invoke(new Bike())); System.out.println((String) methodHandle.invoke(new Animal())); System.out.println((String) methodHandle.invoke(new Man())); &#125;&#125; 方法getSoundMh()中实际上是模拟了invokevirtual指令的执行过程，只不过它的分派逻辑并非固化在Class文件的字节码上，而是通过一个由用户设计的Java方法来实现。而这个方法本身的返回值（MethodHandle对象），可以视为对最终调用方法的一个“引用”。以此为基础，有了MethodHandle就可以写出类似于C&#x2F;C++那样的函数声明了： 使用MethodHandle并没有多少困难，不过看完它的用法之后，读者大概就会产生疑问，相同的事情，用反射不是早就可以实现了吗？ 仅站在Java语言的角度看，MethodHandle在使用方法和效果上与Reflection有众多相似之处。不过，它们也有以下这些区别： Reflection和MethodHandle机制本质上都是在模拟方法调用，但是Reflection是在模拟Java代码层次的方法调用，而MethodHandle是在模拟字节码层次的方法调用。在MethodHandles.Lookup上的3个方法findStatic()、findVirtual()、findSpecial()正是为了对应于invokestatic、invokevirtual（以及invokeinterface）和invokespecial这几条字节码指令的执行权限校验行为，而这些底层细节在使用Reflection API时是不需要关心的。 Reflection中的java.lang.reflect.Method对象远比MethodHandle机制中的java.lang.invoke.MethodHandle对象所包含的信息来得多。前者是方法在Java端的全面映像，包含了方法的签名、描述符以及方法属性表中各种属性的Java端表示方式，还包含执行权限等的运行期信息。而后者仅包含执行该方法的相关信息。用开发人员通俗的话来讲，Reflection是重量级，而MethodHandle是轻量级。 由于MethodHandle是对字节码的方法指令调用的模拟，那理论上虚拟机在这方面做的各种优化（如方法内联），在MethodHandle上也应当可以采用类似思路去支持（但目前实现还在继续完善中），而通过反射去调用方法则几乎不可能直接去实施各类调用点优化措施。 MethodHandle与Reflection除了上面列举的区别外，最关键的一点还在于去掉前面讨论施加的前提“仅站在Java语言的角度看”之后：Reflection API的设计目标是只为Java语言服务的，而MethodHandle则设计为可服务于所有Java虚拟机之上的语言，其中也包括了Java语言而已，而且Java在这里并不是主角。 MethodTypeMethodType 表示一个方法类型的对象，每个 MethodHandle 都有一个 MethodType 实例，MethodType 用来指明方法的返回类型和参数类型。其有多个工厂方法的重载。 LookupMethodHandle.Lookup 可以通过相应的 findxxx 方法得到相应的 MethodHandle，相当于 MethodHandle 的工厂方法。查找对象上的工厂方法对应于方法、 构造函数和字段的所有主要用例。 findStatic 相当于得到的是一个 static 方法的句柄(类似于 invokestatic 的作用)，findVirtual 找的是普通方法(类似于 invokevirtual 的作用) invoke其中需要注意的是 invoke 和 invokeExact,前者在调用的时候可以进行返回值和参数的类型转换工作，而后者是精确匹配的。 所以一般在使用是，往往 invoke 使用比 invokeExact 要多，因为 invokeExact 如果类型不匹配，则会抛错。 invokedynamic某种意义上可以说invokedynamic指令与MethodHandle机制的作用是一样的，都是为了解决原有4条“invoke*”指令方法分派规则完全固化在虚拟机之中的问题，把如何查找目标方法的决定权从虚拟机转嫁到具体用户代码之中，让用户（广义的用户，包含其他程序语言的设计者）有更高的自由度。而且，它们两者的思路也是可类比的，都是为了达成同一个目的，只是一个用上层代码和API来实现，另一个用字节码和Class中其他属性、常量来完成。 每一处含有invokedynamic指令的位置都被称作“动态调用点（Dynamically-Computed Call Site）”，这条指令的第一个参数不再是代表方法符号引用的CONSTANT_Methodref_info常量，而是变为JDK 7时新加入的CONSTANT_InvokeDynamic_info常量，从这个新常量中可以得到3项信息：引导方法（Bootstrap Method，该方法存放在新增的BootstrapMethods属性中）、方法类型（MethodType）和名称。引导方法是有固定的参数，并且返回值规定是java.lang.invoke.CallSite对象，这个对象代表了真正要执行的目标方法调用。根据CONSTANT_InvokeDynamic_info常量中提供的信息，虚拟机可以找到并且执行引导方法，从而获得一个CallSite对象，最终调用到要执行的目标方法上。 1234567891011121314151617181920212223242526272829303132//InvokeDynamic指令演示import java.lang.invoke.*;public class InvokeDynamicTest &#123; public static void testMethod(String s) &#123; System.out.println(&quot;hello string:&quot; + s); &#125; public static CallSite BootstrapMethod(MethodHandles.Lookup lookup, String name, MethodType mt) throws Throwable &#123; return new ConstantCallSite(lookup.findStatic(InvokeDynamicTest.class, name, mt)); &#125; private static MethodType MT_BootstrapMethod() &#123; return MethodType.fromMethodDescriptorString( &quot;(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite;&quot;, null); &#125; private static MethodHandle MH_BootstrapMethod() throws Throwable &#123; return MethodHandles.lookup() .findStatic(InvokeDynamicTest.class, &quot;BootstrapMethod&quot;, MT_BootstrapMethod()); &#125; private static MethodHandle INDY_BootstrapMethod() throws Throwable&#123; CallSite cs = (CallSite)MH_BootstrapMethod().invokeWithArguments(MethodHandles.lookup(),&quot;testMethod&quot;,MethodType.fromMethodDescriptorString(&quot;(Ljava/lang/String;)V&quot;,null)); return cs.dynamicInvoker(); &#125; public static void main(String[] args) throws Throwable &#123; INDY_BootstrapMethod().invokeExact(&quot;hyl&quot;); &#125;&#125; 使用Indify—把程序的字节码转换为使用invokedynamic的简单工具生成一个class文件，然后用javap输出字节码： 从main()方法的字节码中可见，原本的方法调用指令已经被替换为invokedynamic了，它的参数为第123项常量（第二个值为0的参数在虚拟机中不会直接用到，这与invokeinterface指令那个的值为0的参数一样是占位用的，目的都是为了给常量池缓存留出足够的空间）： 12: invokedynamic #123, 0 // InvokeDynamic #0:testMethod:(Ljava/lang/String;)V 从常量池中可见，第123项常量显示“#123=InvokeDynamic#0：#121”说明它是一项CONSTANT_InvokeDynamic_info类型常量，常量值中前面“#0”代表引导方法取Bootstrap Methods属性表的第0项（javap没有列出属性表的具体内容，不过示例中仅有一个引导方法，即BootstrapMethod()），而后面的“#121”代表引用第121项类型为CONSTANT_NameAndType_info的常量，从这个常量中可以获取到方法名称和描述符，即后面输出的“testMethod：(Ljava/lang/String；)V”。再看BootstrapMethod()，这个方法在Java源码中并不存在，是由INDY产生的，但是它的字节码很容易读懂，所有逻辑都是调用MethodHandles$Lookup的findStatic()方法，产生testMethod()方法的MethodHandle，然后用它创建一个ConstantCallSite对象。最后，这个对象返回给invokedynamic指令实现对testMethod()方法的调用，invokedynamic指令的调用过程到此就宣告完成了。 Lambda 语法这个指令通常在 Lambda 语法中出现，我们来看一下一小段代码: 123456public class TestTT &#123; public static void main(String[] args) &#123; Runnable run = () -&gt; System.out.println(&quot;测试Lambda&quot;); run.run(); &#125;&#125; 使用 javap -p -v 命令可以在 main 方法中看到 invokedynamic 指令: BootstrapMethods 属性在 Java 1.7 以后才有，位于类文件的属性列表中，这个属性用于保存 invokedynamic 指令引用的引导方法限定符。 和上面介绍的四个指令不同，invokedynamic 并没有确切的接受对象，取而代之的，是一个叫 CallSite 的对象。 Lambda 表达式的捕获与非捕获当 Lambda 表达式访问一个定义在 Lambda 表达式体外的非静态变量或者对象时，这个 Lambda 表达式称为“捕获的” 那么“非捕获”的 Lambda 表达式来就是 Lambda 表达式没有访问一个定义在 Lambda 表达式体外的非静态变量或者对象 Lambda 表达式是否是捕获的和性能悄然相关。一个非捕获的 lambda 通常比捕获的更高效，非捕获的 lambda 只需要计算一次. 然后每次使用到它都会返 回一个唯一的实例。而捕获的 lambda 表达式每次使用时都需要重新计算一次，而且从目前实现来看，它很像实例化一个匿名内部类的实例。 lambda 最差的情况性能内部类一样， 好的情况肯定比内部类性能高。 总结Lambda 语言实际上是通过方法句柄来完成的，大致这么实现(JVM 编译的时候使用 invokedynamic 实现 Lambda 表达式，invokedynamic 的是使用 MethodHandle 实现的，所以 JVM 会根据你编写的 Lambda 表达式的代码，编译出一套可以去调用 MethodHandle 的字节码代码 句柄类型(MethodType)是我们对方法的具体描述，配合方法名称，能够定位到一类函数。访问方法句柄和调用原来的指令基本一致，但它的调用异常， 包括一些权限检查，在运行时才能被发现。 案例中，我们完成了动态语言的特性，通过方法名称和传入的对象主体，进行不同的调用，而 Bike 和 Man 类，可以没有任何关系。 可以看到 Lambda 语言实际上是通过方法句柄来完成的，在调用链上自然也多了一些调用步骤，那么在性能上，是否就意味着 Lambda 性能低呢?对于 大部分“非捕获”的 Lambda 表达式来说，JIT 编译器的逃逸分析能够优化这部分差异，性能和传统方式无异;但对于“捕获型”的表达式来说，则需要通过方法句柄，不断地生成适配器，性能自然就低了很多(不过和便捷性相比，一丁点性能损失是可接受的)。 invokedynamic 指令，它实际上是通过方法句柄来实现的。和我们关系最大的就是 Lambda 语法，我们了解原理，可以忽略那些对 Lambda 性能高低的 争论，同时还是要尽量写一些“非捕获”的 Lambda 表达式。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"实战：掌控方法分派规则","slug":"jvm/04方法调用/实战：掌控方法分派规则","date":"2021-11-19T12:00:42.000Z","updated":"2022-03-23T09:03:57.922Z","comments":true,"path":"blog/jvm/04方法调用/实战：掌控方法分派规则/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/04%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8/%E5%AE%9E%E6%88%98%EF%BC%9A%E6%8E%8C%E6%8E%A7%E6%96%B9%E6%B3%95%E5%88%86%E6%B4%BE%E8%A7%84%E5%88%99/","excerpt":"","text":"invokedynamic指令与此前4条传统的“invoke*”指令的最大区别就是它的分派逻辑不是由虚拟机决定的，而是由程序员决定。 在Java程序中，可以通过“super”关键字很方便地调用到父类中的方法，但如果要访问祖类的方法呢？ 在拥有invokedynamic和java.lang.invoke包之前，使用纯粹的Java语言很难处理这个问题（使用ASM等字节码工具直接生成字节码当然还是可以处理的，但这已经是在字节码而不是Java语言层面来解决问题了），原因是在Son类的thinking()方法中根本无法获取到一个实际类型是GrandFather的对象引用，而invokevirtual指令的分派逻辑是固定的，只能按照方法接收者的实际类型进行分派，这个逻辑完全固化在虚拟机中，程序员无法改变。 使用MethodHandle来解决问题： 123456789101112131415161718192021222324252627282930313233343536373839import java.lang.invoke.MethodHandle;import java.lang.invoke.MethodHandles;import java.lang.invoke.MethodType;import java.lang.reflect.Field;class Test &#123; class GrandFather &#123; void thinking() &#123; System.out.println(&quot;i am grandfather&quot;); &#125; &#125; class Father extends GrandFather &#123; void thinking() &#123; System.out.println(&quot;i am father&quot;); &#125; &#125; class Son extends Father &#123; void thinking() &#123; try &#123; /** * 是必须保证findSpecial()查找方法版本时受到的访问约束（譬如对访问控制的限制、对参数类型的限制）应与使用invokespecial指令一样，两者必须保持精确对等，包括在上面的场景中它只能访问到其直接父类中的方法版本。 * 去查看MethodHandles.Lookup类的代码，将会发现需要进行哪些访问保护，在该API实现时是预留了后门的。访问保护是通过一个allowedModes的参数来控制，而且这个参数可以被设置成“TRUSTED”来绕开所有的保护措施。尽管这个参数只是在Java类库本身使用，没有开放给外部设置，但我们通过反射可以轻易打破这种限制。 */ MethodType mt = MethodType.methodType(void.class); Field lookupImpl = MethodHandles.Lookup.class.getDeclaredField(&quot;IMPL_LOOKUP&quot;); lookupImpl.setAccessible(true); MethodHandle mh = ((MethodHandles.Lookup) lookupImpl.get(null)).findSpecial(GrandFather.class, &quot;thinking&quot;, mt, GrandFather.class); mh.invoke(this); &#125; catch (Throwable e) &#123; &#125; &#125; &#125; public static void main(String[] args) &#123; (new Test().new Son()).thinking(); &#125;&#125;","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"字节码生成技术与动态代理的实现——动态代理","slug":"jvm/04方法调用/字节码生成技术与动态代理的实现——动态代理","date":"2021-11-19T12:00:41.000Z","updated":"2022-03-23T09:03:57.921Z","comments":true,"path":"blog/jvm/04方法调用/字节码生成技术与动态代理的实现——动态代理/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/04%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8/%E5%AD%97%E8%8A%82%E7%A0%81%E7%94%9F%E6%88%90%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E7%9A%84%E5%AE%9E%E7%8E%B0%E2%80%94%E2%80%94%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;public class DynamicProxyTest &#123; interface IHello &#123; void sayHello(); &#125; static class Hello implements IHello &#123; @Override public void sayHello() &#123; System.out.println(&quot;hello world&quot;); &#125; &#125; static class DynamicProxy implements InvocationHandler &#123; Object originalObj; Object bind(Object originalObj) &#123; this.originalObj = originalObj; return Proxy.newProxyInstance(originalObj.getClass().getClassLoader(), originalObj.getClass().getInterfaces(), this); &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(&quot;welcome&quot;); return method.invoke(originalObj, args); &#125; &#125; public static void main(String[] args) &#123; IHello hello = (IHello) new DynamicProxy().bind(new Hello()); hello.sayHello(); &#125;&#125; 在上述代码里,唯一的“黑匣子”就是Proxy::newProxyInstance()方法,除此之外再没有任何特殊之处。这个方法返回一个实现了IHello的接口,并且代理了new Hello()实例行为的对象。跟踪这个方法的源码,可以看到程序进行过验证、优化、缓存、同步、生成字节码、显式类加载等操作。这里只分析它最后调用sun.misc.ProxyGenerator::generateProxyClass()方法来完成生成字节码的动作,这个方法会在运行时产生一个描述代理类的字节码byte[]数组。如果想看一看这个在运行时产生的代理类中写了些什么,可以在main()方法中加入下面这句: 1System.getProperties().put(&quot;sun.misc.ProxyGenerator.saveGeneratedFiles&quot;, &quot;true&quot;); 加入这句代码后再次运行程序,磁盘中将会产生一个名为“$Proxy0.class”的代理类Class文件,反编译后可以看见如代码清单： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263final class $Proxy0 extends Proxy implements IHello &#123; private static Method m1; private static Method m3; private static Method m2; private static Method m0; public $Proxy0(InvocationHandler var1) throws &#123; super(var1); &#125; public final boolean equals(Object var1) throws &#123; try &#123; return (Boolean)super.h.invoke(this, m1, new Object[]&#123;var1&#125;); &#125; catch (RuntimeException | Error var3) &#123; throw var3; &#125; catch (Throwable var4) &#123; throw new UndeclaredThrowableException(var4); &#125; &#125; public final void sayHello() throws &#123; try &#123; super.h.invoke(this, m3, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final String toString() throws &#123; try &#123; return (String)super.h.invoke(this, m2, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final int hashCode() throws &#123; try &#123; return (Integer)super.h.invoke(this, m0, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; static &#123; try &#123; m1 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;equals&quot;, Class.forName(&quot;java.lang.Object&quot;)); m3 = Class.forName(&quot;com.xyz.pdf.tes.DynamicProxyTest$IHello&quot;).getMethod(&quot;sayHello&quot;); m2 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;toString&quot;); m0 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;hashCode&quot;); &#125; catch (NoSuchMethodException var2) &#123; throw new NoSuchMethodError(var2.getMessage()); &#125; catch (ClassNotFoundException var3) &#123; throw new NoClassDefFoundError(var3.getMessage()); &#125; &#125;&#125; 这个代理类的实现代码也很简单,它为传入接口中的每一个方法,以及从java.lang.Object中继承来的equals()、hashCode()、toString()方法都生成了对应的实现,并且统一调用了InvocationHandler对象的invoke()方法(代码中的“this.h”就是父类Proxy中保存的InvocationHandler实例变量)来实现这些方法的内容,各个方法的区别不过是传入的参数和Method对象有所不同而已,所以无论调用动态代理的哪一个方法,实际上都是在执行InvocationHandler::invoke()中的代理逻辑。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"基于栈的解释器执行过程","slug":"jvm/04方法调用/基于栈的解释器执行过程","date":"2021-11-19T12:00:40.000Z","updated":"2022-03-23T09:03:57.921Z","comments":true,"path":"blog/jvm/04方法调用/基于栈的解释器执行过程/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/04%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8/%E5%9F%BA%E4%BA%8E%E6%A0%88%E7%9A%84%E8%A7%A3%E9%87%8A%E5%99%A8%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/","excerpt":"","text":"1234567public int calc() &#123; int a = 100; int b = 200; int c = 300; return (a + b) * c; &#125; 用javap输出字节码： 首先,执行偏移地址为0的指令,Bipush指令的作用是将单字节的整型常量值(-128~127)推入操作数栈顶,跟随有一个参数,指明推送的常量值,这里是100。 执行偏移地址为2的指令,istore_1指令的作用是将操作数栈顶的整型值出栈并存放到第1个局部变量槽中。后续4条指令(直到偏移为11的指令为止)都是做一样的事情,也就是在对应代码中把变量a、b、c赋值为100、200、300。这4条指令的图示略过。 执行偏移地址为11的指令,iload_1指令的作用是将局部变量表第1个变量槽中的整型值复制到操作数栈顶。 执行偏移地址为12的指令,iload_2指令的执行过程与iload_1类似,把第2个变量槽的整型值入栈。 执行偏移地址为13的指令,iadd指令的作用是将操作数栈中头两个栈顶元素出栈,做整型加法, 然后把结果重新入栈。在iadd指令执行完毕后,栈中原有的100和200被出栈,它们的和300被重新入栈。 执行偏移地址为14的指令,iload_3指令把存放在第3个局部变量槽中的300入栈到操作数栈中。这时操作数栈为两个整数300。下一条指令imul是将操作数栈中头两个栈顶元素出栈,做整型乘法,然后把结果重新入栈,与iadd完全类似 执行偏移地址为16的指令,ireturn指令是方法返回指令之一,它将结束方法执行并将操作数栈顶的整型值返回给该方法的调用者。 再次强调上面的执行过程仅仅是一种概念模型,虚拟机最终会对执行过程做出一系列优化来提高性能,实际的运作过程并不会完全符合概念模型的描述。更确切地说,实际情况会和上面描述的概念模型差距非常大,差距产生的根本原因是虚拟机中解析器和即时编译器都会对输入的字节码进行优化,即使解释器中也不是按照字节码指令去逐条执行的。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"类加载过程","slug":"jvm/03虚拟机类加载机制/类加载过程","date":"2021-11-19T12:00:39.000Z","updated":"2022-03-23T09:03:57.920Z","comments":true,"path":"blog/jvm/03虚拟机类加载机制/类加载过程/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/03%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/","excerpt":"","text":"类加载的时机一个类型从被加载到虚拟机内存中开始,到卸载出内存为止,它的整个生命周期将会经历加载(Loading)、验证(Verification)、准备(Preparation)、解析(Resolution)、初始化(Initialization)、使用(Using)和卸载(Unloading)七个阶段,其中验证、准备、解析三个部分统称为连接(Linking)加载、验证、准备、初始化和卸载这五个阶段的顺序是确定的，类型的加载过程必须按照这种顺序按部就班地开始，而解析阶段则不一定:它在某些情况下可以在初始化阶段之后再开始， 这是为了支持Java语言的运行时绑定特性(也称为动态绑定或晚期绑定) 只有以下情况必须立即对类进行“初始化” 使用new关键字实例化对象的时候。 读取或设置（（getstatic或putstatic））一个类型的静态字段(被final修饰、已在编译期把结果放入常量池的静态字段除外) 的时候。（被final修饰后，编译器在编译这个字段的时候，会在对应的field_info结构体中增加一个ConstantValue类型的结构体，在赋值的时候使用这个ConstantValue进行赋值） 调用一个类型的静态方法（invokestatic）的时候。 使用java.lang.reflect包的方法对类型进行反射调用的时候，如果类型没有进行过初始化，则需要先触发其初始化。 当初始化类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类(包含main()方法的那个类)，虚拟机会先初始化这个主类。 当使用JDK 7新加入的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果为REF_getStatic、REF_putStatic、REF_invokeStatic、REF_newInvokeSpecial四种类型的方法句柄，并且这个方法句柄对应的类没有进行过初始化，则需要先触发其初始化。 当一个接口中定义了JDK 8新加入的默认方法(被default关键字修饰的接口方法)时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。 上述的行为称为对一个类型进行主动引用。除此之外，所有引用类型的方式都不会触发初始化，称为被动引用。 对于静态字段，只有直接定义这个字段的类才会被初始化，因此通过其子类来引用父类中定义的静态字段，只会触发父类的初始化而不会触发子类的初始化。至于是否要触发子类的加载和验证阶段，在《Java虚拟机规范》中并未明确规定，所以这点取决于虚拟机的具体实现。对于HotSpot虚拟机来说，可通过-XX:+TraceClassLoading参数观察到此操作是会导致子类加载的。 该代码没有触发SuperClass类的初始化，而是出发了另一个名为“[Lorg.fenixsoft.classloading.SuperClass”的类的初始化阶段，对于用户代码来说，这并不是一个合法的类型名称，它是一个由虚拟机自动生成的、直接继承于java.lang.Object的子类，创建动作由字节码指令newarray触发。这个类代表了一个元素类型为org.fenixsoft.classloading.SuperClass的一维数组，数组中应有的属性和方法(用户可直接使用的只有被修饰为public的length属性和clone()方法)都实现在这个类里 没有触发类的初始化，因为对于这种final字段在编译阶段通过常量传播优化，已经将此常量的值“hello world”直接存储在NotInitialization类的常量池中，以后NotInitialization对常量ConstClass.HELLOWORLD的引用，实际都被转化为NotInitialization类对自身常量池的引用了。也就是说，实际上NotInitialization的Class文件之中并没有ConstClass类的符号引用入口，这两个类在编译成Class文件后就已不存在任何联系了。 接口的加载过程与类加载过程稍有不同，针对接口需要做一些特殊说明：接口也有初始化过程，这点与类是一致的，上面的代码都是用静态语句块“static{}”来输出初始化信息的，而接口中不能使用“static{}”语句块，但编译器仍然会为接口生成“&lt;clinit&gt;()”类构造器，用于初始化接口中所定义的成员变量。接口与类真正有所区别的是前面讲述的六种“有且仅有”需要触发初始化场景中的第三种：当一个类在初始化时，要求其父类全部都已经初始化过了，但是一个接口在初始化时，并不要求其父接口全部都完成了初始化，只有在真正使用到父接口的时候（如引用接口中定义的常量）才会初始化。 类加载的过程加载JVM 虚拟机的实现都是使用的懒加载，就是什么时候需要这个类了我才去加载，并不是说一个 jar 文件里面有 200 多个类，但实际我只用到了其中的一个类，我不需要把 200 多个类全部加载进来。 在加载阶段，Java虚拟机需要完成以下三件事情： 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。 《Java虚拟机规范》对这三点要求其实并不是特别具体，留给虚拟机实现与Java应用的灵活度都是相当大的。例如“通过一个类的全限定名来获取定义此类的二进制字节流”这条规则，它并没有指明二进制字节流必须得从某个Class文件中获取，确切地说是根本没有指明要从哪里获取、如何获取。仅仅这一点空隙，Java虚拟机的使用者们就可以在加载阶段搭构建出一个相当开放广阔的舞台，Java发展历程中，充满创造力的开发人员则在这个舞台上玩出了各种花样，许多举足轻重的Java技术都建立在这一基础之上，例如: 从ZIP压缩包中读取，这很常见，最终成为日后JAR、EAR、WAR格式的基础。 从网络中获取，这种场景最典型的应用就是Web Applet。 运行时计算生成，这种场景使用得最多的就是动态代理技术，在java.lang.reflect.Proxy中，就是用了ProxyGenerator.generateProxyClass()来为特定接口生成形式为“*$Proxy”的代理类的二进制字节流。 由其他文件生成，典型场景是JSP应用，由JSP文件生成对应的Class文件。 从数据库中读取，这种场景相对少见些，例如有些中间件服务器（如SAP Netweaver）可以选择把程序安装到数据库中来完成程序代码在集群间的分发。 可以从加密文件中获取，这是典型的防Class文件被反编译的保护措施，通过加载时解密Class文件来保障程序运行逻辑不被窥探。 相对于类加载过程的其他阶段，非数组类型的加载阶段(准确地说，是加载阶段中获取类的二进制字节流的动作)是开发人员可控性最强的阶段。加载阶段既可以使用Java虚拟机里内置的引导类加载器来完成，也可以由用户自定义的类加载器去完成，开发人员通过定义自己的类加载器去控制字节流的获取方式(重写一个类加载器的findClass()或loadClass()方法)，实现根据自己的想法来赋予应用程序获取运行代码的动态性。 于数组类而言，情况就有所不同，数组类本身不通过类加载器创建，它是由Java虚拟机直接在内存中动态构造出来的。但数组类与类加载器仍然有很密切的关系，因为数组类的元素类型(Element Type，指的是数组去掉所有维度的类型)最终还是要靠类加载器来完成加载，一个数组类(下面简称为C)创建过程遵循以下规则: 如果数组的组件类型(Component Type，指的是数组去掉一个维度的类型，注意和前面的元素类型区分开来)是引用类型，那就递归采用本节中定义的加载过程去加载这个组件类型，数组C将被标识在加载该组件类型的类加载器的类名称空间上(这点很重要，一个类型必须与类加载器一起确定唯一性)。 如果数组的组件类型不是引用类型(例如int[]数组的组件类型为int)，Java虚拟机将会把数组C 标记为与引导类加载器关联。 数组类的可访问性与它的组件类型的可访问性一致，如果组件类型不是引用类型，它的数组类的可访问性将默认为public，可被所有的类和接口访问到。 加载阶段结束后，Java虚拟机外部的二进制字节流就按照虚拟机所设定的格式存储在方法区之中了。类型数据妥善安置在方法区之后，会在Java堆内存中实例化一个java.lang.Class类的对象，这个对象将作为程序访问方法区中的类型数据的外部接口。 加载阶段与连接阶段的部分动作(如一部分字节码文件格式验证动作)是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的一部分，这两个阶段的开始时间仍然保持着固定的先后顺序。 验证验证是连接阶段的第一步，这一阶段的目的是确保Class文件的字节流中包含的信息符合《Java虚拟机规范》的全部约束要求，保证这些信息被当作代码运行后不会危害虚拟机自身的安全。 验证阶段对于虚拟机的类加载机制来说，是一个非常重要的、 但却不是必须要执行的阶段，因为验证阶段只有通过或者不通过的差别，只要通过了验证， 其后就对程序运行期没有任何影响了。如果程序运行的全部代码(包括自己编写的、第三方包中的、从外部加载的、动态生成的等所有代码)都已经被反复 使用和验证过，在生产环境的实施阶段就可以考虑使用-Xverify:none 参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 准备准备阶段是正式为类中定义的变量(即静态变量，被static修饰的变量)分配内存并设置类变量初始值的阶段（下面详细说下这两个操作），从概念上讲，这些变量所使用的内存都应当在方法区中进行分配，但必须注意到方法区本身是一个逻辑上的区域，在JDK 7及之前，HotSpot使用永久代来实现方法区时，实现是完全符合这种逻辑概念的；而在JDK 8及之后，类变量则会随着Class对象一起存放在Java堆中，这时候“类变量在方法区”就完全是一种对逻辑概念的表述了。 进行内存分配的仅包括类变量，而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在Java堆中。其次是这里所说的初始值“通常情况”下是数据类型的零值。比如 12public static int value = 123;public static Date date = new Date(); 上面的代码在这里阶段的值为 value&#x3D;0，date&#x3D;null123，new Date()会在初始化阶段进行赋值，也就是执行了类构造器&lt;clinit&gt;()方法之后，才真正的为引用赋值程序定义的值。 有一种特殊的情况：如果使用final和static同时修饰一个field字段，并且这个字段是基本类型或者String类型的，那么编译器在编译这个字段的时候，会在对应的field_info结构体中增加一个ConstantValue类型的结构体（也就是类字段的字段属性表中存在ConstantValue属性），在这一阶段变量值就会被初始化为ConstantValue属性所指定的初始值，假设上面类变量value的定义修改为: 1public static final int value = 123; 编译时Javac将会为value生成ConstantValue属性，在准备阶段虚拟机就会根据ConstantValue的设置将value赋值为123。也就是说，对于这样的类变量，如果通过类型.变量的方式使用，是不会触发类的类的初始化阶段的。 解析解析阶段是Java虚拟机将常量池内的符号引用替换为直接引用的过程。 符号引用是一种定义，可以是任何字面上的含义，而直接引用就是直接指向目标的指针、相对偏移量。 直接引用的对象都存在于内存中，你可以把通讯录里的女友手机号码，类比为符号引用，把面对面和你吃饭的女朋友，类比为直接引用。 符号引用在类文件结构中讲解Class 文件格式的时候已经出现过多次，在Class文件中它以CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info等类型的常量出现。 符号引用(Symbolic References):符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标并不一定是已经加载到虚拟机内存当中的内容。各种虚拟机实现的内存布局可以各不相同， 但是它们能接受的符号引用必须都是一致的，因为符号引用的字面量形式明确定义在《Java虚拟机规范》的Class文件格式中。 直接引用(Direct References):直接引用是可以直接指向目标的指针、相对偏移量或者是一个能间接定位到目标的句柄。直接引用是和虚拟机实现的内存布局直接相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那引用的目标必定已经在虚拟机的内存中存在。 对同一个符号引用进行多次解析请求是很常见的事情，除invokedynamic指令以外，虚拟机实现可以对第一次解析的结果进行缓存，譬如在运行时直接引用常量池中的记录，并把常量标识为已解析状态，从而避免解析动作重复进行。无论是否真正执行了多次解析动作，Java虚拟机都需要保证的是在同一个实体中，如果一个符号引用之前已经被成功解析过，那么后续的引用解析请求就应当一直能够成功；同样地，如果第一次解析失败了，其他指令对这个符号的解析请求也应该收到相同的异常，哪怕这个请求的符号在后来已成功加载进Java虚拟机内存之中。 不过对于invokedynamic指令，上面的规则就不成立了。当碰到某个前面已经由invokedynamic指令触发过解析的符号引用时，并不意味着这个解析结果对于其他invokedynamic指令也同样生效。因为invokedynamic指令的目的本来就是用于动态语言支持（到了JDK 8时代，Java有了Lambda表达式和接口的默认方法，它们在底层调用时就会用到invokedynamic指令，这时再提动态语言支持其实已不完全切合，我们就只把它当个代称吧），它对应的引用称为“动态调用点限定符(Dynamically-Computed Call Site Specifier)”，这里“动态”的含义是指必须等到程序实际运行到这条指令时，解析动作才能进行。相对地，其余可触发解析的指令都是“静态”的，可以在刚刚完成加载阶段，还没有开始执行代码时就提前进行解析。 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符这7 类符号引用进行，分别对应于常量池的CONSTANT_Class_info、CONSTANT_String_infoCON-STANT_Fieldref_info、CONSTANT_Methodref_info、CONSTANT_InterfaceMethodref_info、CONSTANT_MethodType_info、CONSTANT_MethodHandle_info、CONSTANT_Dyna-mic_infoCONSTANT_InvokeDynamic_info8种常量类型。 类或接口的解析假设当前代码所处的类为D，如果要把一个从未解析过的符号引用N解析为一个类或接口C的直接引用，那虚拟机完成整个解析的过程需要包括以下3个步骤:如果C不是一个数组类型，那虚拟机将会把代表N的全限定名传递给D的类加载器去加载这个类C。在加载过程中，由于元数据验证、字节码验证的需要，又可能触发其他相关类的加载动作，例如加载这个类的父类或实现的接口。一旦这个加载过程出现了任何异常，解析过程就将宣告失败。 如果C是一个数组类型，并且数组的元素类型为对象，也就是N的描述符会是类似“[Ljava&#x2F;lang&#x2F;Integer”的形式，那将会按照第一点的规则加载数组元素类型。如果N的描述符如前面所假设的形式，需要加载的元素类型就是“java.lang.Integer”，接着由虚拟机生成一个代表该数组维度和元素的数组对象。 如果上面两步没有出现任何异常，那么C在虚拟机中实际上已经成为一个有效的类或接口了， 但在解析完成前还要进行符号引用验证，确认D是否具备对C的访问权限。如果发现不具备访问权限， 将抛出java.lang.IllegalAccessError异常。 字段解析要解析一个未被解析过的字段符号引用，首先将会对字段表内class_index项中索引的CONSTANT_Class_info符号引用进行解析，也就是字段所属的类或接口的符号引用。如果在解析这个类或接口符号引用的过程中出现了任何异常，都会导致字段符号引用解析的失败。如果解析成功完成，那把这个字段所属的类或接口用C表示，《Java虚拟机规范》要求按照如下步骤对C进行后续字段的搜索: 如果C本身就包含了简单名称和字段描述符都与目标相匹配的字段，则返回这个字段的直接引用，查找结束。 否则，如果在C中实现了接口，将会按照继承关系从下往上递归搜索各个接口和它的父接口， 如果接口中包含了简单名称和字段描述符都与目标相匹配的字段，则返回这个字段的直接引用，查找结束。 否则，如果C不是java.lang.Object的话，将会按照继承关系从下往上递归搜索其父类，如果在父类中包含了简单名称和字段描述符都与目标相匹配的字段，则返回这个字段的直接引用，查找结束。 否则，查找失败，抛出java.lang.NoSuchFieldError异常。 如果查找过程成功返回了引用，将会对这个字段进行权限验证，如果发现不具备对字段的访问权限，将抛出java.lang.IllegalAccessError异常。 方法解析方法解析的第一个步骤与字段解析一样，也是需要先解析出方法表的class_index项中索引的方法所属的类或接口的符号引用，如果解析成功，那么我们依然用C表示这个类，接下来虚拟机将会按照如下步骤进行后续的方法搜索: 由于Class文件格式中类的方法和接口的方法符号引用的常量类型定义是分开的，如果在类的方法表中发现class_index中索引的C是个接口的话，那就直接抛出java.lang.IncompatibleClassChangeError 异常。 如果通过了第一步，在类C中查找是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。 否则，在类C的父类中递归查找是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。 否则，在类C实现的接口列表及它们的父接口之中递归查找是否有简单名称和描述符都与目标相匹配的方法，如果存在匹配的方法，说明类C是一个抽象类，这时候查找结束，抛出java.lang.AbstractMethodError异常。 否则，宣告方法查找失败，抛出java.lang.NoSuchMethodError。 最后，如果查找过程成功返回了直接引用，将会对这个方法进行权限验证，如果发现不具备对此方法的访问权限，将抛出java.lang.IllegalAccessError异常。 接口方法解析接口方法也是需要先解析出接口方法表的class_index项中索引的方法所属的类或接口的符号引用，如果解析成功，依然用C表示这个接口，接下来虚拟机将会按照如下步骤进行后续的接口方法搜索: 与类的方法解析相反，如果在接口方法表中发现class_index中的索引C是个类而不是接口，那么就直接抛出java.lang.IncompatibleClassChangeError异常。 否则，在接口C中查找是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。 否则，在接口C的父接口中递归查找，直到java.lang.Object类(接口方法的查找范围也会包括Object类中的方法)为止，看是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。 对于规则3，由于Java的接口允许多重继承，如果C的不同父接口中存有多个简单名称和描述符都与目标相匹配的方法，那将会从这多个方法中返回其中一个并结束查找，《Java虚拟机规范》中并没有进一步规则约束应该返回哪一个接口方法。但与之前字段查找类似地，不同发行商实现的Javac编译器有可能会按照更严格的约束拒绝编译这种代码来避免不确定性。 否则，宣告方法查找失败，抛出java.lang.NoSuchMethodError异常。 初始化进行准备阶段时，变量已经赋过一次系统要求的初始零值，而在初始化阶段，则会根据程序员通过程序编码制定的主观计划去初始化类变量和其他资源。或者说初始化阶段就是执行类构造器&lt;clinit&gt;()方法的过程。 &lt;clinit&gt;()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块(static{}块)中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问。 &lt;clinit&gt;()方法与类的构造函数(即在虚拟机视角中的实例构造器&lt;init&gt;()方法)不同，它不需要显式地调用父类构造器，Java虚拟机会保证在子类的&lt;clinit&gt;()方法执行前，父类的&lt;clinit&gt;()方法已经执行完毕。因此在Java虚拟机中第一个被执行的&lt;clinit&gt;()方法的类型肯定是java.lang.Object。由于父类的&lt;clinit&gt;()方法先执行,也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作。 &lt;clinit&gt;()方法对于类或接口来说并不是必需的，如果一个类中没有静态语句块，也没有对变量的赋值操作，那么编译器可以不为这个类生成&lt;clinit&gt;()方法。 接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此接口与类一样都会生成&lt;clinit&gt;()方法。但接口与类不同的是，执行接口的&lt;clinit&gt;()方法不需要先执行父接口的&lt;clinit&gt;()方法， 因为只有当父接口中定义的变量被使用时，父接口才会被初始化。此外，接口的实现类在初始化时也一样不会执行接口的&lt;clinit&gt;()方法。 Java虚拟机必须保证一个类的&lt;clinit&gt;()方法在多线程环境中被正确地加锁同步，如果多个线程同时去初始化一个类，那么只会有其中一个线程去执行这个类的&lt;clinit&gt;()方法，其他线程都需要阻塞等待，直到活动线程执行完毕&lt;clinit&gt;()方法。如果在一个类的&lt;clinit&gt;()方法中有耗时很长的操作，那就可能造成多个进程阻塞(需要注意，其他线程虽然会被阻塞，但如果执行&lt;clinit&gt;()方法的那条线程退出&lt;clinit&gt;()方法后，其他线程唤醒后则不会再次进入&lt;clinit&gt;()方法。同一个类加载器下，一个类型只会被初始化一次。) 类加载器（类加载过程的加载阶段）对于任意一个类，都必须由加载它的类加载器和这个类本身一起共同确立其在Java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类名称空间。这句话可以表达得更通俗一些：比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来源于同一个Class文件，被同一个Java虚拟机加载，只要加载它们的类加载器不同，那这两个类就必定不相等。 这里所指的“相等”，包括代表类的Class对象的equals()方法、isAssignableFrom()方法、isInstance() 方法的返回结果，也包括了使用instanceof关键字做对象所属关系判定等各种情况。 不同的类加载器对instanceof关键字运算的结果的影响 1234567891011121314151617181920212223242526public class ClassLoaderTest &#123; public static void main(String[] args) throws ClassNotFoundException, InstantiationException, IllegalAccessException &#123; ClassLoader myLoader = new ClassLoader() &#123; @Override public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; try &#123; String fileName = name.substring(name.lastIndexOf(&quot;.&quot;) + 1) + &quot;.class&quot;; InputStream is = getClass().getResourceAsStream(fileName); if (is == null) &#123; return super.loadClass(name); &#125; byte[] b = new byte[is.available()]; is.read(b); return defineClass(name, b, 0, b.length); &#125; catch (IOException e) &#123; throw new ClassNotFoundException(name); &#125; &#125; &#125;; Object obj = myLoader.loadClass(&quot;com.xyz.localcode.ClassLoaderTest&quot;).newInstance(); System.out.println(obj.getClass()); System.out.println(obj instanceof com.xyz.localcode.ClassLoaderTest); &#125;&#125; 运行结果： 12class com.xyz.localcode.ClassLoaderTestfalse 这是因为Java虚拟机中同时存在了两个ClassLoaderTest类,一个是由虚拟机的应用程序类加载器所加载的,另外一个是由我们自定义的类加载器加载的,虽然它们都来自同一个Class文件,但在Java虚拟机中仍然是两个互相独立的类。 双亲委派模型站在Java虚拟机的角度来看，只存在两种不同的类加载器：一种是启动类加载器(Bootstrap ClassLoader)，这个类加载器使用C++语言实现，是虚拟机自身的一部分；另外一种就是其他所有的类加载器，这些类加载器都由Java语言实现，独立存在于虚拟机外部，并且全都继承自抽象类java.lang.ClassLoader。 将针对JDK 8及之前版本的Java来介绍什么是三层类加载器，以及什么是双亲委派模型。对于这个时期的Java应用，绝大多数Java程序都会使用到以下3个系统提供的类加载器来进行加载。 启动类加载器(Bootstrap Class Loader)：前面已经介绍过，这个类加载器负责加载存放在&lt;JAVA_HOME&gt;\\lib目录，或者被-Xbootclasspath参数所指定的路径中存放的，而且是Java虚拟机能够识别的(按照文件名识别，如rt.jar、tools.jar，名字不符合的类库即使放在lib目录中也不会被加载)类库加载到虚拟机的内存中。启动类加载器无法被Java程序直接引用，用户在编写自定义类加载器时， 如果需要把加载请求委派给引导类加载器去处理，那直接使用null代替即可。 扩展类加载器(Extension Class Loader)：这个类加载器是在类sun.misc.Launcher$ExtClassLoader 中以Java代码的形式实现的。它负责加载&lt;JAVA_HOME&gt;\\lib\\ext目录中，或者被java.ext.dirs系统变量所指定的路径中所有的类库。根据“扩展类加载器”这个名称，就可以推断出这是一种Java系统类库的扩展机制，JDK的开发团队允许用户将具有通用性的类库放置在ext目录里以扩展Java SE的功能，在JDK 9之后，这种扩展机制被模块化带来的天然的扩展能力所取代。由于扩展类加载器是由Java代码实现的，开发者可以直接在程序中使用扩展类加载器来加载Class文件。 应用程序类加载器(Application Class Loader)：这个类加载器由sun.misc.Launcher$AppClassLoader来实现。由于应用程序类加载器是ClassLoader类中的getSystemClassLoader()方法的返回值，所以有些场合中也称它为“系统类加载器”。它负责加载用户类路径(ClassPath)上所有的类库，开发者同样可以直接在代码中使用这个类加载器。 双亲委派模型要求除了顶层的启动类加载器外，其余的类加载器都应有自己的父类加载器。不过这里类加载器之间的父子关系一般不是以继承(Inheritance)的关系来实现的，而是通常使用组合(Composition)关系来复用父加载器的代码。 双亲委派模型的工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到最顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求(它的搜索范围中没有找到所需的类)时，子加载器才会尝试自己去完成加载。 使用双亲委派模型来组织类加载器之间的关系，一个显而易见的好处就是Java中的类随着它的类加载器一起具备了一种带有优先级的层次关系。例如类java.lang.Object，它存放在rt.jar之中，无论哪一个类加载器要加载这个类，最终都是委派给处于模型最顶端的启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都能够保证是同一个类。 java.lang.ClassLoader的loadClass()方法: 这段代码的逻辑清晰易懂：先检查请求加载的类型是否已经被加载过，若没有则调用父加载器的loadClass()方法，若父加载器为空则默认使用启动类加载器作为父加载器。假如父类加载器加载失败，抛出ClassNotFoundException异常的话，才调用自己的findClass()方法尝试进行加载。 Tomcat：正统的类加载器架构在部署Web应用时，单独的一个ClassPath就不能满足需求了，所以各种Web服务器都不约而同地提供了好几个有着不同含义的ClassPath路径供用户存放第三方类库，这些路径一般会以“lib”或“classes”命名。被放置到不同路径中的类库，具备不同的访问范围和服务对象，通常每一个目录都会有一个相应的自定义类加载器去加载放置在里面的Java类库。 在Tomcat目录结构中，可以设置3组目录(&#x2F;common&#x2F;*、&#x2F;server&#x2F;*和&#x2F;shared&#x2F;*，但默认不一定是开放的，可能只有&#x2F;lib&#x2F;*目录存在)用于存放Java类库，另外还应该加上Web应用程序自身的“&#x2F;WEB- INF&#x2F;*”目录，一共4组。把Java类库放置在这4组目录中，每一组都有独立的含义，分别是: 放置在&#x2F;common目录中。类库可被Tomcat和所有的Web应用程序共同使用。 放置在&#x2F;server目录中。类库可被Tomcat使用，对所有的Web应用程序都不可见。 放置在&#x2F;shared目录中。类库可被所有的Web应用程序共同使用，但对Tomcat自己不可见。 放置在&#x2F;WebApp&#x2F;WEB-INF目录中。类库仅仅可以被该Web应用程序使用，对Tomcat和其他Web应用程序都不可见。 为了支持这套目录结构，并对目录里面的类库进行加载和隔离，Tomcat自定义了多个类加载器， 这些类加载器按照经典的双亲委派模型来实现，其关系如图： 灰色背景的3个类加载器是JDK(以JDK 9之前经典的三层类加载器为例)默认提供的类加载器， 这3个加载器的作用在第7章中已经介绍过了。而Common类加载器、Catalina类加载器(也称为Server类加载器)、Shared类加载器和Webapp类加载器则是Tomcat自己定义的类加载器，它们分别加载/common/*、/server/*、/shared/*和/WebApp/WEB-INF/*中的Java类库。其中WebApp类加载器和JSP类加载器通常还会存在多个实例，每一个Web应用程序对应一个WebApp类加载器，每一个JSP文件对应一个JasperLoader类加载器。 从图的委派关系中可以看出，Common类加载器能加载的类都可以被Catalina类加载器和Shared 类加载器使用，而Catalina类加载器和Shared类加载器自己能加载的类则与对方相互隔离。WebApp类加载器可以使用Shared类加载器加载到的类，但各个WebApp类加载器实例之间相互隔离。而JasperLoader的加载范围仅仅是这个JSP文件所编译出来的那一个Class文件，它存在的目的就是为了被丢弃:当服务器检测到JSP文件被修改时，会替换掉目前的JasperLoader的实例，并通过再建立一个新的JSP类加载器来实现JSP文件的HotSwap功能。 本例中的类加载结构在Tomcat 6以前是它默认的类加载器结构，在Tomcat 6及之后的版本简化了默认的目录结构，只有指定了tomcat/conf/catalina.properties配置文件的server.loader和share.loader项后才会真正建立Catalina类加载器和Shared类加载器的实例，否则会用到这两个类加载器的地方都会用Common类加载器的实例代替，而默认的配置文件中并没有设置这两个loader项，所以Tomcat 6之后也顺理成章地把/common、/server和/shared这3个目录默认合并到一起变成1个&#x2F;lib目录，这个目录里的类库相当于以前&#x2F;common目录中类库的作用，是Tomcat的开发团队为了简化大多数的部署场景所做的一项易用性改进。如果默认设置不能满足需要，用户可以通过修改配置文件指定server.loader和share.loader的方式重新启用原来完整的加载器架构。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"虚拟机字节码指令表","slug":"jvm/02类文件结构/虚拟机字节码指令表","date":"2021-11-19T12:00:38.000Z","updated":"2022-03-23T09:03:57.912Z","comments":true,"path":"blog/jvm/02类文件结构/虚拟机字节码指令表/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/02%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84/%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%97%E8%8A%82%E7%A0%81%E6%8C%87%E4%BB%A4%E8%A1%A8/","excerpt":"","text":"","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"24class文件其他结构","slug":"jvm/02类文件结构/24class文件其他结构","date":"2021-11-19T12:00:37.000Z","updated":"2022-03-23T09:03:57.912Z","comments":true,"path":"blog/jvm/02类文件结构/24class文件其他结构/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/02%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84/24class%E6%96%87%E4%BB%B6%E5%85%B6%E4%BB%96%E7%BB%93%E6%9E%84/","excerpt":"","text":"访问标志(access_flags)能够表示什么？访问标志（access_flags）紧接着常量池后，占有两个字节，总共16位，如下图所示： 类索引(this_class)是什么？类索引的作用，就是为了指出class文件所描述的这个类叫什么名字。 类索引紧接着访问标志的后面，占有两个字节，在这两个字节中存储的值是一个指向常量池的一个索引，该索引指向的是CONSTANT_Class_info常量池项， 父类索引(super_class)是什么？ Java支持单继承模式，除了java.lang.Object 类除外，每一个类都会有且只有一个父类。class文件中紧接着类索引(this_class)之后的两个字节区域表示父类索引，跟类索引一样，父类索引这两个字节中的值指向了常量池中的某个常量池项CONSTANT_Class_info，表示该class表示的类是继承自哪一个类。 接口索引集合(interfaces)是什么？一个类可以不实现任何接口，也可以实现很多个接口，为了表示当前类实现的接口信息，class文件使用了如下结构体描述某个类的接口实现信息 由于类实现的接口数目不确定，所以接口索引集合的描述的前部分叫做接口计数器（interfaces_count），接口计数器占用两个字节，其中的值表示着这个类实现了多少个接口，紧跟着接口计数器的部分就是接口索引部分了，每一个接口索引占有两个字节，接口计数器的值代表着后面跟着的接口索引的个数。接口索引和类索引和父类索引一样，其内的值存储的是指向了常量池中的常量池项的索引，表示着这个接口的完全限定名。 字段区？Java中的一个Field字段应该包含那些信息？ 针对上述的字段表示，JVM虚拟机规范规定了field_info结构体来描述字段，其表示信息如下： field字段的访问标志 字段的数据类型表示和字段名称表示 属性表集合—–静态field字段的初始化在定义field字段的过程中，我们有时候会很自然地对field字段直接赋值，如下所示： 12public static final int MAX=100;public int count=0; 对于虚拟机而言，上述的两个field字段赋值的时机是不同的： 对于非静态（即无static修饰）的field字段的赋值将会出现在实例构造方法()中 对于静态的field字段，有两个选择：1、没有被final修饰，或者不是基本类型或者String类型，那么将在类构造方法()中赋值；2 、如果使用final和static同时修饰一个field字段，并且这个字段是基本类型或者String类型的，那么编译器在编译这个字段的时候，会在对应的field_info结构体中增加一个ConstantValue类型的结构体，在赋值的时候使用这个ConstantValue进行赋值。 ConstantValue数据项代表了常量池中一个字面量常量 方法区 一个类中的method方法应该包含哪些信息？ 访问标志(access_flags) 名称索引和描述符索引—-一个方法的签名 属性表集合","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"23Class文件中的常量池详解","slug":"jvm/02类文件结构/23Class文件中的常量池详解","date":"2021-11-19T12:00:36.000Z","updated":"2022-03-23T09:03:57.901Z","comments":true,"path":"blog/jvm/02类文件结构/23Class文件中的常量池详解/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/02%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84/23Class%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E5%B8%B8%E9%87%8F%E6%B1%A0%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"在jdk8中不知道是不是去掉了CONSTANT_Integer_info这个常量tag，在class文件中，int类型不会出现在常量池中可以看到，int类型直接在code中，而其他类型需要在常量池中获取。 常量池的里面是怎么组织的？常量池的组织很简单，前端的两个字节占有的位置叫做常量池计数器(constant_pool_count)，它记录着常量池的组成元素 常量池项(cp_info) 的个数。紧接着会排列着constant_pool_count-1个常量池项(cp_info)。如下图所示： 常量池项 (cp_info) 的结构是什么？每个常量池项(cp_info) 都会对应记录着class文件中的某中类型的字面量。让我们先来了解一下常量池项(cp_info)的结构吧：JVM虚拟机规定了不同的tag值和不同类型的字面量对应关系如下：所以根据cp_info中的tag 不同的值，可以将cp_info 更细化为以下结构体：CONSTANT_Utf8_info,CONSTANT_Integer_info,CONSTANT_Float_info,CONSTANT_Long_info, CONSTANT_Double_info,CONSTANT_Class_info,CONSTANT_String_info,CONSTANT_Fieldref_info,CONSTANT_Methodref_info,CONSTANT_InterfaceMethodref_info,CONSTANT_NameAndType_info,CONSTANT_MethodHandle_info,CONSTANT_MethodType_info,CONSTANT_InvokeDynamic_info。现在让我们看一下细化了的常量池的结构会是类似下图所示的样子： 常量池能够表示那些信息？ float数据类型的常量在常量池中是怎样表示和存储的？(CONSTANT_Float_info) Java语言规范规定了 Float 类型的数据类型占用4个字节的空间。那么存在于class字节码文件中的该类型的常量是如何存储的呢？相应地，在常量池中，将Float类型的常量分别使用Constant_float_info表示，他们的结构如下所示： 举例：建下面的类 TestTT.java，在这个类中，我们声明了五个变量，但是取值就两种long类型的1000 和Float类型的11f。然后用编译器编译成TestTT.class字节码文件，我们通过javap -v TestTT 指令来看一下其常量池中的信息，可以看到虽然我们在代码中写了两次1000 和两次次11f，但是常量池中，就只有一个常量1000 和一个常量11f,如下图所示:可以看到，同一个值在常量池中只会出现一个，这可以省掉一点空间，除了int基础类型，其他类型都一样。但由于long和double占用8个字符，结构会有点不同。 String类型的字符串常量在常量池中是怎样表示和存储的？（CONSTANT_String_info、CONSTANT_Utf8_info）对于字符串而言，JVM会将字符串类型的字面量以UTF-8 编码格式存储到在class字节码文件中。这么说可能有点摸不着北，我们先从直观的Java源码中中出现的用双引号”” 括起来的字符串来看，在编译器编译的时候，都会将这些字符串转换成CONSTANT_String_info结构体，然后放置于常量池中。其结构如下所示： 如上图所示的结构体，CONSTANT_String_info结构体中的string_index的值指向了CONSTANT_Utf8_info结构体，而字符串的utf-8编码数据就在这个结构体之中。如下图所示： 看这段代码编译后通过javap -v 获取到的claa如下：可以看到，string类型只是保存了一个指针，实际的值是通过一个utf8保存。 类文件中定义的类名和类中使用到的类在常量池中是怎样被组织和存储的？(CONSTANT_Class_info)JVM会将某个Java 类中所有使用到了的类的完全限定名 以二进制形式的完全限定名 封装成CONSTANT_Class_info结构体中，然后将其放置到常量池里。CONSTANT_Class_info 的tag值为 7 。其结构如下：和string和像。 类中引用到的field字段在常量池中是怎样描述的？(CONSTANT_Fieldref_info, CONSTANT_Name_Type_info)一般而言，我们在定义类的过程中会定义一些 field 字段，然后会在这个类的其他地方（如方法中）使用到它。有可能我们在类的方法中只使用field字段一次，也有可能我们会在类定义的方法中使用它很多很多次。举一个简单的例子，我们定一个叫Person的简单java bean，它有name和age两个field字段，如下所示：在上面定义的类中，我们在Person类中的一系列方法里，多次引用到namefield字段 和agefield字段，对于JVM编译器而言，name和age只是一个符号而已，并且它在由于它可能会在此类中重复出现多次，所以JVM把它当作常量来看待，将name和age以field字段常量的形式保存到常量池中。 将它name和age封装成 CONSTANT_Fieldref_info 常量池项，放到常量池中，在类中引用到它的地方，直接放置一个指向field字段所在常量池的索引。怎样描述某一个field字段的引用？ 方法描述符的组成比如上面的person类，编译后注意：方法没有使用过是不会进入到常量池的，set方法就没在","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"22class文件基本组织结构","slug":"jvm/02类文件结构/22class文件基本组织结构","date":"2021-11-19T12:00:35.000Z","updated":"2022-03-23T09:03:57.890Z","comments":true,"path":"blog/jvm/02类文件结构/22class文件基本组织结构/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/02%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84/22class%E6%96%87%E4%BB%B6%E5%9F%BA%E6%9C%AC%E7%BB%84%E7%BB%87%E7%BB%93%E6%9E%84/","excerpt":"","text":"NO1. 魔数(magic)所有的由Java编译器编译而成的class文件的前4个字节都是“0xCAFEBABE” 它的作用在于：当JVM在尝试加载某个文件到内存中来的时候，会首先判断此class文件有没有JVM认为可以接受的“签名”，即JVM会首先读取文件的前4个字节，判断该4个字节是否是“0xCAFEBABE”，如果是，则JVM会认为可以将此文件当作class文件来加载并使用。 NO2.版本号(minor_version,major_version)主版本号和次版本号在class文件中各占两个字节，副版本号占用第5、6两个字节，而主版本号则占用第7，8两个字节 NO3.常量池计数器(constant_pool_count)常量池是class文件中非常重要的结构，它描述着整个class文件的字面量信息。 常量池是由一组constant_pool结构体数组组成的，而数组的大小则由常量池计数器指定。常量池计数器constant_pool_count 的值 &#x3D;constant_pool表中的成员数+ 1。constant_pool表的索引值只有在大于 0 且小于constant_pool_count时才会被认为是有效的。 NO4.常量池数据区(constant_pool[contstant_pool_count-1])常量池，constant_pool是一种表结构,它包含 Class 文件结构及其子结构中引用的所有字符串常量、 类或接口名、字段名和其它常量。 常量池中的每一项都具备相同的格式特征——第一个字节作为类型标记用于识别该项是哪种类型的常量，称为 “tag byte” 。常量池的索引范围是 1 至constant_pool_count−1。常量池的具体细节我们会稍后讨论。 NO5.访问标志(access_flags)访问标志，access_flags 是一种掩码标志，用于表示某个类或者接口的访问权限及基础属性。 NO6.类索引(this_class)类索引，this_class的值必须是对constant_pool表中项目的一个有效索引值。constant_pool表在这个索引处的项必须为CONSTANT_Class_info 类型常量，表示这个 Class 文件所定义的类或接口。 NO7.父类索引(super_class)父类索引，对于类来说，super_class 的值必须为 0 或者是对constant_pool 表中项目的一个有效索引值。 如果它的值不为 0，那 constant_pool 表在这个索引处的项必须为CONSTANT_Class_info 类型常量，表示这个 Class 文件所定义的类的直接父类。当前类的直接父类，以及它所有间接父类的access_flag 中都不能带有ACC_FINAL 标记。对于接口来说，它的Class文件的super_class项的值必须是对constant_pool表中项目的一个有效索引值。constant_pool表在这个索引处的项必须为代表 java.lang.Object 的 CONSTANT_Class_info 类型常量 。 如果 Class 文件的 super_class的值为 0，那这个Class文件只可能是定义的是java.lang.Object类，只有它是唯一没有父类的类。 NO8.接口计数器(interfaces_count)接口计数器，interfaces_count的值表示当前类或接口的直接父接口数量。 NO9.接口信息数据区(interfaces[interfaces_count])接口表，interfaces[]数组中的每个成员的值必须是一个对constant_pool表中项目的一个有效索引值， 它的长度为 interfaces_count。每个成员 interfaces[i] 必须为 CONSTANT_Class_info类型常量，其中 0 ≤ i &lt;interfaces_count。在interfaces[]数组中，成员所表示的接口顺序和对应的源代码中给定的接口顺序（从左至右）一样，即interfaces[0]对应的是源代码中最左边的接口。 N10.字段计数器(fields_count)字段计数器，fields_count的值表示当前 Class 文件 fields[]数组的成员个数。 fields[]数组中每一项都是一个field_info结构的数据项，它用于表示该类或接口声明的类字段或者实例字段。 N11.字段信息数据区(fields[fields_count]) 字段表，fields[]数组中的每个成员都必须是一个fields_info结构的数据项，用于表示当前类或接口中某个字段的完整描述。 fields[]数组描述当前类或接口声明的所有字段，但不包括从父类或父接口继承的部分。 N12.方法计数器(methods_count)方法计数器， methods_count的值表示当前Class 文件 methods[]数组的成员个数。Methods[]数组中每一项都是一个 method_info 结构的数据项。 N13.方法信息数据区(methods[methods_count])方法表，methods[] 数组中的每个成员都必须是一个 method_info 结构的数据项，用于表示当前类或接口中某个方法的完整描述。如果某个method_info 结构的access_flags 项既没有设置 ACC_NATIVE 标志也没有设置ACC_ABSTRACT 标志，那么它所对应的方法体就应当可以被 Java 虚拟机直接从当前类加载，而不需要引用其它类。 method_info结构可以表示类和接口中定义的所有方法，包括实例方法、类方法、实例初始化方法方法和类或接口初始化方法方法 。methods[]数组只描述当前类或接口中声明的方法，不包括从父类或父接口继承的方法。 N14.属性计数器(attributes_count)属性计数器，attributes_count的值表示当前 Class 文件attributes表的成员个数。attributes表中每一项都是一个attribute_info 结构的数据项。 N15.属性信息数据区(attributes[attributes_count])属性表，attributes 表的每个项的值必须是attribute_info结构。 根据上述的叙述，我们可以将class的文件组织结构概括成以下面这个结构体：","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"0Class类文件的结构前言","slug":"jvm/02类文件结构/0Class类文件的结构前言","date":"2021-11-19T12:00:34.000Z","updated":"2022-03-23T09:03:57.886Z","comments":true,"path":"blog/jvm/02类文件结构/0Class类文件的结构前言/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/02%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84/0Class%E7%B1%BB%E6%96%87%E4%BB%B6%E7%9A%84%E7%BB%93%E6%9E%84%E5%89%8D%E8%A8%80/","excerpt":"","text":"实现语言无关性的基础仍然是虚拟机和字节码存储格式。Java虚拟机不与包括Java语言在内的任何程序语言绑定,它只与“Class文件”这种特定的二进制文件格式所关联,Class文件中包含了Java虚拟机指令集、符号表以及若干其他辅助信息。 作为一个通用的、与机器无关的执行平台,任何其他语言的实现者都可以将Java虚拟机作为他们语言的运行基础,以Class文件作为他们产品的交付媒介。例如,使用Java编译器可以把Java代码编译为存储字节码的Class文件,使用JRuby等其他语言的编译器一样可以把它们的源程序代码编译成Class文件。虚拟机丝毫不关心Class的来源是什么语言,它与程序语言之间的关系如图： Java语言中的各种语法、关键字、常量变量和运算符号的语义最终都会由多条字节码指令组合来表达,这决定了字节码指令所能提供的语言描述能力必须比Java语言本身更加强大才行。因此,有一些Java语言本身无法有效支持的语言特性并不代表在字节码中也无法有效表达出来,这为其他程序语言实现一些有别于Java的语言特性提供了发挥空间。 注意： 任何一个Class文件都对应着唯一的一个类或接口的定义信息,但是反过来说,类或接口并不一定都得定义在文件里(譬如类或接口也可以动态生成,直接送入类加载器中)。 Class文件是一组以8个字节为基础单位的二进制流,各个数据项目严格按照顺序紧凑地排列在文件之中,中间没有添加任何分隔符,这使得整个Class文件中存储的内容几乎全部是程序运行的必要数据,没有空隙存在。当遇到需要占用8个字节以上空间的数据项时,则会按照高位在前的方式分割成若干个8个字节进行存储。 根据《Java虚拟机规范》的规定,Class文件格式采用一种类似于C语言结构体的伪结构来存储数据,这种伪结构中只有两种数据类型:“无符号数”和“表”。 无符号数属于基本的数据类型,以u1、u2、u4、u8来分别代表1个字节、2个字节、4个字节和8个字节的无符号数,无符号数可以用来描述数字、索引引用、数量值或者按照UTF-8编码构成字符串值。 表是由多个无符号数或者其他表作为数据项构成的复合数据类型,为了便于区分,所有表的命名都习惯性地以“_info”结尾。表用于描述有层次关系的复合结构的数据,整个Class文件本质上也可以视作是一张表,这张表由下图所示的数据项按严格顺序排列构成。 魔数与Class文件的版本 每个Class文件的头4个字节被称为魔数(Magic Number),它的唯一作用是确定这个文件是否为一个能被虚拟机接受的Class文件。Class文件的魔数取得很有“浪漫气息”, 值为0xCAFEBABE(咖啡宝贝?)。 紧接着魔数的4个字节存储的是Class文件的版本号：第5和第6个字节是次版本号（Minor Version），第7和第8个字节是主版本号（Major Version）。Java的版本号是从45开始的，高版本的JDK能向下兼容以前版本的Class文件，但不能运行以后版本的Class文件，因为《Java虚拟机规范》在Class文件校验部分明确要求了即使文件格式并未发生任何变化，虚拟机也必须拒绝执行超过其版本号的Class文件。 常量池 由于常量池中常量的数量是不固定的，所以在常量池的入口需要放置一项u2类型的数据，代表常量池容量计数值（constant_pool_count）。与Java中语言习惯不同，这个容量计数是从1而不是0开始的。Class文件结构中只有常量池的容量计数是从1开始，对于其他集合类型，包括接口索引集合、字段表集合、方法表集合等的容量计数都与一般习惯相同，是从0开始。 设计者将第0项常量空出来是有特殊考虑的，这样做的目的在于，如果后面某些指向常量池的索引值的数据在特定情况下需要表达“不引用任何一个常量池项目”的含义，可以把索引值设置为0来表示。 常量池中主要存放两大类常量：字面量（Literal）和符号引用（Symbolic References）。字面量比较接近于Java语言层面的常量概念，如文本字符串、被声明为final的常量值等。。而符号引用则属于编译原理方面的概念，主要包括下面几类常量： 被模块导出或者开放的包（Package） 类和接口的全限定名（Fully Qualified Name） 字段的名称和描述符（Descriptor） 方法的名称和描述符 方法句柄和方法类型（Method Handle、Method Type、Invoke Dynamic） 动态调用点和动态常量（Dynamically-Computed Call Site、Dynamically-Computed Constant） Java代码在进行Javac编译的时候，并不像C和C++那样有“连接”这一步骤，而是在虚拟机加载Class文件的时候进行动态连接（TODO后面讲）。也就是说，在Class文件中不会保存各个方法、字段最终在内存中的布局信息，这些字段、方法的符号引用不经过虚拟机在运行期转换的话是无法得到真正的内存入口地址，也就无法直接被虚拟机使用的。当虚拟机做类加载时，将会从常量池获得对应的符号引用，再在类创建时或运行时解析、翻译到具体的内存地址之中。 常量池中每一项常量都是一个表，最初常量表中共有11种结构各不相同的表结构数据，后来为了更好地支持动态语言调用，额外增加了：CONSTANT_MethodHandle_info、CONSTANT_MethodType_info、CONSTANT_InvokeDynamic_info和CONSTANT_Dynamic_info。为了支持Java模块化系统（Jigsaw），又加入了CONSTANT_Module_info和CONSTANT_Package_info两个常量，所以截至JDK 13，常量表中分别有17种不同类型的常量。这17类表都有一个共同的特点，表结构起始的第一位是个u1类型的标志位（tag，取值见表6-3中标志列），代表着当前常量属于哪种常量类型。 用实例讲解12345678package org.fenixsoft.clazz;public class TestClass &#123; private int m; public int inc() &#123; return m + 1; &#125;&#125; 用javac编译后用Hex Fiend打开（win用WinHex），如图： 前8个字节是magic和版本号，0x0016是常量池的数量，0x07是常量池[1]标记为位，对照表6-3可知是CONSTANT_Class_info类型，此类型的常量代表一个类或者接口的符号引用。CONSTANT_Class_info的结构比较简单，如表： 该结构中tag是标志位，它用于区分常量类型；name_index是常量池的索引值，它指向常量池中一个CONSTANT_Utf8_info类型常量，此常量代表了这个类（或者接口）的全限定名，本例中的name_index值为0x0002，也就是指向了常量池中的第二项常量，它的标志位是0x01，查表6-3可知确实是一个CONSTANT_Utf8_info类型的常量。CONSTANT_Utf8_info类型的结构如表6-5所示。 length值说明了这个UTF-8编码的字符串长度是多少字节，它后面紧跟着的长度为length字节的连续数据是一个使用UTF-8缩略编码表示的字符串。UTF-8缩略编码与普通UTF-8编码的区别是：从’\\u0001’到’\\u007f’之间的字符（相当于1～127的ASCII码）的缩略编码使用一个字节表示，从’\\u0080’到’\\u07ff’之间的所有字符的缩略编码用两个字节表示，从’\\u0800’开始到’\\uffff’之间的所有字符的缩略编码就按照普通UTF-8编码规则使用三个字节表示。 由于Class文件中方法、字段等都需要引用CONSTANT_Utf8_info型常量来描述名称，所以CONSTANT_Utf8_info型常量的最大长度也就是Java中方法、字段名的最大长度。而这里的最大长度就是length的最大值，既u2类型能表达的最大值65535。所以Java程序中如果定义了超过64KB英文字符的变量或方法名，即使规则和全部字符都是合法的，也会无法编译。 本例中这个字符串的length值（偏移地址：0x0000000E）为0x001D，也就是长29个字节，往后29个字节正好都在1～127的ASCII码范围以内，内容为“org&#x2F;fenixsoft&#x2F;clazz&#x2F;TestClass”。 在JDK的bin目录中，Oracle公司已经为我们准备好一个专门用于分析Class文件字节码的工具：javap。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&gt; javap -v /Users/xyz/local/git/localcode/target/classes/org/fenixsoft/clazz/TestClass.classClassfile /Users/xyz/local/git/localcode/target/classes/org/fenixsoft/clazz/TestClass.class Last modified 2021-7-26; size 393 bytes MD5 checksum ca6d8b2868083171147fd4edb61a731b Compiled from &quot;TestClass.java&quot;public class org.fenixsoft.clazz.TestClass minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #4.#18 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Fieldref #3.#19 // org/fenixsoft/clazz/TestClass.m:I #3 = Class #20 // org/fenixsoft/clazz/TestClass #4 = Class #21 // java/lang/Object #5 = Utf8 m #6 = Utf8 I #7 = Utf8 &lt;init&gt; #8 = Utf8 ()V #9 = Utf8 Code #10 = Utf8 LineNumberTable #11 = Utf8 LocalVariableTable #12 = Utf8 this #13 = Utf8 Lorg/fenixsoft/clazz/TestClass; #14 = Utf8 inc #15 = Utf8 ()I #16 = Utf8 SourceFile #17 = Utf8 TestClass.java #18 = NameAndType #7:#8 // &quot;&lt;init&gt;&quot;:()V #19 = NameAndType #5:#6 // m:I #20 = Utf8 org/fenixsoft/clazz/TestClass #21 = Utf8 java/lang/Object&#123; public org.fenixsoft.clazz.TestClass(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 3: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lorg/fenixsoft/clazz/TestClass; public int inc(); descriptor: ()I flags: ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: aload_0 1: getfield #2 // Field m:I 4: iconst_1 5: iadd 6: ireturn LineNumberTable: line 6: 0 LocalVariableTable: Start Length Slot Name Signature 0 7 0 this Lorg/fenixsoft/clazz/TestClass;&#125;SourceFile: &quot;TestClass.java&quot; 从上边的输出可以看到，计算机已经帮我们把整个常量池的21项常量都计算了出来。仔细看一下会发现，其中有些常量似乎从来没有在代码中出现过，如“I”“V”“”“LineNumberTable”“LocalVariableTable”等，这些看起来在源代码中不存在的常量是哪里来的？这部分常量的确不来源于Java源代码，它们都是编译器自动生成的，会被后面即将讲到的字段表（field_info）、方法表（method_info）、属性表（attribute_info）所引用，它们将会被用来描述一些不方便使用“固定字节”进行表达的内容，譬如描述方法的返回值是什么，有几个参数，每个参数的类型是什么。因为Java中的“类”是无穷无尽的，无法通过简单的无符号数来描述一个方法用到了什么类，因此在描述方法的这些信息时，需要引用常量表中的符号引用进行表达。 常量池中的17种数据类型的结构总表 访问标志 在常量池结束之后，紧接着的2个字节代表访问标志（access_flags），这个标志用于识别一些类或者接口层次的访问信息，包括：这个Class是类还是接口；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否被声明为final；等等。具体的标志位以及标志的含义见图 类索引、父类索引与接口索引集合 类索引（this_class）和父类索引（super_class）都是一个u2类型的数据，而接口索引集合（interfaces）是一组u2类型的数据的集合，Class文件中由这三项数据来确定该类型的继承关系。 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名。由于Java语言不允许多重继承，所以父类索引只有一个，除了java.lang.Object之外，所有的Java类都有父类，因此除了java.lang.Object外，所有Java类的父类索引都不为0。接口索引集合就用来描述这个类实现了哪些接口，这些被实现的接口将按implements关键字（如果这个Class文件表示的是一个接口，则应当是extends关键字）后的接口顺序从左到右排列在接口索引集合中。 类索引、父类索引和接口索引集合都按顺序排列在访问标志之后，类索引和父类索引用两个u2类型的索引值表示，它们各自指向一个类型为CONSTANT_Class_info的类描述符常量，通过CONSTANT_Class_info类型的常量中的索引值可以找到定义在CONSTANT_Utf8_info类型的常量中的全限定名字符串。下图展示了类TestClass的索引查找过程 字段表集合 字段表（field_info）用于描述接口或者类中声明的变量。Java语言中的“字段”（Field）包括类级变量以及实例级变量，但不包括在方法内部声明的局部变量。 字段可以包括的修饰符有字段的作用域（public、private、protected修饰符）、是实例变量还是类变量（static修饰符）、可变性（final）、并发可见性（volatile修饰符，是否强制从主内存读写）、可否被序列化（transient修饰符）、字段数据类型（基本类型、对象、数组）、字段名称。上述这些信息中，各个修饰符都是布尔值，要么有某个修饰符，要么没有，很适合使用标志位来表示。而字段叫做什么名字、字段被定义为什么数据类型，这些都是无法固定的，只能引用常量池中的常量来描述。下图中列出了字段表的最终格式。 字段修饰符放在access_flags项目中，它与类中的access_flags项目是非常类似的，都是一个u2的数据类型，其中可以设置的标志位和含义如下： 跟随access_flags标志的是两项索引值：name_index和descriptor_index。它们都是对常量池项的引用，分别代表着字段的简单名称以及字段和方法的描述符。 解释一下“简单名称”“描述符”以及前面出现过多次的“全限定名”这三种特殊字符串的概念。 全限定名：，“org&#x2F;fenixsoft&#x2F;clazz&#x2F;TestClass”是这个类的全限定名，仅仅是把类全名中的“.”替换成了“&#x2F;”而已，为了使连续的多个全限定名之间不产生混淆，在使用时最后一般会加入一个“；”号表示全限定名结束。 简单名称：指没有类型和参数修饰的方法或者字段名称，这个类中的inc()方法和m字段的简单名称分别就是“inc”和“m”。 描述符：描述符的作用是用来描述字段的数据类型、方法的参数列表（包括数量、类型以及顺序）和返回值。根据描述符规则，基本数据类型（byte、char、double、float、int、long、short、boolean）以及代表无返回值的void类型都用一个大写字符来表示，而对象类型则用字符L加对象的全限定名来表示，详见图： 对于数组类型，每一维度将使用一个前置的“[”字符来描述，如一个定义为“java.lang.String[][]”类型的二维数组将被记录成“[[Ljava&#x2F;lang&#x2F;String；”，一个整型数组“int[]”将被记录成“[I”。 用描述符来描述方法时，按照先参数列表、后返回值的顺序描述，参数列表按照参数的严格顺序放在一组小括号“()”之内。如方法void inc()的描述符为“()V”，方法java.lang.String toString()的描述符为“()Ljava&#x2F;lang&#x2F;String；”，方法int indexOf(char[]source，int sourceOffset，int sourceCount，char[]target，int targetOffset，int targetCount，int fromIndex)的描述符为“([CII[CIII)I” 字段表所包含的固定数据项目到descriptor_index为止就全部结束了，不过在descriptor_index之后跟随着一个属性表集合，用于存储一些额外的信息，字段表可以在属性表中附加描述零至多项的额外信息。对于本例中的字段m，它的属性表计数器为0，也就是没有需要额外描述的信息，但是，如果将字段m的声明改为“final static int m&#x3D;123；”，那就可能会存在一项名称为ConstantValue的属性，其值指向常量123。关于attribute_info的其他内容，将在（TODO）介绍属性表的数据项目时再做进一步讲解。 字段表集合中不会列出从父类或者父接口中继承而来的字段，但有可能出现原本Java代码之中不存在的字段，譬如在内部类中为了保持对外部类的访问性，编译器就会自动添加指向外部类实例的字段。另外，在Java语言中字段是无法重载的，两个字段的数据类型、修饰符不管是否相同，都必须使用不一样的名称，但是对于Class文件格式来讲，只要两个字段的描述符不是完全相同，那字段重名就是合法的。 方法表集合 Class文件存储格式中对方法的描述与对字段的描述采用了几乎完全一致的方式，方法表的结构如同字段表一样. 这些数据项目的含义也与字段表中的非常类似，仅在访问标志和属性表集合的可选项中有所区别。 因为volatile关键字和transient关键字不能修饰方法，所以方法表的访问标志中没有了ACC_VOLATILE标志和ACC_TRANSIENT标志。与之相对，synchronized、native、strictfp和abstract关键字可以修饰方法，方法表的访问标志中也相应地增加了ACC_SYNCHRONIZED、ACC_NATIVE、ACC_STRICTFP和ACC_ABSTRACT标志。对于方法表，所有标志位及其取值可参见图： 方法的定义可以通过访问标志、名称索引、描述符索引来表达清楚，但方法里面的代码去哪里了？方法里的Java代码，经过Javac编译器编译成字节码指令之后，存放在方法属性表集合中一个名为“Code”的属性里面。 与字段表集合相对应地，如果父类方法在子类中没有被重写（Override），方法表集合中就不会出现来自父类的方法信息。但同样地，有可能会出现由编译器自动添加的方法，最常见的便是类构造器“&lt;clinit&gt;()”方法和实例构造器“&lt;init&gt;()”方法。 在Java语言中，要重载（Overload）一个方法，除了要与原方法具有相同的简单名称之外，还要求必须拥有一个与原方法不同的特征签名。特征签名是指一个方法中各个参数在常量池中的字段符号引用的集合，也正是因为返回值不会包含在特征签名之中，所以Java语言里面是无法仅仅依靠返回值的不同来对一个已有方法进行重载的。但是在Class文件格式之中，特征签名的范围明显要更大一些，只要描述符不是完全一致的两个方法就可以共存。也就是说，如果两个方法有相同的名称和特征签名，但返回值不同，那么也是可以合法共存于同一个Class文件中的。 在《Java虚拟机规范》第2版的4.4.4节及《Java语言规范》第3版的8.4.2节中分别都定义了字节码层面的方法特征签名以及Java代码层面的方法特征签名，Java代码的方法特征签名只包括方法名称、参数顺序及参数类型，而字节码的特征签名还包括方法返回值以及受查异常表，请读者根据上下文语境注意区分。 属性表集合 属性表（attribute_info）在前面的讲解之中已经出现过数次，Class文件、字段表、方法表都可以携带自己的属性表集合，以描述某些场景专有的信息。 虚拟机规范预定义的属性： 每一个属性，它的名称都要从常量池中引用一个CONSTANT_Utf8_info类型的常量来表示，而属性值的结构则是完全自定义的，只需要通过一个u4的长度属性去说明属性值所占用的位数即可。一个符合规则的属性表应该满足下图中所定义的结构。 CodeJava程序方法体里面的代码经过Javac编译器处理之后，最终变为字节码指令存储在Code属性内。Code属性出现在方法表的属性集合之中，但并非所有的方法表都必须存在这个属性，譬如接口或者抽象类中的方法就不存在Code属性，如果方法表有Code属性存在，那么它的结构将如表 attribute_name_index是一项指向CONSTANT_Utf8_info型常量的索引，此常量值固定为“Code”，它代表了该属性的属性名称，attribute_length指示了属性值的长度，由于属性名称索引与属性长度一共为6个字节，所以属性值的长度固定为整个属性表长度减去6个字节。 max_stack代表了操作数栈（Operand Stack）深度的最大值。在方法执行的任意时刻，操作数栈都不会超过这个深度。虚拟机运行的时候需要根据这个值来分配栈帧（Stack Frame）中的操作栈深度。 max_locals代表了局部变量表所需的存储空间。在这里，max_locals的单位是变量槽（Slot），变量槽是虚拟机为局部变量分配内存所使用的最小单位。对于byte、char、float、int、short、boolean和returnAddress等长度不超过32位的数据类型，每个局部变量占用一个变量槽，而double和long这两种64位的数据类型则需要两个变量槽来存放。方法参数（包括实例方法中的隐藏参数“this”）、显式异常处理程序的参数（Exception Handler Parameter，就是try-catch语句中catch块中所定义的异常）、方法体中定义的局部变量都需要依赖局部变量表来存放。。注意，并不是在方法中用了多少个局部变量，就把这些局部变量所占变量槽数量之和作为max_locals的值，操作数栈和局部变量表直接决定一个该方法的栈帧所耗费的内存，不必要的操作数栈深度和变量槽数量会造成内存的浪费。Java虚拟机的做法是将局部变量表中的变量槽进行重用，当代码执行超出一个局部变量的作用域时，这个局部变量所占的变量槽可以被其他局部变量所使用，Javac编译器会根据变量的作用域来分配变量槽给各个变量使用，根据同时生存的最大局部变量数量和类型计算出max_locals的大小。 code_length和code用来存储Java源程序编译后生成的字节码指令。code_length代表字节码长度，code是用于存储字节码指令的一系列字节流。既然叫字节码指令，那顾名思义每个指令就是一个u1类型的单字节，当虚拟机读取到code中的一个字节码时，就可以对应找出这个字节码代表的是什么指令，并且可以知道这条指令后面是否需要跟随参数，以及后续的参数应当如何解析。 关于code_length，有一件值得注意的事情，虽然它是一个u4类型的长度值，理论上最大值可以达到2的32次幂，但是《Java虚拟机规范》中明确限制了一个方法不允许超过65535条字节码指令，即它实际只使用了u2的长度，如果超过这个限制，Javac编译器就会拒绝编译。一般来讲，编写Java代码时只要不是刻意去编写一个超级长的方法来为难编译器，是不太可能超过这个最大值的限制的。但是，某些特殊情况，例如在编译一个很复杂的JSP文件时，某些JSP编译器会把JSP内容和页面输出的信息归并于一个方法之中，就有可能因为方法生成字节码超长的原因而导致编译失败。 实例123456789101112131415161718192021package org.fenixsoft.clazz;public class TestClass &#123; private int m; public int inc() &#123; return m + 1; &#125; public int ine() &#123; int x; try &#123; x = 1; return x; &#125; catch (Exception e) &#123; x = 2; return x; &#125; finally &#123; x = 3; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&gt; javap -v /Users/xyz/local/git/localcode/target/classes/org/fenixsoft/clazz/TestClass.classClassfile /Users/xyz/local/git/localcode/target/classes/org/fenixsoft/clazz/TestClass.class Last modified 2021-7-26; size 393 bytes MD5 checksum ca6d8b2868083171147fd4edb61a731b Compiled from &quot;TestClass.java&quot;public class org.fenixsoft.clazz.TestClass minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #4.#18 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Fieldref #3.#19 // org/fenixsoft/clazz/TestClass.m:I #3 = Class #20 // org/fenixsoft/clazz/TestClass #4 = Class #21 // java/lang/Object #5 = Utf8 m #6 = Utf8 I #7 = Utf8 &lt;init&gt; #8 = Utf8 ()V #9 = Utf8 Code #10 = Utf8 LineNumberTable #11 = Utf8 LocalVariableTable #12 = Utf8 this #13 = Utf8 Lorg/fenixsoft/clazz/TestClass; #14 = Utf8 inc #15 = Utf8 ()I #16 = Utf8 SourceFile #17 = Utf8 TestClass.java #18 = NameAndType #7:#8 // &quot;&lt;init&gt;&quot;:()V #19 = NameAndType #5:#6 // m:I #20 = Utf8 org/fenixsoft/clazz/TestClass #21 = Utf8 java/lang/Object&#123;...... public int ine(); descriptor: ()I flags: ACC_PUBLIC Code: stack=1, locals=5, args_size=1 0: iconst_1 1: istore_1 2: iload_1 3: istore_2 4: iload_2 5: ireturn 6: astore_2 7: iconst_2 8: istore_1 9: iload_1 10: istore_3 11: iload_3 12: ireturn 13: astore 4 15: aload 4 17: athrow Exception table: from to target type 0 4 6 Class java/lang/Exception 0 4 13 any 6 11 13 any 13 15 13 any LineNumberTable: line 12: 0 line 13: 2 line 14: 6 line 15: 7 line 16: 9 line 17: 13 line 19: 15 LocalVariableTable: Start Length Slot Name Signature 2 4 1 x I 7 6 2 e Ljava/lang/Exception; 9 4 1 x I 0 18 0 this Lorg/fenixsoft/clazz/TestClass; StackMapTable: number_of_entries = 2 frame_type = 70 /* same_locals_1_stack_item */ stack = [ class java/lang/Exception ] frame_type = 70 /* same_locals_1_stack_item */ stack = [ class java/lang/Throwable ]......&#125;SourceFile: &quot;TestClass.java&quot; 上面Code中有args_size，而且值为1。为什么args_size会为1？而且无论是在参数列表里还是方法体内，都没有定义任何局部变量，那Locals又为什么会等于1？如果有这样疑问的读者，大概是忽略了一条Java语言里面的潜规则：在任何实例方法里面，都可以通过“this”关键字访问到此方法所属的对象。这个访问机制对Java程序的编写很重要，而它的实现非常简单，仅仅是通过在Javac编译器编译的时候把对this关键字的访问转变为对一个普通方法参数的访问，然后在虚拟机调用实例方法时自动传入此参数而已。因此在实例方法的局部变量表中至少会存在一个指向当前对象实例的局部变量，局部变量表中也会预留出第一个变量槽位来存放对象实例的引用，所以实例方法参数值从1开始计算。这个处理只对实例方法有效，如果方法被声明为static，那args_size就不会等于1而是等于0了。 Exceptions属性Exceptions属性的作用是列举出方法中可能抛出的受查异常（Checked Excepitons），也就是方法描述时在throws关键字后面列举的异常。 LineNumberTable属性LineNumberTable属性用于描述Java源码行号与字节码行号（字节码的偏移量）之间的对应关系。它并不是运行时必需的属性，但默认会生成到Class文件之中，可以在Javac中使用-g：none或-g：lines选项来取消或要求生成这项信息。如果选择不生成LineNumberTable属性，对程序运行产生的最主要影响就是当抛出异常时，堆栈中将不会显示出错的行号，并且在调试程序的时候，也无法按照源码行来设置断点。 LocalVariableTableLocalVariableTable属性用于描述栈帧中局部变量表的变量与Java源码中定义的变量之间的关系，它也不是运行时必需的属性，但默认会生成到Class文件之中，可以在Javac中使用-g：none或-g：vars选项来取消或要求生成这项信息。如果没有生成这项属性，最大的影响就是当其他人引用这个方法时，所有的参数名称都将会丢失，譬如IDE将会使用诸如arg0、arg1之类的占位符代替原有的参数名，这对程序运行没有影响，但是会对代码编写带来较大不便，而且在调试期间无法根据参数名称从上下文中获得参数值。 ConstantValue属性ConstantValue属性的作用是通知虚拟机自动为静态变量赋值。只有被static关键字修饰的变量（类变量）才可以使用这项属性。类似“int x&#x3D;123”和“static int x&#x3D;123”这样的变量定义在Java程序里面是非常常见的事情，但虚拟机对这两种变量赋值的方式和时刻都有所不同。 对非static类型的变量（也就是实例变量）的赋值是在实例构造器&lt;init&gt;()方法中进行的；而对于类变量，则有两种方式可以选择：在类构造器&lt;clinit&gt;()方法中或者使用ConstantValue属性。目前Oracle公司实现的Javac编译器的选择是，如果同时使用final和static来修饰一个变量（按照习惯，这里称“常量”更贴切），并且这个变量的数据类型是基本类型或者java.lang.String的话，就将会生成ConstantValue属性来进行初始化；如果这个变量没有被final修饰，或者并非基本类型及字符串，则将会选择在&lt;clinit&gt;()方法中进行初始化。 Deprecated及Synthetic属性Deprecated和Synthetic两个属性都属于标志类型的布尔属性，只存在有和没有的区别，没有属性值的概念。 Deprecated属性用于表示某个类、字段或者方法，已经被程序作者定为不再推荐使用，它可以通过代码中使用“@deprecated”注解进行设置。 Synthetic属性代表此字段或者方法并不是由Java源码直接产生的，而是由编译器自行添加的， Signature属性可以出现于类、字段表和方法表结构的属性表中。任何类、接口、初始化方法或成员的泛型签名如果包含了类型变量（Type Variable）或参数化类型（Parameterized Type），则Signature属性会为它记录泛型签名信息。之所以要专门使用这样一个属性去记录泛型类型，是因为Java语言的泛型采用的是擦除法实现的伪泛型，字节码（Code属性）中所有的泛型信息编译（类型变量、参数化类型）在编译之后都通通被擦除掉。使用擦除法的好处是实现简单（主要修改Javac编译器，虚拟机内部只做了很少的改动）、非常容易实现Backport，运行期也能够节省一些类型所占的内存空间。但坏处是运行期就无法像C#等有真泛型支持的语言那样，将泛型类型与用户定义的普通类型同等对待，例如运行期做反射时无法获得泛型信息。Signature属性就是为了弥补这个缺陷而增设的，现在Java的反射API能够获取的泛型类型，最终的数据来源也是这个属性。 MethodParameters属性MethodParameters是在JDK 8时新加入到Class文件格式中的，它是一个用在方法表中的变长属性。MethodParameters的作用是记录方法的各个形参名称和信息。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"理解JVM垃圾回收器准备","slug":"jvm/01垃圾收集器与内存分配策略/理解JVM垃圾回收器准备","date":"2021-11-19T12:00:33.000Z","updated":"2022-03-23T09:03:57.853Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/理解JVM垃圾回收器准备/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/%E7%90%86%E8%A7%A3JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8%E5%87%86%E5%A4%87/","excerpt":"","text":"固定可作为GC Roots的节点主要在全局性的引用(例如常量或类静态属性)与执行上下文(例如栈帧中的本地变量表)中。迄今为止，所有收集器在根节点枚举这一步骤时都是必须暂停用户线程的，现在可达性分析算法耗时最长的查找引用链的过程已经可以做到与用户线程一起并发，但根节点枚举始终还是必须在一个能保障一致性的快照中才得以进行——这里“一致性”的意思是整个枚举期间执行子系统看起来就像被冻结在某个时间点上，不会出现分析过程中，根节点集合的对象引用关系还在不断变化的情况，若这点不能满足的话，分析结果准确性也就无法保证。这是导致垃圾收集过程必须停顿所有用户线程的其中一个重要原因，即使是号称停顿时间可控，或者（几乎）不会发生停顿的CMS、G1、ZGC等收集器，枚举根节点时也是必须要停顿的。Hotspot使用一组称为OopMap的数据结构来快速得到哪些地方存放着对象引用，一旦类加载动作完成的时候， HotSpot就会把对象内什么偏移量上是什么类型的数据计算出来，在即时编译(见第11章)过程中，也会在特定的位置记录下栈里和寄存器里哪些位置是引用。这样收集器在扫描时就可以直接得知这些信息了，并不需要真正一个不漏地从方法区等GC Roots开始查找。在OopMap的协助下，HotSpot可以快速准确地完成GC Roots枚举 安全点HotSpot没有为每条指令都生成OopMap，只是在“特定的位置”记录了这些信息，这些位置被称为安全点(Safepoint)。有了安全点的设定，也就决定了用户程序执行时并非在代码指令流的任意位置都能够停顿下来开始垃圾收集，而是强制要求必须执行到达安全点后才能够暂停。安全点位置的选取基本上是以“是否具有让程序长时间执行的特征”为标准进行选定的，“长时间执行”的最明显特征就是指令序列的复用，例如方法调用、循环跳转、异常跳转等都属于指令序列复用，所以只有具有这些功能的指令才会产生安全点。 对于安全点，另外一个需要考虑的问题是，如何在垃圾收集发生时让所有线程（这里其实不包括执行JNI调用的线程）都跑到最近的安全点，然后停顿下来。 线程的中断有两种：抢先式中断和主动式中断。前者就是在发生中断时，强制剥夺线程的 CPU资源，后者是在正在执行的线程中断位上标记一下，具体什么时候中断由线程自己来决定。 抢先式中断：在垃圾收集发生时，系统首先把所有用户线程全部中断，如果发现有用户线程中断的地方不在安全点上，就恢复这条线程执行，让它一会再重新中断，直到跑到安全点上。现在几乎没有虚拟机实现采用抢先式中断来暂停线程响应GC事件。 主动式中断：不直接对线程操作，仅仅简单地设置一个标志位，各个线程执行过程时会不停地主动去轮询这个标志，一旦发现中断标志为真时就自己在最近的安全点上主动中断挂起。轮询标志的地方和安全点是重合的，另外还要加上所有创建对象和其他需要在Java堆上分配内存的地方，这是为了检查是否即将要发生垃圾收集，避免没有足够内存分配新对象。 由于轮询在代码中会频繁出现，这要求它必须足够高效。HotSpot使用内存保护陷阱的方式，把轮询操作精简至只有一条汇编指令的程度。 安全区域如果用户线程出去Sleep或者阻塞状态，JVM不可能等这些线程走到安全点，这时就需要引入安全区域（Safe Region）来解决。 安全区域是指能够确保在某一段代码片段之中，引用关系不会发生变化，因此，在这个区域中任意地方开始垃圾收集都是安全的。我们也可以把安全区域看作被扩展拉伸了的安全点。 当用户线程执行到安全区域里面的代码时，首先会标识自己已经进入了安全区域，那样当这段时间里虚拟机要发起垃圾收集时就不必去管这些已声明自己在安全区域内的线程了。当线程要离开安全区域时，它要检查虚拟机是否已经完成了根节点枚举(或者垃圾收集过程中其他需要暂停用户线程的阶段)，如果完成了，那线程就当作没事发生过，继续执行;否则它就必须一直等待，直到收到可以离开安全区域的信号为止。 记忆集与卡表讲解分代收集理论的时候，提到了为解决对象跨代引用所带来的问题，垃圾收集器在新生代中建立了名为记忆集（Remembered Set）的数据结构，用以避免把整个老年代加进GC Roots扫描范围。事实上并不只是新生代、老年代之间才有跨代引用的问题，所有涉及部分区域收集（Partial GC）行为的垃圾收集器，典型的如G1、ZGC和Shenandoah收集器，都会面临相同的问题 记忆集是一种用于记录从非收集区域指向收集区域的指针集合的抽象数据结构。 在垃圾收集的场景中，收集器只需要通过记忆集判断出某一块非收集区域是否存在有指向了收集区域的指针就可以了，并不需要了解这些跨代指针的全部细节。那设计者在实现记忆集的时候，便可以选择更为粗犷的记录粒度来节省记忆集的存储和维护成本。比如： 字长精度：每个记录精确到一个机器字长（就是处理器的寻址位数，如常见的32位或64位，这个精度决定了机器访问物理内存地址的指针长度），该字包含跨代指针。 对象精度：每个记录精确到一个对象，该对象里有字段含有跨代指针。 卡精度：每个记录精确到一块内存区域，该区域内有对象含有跨代指针。 其中，第三种“卡精度”所指的是用一种称为“卡表”（Card Table）的方式去实现记忆集，这也是目前最常用的一种记忆集实现形式。 卡表最简单的形式可以只是一个字节数组，而HotSpot虚拟机确实也是这样做的。以下这行代码是HotSpot默认的卡表标记逻辑： 1CARD_TABLE [this address &gt;&gt; 9] = 0 字节数组CARD_TABLE的每一个元素都对应着其标识的内存区域中一块特定大小的内存块，这个内存块被称作“卡页”（Card Page）。一般来说，卡页大小都是以2的N次幂的字节数，通过上面代码可以看出HotSpot中使用的卡页是2的9次幂，即512字节（地址右移9位，相当于用地址除以512）。那如果卡表标识内存区域的起始地址是0x0000的话，数组CARD_TABLE的第0、1、2号元素，分别对应了地址范围为0x0000～0x01FF、0x0200～0x03FF、0x0400～0x05FF的卡页内存块。 一个卡页的内存中通常包含不止一个对象，只要卡页内有一个（或更多）对象的字段存在着跨代指针，那就将对应卡表的数组元素的值标识为1，称为这个元素变脏（Dirty），没有则标识为0。在垃圾收集发生时，只要筛选出卡表中变脏的元素，就能轻易得出哪些卡页内存块中包含跨代指针，把它们加入GC Roots中一并扫描。 写屏障卡表元素何时变脏的答案是很明确的——有其他分代区域中对象引用了本区域对象时，其对应的卡表元素就应该变脏，变脏时间点原则上应该发生在引用类型字段赋值的那一刻。但问题是如何变脏，即如何在对象赋值的那一刻去更新维护卡表呢？ 在HotSpot虚拟机里是通过写屏障（Write Barrier）技术维护卡表状态的。 写屏障可以看作在虚拟机层面对“引用类型字段赋值”这个动作的AOP切面，在引用对象赋值时会产生一个环形（Around）通知，供程序执行额外的动作，也就是说赋值的前后都在写屏障的覆盖范畴内。在赋值前的部分的写屏障叫作写前屏障（Pre-Write Barrier），在赋值后的则叫作写后屏障（Post-Write Barrier）。HotSpot虚拟机的许多收集器中都有使用到写屏障，但直至G1收集器出现之前，其他收集器都只用到了写后屏障。代码清单3-6 写后屏障更新卡表 123456void oop_field_store(oop* field, oop new_value) &#123; // 引用字段赋值操作 *field = new_value; // 写后屏障，在这里完成卡表状态更新 post_write_barrier(field, new_value);&#125; 应用写屏障后，虚拟机就会为所有赋值操作生成相应的指令，一旦收集器在写屏障中增加了更新卡表操作，无论更新的是不是老年代对新生代对象的引用，每次只要对引用进行更新，就会产生额外的开销，不过这个开销与Minor GC时扫描整个老年代的代价相比还是低得多的。 除了写屏障的开销外，卡表在高并发场景下还面临着“伪共享”（False Sharing）问题。伪共享是处理并发底层细节时一种经常需要考虑的问题，现代中央处理器的缓存系统中是以缓存行（Cache Line）为单位存储的，当多线程修改互相独立的变量时，如果这些变量恰好共享同一个缓存行，就会彼此影响（写回、无效化或者同步）而导致性能降低，这就是伪共享问题。 假设处理器的缓存行大小为64字节，由于一个卡表元素占1个字节，64个卡表元素将共享同一个缓存行。这64个卡表元素对应的卡页总的内存为32KB（64×512字节），也就是说如果不同线程更新的对象正好处于这32KB的内存区域内，就会导致更新卡表时正好写入同一个缓存行而影响性能。为了避免伪共享问题，一种简单的解决方案是不采用无条件的写屏障，而是先检查卡表标记，只有当该卡表元素未被标记过时才将其标记为变脏，即将卡表更新的逻辑变为以下代码所示： 12if (CARD_TABLE [this address &gt;&gt; 9] != 0) CARD_TABLE [this address &gt;&gt; 9] = 0; JDK 7之后，HotSpot虚拟机增加了一个新的参数-XX:+UseCondCardMark，用来决定是否开启卡表更新的条件判断。开启会增加一次额外判断的开销，但能够避免伪共享问题，两者各有性能损耗，是否打开要根据应用实际运行情况来进行测试权衡。3.4.6 并发的可达性分析 并发的可达性分析三色标记 白色:表示对象尚未被垃圾收集器访问过。显然在可达性分析刚刚开始的阶段，所有的对象都是白色的，若在分析结束的阶段，仍然是白色的对象，即代表不可达。 黑色:表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经扫描过。黑色的对象代表已经扫描过，它是安全存活的，如果有其他对象引用指向了黑色对象，无须重新扫描一遍。黑色对象不可能直接(不经过灰色对象)指向某个白色对象。 灰色:表示对象已经被垃圾收集器访问过，但这个对象上至少存在一个引用还没有被扫描过。 从上图可以，当且仅当以下两个条件同时满足时，会产生“对象消失”的问题，即原本应该是黑色的对象被误标为白色： 赋值器插入了一条或多条从黑色对象到白色对象的新引用； 赋值器删除了全部从灰色对象到该白色对象的直接或间接引用。 因此，要解决并发扫描时的对象消失问题有两种解决方案: 增量更新(Incremental Update) 原始快照(Snapshot At The Beginning, SATB) &#x3D;&#x3D;增量更新&#x3D;&#x3D;要破坏的是第一个条件，当黑色对象插入新的指向白色对象的引用关系时，就将这个新插入的引用记录下来，等并发扫描结束之后，再将这些记录过的引用关系中的黑色对象为根，重新扫描一次。这可以简化理解为，&#x3D;&#x3D;黑色对象一旦新插入了指向白色对象的引用之后，它就变回灰色对象了。&#x3D;&#x3D; &#x3D;&#x3D;原始快照&#x3D;&#x3D;要破坏的是第二个条件，当灰色对象要删除指向白色对象的引用关系时，就将这个要删除的引用记录下来，在并发扫描结束之后，再将这些记录过的引用关系中的灰色对象为根，重新扫描一次。这也可以简化理解为，&#x3D;&#x3D;无论引用关系删除与否，都会按照刚刚开始扫描那一刻的对象图快照来进行搜索。&#x3D;&#x3D; 以上无论是对引用关系记录的插入还是删除，虚拟机的记录操作都是通过写屏障实现的。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"垃圾收集器的jvm参数","slug":"jvm/01垃圾收集器与内存分配策略/垃圾收集器的jvm参数","date":"2021-11-19T12:00:32.000Z","updated":"2022-03-23T09:03:57.850Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/垃圾收集器的jvm参数/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E7%9A%84jvm%E5%8F%82%E6%95%B0/","excerpt":"","text":"新生代 垃圾收集器 jvm参数 算法 Serial收集器 JVM-XX:+/-UseSerialGC，默认组合Serial+Serial Old 复制算法 ParNew收集器 JVM参数：-XX:+/-UseConcMarkSweepGC，开启CMS，默认组合ParNew+CMS+Serial Old（当CMS收集器发生失败时的后备预案） 复制算法 Parallel Scavenge收集器 JVM参数：-XX:+/-UseParallelGC，开启后默认组合Parallel Scavenge+Serial Old。-XX:MaxGCPauseMillis、-XX:GCTimeRatio、-XX:+UseAdaptiveSizePolicy 老年代 垃圾收集器 jvm参数 算法 Serial Old收集器 标记整理 Parallel Old收集器 JVM参数：-XX:+/-UseParallelOldGC，默认组合Parallel Scavenge+Parallel Old 标记整理 CMS收集器 JVM参数：-XX:+/-UseConcMarkSweepGC，开启CMS，默认组合ParNew+CMS+Serial Old（当CMS收集器发生失败时的后备预案） 标记-清除 G1收集器JVM参数：-XX:+UseG1GC，默认组合G1+G1，jdk8默认收集器 其他jvm参数： -XX:MaxGCPauseMillis：指定最大的收集停顿时间，单位是毫秒，默认值是200毫秒 -XX:G1HeapRegionSize：指定Region的大小取值范围为1MB~32MB，比如：-XX:G1HeapRegionSize&#x3D;16m Epsilon收集器一旦可用堆内存用完, JVM就会退出 用法：XX:+UnlockExperimentalVMOptions -XX:+UseEpsilonGC 如果应用只要运行数分钟甚至数秒， 只要Java虚拟机能正确分配内存，在堆耗尽之前就会退出，那显然运行负载极小、没有任何回收行为的Epsilon便是很恰当的选择。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"JVM日志","slug":"jvm/01垃圾收集器与内存分配策略/JVM日志","date":"2021-11-19T12:00:31.000Z","updated":"2022-03-23T09:03:57.844Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/JVM日志/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/JVM%E6%97%A5%E5%BF%97/","excerpt":"","text":"前面添加-XX:比如：-XX:+UseSerialGC JDK 9后可以使用-Xlog参数，查看JVM日志-Xlog[:[selector][:[output][:[decorators][:output-options]]]]其中:output为执行日志输出到的文件地址 查看GC基本信息在JDK 9之前使用-XX:+PrintGC，JDK 9后使用-Xlog:gc 查看GC详细信息在JDK 9之前使用-XX:+PrintGCDetails，在JDK 9之后使用-Xlog:gc* 查看GC前后的堆、方法区可用容量变化JDK 9之前使用-XX:+PrintHeapAtGC，JDK 9之后使用-Xlog:gc+heap=debug 查看GC过程中用户线程并发时间以及停顿的时间在JDK 9之前使用-XX:+PrintGCApplicationConcurrentTime以及-XX:+PrintGCApplicationStoppedTime，JDK 9之后使用-Xlog:safepoint 查看收集器Ergonomics机制(自动设置堆空间各分代区域大小、收集目标等内容，从Parallel收集器开始支持)自动调节的相关信息在JDK 9之前使用-XX:+PrintAdaptive-SizePolicy，JDK 9之后使用-Xlog:gc+ergo*=trace 查看熬过收集后剩余对象的年龄分布信息在JDK 9前使用-XX:+PrintTenuring-Distribution, JDK 9之后使用-Xlog:gc+age=trace","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"JVM常用内存参数配置(网上)","slug":"jvm/01垃圾收集器与内存分配策略/JVM常用内存参数配置(网上)","date":"2021-11-19T12:00:30.000Z","updated":"2022-03-23T09:03:57.831Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/JVM常用内存参数配置(网上)/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/JVM%E5%B8%B8%E7%94%A8%E5%86%85%E5%AD%98%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE(%E7%BD%91%E4%B8%8A)/","excerpt":"","text":"-Xms JVM启动时申请的初始Heap值，默认为操作系统物理内存的1&#x2F;64但小于1G。默认当空余堆内存大于70%时，JVM会减小heap的大小到-Xms指定的大小，可通过-XX:MaxHeapFreeRation&#x3D;来指定这个比列。Server端JVM最好将-Xms和-Xmx设为相同值，避免每次垃圾回收完成后JVM重新分配内存；开发测试机JVM可以保留默认值。(例如：-Xms4g) -Xmx JVM可申请的最大Heap值，默认值为物理内存的1&#x2F;4但小于1G，默认当空余堆内存小于40%时，JVM会增大Heap到-Xmx指定的大小，可通过**-XX:MinHeapFreeRation&#x3D;**来指定这个比列。最佳设值应该视物理内存大小及计算机内其他内存开销而定。(例如：-Xmx4g) -Xmn Java Heap Young区大小。整个堆大小&#x3D;年轻代大小 + 年老代大小 + 持久代大小(相对于HotSpot 类型的虚拟机来说)。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3&#x2F;8。(例如：-Xmn2g) 程序新创建的对象都是从年轻代分配内存，年轻代由Eden Space和两块相同大小的SurvivorSpace(通常又称S0和S1或From和To)构成，可通过-Xmn参数来指定年轻代的大小，也可以通过-XX:SurvivorRation来调整Eden Space及SurvivorSpace的大小。 老年代用于存放经过多次新生代GC仍然存活的对象，例如缓存对象，新建的对象也有可能直接进入老年代，主要有两种情况：1、大对象，可通过启动参数设置-XX:PretenureSizeThreshold&#x3D;1024(单位为字节，默认为0)来代表超过多大时就不在新生代分配，而是直接在老年代分配。2、大的数组对象，且数组中无引用外部对象。老年代所占的内存大小为-Xmx对应的值减去-Xmn对应的值。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 -Xss Java每个线程的Stack大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。根据应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。(例如：-Xss1024K) -XX:PermSize 持久代（方法区）的初始内存大小。（例如：-XX:PermSize&#x3D;64m） -XX:MaxPermSize 持久代（方法区）的最大内存大小。（例如：-XX:MaxPermSize&#x3D;512m） -XX:+UseSerialGC 串行（SerialGC）是jvm的默认GC方式，一般适用于小型UseAdaptiveSizePolicy应用和单处理器，算法比较简单，GC效率也较高，但可能会给应用带来停顿。 -XX:+UseParallelGC 并行（ParallelGC）是指多个线程并行执行GC，一般适用于多处理器系统中，可以提高GC的效率，但算法复杂，系统消耗较大。（配合使用：-XX:ParallelGCThreads&#x3D;8，并行收集器的线程数，此值最好配置与处理器数目相等） -XX:+UseParNewGC 设置年轻代为并行收集，JKD5.0以上，JVM会根据系统配置自行设置，所以无需设置此值。 -XX:+UseParallelOldGC 设置年老代为并行收集，JKD6.0出现的参数选项。 -XX:+UseConcMarkSweepGC 并发（ConcMarkSweepGC）是指GC运行时，对应用程序运行几乎没有影响（也会造成停顿，不过很小而已），GC和app两者的线程在并发执行，这样可以最大限度不影响app的运行。 -XX:+UseCMSCompactAtFullCollection 在Full GC的时候，对老年代进行压缩整理。因为CMS是不会移动内存的，因此非常容易产生内存碎片。因此增加这个参数就可以在FullGC后对内存进行压缩整理，消除内存碎片。当然这个操作也有一定缺点，就是会增加CPU开销与GC时间，所以可以通过-XX:CMSFullGCsBeforeCompaction&#x3D;3 这个参数来控制多少次Full GC以后进行一次碎片整理。 -XX:+CMSInitiatingOccupancyFraction&#x3D;80 代表老年代使用空间达到80%后，就进行Full GC。CMS收集器在进行垃圾收集时，和应用程序一起工作，所以，不能等到老年代几乎完全被填满了再进行收集，这样会影响并发的应用线程的空间使用，从而再次触发不必要的Full GC。 -XX:+MaxTenuringThreshold&#x3D;10 垃圾的最大年龄，代表对象在Survivor区经过10次复制以后才进入老年代。如果设置为0，则年轻代对象不经过Survivor区，直接进入老年代。 二、分类** JVM启动参数共分为三类：** 1、标准参数（-），所有的JVM实现都必须实现这些参数的功能，而且向后兼容。例如：-verbose:class（输出jvm载入类的相关信息，当jvm报告说找不到类或者类冲突时可此进行诊断）；-verbose:gc（输出每次GC的相关情况）；-verbose:jni（输出native方法调用的相关情况，一般用于诊断jni调用错误信息）。 2、非标准参数（-X），默认jvm实现这些参数的功能，但是并不保证所有jvm实现都满足，且不保证向后兼容。例如：-Xms512m；-Xmx512m；-Xmn200m；-Xss128k；-Xloggc:file（与-verbose:gc功能类似，只是将每次GC事件的相关情况记录到一个文件中，文件的位置最好在本地，以避免网络的潜在问题。若与verbose命令同时出现在命令行中，则以-Xloggc为准）。 3、非Stable参数（-XX），此类参数各个jvm实现会有所不同，将来可能会随时取消，需要慎重使用。例如：-XX:PermSize&#x3D;64m；-XX:MaxPermSize&#x3D;512m。 三、 引用 主要介绍JVM内存参数的详细配置，与GC策略（原：http://www.cnblogs.com/redcreen/archive/2011/05/04/2037057.html）。 不管是YGC还是Full GC,GC过程中都会对导致程序运行中中断,正确的选择不同的GC策略,调整JVM、GC的参数，可以极大的减少由于GC工作，而导致的程序运行中断方面的问题，进而适当的提高Java程序的工作效率。但是调整GC是以个极为复杂的过程，由于各个程序具备不同的特点，如：web和GUI程序就有很大区别（Web可以适当的停顿，但GUI停顿是客户无法接受的），而且由于跑在各个机器上的配置不同（主要cup个数，内存不同），所以使用的GC种类也会不同(如何选择见GC种类及如何选择)。本文将注重介绍JVM、GC的一些重要参数的设置来提高系统的性能。 JVM内存组成及GC相关内容请见之前的文章:JVM内存组成 GC策略&amp;内存申请。JVM参数的含义实例见实例分析如下： ** 并行收集器相关参数：** ** CMS相关参数：** ** 辅助信息：** 3.1 GC性能方面的考虑 对于GC的性能主要有2个方面的指标：吞吐量throughput（工作时间不算gc的时间占总的时间比）和暂停pause（gc发生时app对外显示的无法响应）。 1、Total Heap，默认情况下，vm会增加&#x2F;减少heap大小以维持free space在整个vm中占的比例，这个比例由MinHeapFreeRatio和MaxHeapFreeRatio指定。一般而言，server端的app会有以下规则：对vm分配尽可能多的memory；将Xms和Xmx设为一样的值。如果虚拟机启动时设置使用的内存比较小，这个时候又需要初始化很多对象，虚拟机就必须重复地增加内存。处理器核数增加，内存也跟着增大。 2、The Young Generation，另外一个对于app流畅性运行影响的因素是young generation的大小。younggeneration越大，minor collection越少；但是在固定heap size情况下，更大的young generation就意味着小的tenured generation，就意味着更多的major collection(major collection会引发minor collection)。NewRatio反映的是young和tenured generation的大小比例。NewSize和MaxNewSize反映的是young generation大小的下限和上限，将这两个值设为一样就固定了young generation的大小（同Xms和Xmx设为一样）。如果希望，SurvivorRatio也可以优化survivor的大小，不过这对于性能的影响不是很大。SurvivorRatio是eden和survior大小比例。一般而言，server端的app会有以下规则：首先决定能分配给vm的最大的heap size，然后设定最佳的young generation的大小；如果heap size固定后，增加young generation的大小意味着减小tenured generation大小。让tenured generation在任何时候够大，能够容纳所有live的data（留10%-20%的空余）。 3.2 经验&amp;规则 年轻代大小选择：响应时间优先的应用:尽可能设大,直到接近系统的最低响应时间限制(根据实际情况选择).在此种情况下,年轻代收集发生的频率也是最小的.同时,减少到达年老代的对象；吞吐量优先的应用:尽可能的设置大,可能到达Gbit的程度.因为对响应时间没有要求,垃圾收集可以并行进行,一般适合8CPU以上的应用；避免设置过小.当新生代设置过小时会导致:1.YGC次数更加频繁 2.可能导致YGC对象直接进入旧生代,如果此时旧生代满了,会触发FGC. 年老代大小选择：响应时间优先的应用:年老代使用并发收集器,所以其大小需要小心设置,一般要考虑并发会话率和会话持续时间等一些参数.如果堆设置小了,可以会造成内存碎 片,高回收频率以及应用暂停而使用传统的标记清除方式;如果堆大了,则需要较长的收集时间.最优化的方案,一般需要参考以下数据获得，并发垃圾收集信息、持久代并发收集次数、传统GC信息、花在年轻代和年老代回收上的时间比例。吞吐量优先的应用:一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代.原因是,这样可以尽可能回收掉大部分短期对象,减少中期的对象,而年老代尽存放长期存活对象。 较小堆引起的碎片问题：因为年老代的并发收集器使用标记,清除算法,所以不会对堆进行压缩.当收集器回收时,他会把相邻的空间进行合并,这样可以分配给较大的对象.但是,当堆空间较小时,运行一段时间以后,就会出现”碎片”,如果并发收集器找不到足够的空间,那么并发收集器将会停止,然后使用传统的标记,清除方式进行回收.如果出现”碎片”,可能需要进行如下配置:-XX:+UseCMSCompactAtFullCollection:使用并发收集器时,开启对年老代的压缩；-XX:CMSFullGCsBeforeCompaction&#x3D;0:上面配置开启的情况下,这里设置多少次Full GC后,对年老代进行压缩。 使用CMS的好处是用尽量少的新生代，经验值是128M－256M， 然后老生代利用CMS并行收集， 这样能保证系统低延迟的吞吐效率。 实际上cms的收集停顿时间非常的短，2G的内存， 大约20－80ms的应用程序停顿时间 系统停顿的时候可能是GC的问题也可能是程序的问题，多用jmap和jstack查看，或者killall -3 java，然后查看java控制台日志，能看出很多问题。(相关工具的使用方法将在后面的blog中介绍)。 仔细了解自己的应用，如果用了缓存，那么年老代应该大一些，缓存的HashMap不应该无限制长，建议采用LRU算法的Map做缓存，LRUMap的最大长度也要根据实际情况设定。 采用并发回收时，年轻代小一点，年老代要大，因为年老大用的是并发回收，即使时间长点也不会影响其他程序继续运行，网站不会停顿。 JVM参数的设置(特别是 –Xmx –Xms –Xmn-XX:SurvivorRatio -XX:MaxTenuringThreshold等参数的设置没有一个固定的公式，需要根据PV old区实际数据 YGC次数等多方面来衡量。为了避免promotion faild可能会导致xmn设置偏小，也意味着YGC的次数会增多，处理并发访问的能力下降等问题。每个参数的调整都需要经过详细的性能测试，才能找到特定应用的最佳配置。 3.3 PromotionFailed: 垃圾回收时promotionfailed是个很头痛的问题，一般可能是两种原因产生，第一个原因是救助空间不够，救助空间里的对象还不应该被移动到年老代，但年轻代又有很多对象需要放入救助空间；第二个原因是年老代没有足够的空间接纳来自年轻代的对象；这两种情况都会转向Full GC，网站停顿时间较长。 解决方方案一： 第一个原因我的最终解决办法是去掉救助空间，设置-XX:SurvivorRatio&#x3D;65536 -XX:MaxTenuringThreshold&#x3D;0即可，第二个原因我的解决办法是设置CMSInitiatingOccupancyFraction为某个值（假设70），这样年老代空间到70%时就开始执行CMS，年老代有足够的空间接纳来自年轻代的对象。 解决方案一的改进方案： 又有改进了，上面方法不太好，因为没有用到救助空间，所以年老代容易满，CMS执行会比较频繁。我改善了一下，还是用救助空间，但是把救助空间加大，这样也不会有promotion failed。具体操作上，32位Linux和64位Linux好像不一样，64位系统似乎只要配置MaxTenuringThreshold参数，CMS还是有暂停。为了解决暂停问题和promotion failed问题，最后我设置-XX:SurvivorRatio&#x3D;1 ，并把MaxTenuringThreshold去掉，这样即没有暂停又不会有promotoin failed，而且更重要的是，年老代和永久代上升非常慢（因为好多对象到不了年老代就被回收了），所以CMS执行频率非常低，好几个小时才执行一次，这样，服务器都不用重启了。 -Xmx4000M -Xms4000M -Xmn600M-XX:PermSize&#x3D;500M -XX:MaxPermSize&#x3D;500M -Xss256K -XX:+DisableExplicitGC-XX:SurvivorRatio&#x3D;1 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC-XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction&#x3D;0-XX:+CMSClassUnloadingEnabled -XX:LargePageSizeInBytes&#x3D;128M-XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly-XX:CMSInitiatingOccupancyFraction&#x3D;80 -XX:SoftRefLRUPolicyMSPerMB&#x3D;0-XX:+PrintClassHistogram -XX:+PrintGCDetails -XX:+PrintGCTimeStamps-XX:+PrintHeapAtGC -Xloggc:log&#x2F;gc.log 3.4 CMSInitiatingOccupancyFraction值与Xmn的关系公式 上面介绍了promontionfaild产生的原因是EDEN空间不足的情况下将EDEN与From survivor中的存活对象存入To survivor区时,To survivor区的空间不足，再次晋升到old gen区，而old gen区内存也不够的情况下产生了promontion faild从而导致full gc.那可以推断出：eden+from survivor &lt; old gen区剩余内存时，不会出现promontion faild的情况，即： (Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction&#x2F;100)&gt;&#x3D;(Xmn-Xmn&#x2F;(SurvivorRatior+2)) 进而推断出：CMSInitiatingOccupancyFraction&lt;&#x3D;((Xmx-Xmn)-(Xmn-Xmn&#x2F;(SurvivorRatior+2)))&#x2F;(Xmx-Xmn)*100 例如：当xmx&#x3D;128 xmn&#x3D;36SurvivorRatior&#x3D;1时CMSInitiatingOccupancyFraction&lt;&#x3D;((128.0-36)-(36-36&#x2F;(1+2)))&#x2F;(128-36)*100&#x3D;73.913; 当xmx&#x3D;128 xmn&#x3D;24SurvivorRatior&#x3D;1时CMSInitiatingOccupancyFraction&lt;&#x3D;((128.0-24)-(24-24&#x2F;(1+2)))&#x2F;(128-24)*100&#x3D;84.615… 当xmx&#x3D;3000 xmn&#x3D;600SurvivorRatior&#x3D;1时 CMSInitiatingOccupancyFraction&lt;&#x3D;((3000.0-600)-(600-600&#x2F;(1+2)))&#x2F;(3000-600)*100&#x3D;83.33 CMSInitiatingOccupancyFraction低于70% 需要调整xmn或SurvivorRatior值。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"JVM垃圾收集相关的常用参数","slug":"jvm/01垃圾收集器与内存分配策略/JVM垃圾收集相关的常用参数","date":"2021-11-19T12:00:29.000Z","updated":"2022-03-23T09:03:57.826Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/JVM垃圾收集相关的常用参数/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/JVM%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%9B%B8%E5%85%B3%E7%9A%84%E5%B8%B8%E7%94%A8%E5%8F%82%E6%95%B0/","excerpt":"","text":"前面添加-XX:比如：-XX:+UseSerialGC","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"GC 参数（整理）","slug":"jvm/01垃圾收集器与内存分配策略/GC 参数（整理）","date":"2021-11-19T12:00:28.000Z","updated":"2022-03-23T09:03:57.822Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/GC 参数（整理）/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/GC%20%E5%8F%82%E6%95%B0%EF%BC%88%E6%95%B4%E7%90%86%EF%BC%89/","excerpt":"","text":"GC 日志详解 垃圾收集器 -XX:+&#x2F;-UseSerialGC：年轻代启用或不实用Serial收集器，默认组合Serial+Serial Old -XX:+&#x2F;-UseConcMarkSweepGC：开启或关系CMS，默认组合ParNew+CMS+Serial Old（当CMS收集器发生失败时的后备预案） -XX:+&#x2F;-UseParallelGC：启用或者关系Parallel Scavenge收集器，开启后默认组合Parallel Scavenge+Serial Old。 -XX:+&#x2F;-UseG1GC：启用或关闭G1收集器。默认组合G1+G1，jdk8默认收集器 XX:+UnlockExperimentalVMOptions -XX:+UseEpsilonGC：使用Epsilon收集器，一旦可用堆内存用完, JVM就会退出 GC 常用参数 -Xmx：-Xmx2048m最大堆大小 -Xms：-Xms2048m初始堆大小 -Xmn：-Xmn1024m年轻代大小 -Xss：-Xss512k每个线程栈大小，JDK5.0以后每个线程堆栈大小为1M 设置Matespace内存大小的参数： -XX:MetaspaceSize=256m-XX:MaxMetaspaceSize=512M -XX:MaxDirectMemorySize： -XX:MaxDirectMemorySize=512M直接内存最大值 -XX:SurvivorRatio： -XX:SurvivorRatio=8Eden区与Survivor区的大小比值，设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1&#x2F;10 -XX:+UseTLAB：使用 TLAB（本地线程分配缓存）默认打开 -XX:+DoEscapeAnalysis：开启逃逸分析 详情看&gt;&gt; -XX:+EliminateAllocations：标量替换 详情看&gt;&gt; +XX:+EliminateLocks：开启同步消除 详情看&gt;&gt; -XX:+PrintTLAB：打印 TLAB 的使用情况 -XX:TLABSize：设置 TLAB 大小 -XX:+DisableExplicitGC：禁止在启动期间显式调用System.gc() -XX:+PrintGC：打印GC基本信息 -XX:+PrintGCDetails：打印GC详细信息 -XX:+PrintGCTimeStamps：打印CG发生的时间戳 -XX:+PrintHeapAtGC：每一次GC前和GC后，都打印堆信息 -XX:+TraceClassLoading：监控类的加载 -XX:PretenureSizeThreshold： -XX:PretenureSizeThreshold=10m指定大于该设置值的对象直接在老年代分配 -XX:MaxTenuringThreshold： -XX:MaxTenuringThreshold=n对象晋升老年代的年龄阈值，对象在Survivor区中每熬过一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度(默认为15)，就会被晋升到老年代中。 -XX:HandlePromotionFailure： -XX:HandlePromotionFailure=true/false是否允许空间分配担保失败（Handle Promotion Failure）如果允许，那会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者-XX:HandlePromotionFailure设置不允许冒险，那这时就要改为进行一次Full GC。 -XX:+&#x2F;-UseAdaptiveSizePolicy：会根据GC的情况自动计算计算 Eden、From 和 To 区的大小； 对于面向外部的大流量、低延迟系统，不建议启用此参数，建议关闭该参数。 -XX:+HeapDumpOnOutOfMemoryError：OOM时导出堆到文件 -XX:HeapDumpPath： -XX:HeapDumpPath=d:/a.dump导出OOM的路径 -XX:+UseSpinning：开启自旋锁，在JDK 6后默认开启，关闭只需要-XX:-UseSpinning&#x3D;false -XX:PreBlockSpin：自旋锁的次数，默认10次 数-XX:+UseBiasedLocking：开启偏向锁，是JDK 6起HotSpot虚拟机的默认值，但是它在应用程序启动几秒钟之后才激活，如有必要可以使用JVM参数来关闭延迟:-XX:BiasedLockingStartupDelay&#x3D;0。关闭偏心锁-XX:-UseBiasedLocking=false Parallel 常用参数 -XX:+ParallelGCThreads：并行收集器的线程数，同样适用于 CMS，一般设为和 CPU 核数相同 CMS 常用参数 -XX:+ParallelGCThreads：并行收集器的线程数，同样适用于 CMS，一般设为和 CPU 核数相同 -XX:CMSInitiatingOccupancyFraction：使用多少比例的老年代后开始 CMS 收集，默认是 68%(近似值)，如果频繁发生 SerialOld 卡顿，应该调小 -XX:+UseCMSCompactAtFullCollection： 在Full GC的时候，对老年代进行压缩整理。因为CMS是不会移动内存的，因此非常容易产生内存碎片。因此增加这个参数就可以在FullGC后对内存进行压缩整理，消除内存碎片。当然这个操作也有一定缺点，就是会增加CPU开销与GC时间，所以可以通过-XX:CMSFullGCsBeforeCompaction&#x3D;3 这个参数来控制多少次Full GC以后进行一次碎片整理。 -XX:CMSFullGCsBeforeCompaction：配合上边的使用 -XX:+CMSInitiatingOccupancyFraction： -XX:+CMSInitiatingOccupancyFraction=80代表老年代使用空间达到80%后，就进行Full GC。CMS收集器在进行垃圾收集时，和应用程序一起工作，所以，不能等到老年代几乎完全被填满了再进行收集，这样会影响并发的应用线程的空间使用，从而再次触发不必要的Full GC -XX:+CMSClassUnloadingEnabled：使用并发标记扫描(CMS)垃圾收集器时，启用类卸载。默认情况下启用此选项。 G1 常用参数 -XX:MaxGCPauseMillis： -XX:MaxGCPauseMillis=200设置垃圾收集暂停时间最大值指标，如果是收集器为G1，默认值为200 。这是一个软目标，Java虚拟机将尽最大努力实现它，通常配合G1或者Parallel Scavenge使用 -XX:G1HeapRegionSize：设置每个Region的大小，取值范围为1MB~32MB，且应为2的N次幂。 -XX:G1HeapRegionSize=16m -XX:G1NewSizePercent：新生代的最少比例，默认值5% -XX:G1MaxNewSizePercent：新生代的最大比例，默认值5% -XX:GCTimeRatioGC：时间建议比例，G1会根据这个值来调整堆空间 -XX:ConcGCThreads：并发垃圾收集器使用的线程数量 -XX:InitiatingHeapOccpancyPercent： -XX:InitiatingHeapOccpancyPercent=n指定整个堆的使用率达到多少时, 执行一次并发标记周期, 默认45， 过大会导致并发标记周期迟迟不能启动, 增加FullGC的可能, 过小会导致GC频繁, 会导致应用程序性能有所下降 性能调优参数列表设置用于Java堆的大页面尺寸-XX:LargePageSizeInBytes=4m GC后java堆中空闲量占的最大比例-XX:MaxHeapFreeRatio=70 GC后java堆中空闲量占的最小比例-XX:MinHeapFreeRatio=40 新生代对象生成时占用内存的默认值-XX:NewSize=2.125m 新生成对象能占用内存的最大值-XX:MaxNewSize=size 老生代对象能占用内存的最大值-XX:MaxPermSize=64m 新生代内存容量与老生代内存容量的比例-XX:NewRatio=2 遇到Ctrl-Break后打印并发锁的相关信息，与jstack -l功能相同-XX:-PrintConcurrentLocks","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"21调优案例分析与实战","slug":"jvm/01垃圾收集器与内存分配策略/21调优案例分析与实战","date":"2021-11-19T12:00:27.000Z","updated":"2022-03-23T09:03:57.821Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/21调优案例分析与实战/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/21%E8%B0%83%E4%BC%98%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E6%88%98/","excerpt":"","text":"看《深入理解Java虚拟机：JVM高级特性与最佳实践（第3版》第5章","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"20虚拟机性能监控、故障处理工具","slug":"jvm/01垃圾收集器与内存分配策略/20虚拟机性能监控、故障处理工具","date":"2021-11-19T12:00:26.000Z","updated":"2022-03-23T09:03:57.821Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/20虚拟机性能监控、故障处理工具/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/20%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E3%80%81%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7/","excerpt":"","text":"jps：虚拟机进程状况工具可以列出正在运行的虚拟机进程，并显示虚拟机执行主类(Main Class，main()函数所在的类)名称以及这些进程的本地虚拟机唯一ID(LVMID，Local Virtual Machine Identifier)。 jps命令格式:jps [options] [hostid] jstat：虚拟机统计信息监视工具jstat(JVM Statistics Monitoring Tool)是用于监视虚拟机各种运行状态信息的命令行工具。它可以显示本地或者远程虚拟机进程中的类加载、内存、垃圾收集、即时编译等运行时数据。 jstat命令格式为:jstat [-命令选项] [vmid] [间隔时间/毫秒] [查询次数] 类加载统计：1234# 2060是pidjstat -class 2060Loaded Bytes Unloaded Bytes Time 15756 17355.6 0 0.0 11.29 Loaded:加载class的数量 Bytes：所占用空间大小 Unloaded：未加载数量 Bytes:未加载占用空间 Time：时间 编译统计123jstat -compiler 2060Compiled Failed Invalid Time FailedType FailedMethod 9142 1 0 5.01 1 org/apache/felix/resolver/ResolverImpl mergeCandidatePackages Compiled：编译数量。 Failed：失败数量 Invalid：不可用数量 Time：时间 FailedType：失败类型 FailedMethod：失败的方法 垃圾回收统计123jstat -gc 2060 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT20480.0 20480.0 0.0 13115.3 163840.0 113334.2 614400.0 436045.7 63872.0 61266.5 0.0 0.0 149 3.440 8 0.295 3.735 S0C：第一个幸存区的大小 S1C：第二个幸存区的大小 S0U：第一个幸存区的使用大小 S1U：第二个幸存区的使用大小 EC：伊甸园区的大小 EU：伊甸园区的使用大小 OC：老年代大小 OU：老年代使用大小 MC：方法区大小 MU：方法区使用大小 CCSC:压缩类空间大小 CCSU:压缩类空间使用大小 YGC：年轻代垃圾回收次数 YGCT：年轻代垃圾回收消耗时间 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 堆内存统计123jstat -gccapacity 2060 NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC MCMN MCMX MC CCSMN CCSMX CCSC YGC FGC204800.0 204800.0 204800.0 20480.0 20480.0 163840.0 614400.0 614400.0 614400.0 614400.0 0.0 63872.0 63872.0 0.0 0.0 0.0 149 8 NGCMN：新生代最小容量 NGCMX：新生代最大容量 NGC：当前新生代容量 S0C：第一个幸存区大小 S1C：第二个幸存区的大小 EC：伊甸园区的大小 OGCMN：老年代最小容量 OGCMX：老年代最大容量 OGC：当前老年代大小 OC:当前老年代大小 MCMN:最小元数据容量 MCMX：最大元数据容量 MC：当前元数据空间大小 CCSMN：最小压缩类空间大小 CCSMX：最大压缩类空间大小 CCSC：当前压缩类空间大小 YGC：年轻代gc次数 FGC：老年代GC次数 新生代垃圾回收统计123C:\\Users\\Administrator&gt;jstat -gcnew 7172 S0C S1C S0U S1U TT MTT DSS EC EU YGC YGCT40960.0 40960.0 25443.1 0.0 15 15 20480.0 327680.0 222697.8 12 0.736 S0C：第一个幸存区大小 S1C：第二个幸存区的大小 S0U：第一个幸存区的使用大小 S1U：第二个幸存区的使用大小 TT:对象在新生代存活的次数 MTT:对象在新生代存活的最大次数 DSS:期望的幸存区大小 EC：伊甸园区的大小 EU：伊甸园区的使用大小 YGC：年轻代垃圾回收次数 YGCT：年轻代垃圾回收消耗时间 新生代内存统计123jstat -gcnewcapacity 7172 NGCMN NGCMX NGC S0CMX S0C S1CMX S1C ECMX EC YGC FGC 409600.0 409600.0 409600.0 40960.0 40960.0 40960.0 40960.0 327680.0 327680.0 12 0 NGCMN：新生代最小容量 NGCMX：新生代最大容量 NGC：当前新生代容量 S0CMX：最大幸存1区大小 S0C：当前幸存1区大小 S1CMX：最大幸存2区大小 S1C：当前幸存2区大小 ECMX：最大伊甸园区大小 EC：当前伊甸园区大小 YGC：年轻代垃圾回收次数 FGC：老年代回收次数 老年代垃圾回收统计123jstat -gcold 7172 MC MU CCSC CCSU OC OU YGC FGC FGCT GCT 33152.0 31720.8 0.0 0.0 638976.0 184173.0 12 0 0.000 0.736 MC：方法区大小 MU：方法区使用大小 CCSC:压缩类空间大小 CCSU:压缩类空间使用大小 OC：老年代大小 OU：老年代使用大小 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 老年代内存统计123jstat -gcoldcapacity 7172 OGCMN OGCMX OGC OC YGC FGC FGCT GCT 638976.0 638976.0 638976.0 638976.0 12 0 0.000 0.736 OGCMN：老年代最小容量 OGCMX：老年代最大容量 OGC：当前老年代大小 OC：老年代大小 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 元数据空间统计123jstat -gcmetacapacity 7172 MCMN MCMX MC CCSMN CCSMX CCSC YGC FGC FGCT GCT 0.0 33152.0 33152.0 0.0 0.0 0.0 12 0 0.000 0.736 MCMN:最小元数据容量 MCMX：最大元数据容量 MC：当前元数据空间大小 CCSMN：最小压缩类空间大小 CCSMX：最大压缩类空间大小 CCSC：当前压缩类空间大小 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 总结垃圾回收统计123jstat -gcutil 7172 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 62.12 0.00 81.36 28.82 95.68 - 12 0.736 0 0.000 0.736 S0：幸存1区当前使用比例 S1：幸存2区当前使用比例 E：伊甸园区使用比例 O：老年代使用比例 M：元数据区使用比例 CCS：压缩使用比例 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 JVM编译方法统计123C:\\Users\\Administrator&gt;jstat -printcompilation 7172Compiled Size Type Method 4608 16 1 org/eclipse/emf/common/util/SegmentSequence$SegmentSequencePool$SegmentsAccessUnit reset Compiled：最近编译方法的数量 Size：最近编译方法的字节码数量 Type：最近编译方法的编译类型。 Method：方法名标识。 jinfo：Java配置信息工具jinfo(Configuration Info for Java)的作用是实时查看和调整虚拟机各项参数。 使用jps命令的-v参数可以查看虚拟机启动时显式指定的参数列表，但如果想知道未被显式指定的参数的系统默认值，除了去找资料外，就只能使用jinfo的-flag选项进行查询了。 jinfo还可以使用-sysprops选项把虚拟机进程的System.getProperties()的内容打印出来 jinfo命令格式:jinfo [ option ] pid option： -flag -flags -sysprops 可以使用-flag[+|-]name或者-flag name=value在运行期修改一部分运行期可写的虚拟机参数值。 java -XX:+PrintFlagsFinal –version 查询所有-XX 的 注意:manageable 的参数，代表可以运行时修改。 演示例子如下: 1234&gt; jinfo -flag PrintGC 1364-XX:-PrintGC #PrintGC参数是关闭的&gt; jinfo -flag +PrintGC 1364 # 该命令能启用PrintGC jmap：Java内存映像工具jmap(Memory Map for Java)命令用于生成堆转储快照(一般称为heapdump或dump文件) jmap命令格式:jmap [ option ] vmid jhat：虚拟机堆转储快照分析工具JDK提供jhat(JVM Heap Analysis Tool)命令与jmap搭配使用，来分析jmap生成的堆转储快照。 jhat内置了一个微型的HTTP&#x2F;Web服务器，生成堆转储快照的分析结果后，可以在浏览器中查看。不过实事求是地说，在实际工作中，除非手上真的没有别的工具可用，否则多数人是不会直接使用jhat命令来分析堆转储快照文件的，主要原因有两个方面。一是一般不会在部署应用程序的服务器上直接分析堆转储快照，即使可以这样做，也会尽量将堆转储快照文件复制到其他机器[1]上进行分析，因为分析工作是一个耗时而且极为耗费硬件资源的过程，既然都要在其他机器上进行，就没有必要再受命令行工具的限制了。另外一个原因是jhat的分析功能相对来说比较简陋，很多工具都比它好，比如**VisualVM** jstack：Java堆栈跟踪工具jstack(Stack Trace for Java)命令用于生成虚拟机当前时刻的线程快照(一般称为threaddump或者javacore文件)。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的目的通常是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间挂起等，都是导致线程长时间停顿的常见原因。线程出现停顿时通过jstack来查看各个线程的调用堆栈， 就可以获知没有响应的线程到底在后台做些什么事情，或者等待着什么资源。 jstack命令格式:jstack [ option ] vmid","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"19内存分配与回收策略（记住）","slug":"jvm/01垃圾收集器与内存分配策略/19内存分配与回收策略（记住）","date":"2021-11-19T12:00:25.000Z","updated":"2022-03-23T09:03:57.810Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/19内存分配与回收策略（记住）/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/19%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E4%B8%8E%E5%9B%9E%E6%94%B6%E7%AD%96%E7%95%A5%EF%BC%88%E8%AE%B0%E4%BD%8F%EF%BC%89/","excerpt":"","text":"大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC。 HotSpot虚拟机提供了-XX:+PrintGCDetails这个收集器日志参数，告诉虚拟机在发生垃圾收集行为时打印内存回收日志，并且在进程退出的时候输出当前的内存各区域分配情况。在实际的问题排查中，收集器日志常会打印到文件后通过工具进行分析 大对象直接进入老年代大对象就是指需要大量连续内存空间的Java对象，最典型的大对象便是那种很长的字符串，或者元素数量很庞大的数组. 在Java虚拟机中要避免大对象的原因是，在分配空间时，它容易导致内存明明还有不少空间时就提前触发垃圾收集，以获取足够的连续空间才能安置好它们，而当复制对象时，大对象就意味着高额的内存复制开销。HotSpot虚拟机提供了-XX:PretenureSizeThreshold=10m参数，指定大于该设置值的对象直接在老年代分配，这样做的目的就是避免在Eden区及两个Survivor区之间来回复制，产生大量的内存复制操作。 长期存活的对象将进入老年代HotSpot虚拟机中多数收集器都采用了分代收集来管理堆内存，那内存回收时就必须能决策哪些存活对象应当放在新生代，哪些存活对象放在老年代中。为做到这点，虚拟机给每个对象定义了一个对象年龄(Age)计数器，存储在对象头中。对象通常在Eden区里诞生，如果经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，该对象会被移动到Survivor空间中，并且将其对象年龄设为1岁。对象在Survivor区中每熬过一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度(默认为15)，就会被晋升到老年代中。对象晋升老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold=n设置。 动态对象年龄判定为了能更好地适应不同程序的内存状况，HotSpot虚拟机并不是永远要求对象的年龄必须达到-XX:MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到-XX:MaxTenuringThreshold中要求的年龄。 空间分配担保在发生Minor GC之前，虚拟机必须先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那这一次Minor GC可以确保是安全的。如果不成立，则虚拟机会先查看-XX:HandlePromotionFailure参数的设置值是否允许担保失败（Handle Promotion Failure）；如果允许，那会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者-XX:HandlePromotionFailure设置不允许冒险，那这时就要改为进行一次Full GC。 通常情况下都还是会将-XX:HandlePromotionFailure开关打开，避免Full GC过于频繁。 -XX:HandlePromotionFailure&#x3D;true&#x2F;false","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"18Epsilon收集器","slug":"jvm/01垃圾收集器与内存分配策略/18Epsilon收集器","date":"2021-11-19T12:00:24.000Z","updated":"2022-03-23T09:03:57.810Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/18Epsilon收集器/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/18Epsilon%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"一旦可用堆内存用完, JVM就会退出用法：XX:+UnlockExperimentalVMOptions -XX:+UseEpsilonGC 如果应用只要运行数分钟甚至数秒， 只要Java虚拟机能正确分配内存，在堆耗尽之前就会退出，那显然运行负载极小、没有任何回收行为的Epsilon便是很恰当的选择。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"17ZGC收集器","slug":"jvm/01垃圾收集器与内存分配策略/17ZGC收集器","date":"2021-11-19T12:00:23.000Z","updated":"2022-03-23T09:03:57.809Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/17ZGC收集器/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/17ZGC%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"ZGC的内存布局说起。与Shenandoah和G1一样，ZGC也采用基于Region的堆内存布局，但与它们不同的是，ZGC的Region(在一些官方资料中将它称为Page或者ZPage，本章为行文一致继续称为Region)具有动态性——动态创建和销毁，以及动态的区域容量大小。在x64硬件平台下，ZGC的Region可以具有如图3-19所示的大、中、小三类容量: 小型Region(Small Region):容量固定为2MB，用于放置小于256KB的小对象。 中型Region(Medium Region):容量固定为32MB，用于放置大于等于256KB但小于4MB的对象。 大型Region(Large Region):容量不固定，可以动态变化，但必须为2MB的整数倍，用于放置4MB或以上的大对象。每个大型Region中只会存放一个大对象，这也预示着虽然名字叫作“大型Region”，但它的实际容量完全有可能小于中型Region，最小容量可低至4MB。大型Region在ZGC的实现中是不会被重分配(重分配是ZGC的一种处理动作，用于复制对象的收集器阶段，稍后会介绍到) 的，因为复制一个大对象的代价非常高昂。 ZGC使用了染色指针导致ZGC能够管理的内存不可以超过4TB(2的42次幂)，能支持32位平台，不能支持压缩指针(-XX:+/-UseCompressedOops)等诸多约束 详情看《深入理解Java虚拟机：JVM高级特性与最佳实践（第3版》","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"16Shenandoah收集器","slug":"jvm/01垃圾收集器与内存分配策略/16Shenandoah收集器","date":"2021-11-19T12:00:22.000Z","updated":"2022-03-23T09:03:57.807Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/16Shenandoah收集器/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/16Shenandoah%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"只有在OpenJDK才包含的收集器，而且JDK版本时12及以上。 Shenandoah反而更像是G1 的下一代继承者，虽然Shenandoah也是使用基于Region的堆内存布局，同样有着用于存放大对象的Humongous Region，默认的回收策略也同样是优先处理回收价值最大的Region…… 在管理堆内存方面，它与G1至少有三个明显的不同之处，最重要的当然是支持并发的整理算法，G1的回收阶段是可以多线程并行的，但却不能与用户线程并发，而Shenandoah允许和用户线程并发 Shenandoah是默认不使用分代收集（以后可能会加上） Shenandoah摒弃了在G1中耗费大量内存和计算资源去维护的记忆集，改用名为“连接矩阵”(Connection Matrix)的全局数据结构来记录跨Region的引用关系，降低了处理跨代指针时的记忆集维护消耗，也降低了伪共享问题(见3.4.4节)的发生概率。连接矩阵可以简单理解为一张二维表格，如果Region N有对象指向Region M，就在表格的N行M列中打上一个标记，如图如果Region 5中的对象Baz 引用了Region 3的Foo，Foo又引用了Region 1的Bar，那连接矩阵中的5行3列、3行1列就应该被打上标记。在回收时通过这张表格就可以得出哪些Region之间产生了跨代引用。 Shenandoah收集器的工作过程大致可以划分为以下九个阶段： 初始标记(Initial Marking):与G1一样，首先标记与GC Roots直接关联的对象，这个阶段仍是“Stop The World”的，但停顿时间与堆大小无关，只与GC Roots的数量相关。 并发标记(Concurrent Marking):与G1一样，遍历对象图，标记出全部可达的对象，这个阶段是与用户线程一起并发的，时间长短取决于堆中存活对象的数量以及对象图的结构复杂程度。 最终标记(Final Marking):与G1一样，处理剩余的SATB（原始快照）扫描，并在这个阶段统计出回收价值最高的Region，将这些Region构成一组回收集(Collection Set)。最终标记阶段也会有一小段短暂的停顿。 并发清理(Concurrent Cleanup):这个阶段用于清理那些整个区域内连一个存活对象都没有找到的Region(这类Region被称为Immediate Garbage Region)。 并发回收(Concurrent Evacuation):并发回收阶段是Shenandoah与之前HotSpot中其他收集器的核心差异。在这个阶段，Shenandoah要把回收集里面的存活对象先复制一份到其他未被使用的Region之中。复制时，用户线程仍然可能不停对被移动的对象进行读写访问，移动对象是一次性的行为，但移动之后整个内存中所有指向该对象的引用都还是旧对象的地址，这是很难一瞬间全部改变过来的。为了解决这个问题，Shenandoah将会通过读屏障和被称为“Brooks Pointers”的转发指针来解决。并发回收阶段运行的时间长短取决于回收集的大小。 初始引用更新(Initial Update Reference):并发回收阶段复制对象结束后，还需要把堆中所有指向旧对象的引用修正到复制后的新地址，这个操作称为引用更新。引用更新的初始化阶段实际上并未做什么具体的处理，设立这个阶段只是为了建立一个线程集合点，确保所有并发回收阶段中进行的收集器线程都已完成分配给它们的对象移动任务而已。初始引用更新时间很短，会产生一个非常短暂的停顿。 并发引用更新(Concurrent Update Reference):真正开始进行引用更新操作，这个阶段是与用户线程一起并发的，时间长短取决于内存中涉及的引用数量的多少。并发引用更新与并发标记不同，它不再需要沿着对象图来搜索，只需要按照内存物理地址的顺序，线性地搜索出引用类型，把旧值改为新值即可。 最终引用更新(Final Update Reference):解决了堆中的引用更新后，还要修正存在于GC Roots 中的引用。这个阶段是Shenandoah的最后一次停顿，停顿时间只与GC Roots的数量相关。 并发清理(Concurrent Cleanup):经过并发回收和引用更新之后，整个回收集中所有的Region已再无存活对象，这些Region都变成Immediate Garbage Regions了，最后再调用一次并发清理过程来回收这些Region的内存空间，供以后新对象分配使用。 黄色的区域代表的是被选入回收集的Region,绿色部分就代表还存活的对象,蓝色就是用户线程可以用来分配对象的内存Region Shenandoah中的转发指针在并发回收时使用。在复制对象上添加了一个Brooks Pointers指针，指向复制后的对象。当被的线程通过引用反问到旧对象时，通过这个指针把操作寻找到真实需要操作的对象。Brooks Pointers指针会有并发问题，使用的是cas操作解决","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"15Garbage First收集器","slug":"jvm/01垃圾收集器与内存分配策略/15Garbage First收集器","date":"2021-11-19T12:00:21.000Z","updated":"2022-03-23T09:03:57.803Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/15Garbage First收集器/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/15Garbage%20First%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"在G1收集器出现之前的所有其他收集器，包括CMS在内，垃圾收集的目标范围要么是整个新生代(Minor GC)，要么就是整个老年代(Major GC)，再要么就是整个Java堆(Full GC)。而G1跳出了这个樊笼，它可以面向堆内存任何部分来组成回收集(Collection Set，一般简称CSet)进行回收，衡量标准不再是它属于哪个分代，而是哪块内存中存放的垃圾数量最多，回收收益最大，这就是G1收集器的Mixed GC模式。 G1开创的基于**Region的堆内存布局是它能够实现这个目标的关键。虽然G1也仍是遵循分代收集理论设计的，但其堆内存的布局与其他收集器有非常明显的差异：G1不再坚持固定大小以及固定数量的分代区域划分，而是把连续的Java堆划分为多个大小相等的独立区域(Region)，每一个Region都可以根据需要，扮演新生代的Eden空间、Survivor空间，或者老年代空间**。收集器能够对扮演不同角色的Region采用不同的策略去处理，这样无论是新创建的对象还是已经存活了一段时间、熬过多次收集的旧对象都能获取很好的收集效果。 Region中还有一类特殊的Humongous区域，专门用来存储大对象。G1认为只要大小超过了一个Region容量一半的对象即可判定为大对象。每个Region的大小可以通过参数-XX:G1HeapRegionSize设定，取值范围为1MB~32MB，且应为2的N次幂。而对于那些超过了整个Region容量的超级大对象， 将会被存放在N个连续的**Humongous Region**之中，G1的大多数行为都把Humongous Region作为老年代的一部分来进行看待。 虽然G1仍然保留新生代和老年代的概念，但新生代和老年代不再是固定的了，它们都是一系列区域(不需要连续)的动态集合。G1收集器之所以能建立可预测的停顿时间模型，是因为它将Region作为单次回收的最小单元，即每次收集到的内存空间都是Region大小的整数倍，这样可以有计划地避免在整个Java堆中进行全区域的垃圾收集。更具体的处理思路是让G1收集器去跟踪各个Region里面的垃圾堆积的“价值”大小，价值即回收所获得的空间大小以及回收所需时间的经验值，然后在后台维护一个优先级列表，每次根据用户设定允许的收集停顿时间(使用参数-XX:MaxGCPauseMillis指定，默认值是200毫秒)，优先处理回收价值收益最大的那些Region，这也就是“Garbage First”名字的由来。这种使用Region划分内存空间，以及具有优先级的区域回收方式，保证了G1收集器在有限的时间内获取尽可能高的收集效率。 将Java堆分成多个独立Region后，Region里面存在的跨Region引用对象如何解决?解决的思路我们已经知道：使用记忆集避免全堆作为GC Roots扫描，但在G1收集器上记忆集的应用其实要复杂很多，它的每个Region都维护有自己的记忆集，这些记忆集会记录下别的Region 指向自己的指针，并标记这些指针分别在哪些卡页的范围之内。G1的记忆集在存储结构的本质上是一种哈希表，Key是别的Region的起始地址，Value是一个集合，里面存储的元素是卡表的索引号。这种“双向”的卡表结构(卡表是“我指向谁”，这种结构还记录了“谁指向我”)比原来的卡表实现起来更复杂，同时由于Region数量比传统收集器的分代数量明显要多得多，因此G1收集器要比其他的传统垃圾收集器有着更高的内存占用负担。根据经验，G1至少要耗费大约相当于Java堆容量10%至20%的额外内存来维持收集器工作。 G1为每一个Region设计了两个名为TAMS(Top at Mark Start)的指针，把Region中的一部分空间划分出来用于并发回收过程中的新对象分配，并发回收时新分配的对象地址都必须要在这两个指针位置以上。G1收集器默认在这个地址以上的对象是被隐式标记过的，即默认它们是存活的，不纳入回收范围。与CMS中的“Concurrent Mode Failure”失败会导致Full GC类似，如果内存回收的速度赶不上内存分配的速度， G1收集器也要被迫冻结用户线程执行，导致Full GC而产生长时间“Stop The World”。 G1收集器的运作过程大致可划分为以下四个步骤 初始标记(Initial Marking)：仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS 指针的值，让下一阶段用户线程并发运行时，能正确地在可用的Region中分配新对象。这个阶段需要停顿线程，但耗时很短，而且是借用进行Minor GC的时候同步完成的，所以G1收集器在这个阶段实际并没有额外的停顿。 并发标记(Concurrent Marking)：从GC Root开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象，这阶段耗时较长，但可与用户程序并发执行。当对象图扫描完成以后，还要重新处理SATB(原始快照)记录下的在并发时有引用变动的对象。 最终标记(Final Marking)：对用户线程做另一个短暂的暂停，用于处理并发阶段结束后仍遗留下来的最后那少量的SATB(原始快照)记录。 筛选回收(Live Data Counting and Evacuation)：负责更新Region的统计数据，对各个Region的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个Region 构成回收集，然后把决定回收的那一部分Region的存活对象复制到空的Region中，再清理掉整个旧Region的全部空间。这里的操作涉及存活对象的移动，是必须暂停用户线程，由多条收集器线程并行完成的。 G1收集器除了并发标记外，其余阶段也是要完全暂停用户线程的， 换言之，它并非纯粹地追求低延迟，官方给它设定的目标是在延迟可控的情况下获得尽可能高的吞吐量 使用-XX:MaxGCPauseMillis设置（收集停顿时间）期望值时，不能过小，因为这会导致频繁的GC；也不能过大，这会导致由于收集太多的内存，容易导致Full GC。所以通常把期望停顿时间设置为一两百毫秒或者两三百毫秒会是比较合理的。 JVM参数：-XX:+UseG1GC，默认组合G1+G1，jdk8默认收集器","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"14CMS收集器","slug":"jvm/01垃圾收集器与内存分配策略/14CMS收集器","date":"2021-11-19T12:00:20.000Z","updated":"2022-03-23T09:03:57.800Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/14CMS收集器/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/14CMS%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。JDK 9发布之日，G1宣告取代Parallel Scavenge加Parallel Old组合，成为服务端模式下的默认垃圾收集器，而CMS则沦落至被声明为不推荐使用（Deprecate）的收集器。 CMS收集器是基于标记-清除算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为四个步骤， 初始标记(CMS initial mark) 并发标记(CMS concurrent mark) 重新标记(CMS remark) 并发清除(CMS concurrent sweep) 初始标记只是标记一下GC Roots能直接关联到的对象。 并发标记阶段就是从GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行。 重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录（增量更新(Incremental Update)）。 并发清除阶段，清理删除掉标记阶段判断的已经死亡的对象，由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的。 存在以下缺点： CPU敏感：CMS 对处理器资源敏感，毕竟采用了并发的收集、当处理核心数不足 4 个时，CMS 对用户的影响较大 浮动垃圾：由于 CMS 并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS 无法 在当次收集中处理掉它们，只好留待下一次 GC 时再清理掉。这一部分垃圾就称为“浮动垃圾”。 由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。在 1.6 的版本中老年代空间使用率阈值(92%，可以通过-XX:CMSInitiatingOccupancyFraction)。如果预留的内存不够存放浮动垃圾，就会出现“并发失败”（Concurrent Mode Failure），这时候虚拟机将不得不启动后备预案：冻结用户线程的执行，临时启用Serial Old收集器来重新进行老年代的垃圾收集，但这样停顿时间就很长了。 会产生空间碎片：标记 - 清除算法会导致产生不连续的空间碎片，所以会有内存碎片，当碎片较多时，给大对象的分配带来很大的麻烦，为了解决这个问题，CMS 提供一个 参数：-XX:+UseCMSCompactAtFullCollection&#96;，一般是开启的，如果分配不了大对象，就进行内存碎片的整理过程。 CMS 总结 CMS 问题比较多，所以现在没有一个版本默认是 CMS，只能手工指定。但是它毕竟是第一个并发垃圾回收器，对于了解并发垃圾回收具有一定意义，所以我们必须了解。 为什么 CMS 采用标记-清除，在实现并发的垃圾回收时，如果采用标记整理算法，那么还涉及到对象的移动(对象的移动必定涉及到引用的变化，这个需 要暂停业务线程来处理栈信息，这样使得并发收集的暂停时间更长)，所以使用简单的标记-清除算法才可以降低 CMS 的 STW 的时间。 JVM参数：-XX:+/-UseConcMarkSweepGC，开启CMS，默认组合ParNew+CMS+Serial Old（当CMS收集器发生失败时的后备预案）","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"13Parallel Old收集器","slug":"jvm/01垃圾收集器与内存分配策略/13Parallel Old收集器","date":"2021-11-19T12:00:19.000Z","updated":"2022-03-23T09:03:57.798Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/13Parallel Old收集器/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/13Parallel%20Old%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"Parallel Old是Parallel Scavenge收集器的老年代版本，支持多线程并发收集，基于标记-整理算法实现。这个收集器是直到JDK 6时才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于相当尴尬的状态，原因是如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old(PS MarkSweep)收集器以外别无选择，其他表现良好的老年代收集器，如CMS无法与它配合工作。由于老年代Serial Old收集器在服务端应用性能上的“拖累”，使用Parallel Scavenge收集器也未必能在整体上获得吞吐量最大化的效果。同样，由于单线程的老年代收集中无法充分利用服务器多处理器的并行处理能力，在老年代内存空间很大而且硬件规格比较高级的运行环境中，这种组合的总吞吐量甚至不一定比ParNew加CMS的组合来得优秀。 JVM参数：-XX:+/-UseParallelOldGC，默认组合Parallel Scavenge+Parallel Old","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"12Serial Old收集器","slug":"jvm/01垃圾收集器与内存分配策略/12Serial Old收集器","date":"2021-11-19T12:00:18.000Z","updated":"2022-03-23T09:03:57.797Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/12Serial Old收集器/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/12Serial%20Old%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。这个收集器的主要意义也是供客户端模式下的HotSpot虚拟机使用。如果在服务端模式下，它也可能有两种用途:一种是在JDK 5以及之前的版本中与Parallel Scavenge收集器搭配使用，另外一种就是作为CMS 收集器发生失败时的后备预案。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"11Parallel Scavenge收集器","slug":"jvm/01垃圾收集器与内存分配策略/11Parallel Scavenge收集器","date":"2021-11-19T12:00:17.000Z","updated":"2022-03-23T09:03:57.796Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/11Parallel Scavenge收集器/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/11Parallel%20Scavenge%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"Parallel Scavenge收集器也是一款新生代收集器,它同样是基于标记-复制算法实现的收集器,也是能够并行收集的多线程收集器。 Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量(Throughput)。所谓吞吐量就是处理器用于运行用户代码的时间与处理器总消耗时间的比值， 即: Parallel Scavenge收集器提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间的-XX:MaxGCPauseMillis参数以及直接设置吞吐量大小的-XX:GCTimeRatio参数。 -XX:MaxGCPauseMillis参数允许的值是一个大于0的毫秒数，收集器将尽力保证内存回收花费的时间不超过用户设定值。不过大家不要异想天开地认为如果把这个参数的值设置得更小一点就能使得系统的垃圾收集速度变得更快，垃圾收集停顿时间缩短是以牺牲吞吐量和新生代空间为代价换取的: 系统把新生代调得小一些，收集300MB新生代肯定比收集500MB快，但这也直接导致垃圾收集发生得更频繁，原来10秒收集一次、每次停顿100毫秒，现在变成5秒收集一次、每次停顿70毫秒。停顿时间的确在下降，但吞吐量也降下来了。 -XX:GCTimeRatio参数的值则应当是一个大于0小于100的整数，也就是垃圾收集时间占总时间的比率，相当于吞吐量的倒数。譬如把此参数设置为19，那允许的最大垃圾收集时间就占总时间的5% (即1&#x2F;(1+19))，默认值为99，即允许最大1%(即1&#x2F;(1+99))的垃圾收集时间。 -XX:+UseAdaptiveSizePolicy当这个参数被激活之后，就不需要人工指定新生代的大小(-Xmn)、Eden与Survivor区比例(-XX:SurvivorRatio)、晋升老年代对象大小(-XX:PretenureSizeThreshold)等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。这种调节方式称为垃圾收集的自适应的调节策略(GC Ergonomics)。 手工优化存在困难的话，使用Parallel Scavenge收集器配合自适应调节策略，把内存管理的调优任务交给虚拟机去完成也许是一个很不错的选择。只需要把基本的内存数据设置好(如-Xmx设置最大堆)，然后使用-XX:MaxGCPauseMillis参数(更关注最大停顿时间)或- XX:GCTimeRatio(更关注吞吐量)参数给虚拟机设立一个优化目标，那具体细节参数的调节工作就由虚拟机完成了。自适应调节策略也是Parallel Scavenge收集器区别于ParNew收集器的一个重要特性。 JVM参数：-XX:+/-UseParallelGC，开启后默认组合Parallel Scavenge+Serial Old。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"10ParNew收集器","slug":"jvm/01垃圾收集器与内存分配策略/10ParNew收集器","date":"2021-11-19T12:00:16.000Z","updated":"2022-03-23T09:03:57.795Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/10ParNew收集器/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/10ParNew%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"ParNew收集器实质上是Serial收集器的多线程并行版本。该收集器是在JDK 7之前的遗留系统中首选的新生代收集器，其中有一个与功能、性能无关但其实很重要的原因是:除了Serial收集器外，目前只有它能与CMS 收集器配合工作。 ParNew收集器是激活CMS后(使用-XX:+UseConcMarkSweepGC选项)的默认新生代收集器,也可以使用-XX:+/-UseParNewGC选项来强制指定或者禁用它。 自JDK 9开始，直接取消了-XX：+UseParNewGC参数，这意味着ParNew和CMS从此只能互相搭配使用，再也没有其他收集器能够和它们配合了，ParNew合并入CMS，成为它专门处理新生代的组成部分。 JVM参数：-XX:+/-UseConcMarkSweepGC，开启CMS，默认组合ParNew+CMS+Serial Old（当CMS收集器发生失败时的后备预案）","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"09Serial收集器","slug":"jvm/01垃圾收集器与内存分配策略/09Serial收集器","date":"2021-11-19T12:00:15.000Z","updated":"2022-03-23T09:03:57.793Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/09Serial收集器/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/09Serial%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"这个收集器是一个单线程工作的收集器，但它的“单线程”的意义并不仅仅是说明它只会使用一个处理器或一条收集线程去完成垃圾收集工作，更重要的是强调在它进行垃圾收集时，必须暂停其他所有工作线程，直到它收集结束。“Stop The World”这个词语也许听起来很酷，但这项工作是由虚拟机在后台自动发起和自动完成的，在用户不可知、不可控的情况下把用户的正常工作的线程全部停掉。它在HotSpot虚拟机运行在客户端模式下的默认新生代收集器，有着优于其他收集器的地方，那就是简单而高效(与其他收集器的单线程相比)，对于内存资源受限的环境，它是所有收集器里额外内存消耗(Memory Footprint)最小的;对于单核处理器或处理器核心数较少的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。在用户桌面的应用场景以及近年来流行的部分微服务应用中，分配给虚拟机管理的内存一般来说并不会特别大，收集几十兆甚至一两百兆的新生代(仅仅是指新生代使用的内存，桌面应用甚少超过这个容量)，垃圾收集的停顿时间完全可以控制在十几、几十毫秒，最多一百多毫秒以内，只要不是频繁发生收集，这点停顿时间对许多用户来说是完全可以接受的。所以，Serial收集器对于运行在客户端模式下的虚拟机来说是一个很好的选择。 JVM-XX:+/-UseSerialGC，默认组合Serial+Serial Old","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"08经典垃圾收集器","slug":"jvm/01垃圾收集器与内存分配策略/08经典垃圾收集器","date":"2021-11-19T12:00:14.000Z","updated":"2022-03-23T09:03:57.791Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/08经典垃圾收集器/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/08%E7%BB%8F%E5%85%B8%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"图3-6展示了七种作用于不同分代的收集器,如果两个收集器之间存在连线,就说明它们可以搭配使用,图中收集器所处的区域,则表示它是属于新生代收集器抑或是老年代收集器。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"07垃圾收集算法","slug":"jvm/01垃圾收集器与内存分配策略/07垃圾收集算法","date":"2021-11-19T12:00:13.000Z","updated":"2022-03-23T09:03:57.789Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/07垃圾收集算法/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/07%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95/","excerpt":"","text":"注意： 新生代收集(Minor GC&#x2F;Young GC):指目标只是新生代的垃圾收集。 老年代收集(Major GC&#x2F;Old GC):指目标只是老年代的垃圾收集。目前只有CMS收集器会有单独收集老年代的行为 混合收集(Mixed GC):指目标是收集整个新生代以及部分老年代的垃圾收集。目前只有G1收集器会有这种行为。 整堆收集(Full GC):收集整个Java堆和方法区的垃圾收集 分代收集理论当前商业虚拟机的垃圾收集器，大多数都遵循了“分代收集”(Generational Collection)[1]的理论进行设计，分代收集名为理论，实质是一套符合大多数程序运行实际情况的经验法则，它建立在三个分代假说之上: 弱分代假说(Weak Generational Hypothesis):绝大多数对象都是朝生夕灭的。 强分代假说(Strong Generational Hypothesis):熬过越多次垃圾收集过程的对象就越难以消亡。 这两个分代假说共同奠定了多款常用的垃圾收集器的一致的设计原则:收集器应该将Java堆划分出不同的区域，然后将回收对象依据其年龄(年龄即对象熬过垃圾收集过程的次数)分配到不同的区域之中存储。 在Java堆划分出不同的区域之后，垃圾收集器才可以每次只回收其中某一个或者某些部分的区域——因而才有了“Minor GC”“Major GC”“Full GC”这样的回收类型的划分；也才能够针对不同的区域安排与里面存储对象存亡特征相匹配的垃圾收集算法——因而发展出了“标记-复制算法”“标记-清除算法”“标记-整理算法”等针对性的垃圾收集算法。 把分代收集理论具体放到现在的商用Java虚拟机里，设计者一般至少会把Java堆划分为新生代(Young Generation)和老年代(Old Generation)两个区域。顾名思义，在新生代中，每次垃圾收集时都发现有大批对象死去，而每次回收后存活的少量对象，将会逐步晋升到老年代中存放。 跨代引用假说(Intergenerational Reference Hypothesis):跨代引用相对于同代引用来说仅占极少数。 存在互相引用关系的两个对象，是应该倾向于同时生存或者同时消亡的。举个例子，如果某个新生代对象存在跨代引用，由于老年代对象难以消亡，该引用会使得新生代对象在收集时同样得以存活，进而在年龄增长之后晋升到老年代中，这时跨代引用也随即被消除了。 依据这条假说，我们就不应再为了少量的跨代引用去扫描整个老年代，也不必浪费空间专门记录每一个对象是否存在及存在哪些跨代引用，只需在新生代上建立一个全局的数据结构(该结构被称为“记忆集”，Remembered Set)，这个结构把老年代划分成若干小块，标识出老年代的哪一块内存会存在跨代引用。此后当发生Minor GC时，只有包含了跨代引用的小块内存里的对象才会被加入到GC Roots进行扫描。虽然这种方法需要在对象改变引用关系(如将自己或者某个属性赋值)时维护记录数据的正确性，会增加一些运行时的开销，但比起收集时扫描整个老年代来说仍然是划算的。 标记-清除算法（Mark-Sweep）算法分为“标记”和“清除”两个阶段:首先标记出所有需要回收的对象，在标记完成后，统一回收掉所有被标记的对象。 它的主要缺点有两个: 第一个是执行效率不稳定,如果Java堆中包含大量对象,而且其中大部分是需要被回收的,这时必须进行大量标记和清除的动作,导致标记和清除两个过程的执行效率都随对象数量增长而降低 第二个是内存空间的碎片化问题,标记、清除之后会产生大量不连续的内存碎片,空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 标记-复制算法半区复制”(Semispace Copying)的垃圾收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。缺点很明显，内存浪费。 优化后的半区复制算法。HotSpot虚拟机的Serial、ParNew等新生代收集器均采用了这种策略来设计新生代的内存布局。把新生代分为一块较大的Eden空间和两块较小的Survivor空间，每次分配内存只使用Eden和其中一块Survivor。发生垃圾搜集时，将Eden和Survivor中仍然存活的对象一次性复制到另外一块Survivor空间上，然后直接清理掉Eden和已用过的那块Survivor空间。 HotSpot虚拟机默认Eden和Survivor的大小比例是8∶1。当Survivor空间不足以容纳一次Minor GC之后存活的对象时，就需要依赖其他内存区域(实际上大多就是老年代)进行分配担保(Handle Promotion)。如果另外一块Survivor空间没有足够空间存放上一次新生代收集下来的存活对象，这些对象便将通过分配担保机制直接进入老年代，这对虚拟机来说就是安全的。 标记-复制算法在对象存活率较高时就要进行较多的复制操作，效率将会降低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。 标记-整理算法标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存 如果移动存活对象，尤其是在老年代这种每次回收都有大量对象存活区域，移动存活对象并更新所有引用这些对象的地方将会是一种极为负重的操作，而且这种对象移动操作必须全程暂停用户应用程序才能进行，这就更加让使用者不得不小心翼翼地权衡其弊端了，像这样的停顿被最初的虚拟机设计者形象地描述为“Stop The World”","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"06jdk的引用","slug":"jvm/01垃圾收集器与内存分配策略/06jdk的引用","date":"2021-11-19T12:00:12.000Z","updated":"2022-03-23T09:03:57.787Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/06jdk的引用/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/06jdk%E7%9A%84%E5%BC%95%E7%94%A8/","excerpt":"","text":"在JDK 1.2版之后，Java对引用的概念进行了扩充，将引用分为强引用(Strongly Re-ference)、软引用(Soft Reference)、弱引用(Weak Reference)和虚引用(Phantom Reference)4种，这4种引用强度依次逐渐减弱。 强引用是最传统的“引用”的定义，是指在程序代码之中普遍存在的引用赋值，即类似“Object obj&#x3D;new Object()”这种引用关系。无论任何情况下，只要强引用关系还存在，垃圾收集器就永远不会回收掉被引用的对象。 软引用是用来描述一些还有用，但非必须的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存， 才会抛出内存溢出异常。在JDK 1.2版之后提供了SoftReference类来实现软引用。 弱引用也是用来描述那些非必须对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生为止。当垃圾收集器开始工作，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK 1.2版之后提供了WeakReference类来实现弱引用。 虚引用也称为“幽灵引用”或者“幻影引用”，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的只是为了能在这个对象被收集器回收时收到一个系统通知。在JDK 1.2版之后提供了PhantomReference类来实现虚引用。 上面的实质就是 1this.referent = referent; 在到了触发为止就 1this.referent = null; 如果有一个强引用指向了对象， 这个对象还是不会回收的","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"05对象已死？","slug":"jvm/01垃圾收集器与内存分配策略/05对象已死？","date":"2021-11-19T12:00:11.000Z","updated":"2022-03-23T09:03:57.786Z","comments":true,"path":"blog/jvm/01垃圾收集器与内存分配策略/05对象已死？/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/05%E5%AF%B9%E8%B1%A1%E5%B7%B2%E6%AD%BB%EF%BC%9F/","excerpt":"","text":"引用计数算法在对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加一;当引用失效时，计数器值就减一；任何时刻计数器为零的对象就是不可能再被使用的。 引用计数算法(Reference Counting)虽然占用了一些额外的内存空间来进行计数,但它的原理简单,判定效率也很高。但很难解决对象之间相互循环引用的问题。 可达性分析算法思路就是通过一系列称为“GC Roots”的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链”(Reference Chain)，如果从GC Roots到这个对象不可达时，则证明此对象是不可能再被使用的。 在Java技术体系里面，固定可作为GC Roots的对象包括以下几种： 在虚拟机栈（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。 在方法区中类静态属性引用的对象，譬如Java类的引用类型静态变量。 在方法区中常量引用的对象，譬如字符串常量池（String Table）里的引用。 在本地方法栈中JNI（即通常所说的Native方法）引用的对象。 Java虚拟机内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象（比如NullPointExcepiton、OutOfMemoryError）等，还有系统类加载器。 所有被同步锁（synchronized关键字）持有的对象。 反映Java虚拟机内部情况的JMXBean、JVMTI中注册的回调、本地代码缓存等 生存还是死亡?即使在可达性分析算法中判定为不可达的对象，也不是“非死不可”的，这时候它们暂时还处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程:如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记，随后进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法（任何一个对象的finalize()方法都只会被系统自动调用一次,如果对象面临下一次回收,它的finalize()方法不会被再次执行）。假如对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，那么虚拟机将这两种情况都视为“没有必要执行”。 如果这个对象被判定为确有必要执行finalize()方法，那么该对象将会被放置在一个名为F-Queue的队列之中，并在稍后由一条由虚拟机自动建立的、低调度优先级的Finalizer线程去执行它们的finalize() 方法。这里所说的“执行”是指虚拟机会触发这个方法开始运行，但并不承诺一定会等待它运行结束。这样做的原因是，如果某个对象的finalize()方法执行缓慢，或者更极端地发生了死循环，将很可能导致F-Queue队列中的其他对象永久处于等待，甚至导致整个内存回收子系统的崩溃。finalize()方法是对象逃脱死亡命运的最后一次机会，稍后收集器将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize()中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可，譬如把自己(this关键字)赋值给某个类变量或者对象的成员变量，那在第二次标记时它将被移出“即将回收”的集合;如果对象这时候还没有逃脱，那基本上它就真的要被回收了。 上面关于对象死亡时finalize()方法的描述可能带点悲情的艺术加工，但是并不推荐使用这个方法，它的运行代价高昂,不确定性大,无法保证各个对象的调用顺序,如今已被官方明确声明为不推荐使用的语法。finalize()能做的所有工作,使用try-finally或者其他方式都可以做得更好、更及时。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"自动装箱、拆箱与遍历循环","slug":"jvm/自动装箱、拆箱与遍历循环","date":"2021-11-19T12:00:10.000Z","updated":"2022-03-23T09:03:57.785Z","comments":true,"path":"blog/jvm/自动装箱、拆箱与遍历循环/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/%E8%87%AA%E5%8A%A8%E8%A3%85%E7%AE%B1%E3%80%81%E6%8B%86%E7%AE%B1%E4%B8%8E%E9%81%8D%E5%8E%86%E5%BE%AA%E7%8E%AF/","excerpt":"","text":"12345678public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4); int sum = 0; for (int i : list) &#123; sum += i; &#125; System.out.println(sum);&#125; 上边是java的语法糖，看经过语法糖经过编译后是怎样的 12345678910111213public static void main(String[] args) &#123; List list = Arrays.asList( new Integer[] &#123; Integer.valueOf(1), Integer.valueOf(2), Integer.valueOf(3), Integer.valueOf(4) &#125;); int sum = 0; for (Iterator localIterator = list.iterator(); localIterator.hasNext(); ) &#123; int i = ((Integer)localIterator.next()).intValue(); sum += i; &#125; System.out.println(sum); &#125; 自动装箱、拆箱在编译之后被转化成了对应的包装和还原方法，如本例中的Integer.valueOf()与Integer.intValue()方法，而遍历循环则是把代码还原成了迭代器的实现，这也是为何遍历循环需要被遍历的类实现Iterable接口的原因。变长参数,它在调用的时候变成了一个数组类型的参数。 看下面的代码 1234567891011121314151617181920212223//自动装箱的陷阱public static void main(String[] args) &#123; Integer a = 1; Integer b = 2; Integer c = 3; Integer d = 3; Integer e = 321; Integer f = 321; Long g = 3L; System.out.println(c == d); System.out.println(e == f); System.out.println(c == (a + b)); System.out.println(c.equals(a + b)); System.out.println(g == (a + b)); System.out.println(g.equals(a + b));&#125;结果：truefalsetruetruetruefalse System.out.println(c == d);这打印为true就是c和d进过自动装箱（Integer.valueOf()）后会变成变量，但为了性能的考虑，java在Integer的源码中都会使用了缓存。看源码： 所以，在默认的情况下，java会缓存[-127,127]这个范围内的对象。所以 c &#x3D;&#x3D; d才为true System.out.println(c == (a + b));这里先试计算(a + b)，这时会进行拆箱操作，变为基础数据类型，接着进行 Integer对象 &#x3D;&#x3D; 基础数据类型，这时基础数据类型会进行装箱，接着缓存命中，所以返回了true System.out.println(g == (a + b));一开始也是计算(a + b)，结果为int类型，而之所以这代码返回true，可以从编译后的字节码可以得出答案： 它进行了类型的转换了，所以变为long后，后面的操作和3一样。 System.out.println(g.equals(a + b));这里返回false直接看Long::equals就可以知道了：","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"篇与面试官和蔼交流的深入了解 JVM（JDK8","slug":"jvm/篇与面试官和蔼交流的深入了解 JVM（JDK8","date":"2021-11-19T12:00:09.000Z","updated":"2022-03-23T09:03:57.783Z","comments":true,"path":"blog/jvm/篇与面试官和蔼交流的深入了解 JVM（JDK8/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/%E7%AF%87%E4%B8%8E%E9%9D%A2%E8%AF%95%E5%AE%98%E5%92%8C%E8%94%BC%E4%BA%A4%E6%B5%81%E7%9A%84%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3%20JVM%EF%BC%88JDK8/","excerpt":"","text":"本文由 简悦 SimpRead 转码， 原文地址 mp.weixin.qq.com 文章目录 15.1、100%CPU 的排查 15.2、死锁的检查 13.1、CMS（“标记 - 清除” 算法, -XX:+UseConcMarkSweepGC(old） 13.2、G1 13.3、ZGC 13.1.6.1、增量更新 (Incremental Update)+ 写屏障 13.1.6.2、原始快照（Snapshot At The Beginning，SATB）+ 写屏障 13.1.6.3、并发标记时对漏标的处理方案 13.1.1、运作过程 (5 大步骤) 13.1.2、三色标记法 13.1.3、concurrent model failure（浮动垃圾） 13.1.4、background&#x2F;foreground collector 13.1.5、为什么 G1 用 SATB？CMS 用增量更新？ 13.1.6、漏标 - 读写屏障 (解决方案) 13.1.7、promotion failed 13.1.8、过早提升和提升失败 13.1.9、早提升的原因 13.1.10、提升失败原因 13.2.1、运作流程 13.2.2、Remembered Set（记录集）&#x2F;Card Table（卡表） 13.2.3、Collect Set 13.2.4、young gc 的完整流程 13.2.5、Mixed GC 的完整流程 13.2.6、Full GC 13.2.7、Marking bitmaps&#x2F;TAMS 13.2.8、Pause Prediction Model 13.2.9、G1 收集器参数设置 13.2.10、G1 垃圾收集器优化建议（ -XX:MaxGCPauseMills&#x3D;50ms） 13.2.11、什么场景适合使用 G1 13.3.1、主要目标 13.3.2、color poin（颜色指针） 13.3.3、运作过程 13.3.4、存在的问题，怎么解决 13.3.5、安全点与安全区域 13.3.6、ZGC 参数 13.3.7、ZGC 触发时机 11.1、分代收集理论 11.2、标记 - 复制算法 11.3、标记 - 清除算法 11.4、标记 - 整理算法 10.1、引用计数法 10.2、可达性分析算法（gcroot） 6.1、栈上分配 6.2、对象在 Eden 区分配 (大部分情况，当 Eden 区没有足够空间进行分配时，出现 Young GC) 6.3、大对象直接进入老年代 6.4、长期存活的对象将进入老年代 6.5、对象动态年龄判断 6.6、老年代空间分配担保机制） 5.1.1、对象大小 5.1.2、什么是 java 对象的指针压缩？ 5.1.3、为什么要进行指针压缩？ 5.1、对象大小与指针压缩 4.1、线程私有区域 4.2、线程共享区域 2.1、类加载器 2.2、加载器初始化过程 2.3、双亲委派机制 2.4、为什么要设计双亲委派机制？ 2.5、全盘负责委托机制 2.6、自定义类加载器示例 1、类加载机制 2、双亲委派机制 (先找父亲加载，不行再由儿子自己加载) 3、tomcat 怎么破解类加载机制 4、内存模型 5、对象的创建 6、对象的分配过程 7、如何判断一个类是无用的类 8、finalize() 方法最终判定对象是否存活 9、常见引用类型 (四大引用) 10、对象回收 11、四大垃圾回收算法 12、常见 oom 13、垃圾收集器 14、如何选择垃圾收集器 15、各种命令（例如 100%cpu 的排查、死锁的检查） 16、JIT(即时编译器) 17、逃逸分析 1、类加载机制1类加载过程分为 加载 &gt;&gt; 验证 &gt;&gt; 准备 &gt;&gt; 解析 &gt;&gt; 初始化 &gt;&gt; 使用 &gt;&gt; 卸载 1、加载 在硬盘上查找并通过IO读入字节码文件，使用到类时才会加载，例如调用类的main()方法，new对象 等等，在加载阶段会在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口 2、验证 校验字节码文件的正确性 3、准备 给类的静态变量分配内存，并赋予默认值 4、解析 将符号引用替换为直接引用，该阶段会把一些静态方法(符号引用，比如main()方法)替换为指向数据 所存内存的指针或句柄等(直接引用)，这是所谓的静态链接过程(类加载期间完成)，动态链接是在程 序运行期间完成的将符号引用替换为直接引用，下节课会讲到动态链接 5、初始化 对类的静态变量初始化为指定的值，执行静态代码块 2、双亲委派机制 (先找父亲加载，不行再由儿子自己加载)2.1、类加载器11、根类加载器（**Bootstrap classLoader**）：负责加载支撑JVM运行的位于JRE的lib目录下的核心类库，比如rt.jar、charsets.jar等 2、扩展类加载器（**ExtClassLoader**）：负责加载支撑JVM运行的位于JRE的lib目录下的ext扩展目录中的JAR类包 3、应用加载器（**AppClassLoader**）：负责加载ClassPath路径下的类包，主要就是加载你自己写的那些类,负责加载用户自定义路径下的类包 2.2、加载器初始化过程1类运行加载全过程会创建JVM启动器实例sun.misc.Launcher。sun.misc.Launcher初始化使用了单例模式设计，保证一个JVM虚拟机内只有一个sun.misc.Launcher实例。在Launcher构造方法内部，其创建了两个类加载器，分别是sun.misc.Launcher.ExtClassLoader(扩展类加载器)和sun.misc.Launcher.AppClassLoader(应用类加载器)。 JVM默认使用launcher的`getClassLoader()`方法返回的类加载器`AppClassLoader`的实例来加载我们的应用程序。 2.3、双亲委派机制 应用程序类加载器 AppClassLoader 加载类的双亲委派机制源码，AppClassLoader 的 loadClass 方法最终会调用其父类 ClassLoader 的 loadClass 方法，该方法的大体逻辑如下：首先，检查一下指定名称的类是否已经加载过，如果加载过了，就不需要再加载，直接返回。如果此类没有加载过，那么，再判断一下是否有父加载器；如果有父加载器，则由父加载器加载（即调用 parent.loadClass(name, false);）. 或者是调用 bootstrap 类加载器来加载。如果父加载器及 bootstrap 类加载器都没有找到指定的类，那么调用当前类加载器的 findClass 方法来完成类加载。 2.4、为什么要设计双亲委派机制？ 1、沙箱安全机制：自己写的 java.lang.String.class 类不会被加载，这样便可以防止核心 API 库被随意篡改2、避免类的重复加载：当父亲已经加载了该类时，就没有必要子 ClassLoader 再加载一次，保证被加载类的唯一性 2.5、全盘负责委托机制 “全盘负责” 是指当一个 ClassLoder 装载一个类时，除非显示的使用另外一个 ClassLoder，该类所依赖及引用的类也由这个 ClassLoder 载入 2.6、自定义类加载器示例 自定义类加载器只需要继承 java.lang.ClassLoader 类，该类有两个核心方法，一个是 loadClass(String, boolean)，实现了双亲委派机制，还有一个方法是 findClass，默认实现是空方法，所以我们自定义类加载器主要是重写 findClass 方法。 3、tomcat 怎么破解类加载机制 1、commonLoader：Tomcat 最基本的类加载器，加载路径中的 class 可以被 Tomcat 容器本身以及各个 Webapp 访问； 2、catalinaLoader：Tomcat 容器私有的类加载器，加载路径中的 class 对于 Webapp 不可见； 3、sharedLoader：各个 Webapp 共享的类加载器，加载路径中的 class 对于所有 Webapp 可见，但是对于 Tomcat 容器不可见； 4、WebappClassLoader：各个 Webapp 私有的类加载器，加载路径中的 class 只对当前 Webapp 可见，比如加载 war 包里相关的类， 每个 war 包应用都有自己的 WebappClassLoader，实现相互隔离，比如不同 war 包应用引入了不同的 spring 版本，这样实现就能加载各自的 spring 版本； 5、模拟实现 Tomcat 的 JasperLoader 热加载 原理：后台启动线程监听 jsp 文件变化，如果变化了找到该 jsp 对应的 servlet 类的加载器引用 (gcroot)，重新生成新的 JasperLoader 加载器赋值给引用，然后加载新的 jsp 对应的 servlet 类，之前的那个加载器因为没有 gcroot 引用了，下一次 gc 的时候会被销毁 &#x3D;&gt; 总结：每个 webappClassLoader 加载自己的目录下的 class 文件，不会传递给父类加载器，打破了双亲委派机制。 4、内存模型4.1、线程私有区域 程序计数器：是当前线程所执行的字节码的行号指示器，无 OOM 虚拟机栈：是描述 java 方法执行的内存模型，每个方法在执行的同时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。 栈帧（ Frame）是用来存储数据和部分过程结果的数据结构，同时也被用来处理动态链接 (Dynamic Linking)、 方法返回值和异常分派（ Dispatch Exception）。栈帧随着方法调用而创 建，随着方法结束而销毁——无论方法是正常完成还是异常完成（抛出了在方法内未被捕获的异 常）都算作方法结束。 本地方法栈：和 Java Stack 作用类似, 区别是虚拟机栈为执行 Java 方法服务, 而本地方法栈则为 Native 方法服务, 如果一个 VM 实现使用 C-linkage 模型来支持 Native 调用, 那么该栈将会是一个 C 栈，但 HotSpot VM 直接就把本地方法栈和虚拟机栈合二为一。 4.2、线程共享区域 &#x3D;&#x3D; 堆 - 运行时数据区：&#x3D;&#x3D; 是被线程共享的一块内存区域，创建的对象和数组都保存在 Java 堆内存中，也是垃圾收集器进行垃圾收集的最重要的内存区域。由于现代 VM 采用分代收集算法, 因此 Java 堆从 GC 的角度还可以细分为: 新生代 (Eden 区、From Survivor 区和 To Survivor 区) 和老年代 方法区 &#x2F; 永久代（1.8 之后元空间）：用于存储被 JVM 加载的类信息 、常量、静态变量、 即时编译器编译后的代码等数据. HotSpot VM 把 GC 分代收集扩展至方法区, 即使用 Java 堆的永久代来实现方法区, 这样 HotSpot 的垃圾收集器就可以像管理 Java 堆一样管理这部分内存, 而不必为方法区开发专门的内存管理器 (永久带的内存回收的主要目标是针对常量池的回收和类型的卸载, 因此收益一般很小)。 运行时常量池（Runtime Constant Pool）是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述等信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。 直接内存 jdk1.4 后加入 NIO（New Input&#x2F;Output）类，引入了一种基于通道与缓冲区的 I&#x2F;O 方式，它可以使用 native 函数库直接分配堆外内存，然后通过一个存储在 java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。可以避免在 Java 堆和 Native 堆中来回复制数据 直接内存的分配不会受到 Java 堆大小的限制. 避免大于物理内存的情况 5、对象的创建 1、类加载检查 1虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。new指令对应到语言层面上讲是，new关键词、对象克隆、对象序列化等 2、分配内存 1在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类 加载完成后便可完全确定，为对象分配空间的任务等同于把 一块确定大小的内存从Java堆中划分出来。 //如何划分内存？ 1、“指针碰撞”（Bump the Pointer）(默认用指针碰撞) 如果Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中 间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段 与对象大小相等的距离。 2、“空闲列表”（Free List） 如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简 单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从 列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录。 //解决并发问题的方法 1、CAS（compare and swap） 虚拟机采用CAS配上失败重试的方式保证更新操作的原子性来对分配内存空间的动作进行同 步处理。 2、本地线程分配缓冲（Thread Local Allocation Buffer,TLAB） 把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一 小块内存。通过­XX:+/­UseTLAB参数来设定虚拟机是否使用TLAB(JVM会默认开启 ­XX:+UseTLAB)，­XX:TLABSize指定TLAB大小。 初始化 1内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头）， 如果使用TLAB，这一工作过程也可以提前至TLAB分配时进行。这一步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 设置对象头 1初始化零值之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息存放在对象的对象头Object Header之中。在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、 实例数据（Instance Data）和对齐填充（Padding）。HotSpot虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的运行时数据， 如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时 间戳等。对象头的另外一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 执行方法 1执行&lt;init&gt;方法，即对象按照程序员的意愿进行初始化。对应到语言层面上讲，就是为属性赋值（注意，这与上面的赋零值不同，这是由程序员赋的值），和执行构造方法。 5.1、对象大小与指针压缩5.1.1、对象大小 对象大小可以用 jol­-core 包查看 5.1.2、什么是 java 对象的指针压缩？ jdk1.6 update14 开始，在 64bit 操作系统中，JVM 支持指针压缩 jvm 配置参数: UseCompressedOops，compressed­­ 压缩、oop(ordinary object pointer)­­ 对象指针 启用指针压缩:­XX:+UseCompressedOops(默认开启)，禁止指针压缩:­XX:­UseCompressedOops 5.1.3、为什么要进行指针压缩？ 在 64 位平台的 HotSpot 中使用 32 位指针，内存使用会多出 1.5 倍左右，使用较大指针在主内存和缓存之间移动数据，占用较大宽带，同时 GC 也会承受较大压力 为了减少 64 位平台下内存的消耗，启用指针压缩功能 在 jvm 中，32 位地址最大支持 4G 内存 (2 的 32 次方)，可以通过对对象指针的压缩编码、解码方式进行优化，使得 jvm只用 32 位地址就可以支持更大的内存配置 (小于等于 32G) 堆内存小于 4G 时，不需要启用指针压缩，jvm 会直接去除高 32 位地址，即使用低虚拟地址空间 堆内存大于 32G 时，压缩指针会失效，会强制使用 64 位 (即 8 字节) 来对 java 对象寻址，这就会出现 1 的问题，所以堆内存不要大于 32G 为好 6、对象的分配过程 6.1、栈上分配 我们通过 JVM 内存分配可以知道 JAVA 中的对象都是在堆上进行分配，当对象没有被引用的时候，需要依靠 GC 进行回收内存，如果对象数量较多的时候，会给 GC 带来较大压力，也间接影响了应用的性能。为了减少临时对象在堆内分配的数量，JVM 通过逃逸分析确定该对象不会被外部访问。如果不会逃逸可以将该对象在栈上分配内存，这样该对象所占用的内存空间就可以随栈帧出栈而销毁，就减轻了垃圾回收的压力。 &#x3D;&#x3D; 对象逃逸分析：&#x3D;&#x3D; 就是分析对象动态作用域，当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他地方中。 12345678910public User test1() &#123; User user = new User(); user.setId(1); user.setName(&quot;zhuge&quot;); //TODO 保存到数据库 return user; &#125; public void test2() &#123; User user = new User(); user.setId(1); user.setName(&quot;zhuge&quot;); //TODO 保存到数据库 &#125; 很显然 test1 方法中的 user 对象被返回了，这个对象的作用域范围不确定，test2 方法中的 user 对象我们可以确定当方法结束这个对象就可以认为是无效对象了，对于这样的对象我们其实可以将其分配在栈内存里，让其在方法结束时跟随栈内存一起被回收掉。JVM 对于这种情况可以通过开启逃逸分析参数 (-XX:+DoEscapeAnalysis) 来优化对象内存分配位置，使其通过标量替换优先分配在栈上(栈上分配)，JDK7 之后默认开启逃逸分析，如果要关闭使用参数(-XX:-DoEscapeAnalysis)&#x3D;&#x3D; 标量替换：&#x3D;&#x3D; 通过逃逸分析确定该对象不会被外部访问，并且对象可以被进一步分解时，JVM 不会创建该对象，而是将该对象成员变量分解若干个被这个方法使用的成员变量所代替，这些代替的成员变量在栈帧或寄存器上分配空间，这样就不会因为没有一大块连续空间导致对象内存不够分配。开启标量替换参数 (-XX:+EliminateAllocations)，JDK7 之后默认开启。&#x3D;&#x3D; 标量与聚合量：&#x3D;&#x3D; 标量即不可被进一步分解的量，而 JAVA 的基本数据类型就是标量（如：int，long 等基本数据类型以及 reference 类型等），标量的对立就是可以被进一步分解的量，而这种量称之为聚合量。而在 JAVA 中对象就是可以被进一步分解的聚合量。 结论：栈上分配依赖于逃逸分析和标量替换 6.2、对象在 Eden 区分配 (大部分情况，当 Eden 区没有足够空间进行分配时，出现 Young GC) 大量的对象被分配在 eden 区，eden 区满了后会触发 Young GC，可能会有 99% 以上的对象成为垃圾被回收掉，剩余存活的对象会被挪到 s0 区，下一次 eden 区满了后又会触发 Young GC，把 eden 区和 s0 区垃圾对象回收，把剩余存活的对象一次性挪动到另外一块为空的 s1 区，因为新生代的对象都是朝生夕死的，存活时间很短，所以 JVM 默认的 8:1:1 的比例是很合适的，让 eden 区尽量的大，survivor 区够用即可，JVM 默认有这个参数 - XX:+UseAdaptiveSizePolicy(默认开启)，会导致这个 8:1:1 比例自动变化，如果不想这个比例有变化可以设置参数 - XX:-UseAdaptiveSizePolicy 6.3、大对象直接进入老年代 大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。JVM 参数 -XX:PretenureSizeThreshold 可以设置大对象的大小，如果对象超过设置大小会直接进入老年代，不会进入年轻代，这个参数只在 Serial 和 ParNew 两个收集器下有效。比如设置 JVM 参数：-XX:PretenureSizeThreshold&#x3D;1000000 (单位是字节) -XX:+UseSerialGC ，再执行下上面的第一个程序会发现大对象直接进了老年代 为什么要这样呢？为了避免为大对象分配内存时的复制操作而降低效率。 6.4、长期存活的对象将进入老年代 虚拟机给每个对象一个对象年龄（Age）计数器。如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor空间中，并将对象年龄设为 1。对象在 Survivor 中每熬过一次 MinorGC，年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁，CMS 收集器默认 6 岁，不同的垃圾收集器会略微有点不同），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 6.5、对象动态年龄判断 当前放对象的 Survivor 区域里 (其中一块区域，放对象的那块 s 区)，一批对象的总大小大于这块 Survivor 区域内存大小的 50%(-XX:TargetSurvivorRatio 可以指定)，那么此时大于等于这批对象年龄最大值的对象，就可以直接进入老年代了，例如 Survivor 区域里现在有一批对象，年龄 1 + 年龄 2 + 年龄 n 的多个年龄对象总和超过了 Survivor 区域的 50%，此时就会把年龄 n(含) 以上的对象都放入老年代。这个规则其实是希望那些可能是长期存活的对象，尽早进入老年代。对象动态年龄判断机制一般是在 young gc 之后触发的。 6.6、老年代空间分配担保机制） 年轻代每次 minor gc 之前 JVM 都会计算下老年代剩余可用空间如果这个可用空间小于年轻代里现有的所有对象大小之和 (包括垃圾对象)就会看一个 “-XX:-HandlePromotionFailure”(jdk1.8 默认就设置了) 的参数是否设置了如果有这个参数，就会看看老年代的可用内存大小，是否大于之前每一次 minor gc 后进入老年代的对象的平均大小。如果上一步结果是小于或者之前说的参数没有设置，那么就会触发一次 Full gc，对老年代和年轻代一起回收一次垃圾，如果回收完还是没有足够空间存放新的对象就会发生 “OOM”。 当然，如果 minor gc 之后剩余存活的需要挪动到老年代的对象大小还是大于老年代可用空间，那么也会触发 full gc，full gc 完之后如果还是没有空间放 minor gc 之后的存活对象，则也会发生 “OOM” 7、如何判断一个类是无用的类 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 8、finalize() 方法最终判定对象是否存活 第一次标记并进行一次筛选。 筛选的条件是此对象是否有必要执行 finalize() 方法。 当对象没有覆盖 finalize 方法，对象将直接被回收。 第二次标记 如果这个对象覆盖了 finalize 方法，finalize 方法是对象脱逃死亡命运的最后一次机会，如果对象要在 finalize()中成功拯救 自己，只要重新与引用链上的任何的一个对象建立关联即可，譬如把自己赋值给某个类变量或对象的成员变量，那在第 二次标记时它将移除出 “即将回收” 的集合。如果对象这时候还没逃脱，那基本上它就真的被回收了。 注意：一个对象的 finalize() 方法只会被执行一次，也就是说通过调用 finalize 方法自我救命的机会就一次。 9、常见引用类型 (四大引用) 1、强引用：普通的变量引用 2、软引用（SoftReference）：将对象用 SoftReference 软引用类型的对象包裹，正常情况不会被回收，但是 GC 做完后发现释放不出空间存放新的对象，则会把这些软引用的对象回收掉。软引用可用来实现内存敏感的高速缓存。 使用场景：浏览器的后退按钮 3、弱引用（WeakReference）：将对象用 WeakReference 软引用类型的对象包裹，弱引用跟没引用差不多，GC 会直接回收掉，很少用 4、虚引用：虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系，几乎不用 10、对象回收 什么叫对象回收？ 堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能再被任何途径使用的对象）。 10.1、引用计数法 给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加 1；当引用失效，计数器就减 1；任何时候计数器为 0 的对象就是不可能再被使用的。缺点：循环引用问题 10.2、可达性分析算法（gcroot） 将 “GC Roots” 对象作为起点，从这些节点开始向下搜索引用的对象，找到的对象都标记为非垃圾对象，其余未标记的对象都是垃圾对象GC Roots 根节点：线程栈的本地变量、静态变量、本地方法栈的变量等等 11、四大垃圾回收算法11.1、分代收集理论 当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。比如在新生代中，每次收集都会有大量对象 (近 99%) 死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择 “标记 - 清除” 或“标记 - 整理”算法进行垃圾收集。注意，“标记 - 清除”或 “标记 - 整理” 算法会比复制算法慢 10 倍以上。 11.2、标记 - 复制算法 它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 11.3、标记 - 清除算法 算法分为 “标记” 和“清除”阶段：标记存活的对象， 统一回收所有未被标记的对象(一般选择这种)；也可以反过来，标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象 。它是最基础的收集算法，比较简单，但是会带来两个明显的问题： 效率问题 (如果需要标记的对象太多，效率不高) 空间问题（标记清除后会产生大量不连续的碎片） 11.4、标记 - 整理算法 根据老年代的特点特出的一种标记算法，标记过程仍然与 “标记 - 清除” 算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 12、常见 oom 1、java.lang.StackOverflowError: 报这个错误一般是由于方法深层次的调用，默认的线程栈空间大小一般与具体的硬件平台有关。栈内存为线程私有的空间，每个线程都会创建私有的栈内存。栈空间内存设置过大，创建线程数量较多时会出现栈内存溢出 StackOverflowError。同时，栈内存也决定方法调用的深度，栈内存过小则会导致方法调用的深度较小，如递归调用的次数较少。 2、java.lang.OutOfMemoryError: Java heap space Heap size 设置 JVM 堆的设置是指：java 程序执行过程中 JVM 能够调配使用的内存空间的设置。JVM 在启动的时候会自己主动设置 Heap size 的值，其初始空间 (即 - Xms) 是物理内存的 1&#x2F;64，最大空间 (-Xmx) 是物理内存的 1&#x2F;4。能够利用 JVM 提供的 - Xmn -Xms -Xmx 等选项可进行设置。Heap size 的大小是 Young Generation 和 Tenured Generaion 之和。 3、java.lang.OutOfMemoryError：GC overhead limit exceeded GC 回收时间过长时会抛出的 OutOfMemory。过长是指，超过 98% 的时间都在用来做 GC 并且回收了不到 2% 的堆内存。连续多次的 GC，都回收了不到 2% 的极端情况下才会抛出。假如不抛出 GC overhead limit 错误会发生什么事情呢？那就是 GC 清理出来的一点内存很快又会被再次填满，强迫 GC 再次执行，这样造成恶性循环，CPU 的使用率一直很高，但是 GC 没有任何的进展。 4、java.lang.OutOfMemoryError：Direct buffer memory 写 NIO 程序经常使用到 ByteBuffer 来读取或者写入数据，这是一种基于通道与缓冲区的 I&#x2F;O 方式。它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 java 堆里面的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中提高性能，因为避免了 java 堆和 Native 堆中来回复制数据。 ByteBuffer.allocate(capability) ：这种方式是分配 JVM 堆内存，属于 GC 管辖范围之内。由于需要拷贝，所以速度相对较慢； ByteBuffer.allocateDirect(capability)：这种方式是直接分配 OS 本地内存，不属于 GC 管辖范围之内，由于不需要内存拷贝所以速度相对较快。 但是如果不断分配本地内存，堆内存很少使用，那么 JVM 就不需要执行 GC,DirectByteBuffer 对象就不会被回收。这时候堆内存充足，但是本地内存已经用光了，再次尝试分配的时候就会出现 OutOfMemoryError，那么程序就直接崩溃了。 5、java.lang.OutOfMemoryError：unable to create new native thread 准确的说，这一个异常是和程序运行的平台相关的。导致的原因： 创建了太多的线程，一个应用创建多个线程，超过系统承载极限； 服务器不允许应用程序创建这么多的线程，Linux 系统默认的允许单个进程可以创建的线程数量是 1024 个，当创建多 线程数量多于这个数字的时候就会抛出此异常 如何解决呢？ 想办法减少应用程序创建的线程的数量，分析应用是否真的需要创建这么多的线程。如果不是，改变代码将线程数量降到最低； 对于有的应用，确实需要创建很多的线程，远超过 Linux 限制的 1024 个 限制，那么可以通过修改 Linux 服务器的配置，扩大 Linux 的默认限制。 6、java.lang.OutOfMemoryError：MetaSpace 元空间的本质和永久代类似，都是对 JVM 规范中的方法区的实现。不过元空间与永久代之间最大的区别在于：元空间不在虚拟机中，而是使用的本地内存。因此，默认情况下，元空间的大小仅仅受到本地内存的限制 。 元空间存放了以下的内容： 虚拟机加载的类信息； 常量池； 静态变量； 即时编译后的代码 模拟 MetaSpace 空间溢出，我们不断生成类往元空间里灌，类占据的空间总是会超过 MetaSpace 指定的空间大小的 查看元空间的大小：java -XX:+PrintFlagsInitial 13、垃圾收集器13.1、CMS（“标记 - 清除” 算法, -XX:+UseConcMarkSweepGC(old）定义：以获取最短回收停顿时间为目标的收集器 13.1.1、运作过程 (5 大步骤) 1、初始标记：暂停所有的其他线程 (STW)，并记录下 gc roots 直接能引用的对象，速度很快。2、并发标记：并发标记阶段就是从 GC Roots 的直接关联对象开始遍历整个对象图的过程， 这个过程耗时较长但是不需要停顿用户线程， 可以与垃圾收集线程一起并发运行。因为用户程序继续运行，可能会有导致已经标记过的对象状态发生改变。3、重新标记：重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短。主要用到增量更新算法做重新标记。4、并发清理：开启用户线程，同时 GC 线程开始对未标记的区域做清扫。这个阶段如果有新增对象会被标记为三色标记法里面的黑色不做任何处理5、并发重置：重置本次 GC 过程中的标记数据。 主要优点：并发收集、低停顿。但是它有下面几个明显的缺点： 对 CPU 资源敏感（会和服务抢资源）； 无法处理浮动垃圾 (在并发标记和并发清理阶段又产生垃圾，这种浮动垃圾只能等到下一次 gc 再清理了)； 它使用的回收算法 -“标记 - 清除” 算法会导致收集结束时会有大量空间碎片产生，当然通过参数 &#x3D;&#x3D;-XX:+UseCMSCompactAtFullCollection 可以让 jvm 在执行完标记清除后再做整理 &#x3D;&#x3D; 执行过程中的不确定性，会存在上一次垃圾回收还没执行完，然后垃圾回收又被触发的情况，特别是 在并发标记和并发清理阶段会出现，一边回收，系统一边运行，也许没回收完就再次触发 full gc，也就是 “concurrent mode failure”，此时会进入 stop the world，用 serial old 垃圾收集器来回收 CMS 的相关核心参数 -XX:+UseConcMarkSweepGC：启用 cms -XX:ConcGCThreads：并发的 GC 线程数 -XX:+UseCMSCompactAtFullCollection：FullGC 之后做压缩整理（减少碎片） -XX:CMSFullGCsBeforeCompaction：多少次 FullGC 之后压缩一次，默认是 0，代表每次 FullGC 后都会压缩一次 -XX:CMSInitiatingOccupancyFraction: 当老年代使用达到该比例时会触发 FullGC（默认是 92，这是百分比） -XX:+UseCMSInitiatingOccupancyOnly：只使用设定的回收阈值 (-XX:CMSInitiatingOccupancyFraction 设定的值)，如果不指定，JVM 仅在第一次使用设定值，后续则会自动调整 -XX:+CMSScavengeBeforeRemark：在 CMS GC 前启动一次 minor gc，目的在于减少老年代对年轻代的引用，降低 CMS GC 的标记阶段时的开销，一般 CMS 的 GC 耗时 80% 都在标记阶段 -XX:+CMSParallellnitialMarkEnabled：表示在初始标记的时候多线程执行，缩短 STW -XX:+CMSParallelRemarkEnabled：在重新标记的时候多线程执行，缩短 STW; 13.1.2、三色标记法 黑色：表示对象已经被垃圾收集器访问过， 且这个对象的所有引用都已经扫描过。黑色的对象代表已经扫描过， 它是安全存活的， 如果有其他对象引用指向了黑色对象， 无须重新扫描一遍。黑色对象不可能直接（不经过灰色对象） 指向某个白色对象。 灰色：表示对象已经被垃圾收集器访问过， 但这个对象上至少存在一个引用还没有被扫描过。 白色：表示对象尚未被垃圾收集器访问过。显然在可达性分析刚刚开始的阶段， 所有的对象都是白色的， 若在分析结束的阶段， 仍然是白色的对象， 即代表不可达。 13.1.3、concurrent model failure（浮动垃圾）1在并发标记过程中，如果由于方法运行结束导致部分局部变量(gcroot)被销毁，这个gcroot引用的对象之前又被扫描过(被标记为非垃圾对象)，那么本轮GC不会回收这部分内存。这部分本应该回收但是没有回收到的内存，被称之为“浮动垃圾”。浮动垃圾并不会影响垃圾回收的正确性，只是需要等到下一轮垃圾回收中才被清除。另外，针对并发标记(还有并发清理)开始后产生的新对象，通常的做法是直接全部当成黑色，本轮不会进行清除。这部分对象期间可能也会变为垃圾，这也算是浮动垃圾的一部分。 13.1.4、background&#x2F;foreground collector -XX:ConcGCThreads&#x3D;4 和 - XX:+ExplicitGCInvokesConcurrent 开启 foreground CMS GC，CMS gc 有两种模式，background 和 foreground，正常的 cms gc 使用 background 模式，就是我们平时说的 cms gc；当并发收集失败或者调用了 System.gc() 的时候，就会导致一次 full gc，这个 fullgc 是不是 cms 回收，而是 Serial 单线程回收器，加入了参数 -XX:ConcGCThreads&#x3D;4 后，执行 full gc 的时候，就变成了 CMS foreground gc，它是并行 full gc，只会执行 cms 中 stop the world 阶段的操作，效率比单线程 Serial full GC 要高；需要注意的是它只会回收 old，因为 cms 收集器是老年代收集器；而正常的 Serial 收集是包含整个堆的，加入了参数 &#x3D;&#x3D;-XX:+ExplicitGCInvokesConcurrent&#x3D;&#x3D;, 代表永久带也会被 cms 收集； 13.1.5、为什么 G1 用 SATB？CMS 用增量更新？ SATB 相对增量更新效率会高 (当然 SATB 可能造成更多的浮动垃圾)，因为不需要在重新标记阶段再次深度扫描被删除引用对象，而 CMS 对增量引用的根对象会做深度扫描，G1 因为很多对象都位于不同的 region，CMS 就一块老年代区域，重新深度扫描对象的话 G1 的代价会比 CMS 高，所以 G1 选择 SATB 不深度扫描对象，只是简单标记，等到下一轮 GC 再深度扫描。 13.1.6、漏标 - 读写屏障 (解决方案)13.1.6.1、增量更新 (Incremental Update)+ 写屏障 增量更新就是当黑色对象插入新的指向白色对象的引用关系时， 就将这个新插入的引用记录下来， 等并发扫描结束之后， 再将这些记录过的引用关系中的黑色对象为根， 重新扫描一次。这可以简化理解为， 黑色对象一旦新插入了指向白色对象的引用之后， 它就变回灰色对象了。 写屏障实现增量更新 当对象 A 的成员变量的引用发生变化时，比如新增引用（a.d &#x3D; d），我们可以利用写屏障，将 A 新的成员变量引用对象 D记录下来：void post_write_barrier(oop* field, oop new_value) {remark_set.add(new_value); &#x2F;&#x2F; 记录新引用的对象} 13.1.6.2、原始快照（Snapshot At The Beginning，SATB）+ 写屏障 原始快照就是当灰色对象要删除指向白色对象的引用关系时， 就将这个要删除的引用记录下来， 在并发扫描结束之后，再将这些记录过的引用关系中的灰色对象为根， 重新扫描一次，这样就能扫描到白色的对象，将白色对象直接标记为黑色 (目的就是让这种对象在本轮 gc 清理中能存活下来，待下一轮 gc 的时候重新扫描，这个对象也有可能是浮动垃圾) 以上无论是对引用关系记录的插入还是删除， 虚拟机的记录操作都是通过写屏障实现的。 写屏障实现 SATB 当对象 B 的成员变量的引用发生变化时，比如引用消失（a.b.d &#x3D; null），我们可以利用写屏障，将 B 原来成员变量的引用对象 D 记录下来：void pre_write_barrier(oop* field) {oop old_value &#x3D; *field; &#x2F;&#x2F; 获取旧值remark_set.add(old_value); &#x2F;&#x2F; 记录原来的引用对象} 13.1.6.3、并发标记时对漏标的处理方案 CMS：写屏障 + 增量更新G1，Shenandoah：写屏障 + SATBZGC：读屏障 工程实现中，读写屏障还有其他功能，比如写屏障可以用于记录跨代 &#x2F; 区引用的变化，读屏障可以用于支持移动对象的并发执行等。功能之外，还有性能的考虑，所以对于选择哪种，每款垃圾回收器都有自己的想法。 13.1.7、promotion failed这个异常发生在年轻带回收的时候；在进行 Minor GC 时，Survivor Space 放不下，对象只能放入老年代，而此时老年代也放不下造成的，多数是由于老年带有足够的空闲空间，但是由于碎片较多，新生代要转移到老年带的对象比较大, 找不到一段连续区域存放这个对象导致的， 13.1.8、过早提升和提升失败在 Minor GC 过程中，Survivor Unused 可能不足以容纳 Eden 和另一个 Survivor 中的存活对象， 那么多余的将被移到老年代， 称为过早提升（Premature Promotion）, 这会导致老年代中短期存活对象的增长， 可能会引发严重的性能问题。再进一步， 如果老年代满了， Minor GC 后会进行 Full GC， 这将导致遍历整个堆， 称为提升失败（Promotion Failure）。 13.1.9、早提升的原因 Survivor 空间太小，容纳不下全部的运行时短生命周期的对象，如果是这个原因，可以尝试将 Survivor 调大，否则端生命周期的对象提升过快，导致老年代很快就被占满，从而引起频繁的 full gc； 对象太大，Survivor 和 Eden 没有足够大的空间来存放这些大象； 13.1.10、提升失败原因当提升的时候，发现老年代也没有足够的连续空间来容纳该对象。为什么是没有足够的连续空间而不是空闲空间呢？老年代容纳不下提升的对象有两种情况： 老年代空闲空间不够用了； 老年代虽然空闲空间很多，但是碎片太多，没有连续的空闲空间存放该对象； 解决方法 如果是因为内存碎片导致的大对象提升失败，cms 需要进行空间整理压缩； 如果是因为提升过快导致的，说明 Survivor 空闲空间不足，那么可以尝试调大 Survivor； 如果是因为老年代空间不够导致的，尝试将 CMS 触发的阈值调低 13.2、G1定义：面向服务器的垃圾收集器, 主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC，停顿时间要求的同时, 还具备高吞吐量性能特征 13.2.1、运作流程 G1 将 Java 堆划分为多个大小相等的独立区域（Region），JVM 最多可以有 2048 个 Region。一般 Region 大小等于堆大小除以 2048，比如堆大小为 4096M，则 Region 大小为 2M，当然也可以用参数 “-XX:G1HeapRegionSize” 手动指定 Region 大小，但是推荐默认的计算方式。G1 保留了年轻代和老年代的概念，但不再是物理隔阂了，它们都是（可以不连续）Region 的集合。默认年轻代对堆内存的占比是 5%，如果堆大小为 4096M，那么年轻代占据 200MB 左右的内存，对应大概是 100 个 Region，可以通过 “-XX:G1NewSizePercent” 设置新生代初始占比，在系统运行中，JVM 会不停的给年轻代增加更多的 Region，但是最多新生代的占比不会超过 60%，可以通过 “-XX:G1MaxNewSizePercent” 调整。年轻代中的 Eden 和 Survivor 对应的 region 也跟之前一样，默认 8:1:1，假设年轻代现在有 1000 个 region，eden 区对应 800 个，s0 对应 100 个，s1 对应 100 个。一个 Region 可能之前是年轻代，如果 Region 进行了垃圾回收，之后可能又会变成老年代，也就是说 Region 的区域功能可能会动态变化。G1 垃圾收集器对于对象什么时候会转移到老年代跟之前讲过的原则一样，唯一不同的是对大对象的处理，G1 有专门分配大对象的 Region 叫 Humongous 区，而不是让大对象直接进入老年代的 Region 中。在 G1 中，大对象的判定规则就是一个大对象超过了一个 Region 大小的 50%，比如按照上面算的，每个 Region 是 2M，只要一个大对象超过了 1M，就会被放入 Humongous 中，而且一个大对象如果太大，可能会横跨多个 Region 来存放。 Humongous 区专门存放短期巨型对象，不用直接进老年代，可以节约老年代的空间，避免因为老年代空间不够的 GC 开销。Full GC 的时候除了收集年轻代和老年代之外，也会将 Humongous 区一并回收。 G1 收集器一次 GC 的运作过程大致分为以下 4 个步骤： 初始标记（initial mark，STW）：暂停所有的其他线程，并记录下 gc roots 直接能引用的对象，速度很快 ； 并发标记（Concurrent Marking）：同 CMS 的并发标记 最终标记（Remark，STW）：同 CMS 的重新标记 筛选回收（Cleanup，STW）：筛选回收阶段首先对各个 Region 的 &#x3D;&#x3D; 回收价值和成本进行排序，根据用户所期望的 GC 停顿时间 (可以用 JVM 参数 -XX:MaxGCPauseMillis 指定) 来制定回收计划，&#x3D;&#x3D; 比如说老年代此时有 1000 个 Region 都满了，但是因为根据预期停顿时间，本次垃圾回收可能只能停顿 200 毫秒，那么通过之前回收成本计算得知，可能回收其中 800 个 Region 刚好需要 200ms，那么就只会回收 800 个 Region(Collection Set，要回收的集合)，尽量把 GC 导致的停顿时间控制在我们指定的范围内。这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。不管是年轻代或是老年代，回收算法主要用的是复制算法，将一个 region 中的存活对象复制到另一个 region 中，这种不会像 CMS 那样回收完因为有很多内存碎片还需要整理一次，G1 采用复制算法回收几乎不会有太多内存碎片。(注意：CMS 回收阶段是跟用户线程一起并发执行的，G1 因为内部实现太复杂暂时没实现并发回收，不过到了 Shenandoah 就实现了并发收集，Shenandoah 可以看成是 G1 的升级版本) &#x3D;&#x3D;G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)，比如一个 Region 花 200ms 能回收 10M 垃圾，另外一个 Region 花 50ms 能回收 20M 垃圾，在回收时间有限情况下，G1 当然会优先选择后面这个 Region 回收。&#x3D;&#x3D; 这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率。 被视为 JDK1.7 以上版本 Java 虚拟机的一个重要进化特征。它具备以下特点： 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程来执行 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合：与 CMS 的 “标记–清理” 算法不同，G1 从整体来看是基于 “标记整理” 算法实现的收集器；从局部上来看是基于 “复制” 算法实现的。可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段 (通过参数 “-XX:MaxGCPauseMillis” 指定) 内完成垃圾收集。 毫无疑问， 可以由用户指定期望的停顿时间是 G1 收集器很强大的一个功能， 设置不同的期望停顿时间， 可使得 G1 在不同应用场景中取得关注吞吐量和关注延迟之间的最佳平衡。不过， 这里设置的 “期望值” 必须是符合实际的， 不能异想天开， 毕竟 G1 是要冻结用户线程来复制对象的， 这个停顿时间再怎么低也得有个限度。它默认的停顿目标为两百毫秒， 一般来说， 回收阶段占到几十到一百甚至接近两百毫秒都很正常， 但如果我们把停顿时间调得非常低， 譬如设置为二十毫秒， 很可能出现的结果就是由于停顿目标时间太短， 导致每次选出来的回收集只占堆内存很小的一部分， 收集器收集的速度逐渐跟不上分配器分配的速度， 导致垃圾慢慢堆积。很可能一开始收集器还能从空闲的堆内存中获得一些喘息的时间， 但应用运行时间一长就不行了， 最终占满堆引发 Full GC 反而降低性能， 所以通常把期望停顿时间设置为一两百毫秒或者两三百毫秒会是比较合理的。 13.2.2、Remembered Set（记录集）&#x2F;Card Table（卡表） 在新生代做 GCRoots 可达性扫描过程中可能会碰到跨代引用的对象，这种如果又去对老年代再去扫描效率太低了。为此，在新生代可以引入记录集（Remember Set）的数据结构（记录从非收集区到收集区的指针集合），避免把整个老年代加入 GCRoots 扫描范围。事实上并不只是新生代、 老年代之间才有跨代引用的问题， 所有涉及部分区域收集（Partial GC） 行为的垃圾收集器， 典型的如 G1、 ZGC 和 Shenandoah 收集器， 都会面临相同的问题。垃圾收集场景中，收集器只需通过记忆集判断出某一块非收集区域是否存在指向收集区域的指针即可，无需了解跨代引用指针的全部细节。hotspot 使用一种叫做 “卡表”(cardtable) 的方式实现记忆集，也是目前最常用的一种方式。关于卡表与记忆集的关系，可以类比为 Java 语言中 HashMap 与 Map 的关系。卡表是使用一个字节数组实现：CARD_TABLE[ ]，每个元素对应着其标识的内存区域一块特定大小的内存块，称为 “卡页”。HotSpot 使用的卡页是 2^9 大小，即 512 字节。 一个卡页中可包含多个对象，只要有一个对象的字段存在跨代指针，其对应的卡表的元素标识就变成 1，表示该元素变脏，否则为 0。GC 时，只要筛选本收集区的卡表中变脏的元素加入 GCRoots 里。 卡表如何维护？ 卡表变脏上面已经说了，但是需要知道如何让卡表变脏，即发生引用字段赋值时，如何更新卡表对应的标识为 1。Hotspot 使用写屏障维护卡表状态。 13.2.3、Collect Set Collect Set(CSet) 是指，在 Evacuation 阶段，由 G1 垃圾回收器选择的待回收的 Region 集合。G1 垃圾回收器的软实时的特性就是通过 CSet 的选择来实现的。对应于算法的两种模式 fully-young generational mode 和 partially-young mode，CSet 的选择可以分成两种： 在 fully-young generational mode 下：顾名思义，该模式下 CSet 将只包含 young 的 Region。G1 将调整 young 的 Region 的数量来匹配软实时的目标； 在 partially-young mode 下：该模式会选择所有的 young region，并且选择一部分的 old region。old region 的选择将依据在 Marking cycle phase 中对存活对象的计数。G1 选择存活对象最少的 Region 进行回收。 13.2.4、young gc 的完整流程 YoungGC 并不是说现有的 Eden 区放满了就会马上触发，G1 会计算下现在 Eden 区回收大概要多久时间，如果回收时间远远小于参数 -XX:MaxGCPauseMills 设定的值，那么增加年轻代的 region，继续给新对象存放，不会马上做 Young GC，直到下一次 Eden 区放满，G1 计算回收时间接近参数 -XX:MaxGCPauseMills 设定的值，那么就会触发 Young GC 13.2.5、Mixed GC 的完整流程 不是 FullGC，老年代的堆占有率达到参数 (-XX:InitiatingHeapOccupancyPercent) 设定的值则触发，回收所有的 Young 和部分 Old(根据期望的 GC 停顿时间确定 old 区垃圾收集的优先顺序)以及大对象区，正常情况 G1 的垃圾收集是先做 MixedGC，主要使用复制算法，需要把各个 region 中存活的对象拷贝到别的 region 里去，拷贝过程中如果发现没有足够的空 region 能够承载拷贝对象就会触发一次 Full GC 13.2.6、Full GC 停止系统程序，然后采用单线程进行标记、清理和压缩整理，好空闲出来一批 Region 来供下一次 MixedGC 使用，这个过程是非常耗时的。(Shenandoah 优化成多线程收集了) 13.2.7、Marking bitmaps&#x2F;TAMS Marking bitmap 是一种数据结构，其中的每一个 bit 代表的是一个可用于分配给对象的起始地址。举例来说： bitmap 其中 addrN 代表的是一个对象的起始地址。绿色的块代表的是在该起始地址处的对象是存活对象，而其余白色的块则代表了垃圾对象。G1 使用了两个 bitmap，一个叫做 previous bitmap，另外一个叫做 next bitmap。previous bitmap 记录的是上一次的标记阶段完成之后的构造的 bitmap；next bitmap 则是当前正在标记阶段正在构造的 bitmap。在当前标记阶段结束之后，当前标记的 next bitmap 就变成了下一次标记阶段的 previous bitmap。 TAMS(top at mark start) 变量，是一对用于区分在标记阶段新分配对象的变量，分别被称为 previous TAMS 和 next TAMS。在 previous TAMS 和 next TAMS 之间的对象则是本次标记阶段时候新分配的对象。如图： previous TMAS 和 next TAMS 白色 region 代表的是空闲空间，绿色 region 代表是存活对象，橙色 region 代表的在此次标记阶段新分配的对象。注意的是，在橙色区域的对象，并不能确保它们都事实上是存活的。 13.2.8、Pause Prediction Model 停顿预测模型，通过用户设定的 GC 停顿时间（参数 - XX:MaxGCPauseMillis），G1 以衰减平均值为理论基础，计算需要回收的 Region 数量从而进行满足。 13.2.9、G1 收集器参数设置1-XX:+UseG1GC:使用G1收集器 -XX:ParallelGCThreads:指定GC工作的线程数量 -XX:G1HeapRegionSize:指定分区大小(1MB~32MB，且必须是2的N次幂)，默认将整堆划分为2048个分区 -XX:MaxGCPauseMillis:目标暂停时间(默认200ms) -XX:G1NewSizePercent:新生代内存初始空间(默认整堆5%) -XX:G1MaxNewSizePercent:新生代内存最大空间 -XX:TargetSurvivorRatio:Survivor区的填充容量(默认50%)，Survivor区域里的一批对象(年龄1+年龄2+年龄n的多个年龄对象)总和超过了Survivor区域的50%，此时就会把年龄n(含)以上的对象都放入老年代 -XX:MaxTenuringThreshold:最大年龄阈值(默认15) -XX:InitiatingHeapOccupancyPercent:老年代占用空间达到整堆内存阈值(默认45%)，则执行新生代和老年代的混合收集(MixedGC)，比如我们之前说的堆默认有2048个region，如果有接近1000个region都是老年代的region，则可能就要触发MixedGC了 -XX:G1MixedGCLiveThresholdPercent(默认85%) region中的存活对象低于这个值时才会回收该region，如果超过这个值，存活对象过多，回收的的意义不大。 -XX:G1MixedGCCountTarget:在一次回收过程中指定做几次筛选回收(默认8次)，在最后一个筛选回收阶段可以回收一会，然后暂停回收，恢复系统运行，一会再开始回收，这样可以让系统不至于单次停顿时间过长。 -XX:G1HeapWastePercent(默认5%): gc过程中空出来的region是否充足阈值，在混合回收的时候，对Region回收都是基于复制算法进行的，都是把要回收的Region里的存活对象放入其他Region，然后这个Region中的垃圾对象全部清理掉，这样的话在回收过程就会不断空出来新的Region，一旦空闲出来的Region数量达到了堆内存的5%，此时就会立即停止混合回收，意味着本次混合回收就结束了。 13.2.10、G1 垃圾收集器优化建议（ -XX:MaxGCPauseMills&#x3D;50ms） 假设参数 -XX:MaxGCPauseMills 设置的值很大，导致系统运行很久，年轻代可能都占用了堆内存的 60% 了，此时才触发年轻代 gc。那么存活下来的对象可能就会很多，此时就会导致 Survivor 区域放不下那么多的对象，就会进入老年代中。 或者是你年轻代 gc 过后，存活下来的对象过多，导致进入 Survivor 区域后触发了动态年龄判定规则，达到了 Survivor 区域的 50%，也会快速导致一些对象进入老年代中。所以这里核心还是在于调节 -XX:MaxGCPauseMills 这个参数的值，在保证他的年轻代 gc 别太频繁的同时，还得考虑每次 gc 过后的存活对象有多少, 避免存活对象太多快速进入老年代，频繁触发 mixed gc. 13.2.11、什么场景适合使用 G1 50% 以上的堆被存活对象占用 对象分配和晋升的速度变化非常大 垃圾回收时间特别长，超过 1 秒 8GB 以上的堆内存 (建议值) 停顿时间是 500ms 以内 13.3、ZGC定义：具有实验性质的低延迟垃圾收集器 13.3.1、主要目标 支持 TB 量级的堆。我们生产环境的硬盘还没有上 TB 呢，这应该可以满足未来十年内，所有 JAVA 应用的需求了吧。 最大 GC 停顿时间不超 10ms。目前一般线上环境运行良好的 JAVA 应用 Minor GC 停顿时间在 10ms 左右，Major GC 一般都需要 100ms 以上（G1 可以调节停顿时间，但是如果调的过低的话，反而会适得其反），之所以能做到这一点是因为它的停顿时间主要跟 Root 扫描有关，而 Root 数量和堆大小是没有任何关系的。 奠定未来 GC 特性的基础。 最糟糕的情况下吞吐量会降低 15%。这都不是事，停顿时间足够优秀。至于吞吐量，通过扩容分分钟解决。另外，Oracle 官方提到了它最大的优点是：它的停顿时间不会随着堆的增大而增长！也就是说，几十 G 堆的停顿时间是 10ms 以下，几百 G 甚至上 T 堆的停顿时间也是 10ms 以下。 13.3.2、color poin（颜色指针） Colored Pointers，即颜色指针，如下图所示，ZGC 的核心设计之一。以前的垃圾回收器的 GC 信息都保存在对象头中，而 ZGC 的 GC 信息保存在指针中。 每个对象有一个 64 位指针，这 64 位被分为： 18 位：预留给以后使用； 1 位：Finalizable 标识，此位与并发引用处理有关，它表示这个对象只能通过 finalizer 才能访问； 1 位：Remapped 标识，设置此位的值后，对象未指向 relocation set 中（relocation set 表示需要 GC 的Region 集合）； 1 位：Marked1 标识； 1 位：Marked0 标识，和上面的 Marked1 都是标记对象用于辅助 GC； 42 位：对象的地址（所以它可以支持 2^42&#x3D;4T 内存）： 为什么有 2 个 mark 标记？ 每一个 GC 周期开始时，会交换使用的标记位，使上次 GC 周期中修正的已标记状态失效，所有引用都变成未标记。GC 周期 1：使用 mark0, 则周期结束所有引用 mark 标记都会成为 01。GC 周期 2：使用 mark1, 则期待的 mark 标记 10，所有引用都能被重新标记。通过对配置 ZGC 后对象指针分析我们可知，对象指针必须是 64 位，那么 ZGC 就无法支持 32 位操作系统，同样的也就无法支持压缩指针了（CompressedOops，压缩指针也是 32 位）。 颜色指针的三大优势： 一旦某个 Region 的存活对象被移走之后，这个 Region 立即就能够被释放和重用掉，而不必等待整个堆中所有指向该 Region 的引用都被修正后才能清理，这使得理论上只要还有一个空闲 Region，ZGC 就能完成收集。 颜色指针可以大幅减少在垃圾收集过程中内存屏障的使用数量，ZGC 只使用了读屏障。 颜色指针具备强大的扩展性，它可以作为一种可扩展的存储结构用来记录更多与对象标记、重定位过程相关的数据，以便日后进一步提高性能。 读屏障之前的 GC 都是采用 Write Barrier，这次 ZGC 采用了完全不同的方案读屏障，这个是 ZGC 一个非常重要的特性。在标记和移动对象的阶段，每次「从堆里对象的引用类型中读取一个指针」的时候，都需要加上一个 Load Barriers。那么我们该如何理解它呢？看下面的代码，第一行代码我们尝试读取堆中的一个对象引用 obj.fieldA 并赋给引用 o（fieldA 也是一个对象时才会加上读屏障）。如果这时候对象在 GC 时被移动了，接下来 JVM 就会加上一个读屏障，这个屏障会把读出的指针更新到对象的新地址上，并且把堆里的这个指针 “修正” 到原本的字段里。这样就算 GC 把对象移动了，读屏障也会发现并修正指针，于是应用代码就永远都会持有更新后的有效指针，而且不需要 STW。那么，JVM 是如何判断对象被移动过呢？就是利用上面提到的颜色指针，如果指针是 Bad Color，那么程序还不能往下执行，需要「slow path」，修正指针；如果指针是 Good Color，那么正常往下执行即可： ❝ 这个动作是不是非常像 JDK 并发中用到的 CAS 自旋？读取的值发现已经失效了，需要重新读取。而 ZGC 这里是之前持有的指针由于 GC 后失效了，需要通过读屏障修正指针。❞后面 3 行代码都不需要加读屏障：Object p &#x3D; o 这行代码并没有从堆中读取数据：o.doSomething() 也没有从堆中读取数据；obj.fieldB 不是对象引用，而是原子类型。正是因为 Load Barriers 的存在，所以会导致配置 ZGC 的应用的吞吐量会变低。官方的测试数据是需要多出额外 4% 的开销： 那么，判断对象是 Bad Color 还是 Good Color 的依据是什么呢？就是根据上一段提到的 Colored Pointers 的 4 个颜色位。 当加上读屏障时，根据对象指针中这 4 位的信息，就能知道当前对象是 Bad&#x2F;Good Color 了。 PS：既然低 42 位指针可以支持 4T 内存，那么能否通过预约更多位给对象地址来达到支持更大内存的目的呢？答案肯定是不可以。因为目前主板地址总线最宽只有 48bit，4 位是颜色位，就只剩 44 位了，所以受限于目前的硬件，ZGC 最大只能支持 16T 的内存，JDK13 就把最大支持堆内存从 4T 扩大到了 16T。 13.3.3、运作过程 并发标记（Concurrent Mark）：与 G1 一样，并发标记是遍历对象图做可达性分析的阶段，它的初始标记 (Mark Start) 和最终标记 (Mark End) 也会出现短暂的停顿，与 G1 不同的是， ZGC 的标记是在指针上而不是在对象上进行的， 标记阶段会更新染色指针中的 Marked 0、 Marked 1 标志位。 并发预备重分配（Concurrent Prepare for Relocate）：这个阶段需要根据特定的查询条件统计得出本次收集过程要清理哪些 Region，将这些 Region 组成重分配集（Relocation Set）。ZGC 每次回收都会扫描所有的 Region，用范围更大的扫描成本换取省去 G1 中记忆集的维护成本。 并发重分配（Concurrent Relocate）：重分配是 ZGC 执行过程中的核心阶段，这个过程要把重分配集中的存活对象复制到新的 Region 上，并为重分配集中的每个 Region 维护一个转发表（Forward Table），记录从旧对象到新对象的转向关系。ZGC 收集器能仅从引用上就明确得知一个对象是否处于重分配集之中，如果用户线程此时并发访问了位于重分配集中的对象，这次访问将会被预置的内存屏障 (读屏障) 所截获，然后立即根据 Region 上的转发表记录将访问转发到新复制的对象上，并同时修正更新该引用的值，使其直接指向新对象，ZGC 将这种行为称为指针的“自愈”（Self-Healing）能力。 1 ZGC的颜色指针因为“自愈”（Self‐Healing）能力，所以只有第一次访问旧对象会变慢， 一旦重分配集中某个Region的存活对象都复制完毕后，2 这个Region就可以立即释放用于新对象的分配，但是转发表还得留着不能释放掉， 因为可能还有访问在使用这个转发表。 并发重映射（Concurrent Remap）：重映射所做的就是修正整个堆中指向重分配集中旧对象的所有引用，但是 ZGC 中对象引用存在 “自愈” 功能，所以这个重映射操作并不是很迫切。ZGC 很巧妙地把并发重映射阶段要做的工作，合并到了下一次垃圾收集循环中的并发标记阶段里去完成，反正它们都是要遍历所有对象的，这样合并就节省了一次遍历对象图的开销。一旦所有指针都被修正之后， 原来记录新旧对象关系的转发表就可以释放掉了。 13.3.4、存在的问题，怎么解决 ZGC 最大的问题是浮动垃圾。ZGC 的停顿时间是在 10ms 以下，但是 ZGC 的执行时间还是远远大于这个时间的。假如 ZGC 全过程需要执行 10 分钟，在这个期间由于对象分配速率很高，将创建大量的新对象，这些对象很难进入当次 GC，所以只能在下次 GC 的时候进行回收，这些只能等到下次 GC 才能回收的对象就是浮动垃圾。ZGC 没有分代概念，每次都需要进行全堆扫描，导致一些 “朝生夕死” 的对象没能及时的被回收。 解决方案目前唯一的办法是增大堆的容量，使得程序得到更多的喘息时间，但是这个也是一个治标不治本的方案。如果需要从根本上解决这个问题，还是需要引入分代收集，让新生对象都在一个专门的区域中创建，然后专门针对这个区域进行更频繁、更快的收集。 13.3.5、安全点与安全区域 安全点就是指代码中一些特定的位置, 当线程运行到这些位置时它的状态是确定的, 这样 JVM 就可以安全的进行一些操作, 比如 GC 等，所以 GC 不是想什么时候做就立即触发的，是需要等待所有线程运行到安全点后才能触发。这些特定的安全点位置主要有以下几种: 方法返回之前 调用某个方法之后 抛出异常的位置 循环的末尾 大体实现思想是当垃圾收集需要中断线程的时候， 不直接对线程操作， 仅仅简单地设置一个标志位， 各个线程执行过程时会不停地主动去轮询这个标志， 一旦发现中断标志为真时就自己在最近的安全点上主动中断挂起。轮询标志的地方和安全点是重合的。 安全区域又是什么？ Safe Point 是对正在执行的线程设定的。如果一个线程处于 Sleep 或中断状态，它就不能响应 JVM 的中断请求，再运行到 Safe Point 上。因此 JVM 引入了 Safe Region。Safe Region 是指在一段代码片段中，引用关系不会发生变化。在这个区域内的任意地方开始 GC 都是安全的。 13.3.6、ZGC 参数 13.3.7、ZGC 触发时机ZGC 目前有 4 中机制触发 GC： 1、定时触发，默认为不使用，可通过 ZCollectionInterval 参数配置。2、预热触发，最多三次，在堆内存达到 10%、20%、30% 时触发，主要时统计 GC 时间，为其他 GC 机制使用。3、分配速率，基于正态分布统计，计算内存 99.9% 可能的最大分配速率，以及此速率下内存将要耗尽的时间点，在耗尽之前触发 GC（耗尽时间 - 一次 GC 最大持续时间 - 一次 GC 检测周期时间）。4、主动触发，（默认开启，可通过 ZProactive 参数配置） 距上次 GC 堆内存增长 10%，或超过 5 分钟时，对比距上次 GC 的间隔时间跟（49 * 一次 GC 的最大持续时间），超过则触发。 14、如何选择垃圾收集器 优先调整堆的大小让服务器自己来选择 如果内存小于 100M，使用串行收集器 如果是单核，并且没有停顿时间的要求，串行或 JVM 自己选择 如果允许停顿时间超过 1 秒，选择并行或者 JVM 自己选 如果响应时间最重要，并且不能超过 1 秒，使用并发收集器 4G 以下可以用 parallel，4-8G 可以用 ParNew+CMS，8G 以上可以用 G1，几百 G 以上用 ZGC JDK 1.8 默认使用 Parallel(年轻代和老年代都是)JDK 1.9 默认使用 G1 15、各种命令（例如 100%cpu 的排查、死锁的检查）15.1、100%CPU 的排查 1 、 使用 top 命令查看 cpu 占用资源较高的 PID 2、通过 jps 找到当前用户下的 java 程序 PID（jps -l 能够打印出所有的应用的 PID） 3、使用 pidstat -p 4、找到 cpu 占用较高的线程 TID 5、将 TID 转换为十六进制的表示方式 6、通过 jstack -l（使用 jstack 输出当前 PID 的线程 dunp 信息） 7、 查找 TID 对应的线程 (输出的线程 id 为十六进制)，找到对应的代码 15.2、死锁的检查 方法一、使用 jps + jstack 在 windons 命令窗口，使用 jps -l （找到运行的程序的 PID） 使用 jstack -l PID(上面的) 方法二：使用 jconsole 方法三：使用 Java Visual VM 16、JIT(即时编译器) JIT 是一种提高程序运行效率的方法。通常，程序有两种运行方式：静态编译与动态解释。静态编译的程序在执行前全部被翻译为机器码，而动态解释执行的则是一句一句边运行边翻译。 17、逃逸分析 逃逸分析是指在某个方法之内创建的对象，除了在方法体之内被引用之外，还在方法体之外被其它变量引用到；这样带来的后果是在该方法执行完毕之后，该方法中创建的对象将无法被 GC 回收，由于其被其它变量引用。正常的方法调用中，方法体中创建的对象将在执行完毕之后，将回收其中创建的对象；故由于无法回收，即成为逃逸。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"局部变量表中槽的性质引发的gc问题","slug":"jvm/局部变量表中槽的性质引发的gc问题","date":"2021-11-19T12:00:08.000Z","updated":"2022-03-23T09:03:57.761Z","comments":true,"path":"blog/jvm/局部变量表中槽的性质引发的gc问题/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E8%A1%A8%E4%B8%AD%E6%A7%BD%E7%9A%84%E6%80%A7%E8%B4%A8%E5%BC%95%E5%8F%91%E7%9A%84gc%E9%97%AE%E9%A2%98/","excerpt":"","text":"局部变量表局部变量表(Local Variables Table)是一组变量值的存储空间，用于存放方法参数和方法内部定义的局部变量。 局部变量表的容量以变量槽(Variable Slot)为最小单位。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小（slot的数量） 当一个方法被调用时，Java虚拟机会使用局部变量表来完成参数值到参数变量列表的传递过程， 即实参到形参的传递。如果执行的是实例方法(没有被static修饰的方法)，那局部变量表中第0位索引的变量槽默认是用于传递方法所属对象实例的引用，在方法中可以通过关键字“this”来访问到这个隐含的参数。其余参数则按照参数表顺序排列，占用从1开始的局部变量槽，参数表分配完毕后，再根据方法体内部定义的变量顺序和作用域分配其余的变量槽。 为了尽可能节省栈帧耗用的内存空间，局部变量表中的变量槽是可以重用的，方法体中定义的变量，其作用域并不一定会覆盖整个方法体，如果当前字节码PC计数器的值已经超出了某个变量的作用域，那这个变量对应的变量槽就可以交给其他变量来重用。不过，这样的设计除了节省栈帧空间以外，还会伴随有少量额外的副作用，例如在某些情况下变量槽的复用会直接影响到系统的垃圾收集行为。 如代码 1234567public class TestTT &#123; public static void main(String[] args) &#123; byte[] placeholder = new byte[64 * 1024 * 1024]; System.gc(); &#125;&#125; 编译后： 运行时加上-XX:+PrintGCjvm参数 没有回收掉placeholder所占的内存是能说得过去，因为在执行System.gc()时， 变量placeholder还处于作用域之内，虚拟机自然不敢回收掉placeholder的内存。修改下： 123456public static void main(String[] args) &#123; &#123; byte[] placeholder = new byte[64 * 1024 * 1024]; &#125; System.gc();&#125; placeholder所占的内存还是没有回收掉，再修改下 1234567public static void main(String[] args) &#123; &#123; byte[] placeholder = new byte[64 * 1024 * 1024]; &#125; int i=0; System.gc();&#125; placeholder所占的内存居然回收了！在解析前，先看javap编译后的class文件： 从上图可以看到，变量槽1，引用了变量i了，也就是说，placeholder能否被回收的根本原因就是：局部变量表中的变量槽不存在关于placeholder数组对象的引用！ 第一次修改中，代码虽然已经离开了placeholder的作用域，但在此之后，再没有发生过任何对局部变量表的读写操作，placeholder原本所占用的变量槽还没有被其他变量所复用，所以作为GC Roots一部分的局部变量表仍然保持着对它的关联。这种关联没有被及时打断，绝大部分情况下影响都很轻微。但如果遇到一个方法，其后面的代码有一些耗时很长的操作，而前面又定义了占用了大量内存但实际上已经不会再使用的变量，手动将其设置为null值(用来代替那句int a&#x3D;0，把变量对应的局部变量槽清空)便不见得是一个绝对无意义的操作，这种操作可以作为一种在极特殊情形(对象占用内存大、此方法的栈帧长时间不能被回收、方法调用次数达不到即时编译器的编译条件)下的“奇技”来使用。（注意括号内的前提条件） 第二次修改placeholder原本所占用的变量槽被i&#x3D;0占据，所以placeholder已经是不可达了，所以能被回收。 null操作在某些极端情况下确实是有用的，但不应当对赋null值操作有什么特别的依赖，更没有必要把它当作一个普遍的编码规则来推广。原因有两点，从编码角度讲，以恰当的变量作用域来控制变量回收时间才是最优雅的解决方法。而且，经过JVM即时编译器施加了各种编译优化措施以后，两者的差异就会非常大，赋null值的操作在经过即时编译优化后几乎是一定会被当作无效操作消除掉的，这时候将变量设置为null就是毫无意义的行为。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"java泛型","slug":"jvm/java泛型","date":"2021-11-19T12:00:07.000Z","updated":"2022-03-23T09:03:57.757Z","comments":true,"path":"blog/jvm/java泛型/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/java%E6%B3%9B%E5%9E%8B/","excerpt":"","text":"Java选择的泛型实现方式叫作“类型擦除式泛型”(Type Erasure Generics) Java 语言中的泛型，它只在程序源码中存在，在编译后的字节码文件中，就已经替换为原来的原生类型(Raw Type，也称为裸类 型)了，并且在相应的地方插入了强制转型代码，因此，对于运行期的 Java 语言来说，ArrayList与 ArrayList就是同一 个类，所以泛型技术实际上是 Java 语言的一颗语法糖，Java 语言中的泛型实现方法称为类型擦除，基于这种方法实现的泛型称为伪泛 型。 使用泛型注意事项(小甜点，了解即可，装 B 专用) 上面这段代码是不能被编译的，因为参数 List&lt;Integer&gt;和 List&lt;String&gt;编译之后都被擦除了，变成了一样的原生类型 List&lt;E&gt;，擦除动作导致这两种方法的特征签名变得一模一样(注意在 IDEA 中是不行的，但是 jdk 的编译器是可以，因为 jdk 是根据方法返回值+ 方法名+参数)。 关于泛型的其他问题从一个带范型的接口中发现了一个问题发现一个有意思的，以前没想过这个问题。 定了一个接口 123public interface TestBean&lt;T&gt; &#123; void print(T t);&#125; 实现类是 12345public class SubTestBean implements TestBean&lt;String&gt; &#123; @Override public void print(String s) &#123; &#125;&#125; 这是一个测试： 12345@Testpublic void test6() &#123; Method[] methods = ReflectionUtils.getDeclaredMethods(SubTestBean.class); Arrays.stream(methods).forEach(System.out::println);&#125; 打印的结果： 多了个print(Object)方法。 分析jvm的编译器对于范型是使用类型擦除的，也就是会把T变成Object，这时接口就变成了 123public interface TestBean&lt;Object&gt; &#123; void print(Object t);&#125; 那么实现类中的这个方法 1public void print(String s) &#123;&#125; 只是一个重载的方法，但由于接口的方法是必须在非抽象类中实现的，那么jvm就帮我们写了一个桥接方法，这个方法是这样定义的: 123public void print(Object s) &#123; this.print((String)s);&#125; 所以对于方法 1public void print(String s) &#123;&#125; 这个只是一个重载的方法，重载了桥接方法。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"Stream操作分类","slug":"jvm/Stream操作分类","date":"2021-11-19T12:00:06.000Z","updated":"2022-03-23T09:03:57.753Z","comments":true,"path":"blog/jvm/Stream操作分类/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/Stream%E6%93%8D%E4%BD%9C%E5%88%86%E7%B1%BB/","excerpt":"","text":"介绍和基础官方将 Stream 中的操作分为两大类:终结操作(Terminal operations)和中间操作(Intermediate operations)。 中间操作会返回一个新的流，一个流可以后面跟随零个或多个中间操作。其目的主要是打开流，做出某种程度的数据映射&#x2F;过滤，然后会返回一个新的流，交给下一个操作使用。这类操作都是惰性化的(lazy)，就是说，仅仅调用到这类方法，并没有真正开始流的 遍历。而是在终结操作开始的时候才真正开始执行。 中间操作又可以分为：无状态(Stateless)与有状态(Stateful)操作。 无状态是指元素的处理不受之前元素的影响， 有状态是指该 操作只有拿到所有元素之后才能继续下去。 终结操作是指返回最终的结果。一个流只能有一个终结操作，当这个操作执行后，这个流就被使用“光”了，无法再被操作。所以这必定这个流的最后一个操作。终结操作的执行才会真正开始流的遍历，并且会生成一个结果。 终结操作又可以分为：短路(Short-circuiting)与非短路(Unshort-circuiting)操作。 短路是指遇到某些符合条件的元素就可以得到最终结果， 非短路是指必须处理完所有元素才能得到最终结果。操作分类详情如下图所示: 因为 Stream 操作类型非常多，总结一下常用的 map():将流中的元素进行再次加工形成一个新流，流中的每一个元素映射为另外的元素。 filter(): 返回结果生成新的流中只包含满足筛选条件的数据 limit():返回指定数量的元素的流。返回的是Stream里前面的n个元素。 skip():和limit()相反，将前几个元素跳过(取出)再返回一个流，如果流中的元素小于或者等于n，就会返回一个空的流。 sorted():将流中的元素按照自然排序方式进行排序。 distinct():将流中的元素去重之后输出。 peek():对流中每个元素执行操作，并返回一个新的流，返回的流还是包含原来流中的元素。 代码事例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public class StuWithStream &#123; public static void main(String[] args) &#123; List&lt;Student&gt; studentList =Datainit(); groupBy(studentList); // filter(studentList); total(studentList); MaxAndMin(studentList); &#125; public static List&lt;Student&gt; Datainit()&#123; List&lt;Student&gt; students = Arrays.asList( new Student(&quot;小明&quot;, 168, &quot;男&quot;), new Student(&quot;大明&quot;, 182, &quot;男&quot;), new Student(&quot;小白&quot;, 174, &quot;男&quot;), new Student(&quot;小黑&quot;, 186, &quot;男&quot;), new Student(&quot;小红&quot;, 156, &quot;女&quot;), new Student(&quot;小黄&quot;, 158, &quot;女&quot;), new Student(&quot;小青&quot;, 165, &quot;女&quot;), new Student(&quot;小紫&quot;, 172, &quot;女&quot;)); return students; &#125; //Stream实现分组 public static void groupBy(List&lt;Student&gt; studentsList)&#123; Map&lt;String, List&lt;Student&gt;&gt; groupBy = studentsList .stream() .collect(Collectors.groupingBy(Student::getSex)); System.out.println(&quot;分组后：&quot;+groupBy); &#125; //Stream实现过滤 public static void filter(List&lt;Student&gt; studentsList)&#123; List&lt;Student&gt; filter = studentsList .stream() .filter(student-&gt;student.getHeight()&gt;180) .collect(Collectors.toList()); System.out.println(&quot;过滤后：&quot;+filter); &#125; //Stream实现求和 public static void total(List&lt;Student&gt; studentsList)&#123; int totalHeight = studentsList .stream() .mapToInt(Student::getHeight) .sum(); System.out.println(totalHeight); &#125; //Stream找最大和最小 public static void MaxAndMin(List&lt;Student&gt; studentsList)&#123; int maxHeight = studentsList .stream() .mapToInt(Student::getHeight) .max() .getAsInt(); System.out.println(&quot;max:&quot;+maxHeight); int minHeight = studentsList .stream() .mapToInt(Student::getHeight) .min() .getAsInt(); System.out.println(&quot;min:&quot;+minHeight); &#125; static class Student &#123; private String name; private int height; private String sex; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getHeight() &#123; return height; &#125; public void setHeight(int height) &#123; this.height = height; &#125; public String getSex() &#123; return sex; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public Student( String name, int height,String sex) &#123; this.sex = sex; this.name = name; this.height = height; &#125; &#125;&#125;","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"05后端编译与优化","slug":"jvm/05后端编译与优化","date":"2021-11-19T12:00:05.000Z","updated":"2022-03-23T09:03:57.750Z","comments":true,"path":"blog/jvm/05后端编译与优化/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/05%E5%90%8E%E7%AB%AF%E7%BC%96%E8%AF%91%E4%B8%8E%E4%BC%98%E5%8C%96/","excerpt":"","text":"即时编译器Java程序最初都是通过解释器（Interpreter）进行解释执行的，当虚拟机发现某个方法或代码块的运行特别频繁，就会把这些代码认定为“热点代码”（Hot Spot Code），为了提高热点代码的执行效率，在运行时，虚拟机将会把这些代码编译成本地机器码，并以各种手段尽可能地进行代码优化，运行时完成这个任务的后端编译器被称为即时编译器 编译器优化技术 内容已经非常简化了，但是仍有不少优化的空间第一个要进行的优化是方法内联：去除方法调用的成本(如查找方法版本、建立栈帧等) 第二步进行冗余**访问消除(Redundant Loads Elimination)**：假设代码中间注释掉的“…do stuff…”所代表的操作不会改变b.value的值，那么就可以把“z&#x3D;b.value”替换为“z&#x3D;y”，因为上一句“y&#x3D;b.value”已经保证了变量y与b.value是一致的，这样就可以不再去访问对象b的局部变量了。如果把b.value看作一个表达式，那么也可以把这项优化看作一种公共子表达式消除(Common Subexpression Elimination)。 第三步进行**复写传播(Copy Propagation)**：因为这段程序的逻辑之中没有必要使用一个额外的变量z，它与变量y是完全相等的，因此我们可以使用y来代替z 第四步进行**无用代码消除(Dead Code Elimination)**：无用代码可能是永远不会被执行的代码，也可能是完全没有意义的代码。因此它又被很形象地称为“Dead Code”，在代码清单11-9中，“y&#x3D;y”是没有意义的 上面这些都可以由即时编译器去优化 逃逸分析逃逸分析的基本原理是：分析对象动态作用域，当一个对象在方法里面被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他方法中，这种称为方法逃逸;甚至还有可能被外部线程访问到，譬如赋值给可以在其他线程中访问的实例变量，这种称为线程逃逸、从不逃逸、方法逃逸到线程逃逸，称为对象由低到高的不同逃逸程度。 如果能证明一个对象不会逃逸到方法或线程之外(换句话说是别的方法或线程无法通过任何途径访问到这个对象)，或者逃逸程度比较低(只逃逸出方法而不会逃逸出线程)，则可能为这个对象实例采取不同程度的优化。 JVM中使用-XX:+DoEscapeAnalysis来手动开启逃逸分析， 栈上分配在Java虚拟机中，Java堆上分配创建对象的内存空间几乎是Java程序员都知道的常识，Java堆中的对象对于各个线程都是共享和可见的，只要持有这个对象的引用，就可以访问到堆中存储的对象数据。虚拟机的垃圾收集子系统会回收堆中不再使用的对象，但回收动作无论是标记筛选出可回收对象，还是回收和整理内存，都需要耗费大量资源。如果确定一个对象不会逃逸出线程之外，那让这个对象在栈上分配内存将会是一个很不错的主意，对象所占用的内存空间就可以随栈帧出栈而销毁。在一般应用中，完全不会逃逸的局部对象和不会逃逸出线程的对象所占的比例是很大的，如果能使用栈上分配，那大量的对象就会随着方法的结束而自动销毁了，垃圾收集子系统的压力将会下降很多。栈上分配可以支持方法逃逸，但不能支持线程逃逸。 标量替换一个数据已经无法再分解成更小的数据来表示了，Java虚拟机中的原始数据类型(int、long等数值类型及reference类型等)都不能再进一步分解了，那么这些数据就可以被称为标量。相对的，如果一个数据可以继续分解，那它就被称为聚合量(Aggregate)，Java 中的对象就是典型的聚合量。如果把一个Java对象拆散，根据程序访问的情况，将其用到的成员变量恢复为原始类型来访问，这个过程就称为标量替换。假如逃逸分析能够证明一个对象不会被方法外部访问，并且这个对象可以被拆散，那么程序真正执行的时候将可能不去创建这个对象，而改为直接创建它的若干个被这个方法使用的成员变量来代替。将对象拆分后，除了可以让对象的成员变量在栈上(栈上存储的数据，很大机会被虚拟机分配至物理机器的高速寄存器中存储)分配和读写之外，还可以为后续进一步的优化手段创建条件。标量替换可以视作栈上分配的一种特例，实现更简单(不用考虑整个对象完整结构的分配)，但对逃逸程度的要求更高，它不允许对象逃逸出方法范围内。 JVM中使用-XX:+EliminateAllocations来开启标量替换 同步消除线程同步本身是一个相对耗时的过程，如果逃逸分析能够确定一个变量不会逃逸出线程，无法被其他线程访问，那么这个变量的读写肯定就不会有竞争， 对这个变量实施的同步措施也就可以安全地消除掉。 JVM中使用+XX:+EliminateLocks来开启同步消除","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"04对象的访问定位","slug":"jvm/04对象的访问定位","date":"2021-11-19T12:00:04.000Z","updated":"2022-03-23T09:03:57.748Z","comments":true,"path":"blog/jvm/04对象的访问定位/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/04%E5%AF%B9%E8%B1%A1%E7%9A%84%E8%AE%BF%E9%97%AE%E5%AE%9A%E4%BD%8D/","excerpt":"","text":"主流的访问方式主要有使用句柄和直接指针两种，HotSpot使用的是直接指针 句柄Java堆中将可能会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自具体的地址信息 使用句柄来访问的最大好处就是reference中存储的是稳定句柄地址,在对象被移动(垃圾收集时移动对象是非常普遍的行为)时只会改变句柄中的实例数据指针,而reference本身不需要被修改。 直接指针直接指针访问的话，Java堆中对象的内存布局就必须考虑如何放置访问类型数据的相关信息，reference中存储的直接就是对象地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销 用直接指针来访问最大的好处就是速度更快,它节省了一次指针定位的时间开销,由于对象访问在Java中非常频繁,因此这类开销积少成多也是一项极为可观的执行成本。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"03对象的内存布局","slug":"jvm/03对象的内存布局","date":"2021-11-19T12:00:03.000Z","updated":"2022-03-23T09:03:57.746Z","comments":true,"path":"blog/jvm/03对象的内存布局/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/03%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80/","excerpt":"","text":"在HotSpot虚拟机里，对象在堆内存中的存储布局可以划分为三个部分:对象头(Header)、实例数据(Instance Data)和对齐填充(Padding)。 HotSpot虚拟机对象的对象头部分包括两类信息。 第一类是用于存储对象自身的运行时数据，如哈希码(HashCode)、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据的长度在32位和64位的虚拟机(未开启压缩指针)中分别为32个比特和64个比特，官方称它为“Mark Word”，这是一种动态定义的数据结构。例如在32位的HotSpot虚拟机中，如对象未被同步锁锁定的状态下，Mark Word的32个比特存储空间中的25个比特用于存储对象哈希码，4个比特用于存储对象分代年龄，2个比特用于存储锁标志位，1个比特固定为0，在其他状态(轻量级锁定、重量级锁定、GC标记、可偏向)下对象的存储内容如表2-1所示。 对象头的另外一部分是类型指针，即对象指向它的类型元数据的指针，Java虚拟机通过这个指针来确定该对象是哪个类的实例。如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据。 接下来实例数据部分是对象真正存储的有效信息，即我们在程序代码里面所定义的各种类型的字段内容，无论是从父类继承下来的，还是在子类中定义的字段都必须记录起来。这部分的存储顺序会受到虚拟机分配策略参数（-XX：FieldsAllocationStyle参数）和字段在Java源码中定义顺序的影响。HotSpot虚拟机默认的分配顺序为longs&#x2F;doubles、ints、shorts&#x2F;chars、bytes&#x2F;booleans、oops（Ordinary Object Pointers，OOPs），从以上默认的分配策略中可以看到，相同宽度的字段总是被分配到一起存放，在满足这个前提条件的情况下，在父类中定义的变量会出现在子类之前。如果HotSpot虚拟机的+XX：CompactFields参数值为true（默认就为true），那子类之中较窄的变量也允许插入父类变量的空隙之中，以节省出一点点空间。 对齐填充，这并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。由于HotSpot虚拟机的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说就是任何对象的大小都必须是8字节的整数倍。对象头部分已经被精心设计成正好是8字节的倍数（1倍或者2倍），因此，如果对象实例数据部分没有对齐的话，就需要通过对齐填充来补全。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"02对象的创建","slug":"jvm/02对象的创建","date":"2021-11-19T12:00:02.000Z","updated":"2022-03-23T09:03:57.745Z","comments":true,"path":"blog/jvm/02对象的创建/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/02%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%88%9B%E5%BB%BA/","excerpt":"","text":"当Java虚拟机遇到一条字节码new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 在类加载检查通过后,接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定。这是只需要在java堆中划分一块固定大小的内存即可。有两种情况 假设Java堆中内存是绝对规整的，所有被使用过的内存都被放在一边，空闲的内存被放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间方向挪动一段与对象大小相等的距离，这种分配方式称为“指针碰撞”(Bump The Pointer)。 但如果Java堆中的内存并不是规整的，已被使用的内存和空闲的内存相互交错在一起，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为“空闲列表”(Free List)。 选择哪种分配方式由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有空间压缩整理(Compact)的能力决定。 划分堆空间不是线程安全的，有两种解决方案 使用cas+失败重试的方式 一种是把内存分配的动作按照线程划分在不同的空间之中进行每个线程在java堆中预分配一块小内存，称为本地线程分配缓冲(Thread Local Allocation Buffer，TLAB)，哪个线程要分配内存，就在哪个线程的本地缓冲区中分配，只有本地缓冲区用完了，分配新的缓存区时才需要同步锁定。虚拟机是否使用TLAB，可以通过-XX:+&#x2F;-UseTLAB参数来设定。 内存分配完成之后，虚拟机必须将分配到的内存空间（但不包括对象头）都初始化为零值，如果使用了TLAB的话，这一项工作也可以提前至TLAB分配时顺便进行。这步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，使程序能访问到这些字段的数据类型所对应的零值 接下来，Java虚拟机还要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码(实际上对象的哈希码会延后到真正调用Object::hashCode()方法时才计算)、对象的GC分代年龄等信息。这些信息存放在对象的对象头(Object Header)之中。根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 最后，会执行构造函数（即Class文件中的()方法），按程序员的意愿对对象尽心初始化。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"01运行时数据区域","slug":"jvm/01运行时数据区域","date":"2021-11-19T12:00:01.000Z","updated":"2022-03-23T09:03:57.744Z","comments":true,"path":"blog/jvm/01运行时数据区域/","link":"","permalink":"http://sv.pointcut.cc/blog/jvm/01%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/","excerpt":"","text":"Topic 1.JVM运行时数据区里有什么？ Topic 2. 虚拟机栈是什么？虚拟机栈里有什么？ Topic 3. 栈帧是什么？栈帧里有什么？动态链接时什么？比如执行一个main方法时动态链接就是一个指针，指向了方法区中对应main方法的执行信息 Topic 4. 方法区是什么？方法区里有什么？ 程序计数器程序计数器(Program Counter Register)是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。 由于Java虚拟机的多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器(对于多核处理器来说是一个内核)都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 Java虚拟机栈与程序计数器一样，Java虚拟机栈(Java Virtual Machine Stack)也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的线程内存模型:每个方法被执行的时候，Java虚拟机都会同步创建一个栈帧(Stack Frame)用于存储局部变量表、操作数栈、动态连接、方法出口等信息。每一个方法被调用直至执行完毕的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程 局部变量局部变量表（Local Variables Table）存放了编译期可知的各种Java虚拟机基本数据类型、对象引用和returnAddress 类型(指向了一条字节码指令的地址)。 局部变量表（Local Variables Table）是一组变量值的存储空间，用于存放方法参数和方法内部定义的局部变量。在Java程序被编译为Class文件时，就在方法的Code属性的max_locals数据项中确定了该方法所需分配的局部变量表的最大容量。 这些数据类型在局部变量表中的存储空间以局部变量槽(Slot)来表示，其中64位长度的long和double类型的数据会占用两个变量槽，其余的数据类型只占用一个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的slot的数量。 在编译Java程序源码的时候，栈帧中需要多大的局部变量表，需要多深的操作数栈就已经被分析计算出来，并且写入到方法表的Code属性之中。换言之，一个栈帧需要分配多少内存，并不会受到程序运行期变量数据的影响，而仅仅取决于程序源码和具体的虚拟机实现的栈内存布局形式。 局部变量表详情 操作数栈操作数栈(Operand Stack)也常被称为操作栈，它是一个后入先出(Last In First Out，LIFO) 栈。同局部变量表一样，操作数栈的最大深度也在编译的时候被写入到Code属性的max_stacks数据项之中。操作数栈的每一个元素都可以是包括long和double在内的任意Java数据类型。32位数据类型所占的栈容量为1，64位数据类型所占的栈容量为2。Javac编译器的数据流分析工作保证了在方法执行的任何时候，操作数栈的深度都不会超过在max_stacks数据项中设定的最大值。 当一个方法刚刚开始执行的时候，这个方法的操作数栈是空的，在方法的执行过程中，会有各种字节码指令往操作数栈中写入和提取内容，也就是出栈和入栈操作。譬如在做算术运算的时候是通过将运算涉及的操作数栈压入栈顶后调用运算指令来进行的，又譬如在调用其他方法的时候是通过操作数栈来进行方法参数的传递。举个例子，例如整数加法的字节码指令iadd，这条指令在运行的时候要求操作数栈中最接近栈顶的两个元素已经存入了两个int型的数值，当执行这个指令时，会把这两个int 值出栈并相加，然后将相加的结果重新入栈。 另外在概念模型中，两个不同栈帧作为不同方法的虚拟机栈的元素，是完全相互独立的。但是在大多虚拟机的实现里都会进行一些优化处理，令两个栈帧出现一部分重叠。让下面栈帧的部分操作数栈与上面栈帧的部分局部变量表重叠在一起，这样做不仅节约了一些空间，更重要的是在进行方法调用时就可以直接共用一部分数据，无须进行额外的参数复制传递了。 动态连接每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接(Dynamic Linking)。通过“类文件结构”的讲解，我们知道Class文件的常量池中存有大量的符号引用，字节码中的方法调用指令就以常量池里指向方法的符号引用作为参数。这些符号引用一部分会在类加载阶段或者第一次使用的时候就被转化为直接引用，这种转化被称为静态解析。另外一部分将在每一次运行期间都转化为直接引用，这部分就称为动态连接。 方法返回地址方法退出的过程实际上等同于把当前栈帧出栈，因此退出时可能执行的操作有：恢复上层方法的局部变量表和操作数栈，把返回值（如果有的话）压入调用者栈帧的操作数栈中，调整PC计数器的值以指向方法调用指令后面的一条指令等。 本地方法栈本地方法栈(Native Method Stacks)与虚拟机栈所发挥的作用是非常相似的，其区别只是虚拟机栈为虚拟机执行Java方法(也就是字节码)服务，而本地方法栈则是为虚拟机使用到的本地(Native) 方法服务。 Java堆Java堆(Java Heap)是虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，但并不表示所有的对象都在堆中创建，如果经过逃逸分析，确定对象只在方法内有效，那对象会在虚拟机栈中创建对象。 Java堆是垃圾收集器管理的内存区域，从回收内存的角度看，由于现代垃圾收集器大部分都是基于分代收集理论设计的，所以Java堆中经常会出现“新生代”“老年代”“永久代”“Eden空间”“From Survivor空间”“To Survivor空间”等名词。（不过在G1出现以后，有些回收期已经不遵循这种分代收集理论了） 从分配内存的角度看，所有线程共享的Java堆中可以划分出多个线程私有的分配缓冲区(Thread Local Allocation Buffer，TLAB)，以提升对象分配时的效率。 将Java 堆细分的目的只是为了更好地回收内存，或者更快地分配内存 方法区方法区(Method Area)与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。 在jdk8以前，HotSpot虚拟机团队，使用永久代实现了方法区，这样使得HotSpot的垃圾收集器能够像管理Java堆一样管理这部分内存，省去专门为方法区编写内存管理代码的工作。JDK8后废弃了永久代的概念，使用元空间（Meta-space）替代了方法区。 运行时常量池运行时常量池(Runtime Constant Pool)是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池表(Constant Pool Table)，用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。 直接内存直接内存(Direct Memory)并不是虚拟机运行时数据区的一部分，也不是《Java虚拟机规范》中定义的内存区域。在NIO中经常使用，目的是为了实现零拷贝，减少数据从内存复制到JVM堆中。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"}]},{"title":"并发问题","slug":"juc/并发问题","date":"2021-11-18T12:00:14.000Z","updated":"2022-03-23T09:03:57.735Z","comments":true,"path":"blog/juc/并发问题/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98/","excerpt":"","text":"S，ynchronized 用过吗，其原理是什么?Synchronized 是由 JVM 实现的一种实现互斥同步的一种方式，如果查看被 Synchronized 修饰过的代码块编译后的字节码，会发现，被 Synchronized 修饰过的代码块，在编译前后被编译器生成了monitorenter 和 monitorexit 两个字节码指令 。 在虚拟机执行到 monitorenter 指令时，首先要尝试获取对象的锁: 如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁，把锁的计数器+1;当执行 monitorexit 指令时将锁计数器-1;当计数器 为 0 时，锁就被释放了。 如果获取对象失败了，那当前线程就要阻塞等待，直到对象锁被另外一 个线程释放为止。 还有一点，编译器还会生成一张Exception table，代表着获取锁成功，但代码执行错误后会释放锁。 如果是修饰方法，这又有不同了。 你刚才提到获取对象的锁，这个“锁”到底是什么?如何确定 对象的锁?“锁”的本质其实是一个 Reference 类型的参数，即要锁定和解锁的对象。我们知道，使用Synchronized 可以修饰不同的对象，因此，对应的对象锁可以这么确定。 如果 Synchronized 明确指定了锁对象 如果修饰的是方法，那锁对象为方法对应的对象 如果修饰静态方法，那锁对象为该类的class对象。 JVM 对 Java 的原生锁做了哪些优化?在jdk1.6中，引入了偏向锁(Biased Locking)、轻量级锁和重量级锁 在没有竞争时使用偏向锁： 这是jvm会通过cas操作在对象头上的 Mark Word 部分设置线程 ID，以表示这个对象偏向于当前线程，所以并不涉及真正的互斥锁。这样可以在某些情况下降低无竞争开销 如果有另一线程试图锁定某个被偏斜过的对象，JVM 就撤销偏斜锁， 切换到轻量级锁实现。 这是jvm会通过循环cas操作Mark Word 来试图获取锁，如果重试成功， 就使用普通的轻量级锁;否则，进一步升级为重量级锁。 重量级锁的情况就是直接堵塞了，等待被唤醒。 什么是锁消除和锁粗化?锁消除： 对一些代码上要求同步，但被编译器检查到不可能存在竞争，这是会消除通过操作。 锁粗化： 原则上，同步块的作用范围要尽量小。但是如果一系列的连续 操作都对同一个对象反复加锁和解锁，甚至加锁操作在循环体内，频繁 地进行互斥同步操作也会导致不必要的性能损耗。 锁粗化就是增大锁的作用域。 为什么说 Synchronized 是一个悲观锁?乐观锁的实现原理 又是什么?什么是 CAS，它有什么特性?Synchronized 显然是一个悲观锁，因为它的并发策略是悲观的: 不管是否会产生竞争，任何的数据操作都必须要加锁 乐观锁的策略是，对于共享资源，并需要加锁，如果存在竞争就用cas，如果cas失败就需要重试。 乐观锁一定就是好的吗?乐观锁避免了悲观锁独占对象的现象，同时也提高了并发性能，但它也 有缺点: 长时间自旋可能导致开销大 可能存在ABA 问题","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"ThreadPoolExecutor","slug":"juc/ThreadPoolExecutor","date":"2021-11-18T12:00:13.000Z","updated":"2022-03-23T09:03:57.731Z","comments":true,"path":"blog/juc/ThreadPoolExecutor/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/ThreadPoolExecutor/","excerpt":"","text":"完整的构造函数 123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; corePoolSize核心线程数，代表这个线程池可以保持的线程数，即使处于空闲状态，也不会中断线程（除非设置了allowCoreThreadTimeOut）。 maximumPoolSize池中允许的最大线程数。 keepAliveTime当线程数大于内核数时，这是多余的空闲线程在终止之前等待新任务的最长时间。 workQueue在执行任务之前用于保留任务的队列。 RejectedExecutionHandler handler这个是拒绝策略，在队列满后而且不能再创建线程的情况下，执行的操作。我们一样使用默认的就好了，默认的就是直接抛错。 ThreadFactory threadFactory这个是创建线程的工厂，这里建议使用默认的，因为如果用自己实现的ThreadFactory，如果设计不合理可能会导致addWorker方法抛错或者一直返回false。 从构造函数可知maximumPoolSize &lt; corePoolSize这种情况是不允许的。 之前面试的时候面试官问了这样一个问题maximumPoolSize &lt; corePoolSize的时候会发生什么？ 由于当时已经很久没看ThreadPoolExecutor的源码，有点忘记了，所以我当时回答，这种情况允许吗？我觉得这种情况是不允许的。 但面试官却说这种情况会发生一些有趣的事，让我回去看下源码。 现在看了源码了，知道我当时的回答是正确的，现在回想起来，我觉得面试官都不知道会抛出异常（因为根据之前回答，面试官都会给出一个正确或错误的回复的） 线程池的状态和线程的数量1private static final int COUNT_BITS = Integer.SIZE - 3; #29 这个常量表示int中有效的位数，这个值为29 1private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; 这个表示最大的线程数量，这里 1&lt;&lt;COUNT_BITS后，第30位位0，-1之后就变成了前29位位1了。 123private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125; 首先这里有个重要的参数ctl，这个就是用来保存线程的数量和线程池的状态的。由于ctlOf(RUNNING, 0)中RUNNING | 0返回的是RUNNING，所以默认值是 线程池的状态为RUNNING 线程的数量为0 12345private static final int RUNNING = -1 &lt;&lt; COUNT_BITS; private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS; #0的二进制是00private static final int STOP = 1 &lt;&lt; COUNT_BITS; #1的二进制是01private static final int TIDYING = 2 &lt;&lt; COUNT_BITS; #0的二进制是10private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; #0的二进制是11 状态。从上面的定义可以看到，一个小于0的数字表示的是RUNNING，而32位是符号位，所用使用了31和30位来表示状态。下面说下这些状态代表的意思： RUNNING：接受新任务并处理排队的任务 SHUTDOWN：不接受新任务，但会处理排队的任务，而且还可能会创建新的线程来处理任务。 STOP：不接受新任务，不处理排队任务和中断进行中的任务 TIDYING：所有任务都已终止，workerCount为零，线程池转换为状态TIDYING 并运行terminated()这个钩子方法 TERMINATED：terminated()已完成 这些状态并不需要都达到。但状态变化的顺序一定是按照上边的顺序来的。这里贴一下源码中的注释： 123456789101112131415/** RUNNING -&gt; SHUTDOWN* On invocation of shutdown(), perhaps implicitly in finalize()* (RUNNING or SHUTDOWN) -&gt; STOP* On invocation of shutdownNow()* SHUTDOWN -&gt; TIDYING* When both queue and pool are empty* STOP -&gt; TIDYING* When pool is empty* TIDYING -&gt; TERMINATED* When the terminated() hook method has completed** Threads waiting in awaitTermination() will return when the* state reaches TERMINATED.*/ 1234private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;private static boolean isRunning(int c) &#123; return c &lt; SHUTDOWN; &#125; 上面这3个方法在源码中是经常使用的 runStateOf用来返回线程的状态。~CAPACITY的意思就是把31和30位变为1，其他为变为0。 workerCountOf用来放回线程的数量。 isRunning用来判断线程是否运行中。 123456private boolean compareAndIncrementWorkerCount(int expect) &#123; return ctl.compareAndSet(expect, expect + 1);&#125;private boolean compareAndDecrementWorkerCount(int expect) &#123; return ctl.compareAndSet(expect, expect - 1);&#125; 还有两个重要的方法compareAndIncrementWorkerCount和compareAndDecrementWorkerCount，这两个方法的行为是相反的。 compareAndIncrementWorkerCount：这个方法的作用就是，当添加一个线程后，ctl这参数就要加1。 compareAndDecrementWorkerCount：当减少一个线程后，ctl这参数就要加1。 核心代码——execute在说核心方法时先，先分析下方法addWorker 核心代码——addWorker123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; addWorker这个方法有两个入参，第一个入参Runnable firstTask就是通过execute方法传入的任务。第二个参数结合源码说明。从方法的名字可以看出就是添加Worker的。 true表示添加成功 false表示添加失败 那Worker是什么？分析完源码就知道了。 addWorker的源码可以分为两部分，一部分是判断是否可以添加线程，另一个部分是去创建线程。下面对这两部进行讲解。 第一部分12345678910111213141516171819202122232425262728retry:for (;;) &#123; //前面讲过ctl分别表示线程的数量和线程池的状态 int c = ctl.get(); //这里返回的是线程池的状态 int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; //线程数量 int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125;&#125; retry:表示这个循环的名字。 rs代表着线程池的状态。 把第一个if的判断拆分下： rs &gt;&#x3D; SHUTDOWN &amp;&amp; rs !&#x3D; SHUTDOWN rs &gt;&#x3D; SHUTDOWN &amp;&amp; firstTask !&#x3D; null rs &gt;&#x3D; SHUTDOWN &amp;&amp; workQueue.isEmpty() 而且只要满足其中一个条件，addWorker方法就会放回false。 从这些条件中可以看到得出两个结论： 只需要线程池的状态 &gt; SHUTDOWN，那肯定不会去创建线程了。 只要线程池的状态为SHUTDOWN，而且第一个入参数为null，那就有可能创建新的线程来处理任务。 wc表示的是线程数量 在里面的for循环中，可以看到core ? corePoolSize : maximumPoolSize，这一段代码，通过这段代码我们就知道了addWorker的第二个参数的意思了： core&#x3D;&#x3D;true时，判断线程数量是否大于corePoolSize core&#x3D;&#x3D;false时，判断线程数量是否大于maximumPoolSize 里面的for循环其实做了三件事： 判断当前的线程数量是否大于CAPACITY、corePoolSize或者maximumPoolSize。只要满足一个，就表示线程池已经不能再创建线程了，所以直接放回false 添加线程数量计数器的值，也就是执行了compareAndIncrementWorkerCount这个cas操作。成功的情况，下就表示需要去创建线程，所以直接结束着外出的for。而失败也就意味着有线程的竞争，这时就需要处理 当出现线程竞争的时候，重新获取下ctl的值，并比较下两次的状态是否相同，不同的话就意味着存在线程池状态的竞争，这时直接结束外层for的这次循环，重新进行新一次循环。相同的话也就意味着只是单纯的创建线程的竞争，所以继续执行循环cas就好了。 第二部分1234567891011121314151617181920212223242526272829303132333435363738boolean workerStarted = false;boolean workerAdded = false;Worker w = null;try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125;&#125; finally &#123; if (! workerStarted) addWorkerFailed(w);&#125;return workerStarted; 走到这个方法也就意味着compareAndIncrementWorkerCount这个cas操作成功了，这时就会去创建线程。 先看下Worker的构造函数： 1234567Runnable firstTask;final Thread thread;Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this);&#125; getThreadFactory这个会返回一个创建线程的工厂类，是一个扩展，不过我们可以使用默认的。 getThreadFactory().newThread(this)可以看成new Thread(this) 可以看到，这个Worker就是一个Thread的包装，而且这个Worker同时也是线程线程的Runnable，所以如果要了解线程是怎么工作的，就看这个类的run方法就好了。还有一点，firstTask这个属性代表的就是这个线程的第一个任务，也就是线程会先执行firstTask这个Runnable，然后才会从队列中拿Runnable。下面回到源码。 第二部分的代码就做了就做了三件事： 新建一个新建的Worker对象，并通过ThreadFactory获取一个线程。 在1成功的基础上，把新建的Worker对象放入到一个workers集合中，执行Thread.start方法。 在1或2失败的情况下，才会走到这一步。而什么情况1会失败呢？看Worker的构造函数，线程是通过ThreadFactory获取的，这个是线程池提供的一个扩展，如果使用了自定义的，那就可能导致1情况失败。而2会失败，也是因为同一个原因。线程是通过ThreadFactory获取，那就有可能新建的线程其实已经开始了已经结束了，所以在源码中有这一块的代码，去判断线程的状态。 12if (t.isAlive()) throw new IllegalThreadStateException(); 在这个步骤中，最后会执行addWorkerFailed： 123456789101112private void addWorkerFailed(Worker w) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (w != null) workers.remove(w); decrementWorkerCount(); tryTerminate(); &#125; finally &#123; mainLock.unlock(); &#125;&#125; 这个的逻辑很简单，就是做回滚操作的。不过tryTerminate这个方法先不要探究它的细节，因为这个方法和execute的逻辑是无光的。 总结检查是否可以针对当前池状态和给定的界限（核心或最大值）添加新的Worker。如果可以添加，则将调整线程数量，并在可能的情况下创建并启动一个新的Worker，并将firstTask作为其第一个任务运行。如果池已停止或可以关闭，则此方法返回false。如果在创建Worker对象的时候，ThreadFactory无法创建线程，则它还将返回false。如果线程创建失败，或者是由于线程工厂返回了 null，或者是由于异常（通常是Thread.start()中的OutOfMemoryError）导致的，就会回滚。 核心方法——execute123456789101112131415161718192021222324252627282930313233343536373839public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); /* * Proceed in 3 steps: * * 1. If fewer than corePoolSize threads are running, try to * start a new thread with the given command as its first * task. The call to addWorker atomically checks runState and * workerCount, and so prevents false alarms that would add * threads when it shouldn&#x27;t, by returning false. * * 2. If a task can be successfully queued, then we still need * to double-check whether we should have added a thread * (because existing ones died since last checking) or that * the pool shut down since entry into this method. So we * recheck state and if necessary roll back the enqueuing if * stopped, or start a new thread if there are none. * * 3. If we cannot queue task, then we try to add a new * thread. If it fails, we know we are shut down or saturated * and so reject the task. */ int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command);&#125; 通过上面我们已经了解了addWorker方法，那这里就说明下这段代码中的3种情况： 当线程的数量小于corePoolSize的时候，会去创建一个Worker，并把该任务command作为Worker的第一个任务。但可能由于线程竞争（状态）或者ThreadFactory创建的Thread有问题，导致创建Worker失败了，那么这时会走另外两种情况中的一种。 当线程的数量&gt;&#x3D;corePoolSize或者addWorker返回false的情况下，如果线程池在执行execute瞬间，状态还是RUNNING，而且任务添加队列成功了，那么这时有两种情况。 这时有可能线程池的状态发生改变了（线程竞争导致的），那么就会从队列中移除该任务，而且还是执行决绝逻辑。 在1不符合的情况下，而且线程的数量还是0，就执行addWorker(null, false)，也就是会再次尝试添加线程。发生这种情况的可能性就只有使用了自定的ThreadFactory，并且ThreadFactory.newThread返回了null（看addWorker的源码）。 在1和2都失败的情况下，就会再执行addWorker(command, false)，由于第二个参数传入了false，也就是说，会根据maximumPoolSize去创建额外的线程来执行任务。如果创建失败了就会执行拒绝策略。 上面的说明中，包含的一些特殊的情况，下面会把这些特殊情况去掉，总结出execute的核心逻辑。 在线程池的状态为RUNNING的时候： 1234567891011121314151617181920212223242526st=&gt;start: executecond1=&gt;condition: 当线程的数量小于corePoolSize？state1_y=&gt;operation: 创建Worker对象，并把传入的任务作为该线程的第一个任务。cond2=&gt;condition: 将任务放入到队列中是否成功？cond3=&gt;condition: 当线程的数量是否小于maximumPoolSize？state3_y=&gt;operation: 创建Worker对象，并把传入的任务作为该线程的第一个任务。state3_n=&gt;operation: 执行拒绝逻辑（默认会抛出异常）e=&gt;end: 结束st-&gt;cond1cond1(yes)-&gt;state1_y-&gt;econd1(no)-&gt;cond2cond2(yes)-&gt;econd2(no)-&gt;cond3cond3(yes)-&gt;state3_y-&gt;econd3(no)-&gt;state3_n 在线程池的状态为大于RUNNING的时候就说明，这个线程池已经不能接受任务和创建新的线程了，最后它会执行拒绝逻辑。 线程池中的Worker上面已经分析了Worker是一个Thread的包装，而且这个Worker同时也是线程线程的Runnable，所以如果要了解线程是怎么工作的，就看这个类的run方法就好了。还有一点，firstTask这个属性代表的就是这个线程的第一个任务，也就是线程会先执行firstTask这个Runnable，然后才会从队列中拿Runnable。 现在下Worker是怎么执行任务的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; while (task != null || (task = getTask()) != null) &#123; w.lock(); //只要线程的状态大于等于STOP，就会中断线程 //如果小于STOP，还会恢复线程的中断状态 if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125;private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. //只要线程池的状态 &gt; STOP 那么不管队列是否有任务，都会结束线程， if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // Are workers subject to culling? //allowCoreThreadTimeOut这个是线程池的成员属性，可以通过set设值 //这个变量表示是否允许超时的标记 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125;private void processWorkerExit(Worker w, boolean completedAbruptly) &#123; if (completedAbruptly) // If abrupt, then workerCount wasn&#x27;t adjusted decrementWorkerCount(); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; completedTaskCount += w.completedTasks; workers.remove(w); &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); int c = ctl.get(); //线程池的状态小于STOP if (runStateLessThan(c, STOP)) &#123; //在getTask()返回null时，这个参数的入参值为false //这里保证线程池的线程数最少为某一个值 if (!completedAbruptly) &#123; int min = allowCoreThreadTimeOut ? 0 : corePoolSize; if (min == 0 &amp;&amp; ! workQueue.isEmpty()) min = 1; if (workerCountOf(c) &gt;= min) return; // replacement not needed &#125; //去添加新的线程，也就是说替换线程 addWorker(null, false); &#125;&#125; 线程消费任务的逻辑很简单，就是执行firstTask后通过getTask()从队列中获取新任务。而且getTask()方法还是线程结束的核心方法。只要getTask()放回null，那么线程就会结束执行，在最后会执行processWorkerExit方法去移除workers中对应的Worker对象。而且这个run方法还提供了两个勾子方法beforeExecute和afterExecute，可以在任务开始执行和执行结束后的时候做一些操作。 现在看下getTask()，从源码中可以看到，这个方法放回null有两种情况： 只要线程池的状态 &gt; STOP 那么不管队列是否有任务，都会结束线程。 只要allowCoreThreadTimeOut为true或者线程数大于corePoolSize，都会通过一个带超时时间的方法获取队列的值，而这个超时时间就是keepAliveTime。也就是说allowCoreThreadTimeOut这个参数允许核心线程结束，而keepAliveTime表示，队列空闲后，额外线程的存活时间。 而且这个getTask()方法在返回null值前会对线程计数器执行减1操作。 submit()1234567891011121314151617181920public Future&lt;?&gt; submit(Runnable task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null); execute(ftask); return ftask;&#125;public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result); execute(ftask); return ftask;&#125;public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task); execute(ftask); return ftask;&#125; 这个方法最后还是会执行execute，不同的是会把task封装成RunnableFuture。下面看看是怎样获取到放回值的。 前两个比较少用，因为前两个方法都是已经指定了放回值了，第一个指定的放回值null，第二个位传入的值，它们的作用就是根据返回的Future，可以执行get方法来堵塞到任务执行完。下面重点看第三个方法。 先看newTaskFor方法。 123protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; return new FutureTask&lt;T&gt;(callable);&#125; 可以看到，task的对象类型为FutureTask。而且通过上边已经知道，线程最后都是执行Runnable的run方法的。所以看FutureTask的run方法。 12345678910111213141516171819202122232425262728293031public void run() &#123; if (state != NEW || !RUNNER.compareAndSet(this, null, Thread.currentThread())) return; try &#123; Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; result = c.call(); ran = true; &#125; catch (Throwable ex) &#123; result = null; ran = false; setException(ex); &#125; if (ran) set(result); &#125; &#125; finally &#123; // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125;&#125; 重点看set方法就好了，这个方法的入参就是为任务的放回值。 1234567891011121314151617181920212223242526272829303132protected void set(V v) &#123; if (STATE.compareAndSet(this, NEW, COMPLETING)) &#123; outcome = v; STATE.setRelease(this, NORMAL); // final state finishCompletion(); &#125;&#125;private void finishCompletion() &#123; // assert state &gt; COMPLETING; for (WaitNode q; (q = waiters) != null;) &#123; if (WAITERS.weakCompareAndSet(this, q, null)) &#123; for (;;) &#123; Thread t = q.thread; if (t != null) &#123; q.thread = null; LockSupport.unpark(t); &#125; WaitNode next = q.next; if (next == null) break; q.next = null; // unlink to help gc q = next; &#125; break; &#125; &#125; done(); callable = null; // to reduce footprint&#125; 这个set方法做了四件事 把state属性的值设置为COMPLETING 把返回值赋值给了outcome这个成员属性。 把state属性的值设置为NORMAL 最后唤醒堵塞的线程。 从这里可以推出，COMPLETING表示任务执行完成，但还没赋值。NORMAL表示已经把返回结果成功赋值了。 get方法，其实很简单，无非就是堵塞或返回结果。 123456789101112131415public V get() throws InterruptedException, ExecutionException &#123; int s = state; if (s &lt;= COMPLETING) s = awaitDone(false, 0L); return report(s);&#125;private V report(int s) throws ExecutionException &#123; Object x = outcome; if (s == NORMAL) return (V)x; if (s &gt;= CANCELLED) throw new CancellationException(); throw new ExecutionException((Throwable)x);&#125; 从get方法里也印证了上边对状态的说明。 shutdown()12345678910111213141516171819202122232425262728293031323334353637383940public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); //这里通过循环cas改线程池的状态为SHUTDOWN advanceRunState(SHUTDOWN); //这里会中断一些空闲的Worker。 interruptIdleWorkers(); //一个勾子方法 onShutdown(); // hook for ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate();&#125;//在这里的onlyOne入参为falseprivate void interruptIdleWorkers(boolean onlyOne) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; for (Worker w : workers) &#123; Thread t = w.thread; if (!t.isInterrupted() &amp;&amp; w.tryLock()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; finally &#123; w.unlock(); &#125; &#125; if (onlyOne) break; &#125; &#125; finally &#123; mainLock.unlock(); &#125;&#125; shutdown()方法会把线程状态设置为SHUTDOWN状态。在上面的方法中有一步操作，回去中断一些空闲的Worker，而这里的空闲的定义，就是线程没有执行任务的时候，就被定义为空闲了。因为在runWorker中，线程没次执行任务时都会加锁，也就是说只要任务在执行中了，interruptIdleWorkers中就会获取锁失败，从而不会中断线程。 shutdownNow()1234567891011121314151617public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); //这里通过循环cas改线程池的状态为STOP advanceRunState(STOP); //这里会中断Worker， interruptWorkers(); tasks = drainQueue(); &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); return tasks;&#125; 这个方法会方法会把线程状态设置为STOP状态，并且去中断线程。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"Node","slug":"juc/Node","date":"2021-11-18T12:00:12.000Z","updated":"2022-03-23T09:03:57.729Z","comments":true,"path":"blog/juc/Node/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/Node/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214/** * Wait queue node class. * * &lt;p&gt;The wait queue is a variant of a &quot;CLH&quot; (Craig, Landin, and * Hagersten) lock queue. CLH locks are normally used for * spinlocks. We instead use them for blocking synchronizers, but * use the same basic tactic of holding some of the control * information about a thread in the predecessor of its node. A * &quot;status&quot; field in each node keeps track of whether a thread * should block. A node is signalled when its predecessor * releases. Each node of the queue otherwise serves as a * specific-notification-style monitor holding a single waiting * thread. The status field does NOT control whether threads are * granted locks etc though. A thread may try to acquire if it is * first in the queue. But being first does not guarantee success; * it only gives the right to contend. So the currently released * contender thread may need to rewait. * * &lt;p&gt;To enqueue into a CLH lock, you atomically splice it in as new * tail. To dequeue, you just set the head field. * &lt;pre&gt; * +------+ prev +-----+ +-----+ * head | | &lt;---- | | &lt;---- | | tail * +------+ +-----+ +-----+ * &lt;/pre&gt; * * &lt;p&gt;Insertion into a CLH queue requires only a single atomic * operation on &quot;tail&quot;, so there is a simple atomic point of * demarcation from unqueued to queued. Similarly, dequeuing * involves only updating the &quot;head&quot;. However, it takes a bit * more work for nodes to determine who their successors are, * in part to deal with possible cancellation due to timeouts * and interrupts. * * &lt;p&gt;The &quot;prev&quot; links (not used in original CLH locks), are mainly * needed to handle cancellation. If a node is cancelled, its * successor is (normally) relinked to a non-cancelled * predecessor. For explanation of similar mechanics in the case * of spin locks, see the papers by Scott and Scherer at * http://www.cs.rochester.edu/u/scott/synchronization/ * * &lt;p&gt;We also use &quot;next&quot; links to implement blocking mechanics. * The thread id for each node is kept in its own node, so a * predecessor signals the next node to wake up by traversing * next link to determine which thread it is. Determination of * successor must avoid races with newly queued nodes to set * the &quot;next&quot; fields of their predecessors. This is solved * when necessary by checking backwards from the atomically * updated &quot;tail&quot; when a node&#x27;s successor appears to be null. * (Or, said differently, the next-links are an optimization * so that we don&#x27;t usually need a backward scan.) * * &lt;p&gt;Cancellation introduces some conservatism to the basic * algorithms. Since we must poll for cancellation of other * nodes, we can miss noticing whether a cancelled node is * ahead or behind us. This is dealt with by always unparking * successors upon cancellation, allowing them to stabilize on * a new predecessor, unless we can identify an uncancelled * predecessor who will carry this responsibility. * * &lt;p&gt;CLH queues need a dummy header node to get started. But * we don&#x27;t create them on construction, because it would be wasted * effort if there is never contention. Instead, the node * is constructed and head and tail pointers are set upon first * contention. * * &lt;p&gt;Threads waiting on Conditions use the same nodes, but * use an additional link. Conditions only need to link nodes * in simple (non-concurrent) linked queues because they are * only accessed when exclusively held. Upon await, a node is * inserted into a condition queue. Upon signal, the node is * transferred to the main queue. A special value of status * field is used to mark which queue a node is on. * * &lt;p&gt;Thanks go to Dave Dice, Mark Moir, Victor Luchangco, Bill * Scherer and Michael Scott, along with members of JSR-166 * expert group, for helpful ideas, discussions, and critiques * on the design of this class. */static final class Node &#123; /** Marker to indicate a node is waiting in shared mode */ static final Node SHARED = new Node(); /** Marker to indicate a node is waiting in exclusive mode */ static final Node EXCLUSIVE = null; /** waitStatus value to indicate thread has cancelled */ static final int CANCELLED = 1; /** waitStatus value to indicate successor&#x27;s thread needs unparking */ static final int SIGNAL = -1; /** waitStatus value to indicate thread is waiting on condition */ static final int CONDITION = -2; /** * waitStatus value to indicate the next acquireShared should * unconditionally propagate */ static final int PROPAGATE = -3; /** * Status field, taking on only the values: * SIGNAL: The successor of this node is (or will soon be) * blocked (via park), so the current node must * unpark its successor when it releases or * cancels. To avoid races, acquire methods must * first indicate they need a signal, * then retry the atomic acquire, and then, * on failure, block. * CANCELLED: This node is cancelled due to timeout or interrupt. * Nodes never leave this state. In particular, * a thread with cancelled node never again blocks. * CONDITION: This node is currently on a condition queue. * It will not be used as a sync queue node * until transferred, at which time the status * will be set to 0. (Use of this value here has * nothing to do with the other uses of the * field, but simplifies mechanics.) * PROPAGATE: A releaseShared should be propagated to other * nodes. This is set (for head node only) in * doReleaseShared to ensure propagation * continues, even if other operations have * since intervened. * 0: None of the above * * The values are arranged numerically to simplify use. * Non-negative values mean that a node doesn&#x27;t need to * signal. So, most code doesn&#x27;t need to check for particular * values, just for sign. * * The field is initialized to 0 for normal sync nodes, and * CONDITION for condition nodes. It is modified using CAS * (or when possible, unconditional volatile writes). */ volatile int waitStatus; /** * Link to predecessor node that current node/thread relies on * for checking waitStatus. Assigned during enqueuing, and nulled * out (for sake of GC) only upon dequeuing. Also, upon * cancellation of a predecessor, we short-circuit while * finding a non-cancelled one, which will always exist * because the head node is never cancelled: A node becomes * head only as a result of successful acquire. A * cancelled thread never succeeds in acquiring, and a thread only * cancels itself, not any other node. */ volatile Node prev; /** * Link to the successor node that the current node/thread * unparks upon release. Assigned during enqueuing, adjusted * when bypassing cancelled predecessors, and nulled out (for * sake of GC) when dequeued. The enq operation does not * assign next field of a predecessor until after attachment, * so seeing a null next field does not necessarily mean that * node is at end of queue. However, if a next field appears * to be null, we can scan prev&#x27;s from the tail to * double-check. The next field of cancelled nodes is set to * point to the node itself instead of null, to make life * easier for isOnSyncQueue. */ volatile Node next; /** * The thread that enqueued this node. Initialized on * construction and nulled out after use. */ volatile Thread thread; /** * Link to next node waiting on condition, or the special * value SHARED. Because condition queues are accessed only * when holding in exclusive mode, we just need a simple * linked queue to hold nodes while they are waiting on * conditions. They are then transferred to the queue to * re-acquire. And because conditions can only be exclusive, * we save a field by using special value to indicate shared * mode. */ Node nextWaiter; /** * Returns true if node is waiting in shared mode. */ final boolean isShared() &#123; return nextWaiter == SHARED; &#125; /** * Returns previous node, or throws NullPointerException if null. * Use when predecessor cannot be null. The null check could * be elided, but is present to help the VM. * * @return the predecessor of this node */ final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) throw new NullPointerException(); else return p; &#125; Node() &#123; // Used to establish initial head or SHARED marker &#125; Node(Thread thread, Node mode) &#123; // Used by addWaiter this.nextWaiter = mode; this.thread = thread; &#125; Node(Thread thread, int waitStatus) &#123; // Used by Condition this.waitStatus = waitStatus; this.thread = thread; &#125;&#125;","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"LongAdder","slug":"juc/LongAdder","date":"2021-11-18T12:00:11.000Z","updated":"2022-03-23T09:03:57.729Z","comments":true,"path":"blog/juc/LongAdder/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/LongAdder/","excerpt":"","text":"原理LongAdder类是JDK1.8新增的一个原子性操作类。 AtomicLong通过CAS算法提供了非阻塞的原子性操作，相比受用阻塞算法的同步器来说性能已经很好了，但是JDK开发组并不满足于此，因为非常高并发的请求下AtomicLong的性能是不能让人接受的。 如下AtomicLong 的incrementAndGet的代码，虽然AtomicLong使用CAS算法，但是CAS失败后还是通过无限循环的自旋锁不断的尝试，这就是高并发下CAS性能低下的原因所在。源码如下： 12345678public final long incrementAndGet() &#123; for (;;) &#123; long current = get(); long next = current + 1; if (compareAndSet(current, next)) return next; &#125;&#125; 高并发下N多线程同时去操作一个变量会造成大量线程CAS失败，然后处于自旋状态，导致严重浪费CPU资源，降低了并发性。既然AtomicLong性能问题是由于过多线程同时去竞争同一个变量的更新而降低的，那么如果把一个变量分解为多个变量，让同样多的线程去竞争多个资源。这就能把并发分散，极大的提高了并发性能。这就是分段锁的思想。juc经典实现就是ConcurrentHashMap。 在JDK1.8中就新增了LongAdder也是采用了分段锁的思想来实现高并发下的累加。 其实ConcurrentHashMap的addCount方法实现和LongAdder基本一摸一样. LongAdder则是内部维护一个base变量和个一个Cells数组。base初始值为0，Cells数组初始值为null。通知LongAdder也采用了类似偏向锁的设计思想。 在只有一个线程操作的时候 ，会对base进行累加，随着出现线程竞争，会初始化Cells数组，初始长度为2。并把竞争分散到Cells数组的元素Cell中。 每个Cell里面有一个初始值为0的long型变量，在同等并发量的情况下，争夺单个变量的线程会减少，这是变相的减少了争夺共享资源的并发量，另外多个线程在争夺同一个原子变量时候，如果失败并不是自旋CAS重试，而是尝试获取其他原子变量的锁，最后当获取当前值时候是把所有变量的值累加后再加上base的值返回的。 LongAdder维护了要给延迟初始化的原子性更新数组和一个基值变量base数组的大小保持是2的N次方大小，数组表的下标使用每个线程的hashcode值的掩码表示，数组里面的变量实体是Cell类型。 Cell 类型是Atomic的一个改进，用来减少缓存的争用，对于大多数原子操作字节填充是浪费的，因为原子操作都是无规律的分散在内存中进行的，多个原子性操作彼此之间是没有接触的，但是原子性数组元素彼此相邻存放将能经常共享缓存行，也就是伪共享。所以这在性能上是一个提升。 另外由于Cells占用内存是相对比较大的，所以一开始并不创建，而是在需要时候再创建，也就是惰性加载，在只有一个线程操作的时候 ，会对base进行累加，随着出现线程竞争，会初始化Cells数组。之后的操作都会分散给各个Cell对象完成。 源码LongAdder的类图： Striped64Striped64是一个高并发累加的工具类。 Striped64的设计核心思路就是通过内部的分散计算来避免竞争。 Striped64内部包含一个base和一个Cell[] cells数组，又叫hash表。没有竞争的情况下，要累加的数通过cas累加到base上；如果有竞争的话，会将要累加的数累加到Cells数组中的某个cell元素里面。所以整个Striped64的值为sum=base+∑[0~n]cells Striped64核心属性 transient volatile Cell[] cells; &#x2F;&#x2F; 存放cell的hash表，大小为2乘幂 transient volatile long base; &#x2F;&#x2F; 基础值（1.无竞争时更新，2.cells数组初始化过程不可用时，也会通过cas累加到base） transient volatile int cellsBusy; &#x2F;&#x2F; 对cells更新时使用的锁，通过CAS操作加锁（1.初始化cells数组，2.创建cell单元，3.cells扩容） cellsAtomicInteger只有一个value，所有线程累加都要通过cas竞争value这一个变量，高并发下线程争用非常严重； 而LongAdder则有两个值用于累加，一个是base，它的作用类似于AtomicInteger里面的value，在没有竞争的情况不会用到cells数组，这时使用base做累加，有了竞争后cells数组就上场了，第一次初始化长度为2，以后每次扩容都是变为原来的两倍，直到cells数组的长度大于等于当前服务器cpu的数量为止就不在扩容（CPU能够并行的CAS操作的最大数量是它的核心数），每个线程会通过线程对cells[threadLocalRandomProbe%cells.length]位置的Cell对象中的value做累加，这样相当于将线程绑定到了cells中的某个cell对象上。 12345678910111213141516171819202122232425// 为提高性能，使用注解@sun.misc.Contended，用来避免伪共享// 伪共享简单来说就是会破坏其它线程在缓存行中的值，导致重新从主内存读取，降低性能。@sun.misc.Contended static final class Cell &#123; //用来保存要累加的值 volatile long value; Cell(long x) &#123; value = x; &#125; //使用UNSAFE类的cas来更新value值 final boolean cas(long cmp, long val) &#123; return UNSAFE.compareAndSwapLong(this, valueOffset, cmp, val); &#125; private static final sun.misc.Unsafe UNSAFE; //value在Cell类中存储位置的偏移量； private static final long valueOffset; //这个静态方法用于获取偏移量 static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; ak = Cell.class; valueOffset = UNSAFE.objectFieldOffset (ak.getDeclaredField(&quot;value&quot;)); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;&#125; cellsBusy对cells更新时使用的锁，0为无锁，1为加锁。在对cells进行初始化、初始话cells中的某个元素、对cells进行扩容时使用 LongAdder源码123456789101112131415161718192021public void add(long x) &#123; Cell[] cs; long b, v; int m; Cell c; // 1. cells初始化是null的，所以在一开始就是base字段进行cas累加。 // 2. 如果casBase返回false，就表示存在竞争，这时就需要cells进行初始化，该过程是在longAccumulate中完成 // 3. cells初始化后，所有的操作都由Cell对象完成 if ((cs = cells) != null || !casBase(b = base, b + x)) &#123; // 获取线程对象的threadLocalRandomProbe值 // 等价于ThreadLocalRandom#getProbe // 在没有发生竞争的情况下，该值是不会变的 int index = getProbe(); // 用来表示有没有竞争的字段 boolean uncontended = true; if (cs == null || (m = cs.length - 1) &lt; 0 || // index &amp; m是进行取模操作，对于长度是2^n才生效 (c = cs[index &amp; m]) == null || // 这里会对Cell对象进行cas。 // 如果失败了就需要重新绑定Cell对象，也就是重新计算 !(uncontended = c.cas(v = c.value, v + x))) longAccumulate(x, null, uncontended, index); &#125;&#125; longAccumulate: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129final void longAccumulate(long x, LongBinaryOperator fn, boolean wasUncontended) &#123; //获取当前线程的threadLocalRandomProbe值作为hash值,如果当前线程的threadLocalRandomProbe为0，说明当前线程是第一次进入该方法，则强制设置线程的threadLocalRandomProbe为ThreadLocalRandom类的成员静态私有变量probeGenerator的值，后面会详细将hash值的生成; //另外需要注意，如果threadLocalRandomProbe=0，代表新的线程开始参与cell争用的情况 //1.当前线程之前还没有参与过cells争用（也许cells数组还没初始化，进到当前方法来就是为了初始化cells数组后争用的）,是第一次执行base的cas累加操作失败； //2.或者是在执行add方法时，对cells某个位置的Cell的cas操作第一次失败，则将wasUncontended设置为false，那么这里会将其重新置为true；第一次执行操作失败； //凡是参与了cell争用操作的线程threadLocalRandomProbe都不为0； int h; if ((h = getProbe()) == 0) &#123; //初始化ThreadLocalRandom; ThreadLocalRandom.current(); // force initialization h = getProbe(); //设置未竞争标记为true wasUncontended = true; &#125; //cas冲突标志，表示当前线程hash到的Cells数组的位置，做cas累加操作时与其它线程发生了冲突，cas失败；collide=true代表有冲突，collide=false代表无冲突 boolean collide = false; for (;;) &#123; Cell[] as; Cell a; int n; long v; //这个主干if有三个分支 //1.主分支一：处理cells数组已经正常初始化了的情况（这个if分支处理add方法的四个条件中的3和4） //2.主分支二：处理cells数组没有初始化或者长度为0的情况；（这个分支处理add方法的四个条件中的1和2） //3.主分支三：处理如果cell数组没有初始化，并且其它线程正在执行对cells数组初始化的操作，及cellbusy=1；则尝试将累加值通过cas累加到base上 //先看主分支一 if ((as = cells) != null &amp;&amp; (n = as.length) &gt; 0) &#123; /** *内部小分支一：这个是处理add方法内部if分支的条件3：如果被hash到的位置为null，说明没有线程在这个位置设置过值，没有竞争，可以直接使用，则用x值作为初始值创建一个新的Cell对象，对cells数组使用cellsBusy加锁，然后将这个Cell对象放到cells[m%cells.length]位置上 */ if ((a = as[(n - 1) &amp; h]) == null) &#123; //cellsBusy == 0 代表当前没有线程cells数组做修改 if (cellsBusy == 0) &#123; //将要累加的x值作为初始值创建一个新的Cell对象， Cell r = new Cell(x); //如果cellsBusy=0无锁，则通过cas将cellsBusy设置为1加锁 if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; //标记Cell是否创建成功并放入到cells数组被hash的位置上 boolean created = false; try &#123; Cell[] rs; int m, j; //再次检查cells数组不为null，且长度不为空，且hash到的位置的Cell为null if ((rs = cells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; rs[j = (m - 1) &amp; h] == null) &#123; //将新的cell设置到该位置 rs[j] = r; created = true; &#125; &#125; finally &#123; //去掉锁 cellsBusy = 0; &#125; //生成成功，跳出循环 if (created) break; //如果created为false，说明上面指定的cells数组的位置cells[m%cells.length]已经有其它线程设置了cell了，继续执行循环。 continue; &#125; &#125; //如果执行的当前行，代表cellsBusy=1，有线程正在更改cells数组，代表产生了冲突，将collide设置为false collide = false; /** *内部小分支二：如果add方法中条件4的通过cas设置cells[m%cells.length]位置的Cell对象中的value值设置为v+x失败,说明已经发生竞争，将wasUncontended设置为true，跳出内部的if判断，最后重新计算一个新的probe，然后重新执行循环; */ &#125; else if (!wasUncontended) //设置未竞争标志位true，继续执行，后面会算一个新的probe值，然后重新执行循环。 wasUncontended = true; /** *内部小分支三：新的争用线程参与争用的情况：处理刚进入当前方法时threadLocalRandomProbe=0的情况，也就是当前线程第一次参与cell争用的cas失败，这里会尝试将x值加到cells[m%cells.length]的value ，如果成功直接退出 */ else if (a.cas(v = a.value, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; /** *内部小分支四：分支3处理新的线程争用执行失败了，这时如果cells数组的长度已经到了最大值（大于等于cup数量），或者是当前cells已经做了扩容，则将collide设置为false，后面重新计算prob的值*/ else if (n &gt;= NCPU || cells != as) collide = false; /** *内部小分支五：如果发生了冲突collide=false，则设置其为true；会在最后重新计算hash值后，进入下一次for循环 */ else if (!collide) //设置冲突标志，表示发生了冲突，需要再次生成hash，重试。 如果下次重试任然走到了改分支此时collide=true，!collide条件不成立，则走后一个分支 collide = true; /** *内部小分支六：扩容cells数组，新参与cell争用的线程两次均失败，且符合库容条件，会执行该分支 */ else if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; try &#123; //检查cells是否已经被扩容 if (cells == as) &#123; // Expand table unless stale Cell[] rs = new Cell[n &lt;&lt; 1]; for (int i = 0; i &lt; n; ++i) rs[i] = as[i]; cells = rs; &#125; &#125; finally &#123; cellsBusy = 0; &#125; collide = false; continue; // Retry with expanded table &#125; //为当前线程重新计算hash值 h = advanceProbe(h); //这个大的分支处理add方法中的条件1与条件2成立的情况，如果cell表还未初始化或者长度为0，先尝试获取cellsBusy锁。 &#125;else if (cellsBusy == 0 &amp;&amp; cells == as &amp;&amp; casCellsBusy()) &#123; boolean init = false; try &#123; // Initialize table //初始化cells数组，初始容量为2,并将x值通过hash&amp;1，放到0个或第1个位置上 if (cells == as) &#123; Cell[] rs = new Cell[2]; rs[h &amp; 1] = new Cell(x); cells = rs; init = true; &#125; &#125; finally &#123; //解锁 cellsBusy = 0; &#125; //如果init为true说明初始化成功，跳出循环 if (init) break; &#125; /** *如果以上操作都失败了，则尝试将值累加到base上； */ else if (casBase(v = base, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) // Fall back on using base break; &#125;&#125;","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"LinkedList","slug":"juc/LinkedList","date":"2021-11-18T12:00:10.000Z","updated":"2022-03-23T09:03:57.723Z","comments":true,"path":"blog/juc/LinkedList/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/LinkedList/","excerpt":"","text":"这里重点看get()，迭代器 get1234567891011121314151617181920public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 可以看到，这个get就有点坑了，直接是遍历。虽然做了优化，但每get一个都这么操作的话，性能还是很低的。 迭代器—iterator123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122public Iterator&lt;E&gt; iterator() &#123; return listIterator();&#125;private class Itr implements Iterator&lt;E&gt; &#123; /** * Index of element to be returned by subsequent call to next. */ int cursor = 0; /** * Index of element returned by most recent call to next or * previous. Reset to -1 if this element is deleted by a call * to remove. */ int lastRet = -1; /** * The modCount value that the iterator believes that the backing * List should have. If this expectation is violated, the iterator * has detected concurrent modification. */ int expectedModCount = modCount; public boolean hasNext() &#123; return cursor != size(); &#125; public E next() &#123; checkForComodification(); try &#123; int i = cursor; E next = get(i); lastRet = i; cursor = i + 1; return next; &#125; catch (IndexOutOfBoundsException e) &#123; checkForComodification(); throw new NoSuchElementException(); &#125; &#125; public void remove() &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try &#123; AbstractList.this.remove(lastRet); if (lastRet &lt; cursor) cursor--; lastRet = -1; expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException e) &#123; throw new ConcurrentModificationException(); &#125; &#125; final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125;&#125;private class ListItr extends Itr implements ListIterator&lt;E&gt; &#123; ListItr(int index) &#123; cursor = index; &#125; public boolean hasPrevious() &#123; return cursor != 0; &#125; public E previous() &#123; checkForComodification(); try &#123; int i = cursor - 1; E previous = get(i); lastRet = cursor = i; return previous; &#125; catch (IndexOutOfBoundsException e) &#123; checkForComodification(); throw new NoSuchElementException(); &#125; &#125; public int nextIndex() &#123; return cursor; &#125; public int previousIndex() &#123; return cursor-1; &#125; public void set(E e) &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try &#123; AbstractList.this.set(lastRet, e); expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125; &#125; public void add(E e) &#123; checkForComodification(); try &#123; int i = cursor; AbstractList.this.add(i, e); lastRet = -1; cursor = i + 1; expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125; &#125;&#125; 迭代器常用的方法有hasNext、next、remove hasNext没什么问题，有问题的是next和remove next迭代器的中 12345678910111213public E next() &#123; checkForComodification(); try &#123; int i = cursor; E next = get(i); lastRet = i; cursor = i + 1; return next; &#125; catch (IndexOutOfBoundsException e) &#123; checkForComodification(); throw new NoSuchElementException(); &#125;&#125; 调用了自己的get方法，经过上面的分析，LinkedList.get(index)就是遍历，性能是比较低的。 remove迭代器的中 123456789101112131415public void remove() &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try &#123; AbstractList.this.remove(lastRet); if (lastRet &lt; cursor) cursor--; lastRet = -1; expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException e) &#123; throw new ConcurrentModificationException(); &#125;&#125; AbstractList.this.remove(lastRet);也就是LinkedList.remove(index): 1234567891011121314151617181920public E remove(int index) &#123; checkElementIndex(index); return unlink(node(index));&#125;Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 还是node，还是遍历！！ 总结如果需要遍历的情况，而且元素较多，不要使用LinkedList","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"JCTools介绍","slug":"juc/JCTools/JCTools介绍","date":"2021-11-18T12:00:09.000Z","updated":"2022-03-23T09:03:57.722Z","comments":true,"path":"blog/juc/JCTools/JCTools介绍/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/JCTools/JCTools%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"早在96年就有论文提出了无锁队列的概念，再到后来 Disruptor，高性能已得到生产的验证。此处介绍的 Jctools 中的高性能队列，其性能丝毫不输于 Disruptor。 JCTools (Java Concurrency Tools) 提供了一系列非阻塞并发数据结构（标准 Java 中缺失的），当存在线程争抢的时候，非阻塞并发数据结构比阻塞并发数据结构能提供更好的性能。 JCTools 是一个开源工具包，在 Apache License 2.0 下发布，并在 Netty、Rxjava 等诸多框架中被广泛使用。 JCTools 的开源 Github 仓库：https://github.com/JCTools/JCTools 在 Maven 中引入 JCtools jar 包就能使用 JCTools 了： 12345&lt;dependency&gt; &lt;groupId&gt;org.jctools&lt;/groupId&gt; &lt;artifactId&gt;jctools-core&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt; JCTools 中主要提供了 Map 以及 Queue 的非阻塞并发数据结构： 非阻塞 Map ConcurrentAutoTable（后面几个map&#x2F;set结构的基础） NonBlockingHashMap NonBlockingHashMapLong NonBlockingHashSet NonBlockingIdentityHashMap NonBlockingSetInt NonBlockingHashMap 是对 ConcurrentHashMap 的增强，对多 CPU 的支持以及高并发更新提供更好的性能。 NonBlockingHashMapLong 是 key 为 Long 型的 NonBlockingHashMap。 NonBlockingHashSet 是对 NonBlockingHashMap 的简单包装以支持 set 的接口。 NonBlockingIdentityHashMap 是从 NonBlockingHashMap 改造来的，使用 System.identityHashCode() 来计算哈希。 NonBlockingSetInt 是一个使用 CAS 的简单的 bit-vector。 非阻塞 QueueJCTools 提供的非阻塞队列分为 4 类，可以根据不同的应用场景选择使用： SPSC-单一生产者单一消费者（有界和无界） MPSC-多生产者单一消费者（有界和无界） SPMC-单生产者多消费者（有界） MPMC-多生产者多消费者（有界） “生产者”和“消费者”是指“生产线程”和“消费线程”。 1234567891011// spsc-有界/无界队列Queue&lt;String&gt; spscArrayQueue = new SpscArrayQueue(16);Queue&lt;String&gt; spscUnboundedArrayQueue = new SpscUnboundedArrayQueue(2);// spmc-有界队列Queue&lt;String&gt; spmcArrayQueue = new SpmcArrayQueue&lt;&gt;(16);// mpsc-有界/无界队列Queue&lt;String&gt; mpscArrayQueue = new MpscArrayQueue&lt;&gt;(16);Queue&lt;String&gt; mpscChunkedArrayQueue = new MpscChunkedArrayQueue&lt;&gt;(1024, 8 * 1024);Queue&lt;String&gt; mpscUnboundedArrayQueue = new MpscUnboundedArrayQueue&lt;&gt;(2);// mpmc-有界队列Queue&lt;String&gt; mpmcArrayQueue = new MpmcArrayQueue&lt;&gt;(16);","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"HashMap","slug":"juc/HashMap","date":"2021-11-18T12:00:08.000Z","updated":"2022-03-23T09:03:57.721Z","comments":true,"path":"blog/juc/HashMap/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/HashMap/","excerpt":"","text":"有趣的取模性质：取模a % (2^n) 等价于 a &amp; (2^n - 1)，所以在map里的数组个数一定是2的乘方数，计算key值在哪个元素中的时候，就用位运算来快速定位。 put123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 第一次put会触发resize()，该方法第一次运行时会生成长度为16的数组。 如果经过(n - 1) &amp; hash 运算后（就是取模运算），在数组下标为i的位置没有数据就直接赋值。 如果有数据的情况就会复杂一些。会先判断tab[i]位置的数据的hash、key是否和需要插入的key、hash相同，如果相同就直接更新数据就好了。如果不同，会判断tab[i]位置的节点是否红黑树的节点。如果不是就意味该节点是链表的节点，所以会遍历该链表。如果找到的就覆盖旧值；如果没找到的情况下就新建节点插入到链表尾部。这时会触发。if (binCount &gt;&#x3D; TREEIFY_THRESHOLD - 1)这个校验，也就是说如果链表的节点数量为8个的时候可能需要转化为红黑树。这时看下treeifyBin(tab, hash)的逻辑： 1234567891011121314151617181920final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); &#125;&#125; 从源码可以看到，转化为红黑树的情况还是有限制的，结合上述的if，所以是否转化为红黑树的条件是 链表结构的节点数目达到了8个 数组的长度大于等于64 满足上述条件才会转化为红黑树，不满足的情况只会进行扩容。 resize(): 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 上面的扩容流程就是，扩大为原数组长度的两倍，然后新建一个数组，并把原数组中位置为n的节点上的数据，移动到n位置或者（n+oldlength）的位置。 假设，put时是没有节点的，那走到了 threshold的计算公式可以在resize中看到，第一次执行该方法 123threshold = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);也就是：threshold = (int)(0.75 * 16) = 12 随后，随着数据越来越多，假如下面的if成立 1234++modCount;if (++size &gt; threshold) resize();afterNodeInsertion(evict); 那么就会需要进行扩容，而且threshold需要扩大为原来的两倍。 123threshold = threshold &lt;&lt; 1相当于：threshold = 2 * threshold 最大值是Integer.MAX_VALUE 好了回到代码 如果结点数量大于threshold就必须扩容了。数组变为原来的两倍，threshold也变为原来的两倍，并且要rehash get123456789101112131415161718192021222324public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; hash后，对hash做(tab.length - 1) &amp; hash得到所在的下标，这时如果是链表就遍历链表，如果是红黑树就在红黑树中查找。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"ConcurrentHashMap","slug":"juc/ConcurrentHashMap","date":"2021-11-18T12:00:07.000Z","updated":"2022-03-23T09:03:57.720Z","comments":true,"path":"blog/juc/ConcurrentHashMap/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/ConcurrentHashMap/","excerpt":"","text":"在1.8下的实现： 采用table数组元素作为锁，从而实现了对缩小锁的粒度，进一步减少并发冲突的概率，并大量使用了采用了 CAS + synchronized 来保证并发安全性。 将原先table数组＋单向链表的数据结构，变更为table数组＋单向链表＋红黑树的结构。对于hash表来说，最核心的能力在于将key hash之后能均匀的分布在数组中。如果hash之后散列的很均匀，那么table数组中的每个队列长度主要为0或者1。但实际情况并非总是如此理想，虽然ConcurrentHashMap类默认的加载因子为0.75，但是在数据量过大或者运气不佳的情况下，还是会存在一些队列长度过长的情况，如果还是采用单向列表方式，那么查询某个节点的时间复杂度为O(n)；因此，对于个数超过8(默认值)的列表，jdk1.8中采用了红黑树的结构，那么查询的时间复杂度可以降低到O(logN)，可以改进性能。 ConcurrentHashMap在扩容时，如果发生了put，那么put的线程会去帮助扩容。 重要的参数sizeCtl： 用来控制 table 的初始化和扩容操作。 负数代表正在进行初始化或扩容操作： -1代表正在初始化 -N 表示有N-1个线程正在进行扩容操作 0为默认值，代表当时的table还没有被初始化正数表示初始化大小或Map中的元素达到这个数量时，需要进行扩容了。 get12345678910111213141516171819public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; get方法比较简单，给定一个key来确定value的时候，必须满足两个条件 key相同 hash值相同，对于节点可能在链表或树上的情况，需要分别去查找。 有一点值得注意，这个方法上是没有任何同步操作的。那get方法它能在并发下没问题吗？ 先看下Node的定义 对于一个Node节点来说，hash和key是不会变的，而val和next是会发生变化的，而这个两个变量用了volatile的修饰，保证了这两个变量的可见性，所以在get方法上不需要加锁就能在并发下获取最后一次对这些变量的写入值。 put123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public V put(K key, V value) &#123; return putVal(key, value, false);&#125;static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125;final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; K fk; V fv; //先初始化数组，这个初始化是使用cas来保证的 if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value))) break; // no lock when adding to empty bin &#125; else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else if (onlyIfAbsent // check first node without acquiring lock &amp;&amp; fh == hash &amp;&amp; ((fk = f.key) == key || (fk != null &amp;&amp; key.equals(fk))) &amp;&amp; (fv = f.val) != null) return fv; else &#123; V oldVal = null; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; else if (f instanceof ReservationNode) throw new IllegalStateException(&quot;Recursive update&quot;); &#125; &#125; if (binCount != 0) &#123; // if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount); return null;&#125; 整体流程上，就是首先定义不允许key或value为null的情况放入 对于每一个放入的值，首先利用spread方法对key的hashcode进行一次hash计算，由此来确定这个值在table中的位置。这时存在如下情况 如果数组还没初始化时，这时会使用sizeCtl和cas保证初始化的线程安全。 如果这个位置是空的，那么使用cas把新建的Node放入对应的位置，而且不需要加锁操作 如果发现节点上的hash&#x3D;&#x3D;MOVED，就表示这个时候正在扩容中，而且这个位置上的数据还没有进行rehash，那么就会执行helpTransfer(tab, f)来对这个位置上的节点进行rehash。 如果这个位置存在结点，说明发生了hash碰撞，这时先对tab[i]的节点上锁，接着判断这个节点的类型，如果这个节点的类型已经是树节点的话，直接调用树节点的插入方法进行插入新的值。如果是链表节点，需要依次向后遍历确定这个新加入的值所在位置。如果遇到hash值与key值都与新加入节点是一致的情况，则只需要更新value值即可。否则依次向后遍历，直到链表尾插入这个结点。如果加入这个节点以后链表长度大于8，就把这个链表转换成红黑树，但并不是在任何情况下这个成立的，因为在treeifyBin(tab, i)方法中存在这一段代码：表示如果数组的长度小于64那么会执行tryPresize(n &lt;&lt; 1)方法，而这个方法的核心就是在最后的一个else也就是会去进行扩容。这一点和hashMap还是一样的，变成红黑树的前提是数组长度必须大于等于64 初始化前面已经说过，构造方法中并没有真正初始化，真正的初始化在放在了是在向ConcurrentHashMap中插入元素的时候发生的，具体实现的方法就是initTable。 1234567891011121314151617181920212223private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; //负数代表正在进行初始化或扩容操作 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin else if (U.compareAndSetInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 扩容前面已经说了，扩容后两种情况会触发，put位置上的链表的节点数量已经达到了8个，而且这时数组的长度小于64，这个时候会触发扩容。还有一种情况就是节点数量大于某个值时也会触发扩容。 ConcurrentHashMap的扩容方法的基本思想跟HashMap是很像的，但是由于它是支持并发扩容的，所以要复杂的多。我们不深入源码去讲述，只讲述其大概原理。 整个扩容操作分为两个部分： 第一部分是构建一个nextTable,它的容量是原来的2倍。 第二个部分就是将原来table中的元素复制到nextTable中，这里允许多线程进行操作。整个扩容流程就是遍历和复制： 为null或者已经处理过的节点，会被设置为forwardNode节点，当线程准备扩容时，发现节点是forwardNode节点，跳过这个节点，继续寻找未处理的节点，找到了，对节点上锁， 如果这个位置是Node节点（fh&gt;&#x3D;0），说明它是一个链表，就构造一个反序链表，把他们分别放在nextTable的i和i+n的位置上 如果这个位置是TreeBin节点（fh&lt;0），也做一个反序处理，并且判断是否需要红黑树转链表，把处理的结果分别放在nextTable的i和i+n的位置上 遍历过所有的节点以后就完成了复制工作，这时让nextTable作为新的table，并且更新sizeCtl为新容量的0.75倍 ，完成扩容。 如果在扩容时有有其他线程执行了put操作，那么这个线程回去帮助扩容。 并发扩容其实就是将数据迁移任务拆分成多个小迁移任务，在实现上使用了一个变量stride作为步长控制，每个线程每次负责迁移其中的一部分。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"CompletableFuture原理","slug":"juc/CompletableFuture原理","date":"2021-11-18T12:00:06.000Z","updated":"2022-03-23T09:03:57.718Z","comments":true,"path":"blog/juc/CompletableFuture原理/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/CompletableFuture%E5%8E%9F%E7%90%86/","excerpt":"","text":"执行过程涉及到的类的关系如下： 以run系列API为例，展示执行了runAsync 、thenRun和thenRunAsync后任务的情况，并假设，执行API时，上一个任务都没有完成 第一步 执行了 123CompletableFuture&lt;Void&gt; task1_Future = CompletableFuture.runAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:1&quot;);&#125;, executor); 此时的任务情况如图： 第二步 执行了 123CompletableFuture&lt;Void&gt; task2_Future = task1_Future.thenRun(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:2&quot;);&#125;); 此时的任务情况如图： 其中UniRun类关系如下： 第三步 执行了 123CompletableFuture&lt;Void&gt; task3_Future = task2_Future.thenRunAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:3&quot;);&#125;, executor); 此时的任务情况如图： 现在任务开始执行了，在task1_runner这个AsyncRun中，执行完task1后。接着执行task1_future中的stack里面的子任务task2，然后出栈。接着会执行 task2_future中的stack里面的任务task3，然后出栈。接着执行task3_future中的stack里面的任务，由于task3_future.stack已经为null了，返回一个null到task1_future中，由于task1_future.stack已经为空，任务执行结束了 还有一种情况 123456789101112131415// 第一步CompletableFuture&lt;Void&gt; task1_Future = CompletableFuture.runAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:1&quot;);&#125;, executor);// 第二步task1_Future.thenRun(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:2&quot;);&#125;);// 第三步task1_Future.thenRunAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:3&quot;);&#125;, executor); 执行完后，任务情况如下： 执行还是一样的 源码分析主任务添加源码对应的方法runAsync(Runnable runnable) 123456static CompletableFuture&lt;Void&gt; asyncRunStage(Executor e, Runnable f) &#123; if (f == null) throw new NullPointerException(); CompletableFuture&lt;Void&gt; d = new CompletableFuture&lt;Void&gt;(); e.execute(new AsyncRun(d, f)); return d;&#125; 很简单，只是创建了CompletableFuture后，创建一个AsyncRun对象，然后放到执行器中执行而已。 添加子任务源码对应的方法thenRun(Runnable action) 123456789101112private CompletableFuture&lt;Void&gt; uniRunStage(Executor e, Runnable f) &#123; if (f == null) throw new NullPointerException(); CompletableFuture&lt;Void&gt; d = new CompletableFuture&lt;Void&gt;(); // this是主任务的CompletableFuture // if (e != null || !d.uniRun(this, f, null)) &#123; UniRun&lt;T&gt; c = new UniRun&lt;T&gt;(e, d, this, f); push(c); c.tryFire(SYNC); &#125; return d;&#125; 代码中的this是主任务的CompletableFuture。而d.uniRun(this, f, null)，就是使用子任务的CompletableFuturer进行方法调用，而uniRun方法其实就是尝试使用方法调用者的现场去程执行子方法的逻辑 123456789101112131415161718192021222324252627final boolean uniRun(CompletableFuture&lt;?&gt; a, Runnable f, UniRun&lt;?&gt; c) &#123; Object r; Throwable x; if (a == null || (r = a.result) == null || f == null) // a.result == null 表示主任务还未执行完成，所以返回false // 返回false表示需要把子任务添加到主任务的CompletableFuture的任务栈结构`stack`中 return false; if (result == null) &#123; // 进入到这里，表示子任务还未处理完成，而且父任务处理完成了 if (r instanceof AltResult &amp;&amp; (x = ((AltResult)r).ex) != null) // 父任务执行失败 completeThrowable(x, r); else try &#123; if (c != null &amp;&amp; !c.claim()) // 这里表示，如果子任务有指定执行器，就是用指定的执行器执行 return false; // 没有指定执行器，就用调用者的线程执行 // 可能是thenRun方法的调用者，也可能是执行父任务的现场 f.run(); // 子任务CompletableFuture.result = AltResult completeNull(); &#125; catch (Throwable ex) &#123; completeThrowable(ex); &#125; &#125; return true;&#125; 这也解析了子任务为什么有时有直接使用主线程执行 假设父任务没有处理完成，就会执行 1234// d是子任务的CompletableFuture，this是父任务的CompletableFuture// e是执行器，f是子任务的具体逻辑UniRun&lt;T&gt; c = new UniRun&lt;T&gt;(e, d, this, f);push(c); 会先把子任务包装成一个UniRun对象，接着执行push(c)方法，这方法其实就是把子任务的UniRun对象添加到父任务的CompletableFuture.stack这个子任务栈结构的栈顶位置。 123456789101112131415161718192021222324// 这个方法支持并发final void push(UniCompletion&lt;?,?&gt; c) &#123; if (c != null) &#123; while (result == null &amp;&amp; !tryPushStack(c)) lazySetNext(c, null); // clear on failure &#125;&#125;// 这个方法尝试把子任务的`UniRun`对象添加到父任务的`CompletableFuture.stack`这个子任务栈结构的栈顶位置。// 整个逻辑如下：// Completion h = （父）CompletableFuture.stack// 子任务的`UniRun`对象 c.next = h// 其实就是（父）CompletableFuture.stack = cfinal boolean tryPushStack(Completion c) &#123; Completion h = stack; lazySetNext(c, h); // 可能存在并发，所以使用cas return UNSAFE.compareAndSwapObject(this, STACK, h, c);&#125;// 该方法把c中next属性设置成next参数static void lazySetNext(Completion c, Completion next) &#123; UNSAFE.putOrderedObject(c, NEXT, next);&#125; 添加完子任务后还会尝试实行子任务c.tryFire(SYNC) 1234567891011121314// UniRun 尝试执行子任务final CompletableFuture&lt;Void&gt; tryFire(int mode) &#123; CompletableFuture&lt;Void&gt; d; CompletableFuture&lt;T&gt; a; if ((d = dep) == null || // 这里的dep就是子任务的CompletableFuture，src就是父任务的CompletableFuture // mode &gt; 0表示使用调用者的现场执行，&lt;= 0表示用子任务指定执行器执行 // 这个方法前边已经讲过，false表示父任务还未执行完成，true表示如任务执行完成了，子任务已经执行完或者已经交给执行去去执行了 !d.uniRun(a = src, fn, mode &gt; 0 ? null : this)) return null; // 这里做清除操作，使UniRun失效 dep = null; src = null; fn = null; // a = src return d.postFire(a, mode);&#125; 在最后的d.postFire(a, mode);就是去继续执行子任务的子任务，也就是去遍历子任务的CompletableFuture的&#96;&#96;stack&#96;这个栈结构的元素并执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354final CompletableFuture&lt;T&gt; postFire(CompletableFuture&lt;?&gt; a, int mode) &#123; if (a != null &amp;&amp; a.stack != null) &#123; if (mode &lt; 0 || a.result == null) // 这里只是清除父任务的stack中已经执行完成的元素 a.cleanStack(); else a.postComplete(); &#125; // 注意，调用这个方法的d是子任务的CompletableFuture // result != null 表示子任务已经执行完成了 // stack != null 表示子任务还有子任务 if (result != null &amp;&amp; stack != null) &#123; if (mode &lt; 0) return this; else // 这个方法就是遍历调用对象`CompletableFuture`的stack结构的 postComplete(); &#125; return null;&#125;final void postComplete() &#123; /* * On each step, variable f holds current dependents to pop * and run. It is extended along only one path at a time, * pushing others to avoid unbounded recursion. */ // this就是调用者 CompletableFuture&lt;?&gt; f = this; Completion h; while ((h = f.stack) != null || (f != this &amp;&amp; (h = (f = this).stack) != null)) &#123; CompletableFuture&lt;?&gt; d; Completion t; // 栈顶元素出栈 if (f.casStack(h, t = h.next)) &#123; if (t != null) &#123; if (f != this) &#123; // 如果子任务还有子任务，就会把这些任务加入到主任务的栈中 // 如果当前的元素已经是最后一个元素了，不会进入进入到 if中，而会往后走，直接执行任务 pushStack(h); continue; &#125; h.next = null; // detach &#125; // 执行出栈后的元素tryFire方法，这个放前面讲过了，就是去执行子任务逻辑的 // 这里的f可能不等于this，这是因为子任务执行的时候，发现子任务还有子任务 // 那么就会把子任务对应的CompletableFuture返回，那么就可以遍历子任务的子任务，并执行了。 // 但是，它不是遍历了就马上执行的，因为有这个判断语句在 // if (t != null) 和 if (f != this) // 如果不是最后一个元素，并且子任务还有子任务，就把这些任务放入到主任务的栈中。对于最后一个任务，会直接执行 f = (d = h.tryFire(NESTED)) == null ? this : d; &#125; &#125;&#125; 其实CompletableFuture中的任务结构其实是一颗树形结构，而postComplete方法就是遍历该树并执行任务 所以，对于上边的例子的任务结构就变成了这样： 而由于postComplete方法中有这段代码： 123456789if (t != null) &#123; if (f != this) &#123; // 如果子任务还有子任务，就会把这些任务加入到主任务的栈中 // 如果当前的元素已经是最后一个元素了，不会进入进入到 if中，而会往后走，直接执行任务 pushStack(h); continue; &#125; h.next = null; // detach&#125; 导致执行的顺序会从第三层开始发生变化。 1pushStack(h); 对于第三层，会把非栈尾的子节点加入到根节点的栈中，执行栈尾节点的任务，接着从栈尾节点开始执行其子节点的任务，直到没有子节点为此，然后会从根节点的栈中弹出栈顶元素，这时也就是第三层中倒数第二个元素，一直这样反复执行。 比如上图的执行顺序就如下图： 1A-&gt;B_1-&gt;C_2-&gt;D_3-&gt;C_1-&gt;D_2-&gt;_D_1 主任务AsyncRun执行过程它的run方法如下： 123456789101112131415161718public void run() &#123; CompletableFuture&lt;Void&gt; d; Runnable f; // dep 就是 创建这个任务时，一起创建的CompletableFuture，也就是主人的CompletableFuture if ((d = dep) != null &amp;&amp; (f = fn) != null) &#123; dep = null; fn = null; if (d.result == null) &#123; try &#123; // 主任务逻辑执行 f.run(); d.completeNull(); &#125; catch (Throwable ex) &#123; d.completeThrowable(ex); &#125; &#125; // 遍历主任务的子任务并执行 d.postComplete(); &#125;&#125; 上边是最简单的情况，就是子任务和子任务的子任务都只有一个。也就是这种结构 执行顺序很简单 如果复杂点的，子任务有多个，比如下面的例子： 123456789101112131415161718192021222324252627282930313233CompletableFuture&lt;String&gt; f = // 任务A CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:A&quot;); sleep(5000); return &quot;A&quot;; &#125;, executor);// 任务Bf.thenApply((v) -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; preTask:&quot; + v + &quot; sout:B_1&quot;); return &quot;B&quot;;&#125;);f = f.thenApply((v) -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; preTask:&quot; + v + &quot; sout:B&quot;); return &quot;B&quot;;&#125;);// 任务Cf.thenApply((v) -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; preTask:&quot; + v + &quot; sout:C_1&quot;); return &quot;C&quot;;&#125;);f = f.thenApply((v) -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; preTask:&quot; + v + &quot; sout:C&quot;); return &quot;C&quot;;&#125;);// 任务Df = f.thenApply((v) -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; preTask:&quot; + v + &quot; sout:D&quot;); downLatch.countDown(); return &quot;D&quot;;&#125;); A任务有两个子任务B和B_1 B任务有两个字任务C和C_1 C任务有一个字任务D 这种执行情况会比上边的复杂些。此时的任务树如下： 所以根据该树可以得出他答应的结果是 123456ABC_1CDB_1 程序执行的结果如下：","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"CompletableFuture使用","slug":"juc/CompletableFuture使用","date":"2021-11-18T12:00:05.000Z","updated":"2022-03-23T09:03:57.709Z","comments":true,"path":"blog/juc/CompletableFuture使用/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/CompletableFuture%E4%BD%BF%E7%94%A8/","excerpt":"","text":"JAVA支持的多线程开启方式根据Oracle官方出具的Java文档说明，创建线程的方式只有两种：继承Thread或者实现Runnable接口。 但是这两种方法都存在一个缺陷，没有返回值，也就是说我们无法得知线程执行结果。虽然简单场景下已经满足，但是当我们需要返回值的时候怎么办呢？ Java 1.5 以后的Callable和Future接口就解决了这个问题，我们可以通过向线程池提交一个Callable来获取一个包含返回值的Future对象，从此，我们的程序逻辑就不再是同步顺序。 下面是Java8实战书籍的原文： Future接口在Java5中被引入，设计初衷是对将来某个时刻会产生的结果进行建模。它建模了一种异步运算，返回一个执行结果的引用，当运算结束后，这个引用被返回给调用方。在Future中触发那些潜在耗时的操作完成。如下图： 我们从最初的串行操作变成了并行，在异步的同时，我们还可以做其他事情来节约程序运行时间。 Future接口的局限性当我们得到包含结果的Future时，我们可以使用get方法等待线程完成并获取返回值，Future的get() 方法会阻塞主线程。 Future文档原文如下 A {@code Future} represents the result of an asynchronous computation. Methods are provided to check if the computation is complete, to wait for its completion, and to retrieve the result of the computation. 谷歌翻译： {@code Future}代表异步*计算的结果。提供了一些方法来检查计算是否完成，等待其完成并检索计算结果。 Future执行耗时任务由此我们得知，Future获取得线程执行结果前，我们的主线程get()得到结果需要一直阻塞等待，即使我们使用isDone()方法轮询去查看线程执行状态，但是这样也非常浪费cpu资源。 当Future的线程进行了一个非常耗时的操作，那我们的主线程也就阻塞了。 当我们在简单业务上，可以使用Future的另一个重载方法get(long,TimeUnit)来设置超时时间，避免我们的主线程被无穷尽地阻塞。 不过，有没有更好的解决方案呢？ 我们需要更强大异步能力不仅如此，当我们在碰到一下业务场景的时候，单纯使用Future接口或者FutureTask类并不能很好地完成以下我们所需的业务 将两个异步计算合并为一个，这两个异步计算之间相互独立，同时第二个又依赖于第一个的结果 等待Future集合种的所有任务都完成。 仅等待Future集合种最快结束的任务完成(有可能因为他们试图通过不同的方式计算同一个值)，并返回它的结果。 通过编程方式完成一个Future任务的执行(即以手工设定异步操作结果的方式)。 应对Future的完成时间(即当Future的完成时间完成时会收到通知，并能使用Future的计算结果进行下一步的的操作，不只是简单地阻塞等待操作的结果) 神奇的CompletableFuture什么是CompletableFuture在Java 8中, 新增加了一个包含50个方法左右的类: CompletableFuture，结合了Future的优点，提供了非常强大的Future的扩展功能，可以帮助我们简化异步编程的复杂性，提供了函数式编程的能力，可以通过回调的方式处理计算结果，并且提供了转换和组合CompletableFuture的方法。 CompletableFuture被设计在Java中进行异步编程。异步编程意味着在主线程之外创建一个独立的线程，与主线程分隔开，并在上面运行一个非阻塞的任务，然后通知主线程进展，成功或者失败。 通过这种方式，你的主线程不用为了任务的完成而阻塞&#x2F;等待，你可以用主线程去并行执行其他的任务。 使用这种并行方式，极大地提升了程序的表现。 Java8源码doc注释： 译文： 123456789101112131415161718192021当一个Future可能需要显示地完成时，使用CompletionStage接口去支持完成时触发的函数和操作。当2个以上线程同时尝试完成、异常完成、取消一个CompletableFuture时，只有一个能成功。CompletableFuture实现了CompletionStage接口的如下策略：1.为了完成当前的CompletableFuture接口或者其他完成方法的回调函数的线程，提供了非异步的完成操作。2.没有显式入参Executor的所有async方法都使用ForkJoinPool.commonPool()为了简化监视、调试和跟踪， 所有生成的异步任务都是标记接口AsynchronousCompletionTask的实例。3.所有的CompletionStage方法都是独立于其他共有方法实现的，因此一个方法的行为不会受到子类中其他 方法的覆盖。CompletableFuture实现了Futurre接口的如下策略：1.CompletableFuture无法直接控制完成，所以cancel操作被视为是另一种异常完成形式。 方法isCompletedExceptionally可以用来确定一个CompletableFuture是否以任何异常的方式完成。2.以一个CompletionException为例，方法get()和get(long,TimeUnit)抛出一个ExecutionException， 对应CompletionException。为了在大多数上下文中简化用法，这个类还定义了方法join()和getNow， 而不是直接在这些情况中直接抛出CompletionException。 CompletableFuture API想直接找例子上手的小伙伴可以跳过去后面 实例化CompletableFuture实例化方式 12345public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier);public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier, Executor executor);public static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable);public static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable, Executor executor); 有两种格式，一种是supply开头的方法，一种是run开头的方法 supply开头：这种方法，可以返回异步线程执行之后的结果 run开头：这种不会返回结果，就只是执行线程任务 在实例化方法中，我们是可以指定Executor参数的，当我们不指定的试话，我们所开的并行线程使用的是默认系统及公共线程池ForkJoinPool，而且这些线程都是守护线程。我们在编程的时候需要谨慎使用守护线程，如果将我们普通的用户线程设置成守护线程，当我们的程序主线程结束，JVM中不存在其余用户线程，那么CompletableFuture的守护线程会直接退出，造成任务无法完成的问题。 或者可以使用构造器来创建 123CompletableFuture&lt;String&gt; completableFuture = new CompletableFuture&lt;String&gt;();或者CompletableFuture&lt;String&gt; completableFuture = new CompletableFuture&lt;String&gt;(result); 这种就是把父任务的定义从一开始指定，变成了稍后指定。本质是一样的。 获取结果同步获取结果 1234public T get()public T get(long timeout, TimeUnit unit)public T getNow(T valueIfAbsent)public T join() 简单的例子12CompletableFuture&lt;Integer&gt; future = new CompletableFuture&lt;&gt;();Integer integer = future.get(); get() 方法同样会阻塞直到任务完成，上面的代码，主线程会一直阻塞，因为这种方式创建的future从未完成。有兴趣的小伙伴可以打个断点看看，状态会一直是not completed 前两个方法比较通俗易懂，认真看完上面Future部分的小伙伴肯定知道什么意思。 getNow() 则有所区别，参数valueIfAbsent的意思是当计算结果不存在或者Now时刻没有完成任务，给定一个确定的值。 join() 与get() 区别在于join() 返回计算的结果或者抛出一个unchecked异常(CompletionException)，而get() 返回一个具体的异常. 计算完成后续操作1——complete1234public CompletableFuture&lt;T&gt; whenComplete(BiConsumer&lt;? super T,? super Throwable&gt; action)public CompletableFuture&lt;T&gt; whenCompleteAsync(BiConsumer&lt;? super T,? super Throwable&gt; action)public CompletableFuture&lt;T&gt; whenCompleteAsync(BiConsumer&lt;? super T,? super Throwable&gt; action, Executor executor)public CompletableFuture&lt;T&gt; exceptionally(Function&lt;Throwable,? extends T&gt; fn) 方法1和2的区别在于是否使用异步处理，2和3的区别在于是否使用自定义的线程池，前三个方法都会提供一个返回结果和可抛出异常，我们可以使用lambda表达式的来接收这两个参数，然后自己处理。 方法4，接收一个可抛出的异常，且必须return一个返回值，类型与钻石表达式种的类型一样，详见下文的exceptionally() 部分，更详细 1234567CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; return 10086;&#125;);future.whenComplete((result, error) -&gt; &#123; System.out.println(&quot;拨打&quot;+result); error.printStackTrace();&#125;); 计算完成后续操作2——handle123public &lt;U&gt; CompletableFuture&lt;U&gt; handle(BiFunction&lt;? super T,Throwable,? extends U&gt; fn)public &lt;U&gt; CompletableFuture&lt;U&gt; handleAsync(BiFunction&lt;? super T,Throwable,? extends U&gt; fn)public &lt;U&gt; CompletableFuture&lt;U&gt; handleAsync(BiFunction&lt;? super T,Throwable,? extends U&gt; fn, Executor executor) handle方法集和上面的complete方法集没有区别，同样有两个参数一个返回结果和可抛出异常，区别就在于返回值，虽然同样返回CompletableFuture类型，但是里面的参数类型，handle方法是可以自定义的。 12345678910111213141516// 开启一个异步方法CompletableFuture&lt;List&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;语文&quot;); list.add(&quot;数学&quot;); // 获取得到今天的所有课程 return list;&#125;);// 使用handle()方法接收list数据和error异常CompletableFuture&lt;Integer&gt; future2 = future.handle((list,error)-&gt; &#123; // 如果报错，就打印出异常 // error.printStackTrace(); // 如果不报错，返回一个包含Integer的全新的CompletableFuture return list.size(); // 注意这里的两个CompletableFuture包含的返回类型不同&#125;); 计算完成的后续操作3——run123public CompletableFuture&lt;Void&gt; thenRun(Runnable action)public CompletableFuture&lt;Void&gt; thenRunAsync(Runnable action)public CompletableFuture&lt;Void&gt; thenRunAsync(Runnable action, Executor executor) 任务之间没有联系的可以使用这种API。第2中会使用公共线程池ForkJoinPool来执行任务。 计算完成的后续操作4——apply123public &lt;U&gt; CompletableFuture&lt;U&gt; thenApply(Function&lt;? super T,? extends U&gt; fn)public &lt;U&gt; CompletableFuture&lt;U&gt; thenApplyAsync(Function&lt;? super T,? extends U&gt; fn)public &lt;U&gt; CompletableFuture&lt;U&gt; thenApplyAsync(Function&lt;? super T,? extends U&gt; fn, Executor executor) 为什么这三个方法被称作，计算完成的后续操作2呢，因为apply方法和handle方法一样，都是结束计算之后的后续操作，唯一的不同是，handle方法会给出异常，可以让用户自己在内部处理，而apply方法只有一个返回结果，如果异常了，会被直接抛出，交给上一层处理。 如果不想每个链式调用都处理异常，那么就使用apply吧。 例子：请看下面的exceptionally() 示例 计算完成的后续操作5——accept123public CompletableFuture&lt;Void&gt; thenAccept(Consumer&lt;? super T&gt; action)public CompletableFuture&lt;Void&gt; thenAcceptAsync(Consumer&lt;? super T&gt; action)public CompletableFuture&lt;Void&gt; thenAcceptAsync(Consumer&lt;? super T&gt; action, Executor executor) accept()三个方法只做最终结果的消费，注意此时返回的CompletableFuture是空返回。只消费，无返回，有点像流式编程的终端操作。 例子：请看下面的exceptionally() 示例 捕获中间产生的异常——exceptionally1public CompletableFuture&lt;T&gt; exceptionally(Function&lt;Throwable, ? extends T&gt; fn) exceptionally() 可以帮我们捕捉到所有中间过程的异常，方法会给我们一个异常作为参数，我们可以处理这个异常，同时返回一个默认值，跟服务降级 有点像，默认值的类型和上一个操作的返回值相同。 小贴士 ：向线程池提交任务的时候发生的异常属于外部异常，是无法捕捉到的，毕竟还没有开始执行任务。作者也是在触发线程池拒绝策略的时候发现的。exceptionally() 无法捕捉RejectedExecutionException() 12345678910111213141516171819202122// 实例化一个CompletableFuture,返回值是IntegerCompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; // 返回null return null;&#125;);CompletableFuture&lt;String&gt; exceptionally = future.thenApply(result -&gt; &#123; // 制造一个空指针异常NPE int i = result; return i;&#125;).thenApply(result -&gt; &#123; // 这里不会执行，因为上面出现了异常 String words = &quot;现在是&quot; + result + &quot;点钟&quot;; return words;&#125;).exceptionally(error -&gt; &#123; // 我们选择在这里打印出异常 error.printStackTrace(); // 并且当异常发生的时候，我们返回一个默认的文字 return &quot;出错啊~&quot;;&#125;);exceptionally.thenAccept(System.out::println); 最后输出结果 组合式异步编程组合两个completableFuture还记得我们上面说的Future做不到的事吗 将两个异步计算合并为一个，这两个异步计算之间相互独立，同时第二个又依赖于第一个的结果。 thenApply()假设一个场景，我是一个小学生，我想知道今天我需要上几门课程 此时我需要两个步骤，1.根据我的名字获取我的学生信息 2.根据我的学生信息查询课程 我们可以用下面这种方式来链式调用api，使用上一步的结果进行下一步操作 1234567CompletableFuture&lt;List&lt;Lesson&gt;&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; // 根据学生姓名获取学生信息 return StudentService.getStudent(name);&#125;).thenApply(student -&gt; &#123; // 再根据学生信息获取今天的课程 return LessonsService.getLessons(student);&#125;); 我们根据学生姓名获取学生信息，然后使用把得到的学生信息student传递到apply() 方法再获取得到学生今天的课程列表。 将两个异步计算合并为一个，这两个异步计算之间相互独立，互不依赖 thenCompose()假设一个场景，我是一个小学生，今天有劳技课和美术课，我需要查询到今天需要带什么东西到学校 123456789101112131415161718192021CompletableFuture&lt;List&lt;String&gt;&gt; total = CompletableFuture.supplyAsync(() -&gt; &#123; // 第一个任务获取美术课需要带的东西，返回一个list List&lt;String&gt; stuff = new ArrayList&lt;&gt;(); stuff.add(&quot;画笔&quot;); stuff.add(&quot;颜料&quot;); return stuff;&#125;).thenCompose(list -&gt; &#123; // 向第二个任务传递参数list(上一个任务美术课所需的东西list) CompletableFuture&lt;List&lt;String&gt;&gt; insideFuture = CompletableFuture.supplyAsync(() -&gt; &#123; List&lt;String&gt; stuff = new ArrayList&lt;&gt;(); // 第二个任务获取劳技课所需的工具 stuff.add(&quot;剪刀&quot;); stuff.add(&quot;折纸&quot;); // 合并两个list，获取课程所需所有工具 List&lt;String&gt; allStuff = Stream.of(list, stuff).flatMap(Collection::stream).collect(Collectors.toList()); return allStuff; &#125;); return insideFuture;&#125;);System.out.println(total.join().size()); 我们通过CompletableFuture.supplyAsync() 方法创建第一个任务，获得美术课所需的物品list，然后使用thenCompose() 接口传递list到第二个任务，然后第二个任务获取劳技课所需的物品，整合之后再返回。至此我们完成两个任务的合并。 (说实话，用compose去实现这个业务场景看起来有点别扭，我们看下一个例子) 将两个异步计算合并为一个，这两个异步计算之间相互独立，互不依赖 thenCombine()还是上面那个场景，我是一个小学生，今天有劳技课和美术课，我需要查询到今天需要带什么东西到学校 123456789101112131415161718192021222324CompletableFuture&lt;List&lt;String&gt;&gt; painting = CompletableFuture.supplyAsync(() -&gt; &#123; // 第一个任务获取美术课需要带的东西，返回一个list List&lt;String&gt; stuff = new ArrayList&lt;&gt;(); stuff.add(&quot;画笔&quot;); stuff.add(&quot;颜料&quot;); return stuff; &#125;); CompletableFuture&lt;List&lt;String&gt;&gt; handWork = CompletableFuture.supplyAsync(() -&gt; &#123; // 第二个任务获取劳技课需要带的东西，返回一个list List&lt;String&gt; stuff = new ArrayList&lt;&gt;(); stuff.add(&quot;剪刀&quot;); stuff.add(&quot;折纸&quot;); return stuff; &#125;); CompletableFuture&lt;List&lt;String&gt;&gt; total = painting // 传入handWork列表，然后得到两个CompletableFuture的参数Stuff1和2 .thenCombine(handWork, (stuff1, stuff2) -&gt; &#123; // 合并成新的list List&lt;String&gt; totalStuff = Stream.of(stuff1, stuff1) .flatMap(Collection::stream) .collect(Collectors.toList()); return totalStuff; &#125;); System.out.println(JSONObject.toJSONString(total.join())); 等待Future集合中的所有任务都完成。 获取所有完成结果——allOf1public static CompletableFuture&lt;Void&gt; allOf(CompletableFuture&lt;?&gt;... cfs) allOf方法，当所有给定的任务完成后，返回一个全新的已完成CompletableFuture 12345678910111213141516CompletableFuture&lt;Integer&gt; future1 = CompletableFuture.supplyAsync(() -&gt; &#123; try &#123; //使用sleep()模拟耗时操作 TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return 1;&#125;);CompletableFuture&lt;Integer&gt; future2 = CompletableFuture.supplyAsync(() -&gt; &#123; return 2;&#125;);CompletableFuture.allOf(future1, future1);// 输出3System.out.println(future1.join()+future2.join()); 获取率先完成的任务结果——anyOf 仅等待Future集合种最快结束的任务完成(有可能因为他们试图通过不同的方式计算同一个值)，并返回它的结果。 小贴士 ：如果最快完成的任务出现了异常，也会先返回异常，如果害怕出错可以加个exceptionally() 去处理一下可能发生的异常并设定默认返回值 123456789101112131415161718192021public static CompletableFuture&lt;Object&gt; anyOf(CompletableFuture&lt;?&gt;... cfs)CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; throw new NullPointerException(); &#125;); CompletableFuture&lt;Integer&gt; future2 = CompletableFuture.supplyAsync(() -&gt; &#123; try &#123; // 睡眠3s模拟延时 TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return 1; &#125;); CompletableFuture&lt;Object&gt; anyOf = CompletableFuture .anyOf(future, future2) .exceptionally(error -&gt; &#123; error.printStackTrace(); return 2; &#125;); System.out.println(anyOf.join()); 几个小例子多个方法组合使用 通过编程方式完成一个Future任务的执行(即以手工设定异步操作结果的方式)。 应对Future的完成时间(即当Future的完成时间完成时会收到通知，并能使用Future的计算结果进行下一步的的操作，不只是简单地阻塞等待操作的结果) 1234567891011121314public static void main(String[] args) &#123; CompletableFuture.supplyAsync(() -&gt; 1) .whenComplete((result, error) -&gt; &#123; System.out.println(result); error.printStackTrace(); &#125;) .handle((result, error) -&gt; &#123; error.printStackTrace(); return error; &#125;) .thenApply(Object::toString) .thenApply(Integer::valueOf) .thenAccept((param) -&gt; System.out.println(&quot;done&quot;));&#125; 循环创建并发任务123456789101112131415161718192021222324252627282930313233343536373839public static void main(String[] args) &#123; long begin = System.currentTimeMillis(); // 自定义一个线程池 ExecutorService executorService = Executors.newFixedThreadPool(10); // 循环创建10个CompletableFuture List&lt;CompletableFuture&lt;Integer&gt;&gt; collect = IntStream.range(1, 10).mapToObj(i -&gt; &#123; CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; // 在i=5的时候抛出一个NPE if (i == 5) &#123; throw new NullPointerException(); &#125; try &#123; // 每个依次睡眠1-9s，模拟线程耗时 TimeUnit.SECONDS.sleep(i); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(i); return i; &#125;, executorService) // 这里处理一下i=5时出现的NPE // 如果这里不处理异常，那么异常会在所有任务完成后抛出,小伙伴可自行测试 .exceptionally(Error -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(5); System.out.println(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return 100; &#125;); return future; &#125;).collect(Collectors.toList()); // List列表转成CompletableFuture的Array数组,使其可以作为allOf()的参数 // 使用join()方法使得主线程阻塞，并等待所有并行线程完成 CompletableFuture.allOf(collect.toArray(new CompletableFuture[]&#123;&#125;)).join(); System.out.println(&quot;最终耗时&quot; + (System.currentTimeMillis() - begin) + &quot;毫秒&quot;); executorService.shutdown(); &#125; 使用CompletableFuture场景 执行比较耗时的操作时，尤其是那些依赖一个或多个远程服务的操作，使用异步任务可以改善程序的性能，加快程序的响应速度 使用CompletableFuture类，它提供了异常管理的机制，让你有机会抛出、管理异步任务执行种发生的异常 如果这些异步任务之间相互独立，或者他们之间的的某一些的结果是另一些的输入，你可以讲这些异步任务构造或合并成一个 需要注意的问题问题1123456789101112131415161718192021222324public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(4, 4, 10, TimeUnit.MINUTES, new LinkedBlockingQueue&lt;&gt;()); CompletableFuture&lt;Void&gt; future = // 任务1 CompletableFuture.runAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:1&quot;); &#125;, executor); try &#123; Thread.sleep(1000); &#125; catch (Exception e) &#123; &#125; // 任务2-1 future.thenRun(() -&gt; &#123; System.out.println(2); &#125;); // 或者 // 任务2-2 future.thenRunAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:2&quot;); &#125;, executor);&#125; 任务2是要等到任务1完成才会执行。 任务2-1的执行现场有两种情况 如果在执行future.thenRun的时候，第一个任务已经完成了，那么它会在主线程（也就是main现场）中运行，而不会异步执行。 如果在执行future.thenRun的时候，第一个任务还没有完成，那么它会在执行第一个任务的现场中执行。 这里的第一个任务其实不是很准确，更应该说是上一个任务，因为future.thenRun这种不带Async后缀的API的执行线程正常的情况下是使用上一个任务的执行线程的，比如代码改成这样 123456789101112131415161718192021CompletableFuture&lt;Void&gt; // 任务1f = CompletableFuture.runAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:1&quot;);&#125;, executor);try &#123; Thread.sleep(1000);&#125; catch (Exception e) &#123;&#125;// 任务2-1f = f.thenRunAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:2&quot;); try &#123; Thread.sleep(1000); &#125; catch (Exception e) &#123; &#125;&#125;, executor);// 任务3f = f.thenRun(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:3&quot;);&#125;); 它的输出如下。 但是CompletableFuture会对这种不带Async后缀的API进行优化，如果上一个任务完成后，再调用这种API，就使用主线程来执行，使得执行器的线程会更快的执行完成。所以对于这种API一定要注意，避免新添加的任务导致主线程的执行时间过长。 任务2-2就不会有上边的两种情况，它都会在任务1完成后，在指定的executor中执行。 其他类型的API的执行情况都和上边的一样。 问题21234567891011121314151617181920212223242526public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(4, 4, 1000, TimeUnit.MINUTES, new LinkedBlockingQueue&lt;&gt;()); CompletableFuture&lt;String&gt; f = // 任务A CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; sout:A&quot;); sleep(5000); return &quot;A&quot;; &#125;, executor); // 任务B f.thenApply((v) -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; preTask:&quot; + v + &quot; sout:B&quot;); return &quot;B&quot;; &#125;); // 任务C f.thenApply((v) -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; preTask:&quot; + v + &quot; sout:C&quot;); return &quot;C&quot;; &#125;); // 任务D f.thenApply((v) -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; preTask:&quot; + v + &quot; sout:D&quot;); return &quot;D&quot;; &#125;);&#125; 比如上边的例子，任务B、C、D是没有任何关联的，他们的上一个任务都是任务A。 注意一下A的下一个任务B、C、D的执行顺序，不是按加入到任务A的顺序来执行的，而是后加入先执行的，所以在一个CompletableFuture中，子任务是使用栈来管理的。 这里的加入顺序是A、B、D。 CompletableFuture中有一个属性Completion stack，这是一个栈结构，而且这个属性就是栈顶元素，也就是下一个要执行的任务。子任务执行完了会执行出栈操作，然后执行栈顶的任务，一直直到栈为空为止 就算把上边的API换成异步，执行顺序都一样（加入线程池的顺序）。 但是打印的顺序可能会变了，因为执行的线程不同了。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"CAS","slug":"juc/CAS","date":"2021-11-18T12:00:04.000Z","updated":"2022-03-23T09:03:57.704Z","comments":true,"path":"blog/juc/CAS/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/CAS/","excerpt":"","text":"每一个CAS操作过程都包含三个运算符：一个内存地址V，一个期望的值A和一个新值B，操作的时候如果这个地址上存放的值等于这个期望的值A，则将地址上的值赋为新值B，否则不做任何操作。 CAS的基本思路就是，如果这个地址上的值和期望的值相等，则给其赋予新值，否则不做任何事儿，但是要返回原值是多少。循环CAS就是在一个循环里不断的做cas操作，直到成功为止。 CAS实现原子操作的三大问题ABA问题因为CAS需要在操作值的时候，检查值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。 ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加1，那么A→B→A就会变成1A→2B→3A。 循环时间长开销大自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。 只能保证一个共享变量的原子操作。当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁。 还有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如，有两个共享变量i＝2，j&#x3D;a，合并一下ij&#x3D;2a，然后用CAS来操作ij。从Java 1.5开始，JDK提供了AtomicReference类来保证引用对象之间的原子性，就可以把多个变量放在一个对象里来进行CAS操作。 java中的无锁编程Atomic** 这些类里面的原理都是使用了循环CAS + volatile关键字","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"ArrayList","slug":"juc/ArrayList","date":"2021-11-18T12:00:03.000Z","updated":"2022-03-23T09:03:57.703Z","comments":true,"path":"blog/juc/ArrayList/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/ArrayList/","excerpt":"","text":"其实代码挺简单的， 通过无参构造方法后，默认容量是多少。 123public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; 所以要第一次add才会触发扩容。 1234567891011121314151617public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125;private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125; 由于一开始的时候minCapacity&#x3D;1，而且elementData &#x3D;&#x3D; DEFAULTCAPACITY_EMPTY_ELEMENTDATA，所以第一次add时会把数组扩容为DEFAULT_CAPACITY&#x3D;10 1234/** * Default initial capacity. */private static final int DEFAULT_CAPACITY = 10; 好了，接着看扩容 12345678910111213141516171819private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 可以看到，int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);意思就是变为原来的1.5倍","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"AQS","slug":"juc/AQS","date":"2021-11-18T12:00:02.000Z","updated":"2022-03-23T09:03:57.703Z","comments":true,"path":"blog/juc/AQS/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/AQS/","excerpt":"","text":"看源码前，先看下在这类的内部定义的Node类:Node 线程的2种等待模式： SHARED：表示线程以共享的模式等待锁（如ReadLock） EXCLUSIVE：表示线程以互斥的模式等待锁（如ReetrantLock），互斥就是一把锁只能由一个线程持有，不能同时存在多个线程使用同一个锁 线程在队列中的状态枚举： CANCELLED：值为1，表示线程的获锁请求已经“取消” SIGNAL：值为-1，表示该线程一切都准备好了,就等待锁空闲出来给我 CONDITION：值为-2，表示线程等待某一个条件（Condition）被满足 PROPAGATE：值为-3，当线程处在“SHARED”模式时，该字段才会被使用上 成员变量： waitStatus：该int变量表示线程在队列中的状态，其值就是上述提到的CANCELLED、SIGNAL、CONDITION、PROPAGATE prev：该变量类型为Node对象，表示该节点的前一个Node节点（前驱） next：该变量类型为Node对象，表示该节点的后一个Node节点（后继） thread：该变量类型为Thread对象，表示该节点的代表的线程 nextWaiter：该变量类型为Node对象，表示等待condition条件的Node节点 当前线程获取同步状态失败时，同步器会将当前线程以及等待状态等信息构造成为一个节点（Node）并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点中的线程唤醒，使其再次尝试获取同步状态。同步队列中的节点（Node）用来保存获取同步状态失败的线程引用、等待状态以及前驱和后继节点。 AQS重要的变量。 从ReentrantLock的NonfairSync开始ReentrantLock使用 123456public static void main(String[] args) throws InterruptedException &#123; ReentrantLock lock = new ReentrantLock(); lock.lock(); //do something lock.unlock();&#125; 先看new ReentrantLock(); 1234567/** * Creates an instance of &#123;@code ReentrantLock&#125;. * This is equivalent to using &#123;@code ReentrantLock(false)&#125;. */public ReentrantLock() &#123; sync = new NonfairSync();&#125; 好了，从 NonfairSync开始 123456789101112131415161718static final class NonfairSync extends Sync &#123; private static final long serialVersionUID = 7316153563782823691L; /** * Performs lock. Try immediate barge, backing up to normal * acquire on failure. */ final void lock() &#123; if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires); &#125;&#125; 在这加锁方法的第一步，就是执行了CAS(0,1)，也就是说，只要这个操作成功了，就意味着获取了锁，如果一个线程刚好执行完了，然后有个线程执行这个cas操作成功了，那这个线程就不用进入同步队列了，这也就是为什么说这是不公平锁的原因。 好了，现在假设cas失败，也就是获取锁失败，acquire(1)： 1234567891011121314151617/** * Acquires in exclusive mode, ignoring interrupts. Implemented * by invoking at least once &#123;@link #tryAcquire&#125;, * returning on success. Otherwise the thread is queued, possibly * repeatedly blocking and unblocking, invoking &#123;@link * #tryAcquire&#125; until success. This method can be used * to implement method &#123;@link Lock#lock&#125;. * * @param arg the acquire argument. This value is conveyed to * &#123;@link #tryAcquire&#125; but is otherwise uninterpreted and * can represent anything you like. */public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 先看tryAcquire，跟踪代码，然后到这里: 1234567891011121314151617181920212223242526 protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires); &#125; /** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */ final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125; 在第一步失败后，接着执行第二步，再次去尝试获取锁。先检查状态值state是否为0，如果是的话再一次执行一次cas操作，抢一下锁。如果不为0就判断当前线程是否为获取了锁的线程，如果是的话就state+1（这步是锁的可重入了）不是的话就要执行第三步了。从上诉代码可以看出，tryAcquire实际上就是一次尝试获取锁过程。 先看第三步执行的代码，在acquire(1)：中tryAcquire(arg)返回false了，所以会接着执行acquireQueued(addWaiter(Node.EXCLUSIVE), arg)，先看addWaiter(Node.EXCLUSIVE)，这也是第三步执行的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Creates and enqueues node for current thread and given mode. * * @param mode Node.EXCLUSIVE for exclusive, Node.SHARED for shared * @return the new node */private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125;/** * Inserts node into queue, initializing if necessary. See picture above. * @param node the node to insert * @return node&#x27;s predecessor */private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 这时有两种情况， 第一种就是一开始线程A、B。A线程获取了锁，B失败，这是B线程执行到addWaiter时由于tail仍然为null，所以走了enq方法。从enq方法可以看出，先出初始化了head为一个空节点（并不是说它是null，而是该节点步不保存任何信息），接着就把B线程所代表的Node节点赋值给tail，这时同步队列的情况是： 第二种情况，线程C这时进来发现tail不为null，就把C线程所代表的节点设置为tail，这是同步队列的情况： 所以addWaiter的方法的作用就是把代表当前线程的节点Node放入同步队列的对尾。 好了，进入同步队列成功了，接着看第4步acquireQueued： 1234567891011121314151617181920212223242526272829/** * Acquires in exclusive uninterruptible mode for thread already in * queue. Used by condition wait methods as well as acquire. * * @param node the node * @param arg the acquire argument * @return &#123;@code true&#125; if interrupted while waiting */final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 这时先判断当前节点的前一个节点是不是head，若是，并且尝试获取锁，成功了，那代表前一个线程已经释放锁了，所以这时把head指向当前节点。如果失败的情况下看：shouldParkAfterFailedAcquire： 123456789101112131415161718192021222324252627282930313233343536/** * Checks and updates status for a node that failed to acquire. * Returns true if thread should block. This is the main signal * control in all acquire loops. Requires that pred == node.prev. * * @param pred node&#x27;s predecessor holding status * @param node the node * @return &#123;@code true&#125; if thread should block */private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; if (ws &gt; 0) &#123; /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don&#x27;t park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; 第一个if (ws &#x3D;&#x3D; Node.SIGNAL)代表当前节点的前继节点的状态为Node.SIGNAL，表示前继节点为等待获取锁的状态。 第二个if (ws &gt; 0) 代表当前节点的前继节点已经被取消了，也就是说不用获取锁了，那这是就要调整下同步队列。 最后，上面的if都不满足的情况下，会把当前节点的前继节点状态设置为Node.SIGNAL（如果head为空节点，这时这个节点就会变为Node.SIGNAL了），当前线程又重复一次第4步的步骤，如果还是获取锁失败了，就执行第五步：parkAndCheckInterrupt() 第4步的核心就是把当前节点的前继节点状态设置为Node.SIGNAL，接着看第五步：parkAndCheckInterrupt()： 1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 这步就是堵塞当前线程。 好了，对一个线程的加锁已经完成了，现在看下解锁： 12345678910111213141516171819202122public void unlock() &#123; sync.release(1);&#125;/** * Releases in exclusive mode. Implemented by unblocking one or * more threads if &#123;@link #tryRelease&#125; returns true. * This method can be used to implement method &#123;@link Lock#unlock&#125;. * * @param arg the release argument. This value is conveyed to * &#123;@link #tryRelease&#125; but is otherwise uninterpreted and * can represent anything you like. * @return the value returned from &#123;@link #tryRelease&#125; */public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; 看tryRelease: 123456789101112protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; 可以看到这个方法是没有同步操作的，因为对于同一把锁，同一时刻执行release的线程只有一个。 状态state-1，如果为0表示该线程已经释放完锁了，大于0表示该线程还有锁没有释放。 假设state为0了，看unparkSuccessor： 12345678910111213141516171819202122232425262728293031/** * Wakes up node&#x27;s successor, if one exists. * * @param node the node */private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; 这时经过第4步，已经把head的waitStatus设置为Node.SIGNAL了，一个小于0的数。 123456if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t;&#125; 这段代码的含义就是，如果头节点的后继节点为null，就意味着没有线程在堵塞；如果头节点的后继节点的状态为CANCELLED，就从尾部开始，找到最后一个节点的状态为小于0的节点。 好了最后就是执行LockSupport.unpark唤醒下一个堵塞的线程了。 当一个线程被唤醒后， 123456789101112131415161718192021final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 从 parkAndCheckInterrupt()处开始执行，执行到了if处，这里还是不一定成功的，因为是不公平锁，所以这时有可能会被后来线程捷足先登。假如if放回true，获取锁成功了，这时就把head设置为当前线程的节点。 总结 线程执行了lock()方法后，线程先尝试获取锁，如果成功了就直接放回，执行后续代码；如果失败了，就执行第2步 在这里，如果是获取锁的线程和执行这步的线程是同一个线程的话，就执行锁的可重入操作state+1，lock()方法结束；如果不是会通过state判断了需不需要去尝试获取下锁，如果state&#x3D;0，代表前面的线程已经释放锁了，这是就去尝试获取锁，因为这时可能有别的线程也到这步了，会有竞争，所以不一定会成功；如果这时还是获取锁失败就会执行第3步 这一步会创建一个表示当前线程的Node节点。如果没竞争就直接放入同步队列的尾部；如果有竞争，就通过cas把该节点放入同步队列的尾部 这时还是会尝试获取一下锁，如果还是失败了，就修改前继结点的状态为Node.SIGNAL（小于0的数字），表示前继节点需要被唤醒。修改前继的状态后，当前线程就会进入堵塞状态。 当获取了锁的线程执行了unlock()后，会执行state-1，如果结果是大于0的，表示该线程还有锁未释放；如果等于0了，表示该线程已经释放完锁了，这时会把head节点的状态设置为0，并且从head节点开始，找到第一个节点状态小于0的节点，并且会唤醒该节点代表的线程。 线程被唤醒后，会执行尝试获取锁的操作，这时不一定成功，如果成功了，就把自己的节点设置为head；如果失败了又要把head的状态改为Node.SIGNAL了。 从这些步骤，看了一个锁慢慢膨胀的过程。一开始一个线程cas成功了，如果在某一段时间内同一个线程执行了lock()，只需要state+1就可以了，并不需要做加锁的动作。接着如果有别的线程执行lock()了，那么之前不需要加锁的状态被终止了，这时该线程会创建一个Node，并且进入同步队列的尾部，然后通过循环cas尝试去获取锁，如果成功，就直接放回了，并不需要堵塞，如果失败了，这时循环cas就会被暂停，这时会直接堵塞线程，膨胀成了重量级锁。 也就是无锁状态—-&gt;偏向锁状态—-&gt;轻量级锁状态—-&gt;重量级锁状态 Condition123456Condition condition = lock.newCondition();//线程Acondition.await(); //线程Bcondition.signal(); AQS的Condition和jvm中的wait()和notifyAll()原理很像。 看下ReentrantLock的实现：ConditionObject 一个Condition包含一个等待队列，Condition拥有首节点（firstWaiter）和尾节点（lastWaiter）。 当前线程A调用Condition.await()方法： 12345678910111213141516171819202122232425262728293031323334public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); int interruptMode = 0; while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125;private Node addConditionWaiter() &#123; Node t = lastWaiter; // If lastWaiter is cancelled, clean out. if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; unlinkCancelledWaiters(); t = lastWaiter; &#125; Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node;&#125; 有了AQS的知识，那这里就好理解了，调用await后，会先创建一个调表是该线程的一个Node，并把该Node放入到等待队列的对尾。并且释放该线程占用的锁（fullyRelease(node)）从加入等待队列和释放锁的过程都是没有使用CAS保证，原因在于调用await()方法的线程必定是获取了锁的线程，也就是说该过程是由锁来保证线程安全的。 Lock（更确切地说是同步器）拥有一个同步队列和多个等待队列。 从队列（同步队列和等待队列）的角度看await()方法，当调用await()方法时，相当于同步队列的首节点（获取了锁的节点）移动到Condition的等待队列中。调用该方法的线程成功获取了锁的线程，也就是同步队列中的首节点，该方法会将当前线程构造成节点并加入等待队列中，然后释放同步状态，唤醒同步队列中的后继节点，然后当前线程会进入等待状态。 当前线程B调用Condition.signal()方法： 1234567891011121314151617181920212223242526272829/** * Moves the longest-waiting thread, if one exists, from the * wait queue for this condition to the wait queue for the * owning lock. * * @throws IllegalMonitorStateException if &#123;@link #isHeldExclusively&#125; * returns &#123;@code false&#125; */public final void signal() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first);&#125;/** * Removes and transfers nodes until hit non-cancelled one or * null. Split out from signal in part to encourage compilers * to inline the case of no waiters. * @param first (non-null) the first node on condition queue */private void doSignal(Node first) &#123; do &#123; if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null);&#125; 最后调用了AQS的transferForSignal方法。 1234567891011121314151617181920212223242526/** * Transfers a node from a condition queue onto sync queue. * Returns true if successful. * @param node the node * @return true if successfully transferred (else the node was * cancelled before signal) */final boolean transferForSignal(Node node) &#123; /* * If cannot change waitStatus, the node has been cancelled. */ if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; /* * Splice onto queue and try to set waitStatus of predecessor to * indicate that thread is (probably) waiting. If cancelled or * attempt to set waitStatus fails, wake up to resync (in which * case the waitStatus can be transiently and harmlessly wrong). */ Node p = enq(node); int ws = p.waitStatus; if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) LockSupport.unpark(node.thread); return true;&#125; 调用Condition的signal()方法，将会唤醒在等待队列中等待时间最长的节点（首节点），在唤醒节点之前，会将节点移到同步队列中。调用该方法的前置条件是当前线程必须获取了锁，可以看到signal()方法进行了isHeldExclusively()检查，也就是当前线程必须是获取了锁的线程。接着获取等待队列的首节点，将其移动到同步队列并使用LockSupport唤醒节点中的线程。 被唤醒后的线程，将从await()方法中的while循环中退出（isOnSyncQueue(Node node)方法返回true，节点已经在同步队列中），进而调用同步器的acquireQueued()方法加入到获取同步状态的竞争中。 成功获取同步状态（或者说锁）之后，被唤醒的线程将从先前调用的await()方法返回，此时该线程已经成功地获取了锁。 Condition的signalAll()方法，相当于对等待队列中的每个节点均执行一次signal()方法，效果就是将等待队列中所有节点全部移动到同步队列中，并唤醒每个节点的线程。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"01基础","slug":"juc/01基础","date":"2021-11-18T12:00:01.000Z","updated":"2022-03-23T09:03:57.697Z","comments":true,"path":"blog/juc/01基础/","link":"","permalink":"http://sv.pointcut.cc/blog/juc/01%E5%9F%BA%E7%A1%80/","excerpt":"","text":"stop() 与destory() 函数在Java中，有stop()、destory()之类的函数，但这些函数都是官方明确不建议使用的。原因很简单，如果强制杀死线程，则线程中所使用的资源，例如文件描述符、网络连接等不能正常关闭。 因此，一个线程一旦运行起来，就不要去强行打断它，合理的关闭办法是让其运行完（也就是函数执行完毕），干净地释放掉所有资源，然后退出。如果是一个不断循环运行的线程，就需要用到线程间的通信机制，让主线程通知其退出。 守护线程所谓的守护线程，指的是程序运行时在后台提供的一种通用服务的线程。比如垃圾回收线程就是一个很称职的守护者，并且这种线程并不属于程序中不可或缺的部分。因此，当所有的非守护线程结束时，程序也就终止了，同时会杀死进程中的所有守护线程。反过来说，只要任何非守护线程还在运行，程序就不会终止。 事实上，User Thread（用户线程）和Daemon Thread（守护线程）从本质上来说并没有什么区别，唯一的不同之处就在于虚拟机的离开：如果用户线程已经全部退出运行了，只剩下守护线程存在了，虚拟机也就退出了。 因为没有了被守护者，守护线程也就没有工作可做了，也就没有继续运行程序的必要了。 设置关闭的标志位InterruptedException()函数与interrupt()函数什么情况下会抛出Interrupted异常只有那些声明了会抛出 InterruptedException 的函数才会抛出异常，也就是下面这些常用的函数： public static native void sleep(long millis) throws InterruptedException{…} public final void wait() throws InterruptedException {…} public final void join() throws InterruptedException{…} 注意了：抛出InterruptedException()异常后，会把中断标志位还原 yield()函数暂停当前正在执行的线程对象，并执行其他线程。 yield()应该做的是让当前运行线程回到可运行状态，以允许具有相同优先级的其他线程获得运行机会。因此，使用yield()的目的是让相同优先级的线程之间能适当的轮转执行。但是，实际中无法保证yield()达到让步目的，因为让步的线程还有可能被线程调度程序再次选中。 结论：yield()从未导致线程转到等待&#x2F;睡眠&#x2F;阻塞状态。在大多数情况下，yield()将导致线程从运行状态转到可运行状态，但有可能没有效果。 轻量级阻塞与重量级阻塞能够被中断的阻塞称为轻量级阻塞，对应的线程状态是WAITING或者TIMED_WAITING；而像 synchronized 这种不能被中断的阻塞称为重量级阻塞，对应的状态是 BLOCKED。如图1-1所示的是在调用不同的函数之后，一个线程完整的状态迁移过程。 初始线程处于NEW状态，调用start()之后开始执行，进入RUNNING或者READY状态。如果没有调用任何的阻塞函数，线程只会在RUNNING和READY之间切换，也就是系统的时间片调度。这两种状态的切换是操作系统完成的，开发者基本没有机会介入，除了可以调用yield()函数，放弃对CPU的占用。 一旦调用了图中的任何阻塞函数，线程就会进入WAITING或者TIMED_WAITING状态，两者的区别只是前者为无限期阻塞，后者则传入了一个时间参数，阻塞一个有限的时间。如果使用了synchronized关键字或者synchronized块，则会进入BLOCKED状态。 除了常用的阻塞&#x2F;唤醒函数，还有一对不太常见的阻塞&#x2F;唤醒函数，LockSupport.park()&#x2F;unpark()。这对函数非常关键，Concurrent包中Lock的实现即依赖这一对操作原语。 故而interrupt()的精确含义是“唤醒轻量级阻塞”，而不是字面意思“中断一个线程”。 t.isInterrupted()与Thread.interrupted()的区别因为 interrupt()相当于给线程发送了一个唤醒的信号，所以如果线程此时恰好处于WAITING或者TIMED_WAITING状态，就会抛出一个InterruptedException，并且线程被唤醒。而如果线程此时并没有被阻塞，则线程什么都不会做。但在后续，线程可以判断自己是否收到过其他线程发来的中断信号，然后做一些对应的处理，这也是本节要讲的两个函数。 这两个函数都是线程用来判断自己是否收到过中断信号的，前者是非静态函数，后者是静态函数。二者的区别在于，前者只是读取中断状态，不修改状态；后者不仅读取中断状态，还会重置中断标志位。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"}]},{"title":"13插件开发","slug":"mybatis/13插件开发","date":"2021-11-17T12:00:14.000Z","updated":"2022-03-23T09:03:57.481Z","comments":true,"path":"blog/mybatis/13插件开发/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/13%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/","excerpt":"","text":"mybatis的插件使用了《责任链模式》 插件是用来改变或者扩展 mybatis 的原有的功能，mybaits 的插件就是通过继承 Interceptor 拦截器实现的。 MyBatis 中能使用插件进行拦截的接口如下： Executor StatementHandler ParameterHandler ResultSetHandler 也就是在通过Configuration中的new…方法创建对象的接口才支持。 入门实现Interceptor接口，并加上@Intercepts注解如下： 12345678910111213141516171819202122232425262728293031323334353637@Intercepts(&#123; @Signature(type=StatementHandler.class,method=&quot;query&quot;,args=&#123;Statement.class, ResultHandler.class&#125;)// @Signature(type=StatementHandler.class,method=&quot;query&quot;,args=&#123;MappedStatement.class,Object.class, RowBounds.class, ResultHandler.class&#125;)&#125;)public class ThresholdInterceptor implements Interceptor &#123; private long threshold; @Override public Object intercept(Invocation invocation) throws Throwable &#123; long begin = System.currentTimeMillis(); Object ret = invocation.proceed(); long end=System.currentTimeMillis(); long runTime = end - begin; if(runTime&gt;=threshold)&#123; Object[] args = invocation.getArgs(); Statement stat = (Statement) args[0]; MetaObject metaObjectStat = SystemMetaObject.forObject(stat); PreparedStatementLogger statementLogger = (PreparedStatementLogger)metaObjectStat.getValue(&quot;h&quot;); Statement statement = statementLogger.getPreparedStatement(); System.out.println(&quot;sql语句：“&quot;+statement.toString()+&quot;”执行时间为：&quot;+runTime+&quot;毫秒，已经超过阈值！&quot;); &#125; return ret; &#125; //如果没什么特别要求，都是使用Plugin.wrap这个方法。 @Override public Object plugin(Object target) &#123; return Plugin.wrap(target, this); &#125; @Override public void setProperties(Properties properties) &#123; this.threshold = Long.valueOf(properties.getProperty(&quot;threshold&quot;)); &#125;&#125; Signature注解是要指明这个Interceptor的执行范围。上述代码定义了只有在StatementHandler中的 &lt;E&gt; List&lt;E&gt; query(Statement statement, ResultHandler resultHandler) throws SQLException;这个方法才执行intercept方法。 源码解析初始化阶段会将插件放入Configuration.interceptorChain这个Interceptor链对象中。 在创建4大接口对象时都会执行interceptorChain.pluginAll(executor)方法比如： 由于interceptor的plugin方法绝大部份情况下都是调用Plugin.wrap(target, this)：所以绝大部份情况下都会创建出一条实现了同一接口的责任量，效果如下图:而Proxy的InvocationHandler大部分情况都为Plugin可以看到，只有在signatureMap中的才会执行拦截器的intercept方法","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"12与 spring 结合原理","slug":"mybatis/12与 spring 结合原理","date":"2021-11-17T12:00:13.000Z","updated":"2022-03-23T09:03:57.475Z","comments":true,"path":"blog/mybatis/12与 spring 结合原理/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/12%E4%B8%8E%20spring%20%E7%BB%93%E5%90%88%E5%8E%9F%E7%90%86/","excerpt":"","text":"MyBatis-Spring 是什么Mybatis-Spring 用于帮助你将 MyBatis 代码无缝地整合到 Spring中，集成过程中的增强主 要体现在如下四个方面: Spring 将会加载必要的 MyBatis 工厂类和 session 类; 提供一个简单的方式来注入 MyBatis 数据映射器和 SqlSession 到业务层的 bean中; 方便集成spring事务; 翻译 MyBatis 的异常到 Spring 的 DataAccessException 异常(数据访问异常)中; MyBatis-Spring 集成 可以使用注解，但是注解用xml这样清晰点 在 pom 文件中添加 mybatis-spring 的依赖和mybatis依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.3-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.5.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 配置 SqlSessionFactoryBean 123456&lt;!-- spring和MyBatis完美整合，不需要mybatis的配置映射文件 --&gt;&lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt; &lt;property name=&quot;typeAliasesPackage&quot; value=&quot;com.enjoylearning.mybatis.entity&quot; /&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:sqlmapper/*.xml&quot; /&gt;&lt;/bean&gt; 在 MyBatis-Spring 中， SqlSessionFactoryBean 是用于 创建 Sql SessionFactory 的，几个关键配置选项如下所示: dataSource:用于配置数据源，该属性为必选项，必须通过这个属性配置数据源 mapperLocations:配置SqlSessionFactoryBean扫描XML映射文件的路径，可以使用Ant风格的路径进行配置。 configLocation:用于配置mybatisconfigXML的路径，除了数据源外，对MyBatis的各种配直仍然可以通过这种方式进行，并且配置MyBatissettings时只能使用这种方式。但配置文件中任意环境，数据源和MyBatis的事务管理器都会被忽略; typeAliasesPackage:配置包中类的别名，配置后，包中的类在XML映射文件中使用时可以省略包名部分，直接使用类名。这个配置不支持Ant风格的路径，当需要配置多个包路径时可以使用分号或逗号进行分隔 配置 MapperScannerConfigurer 123&lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.enjoylearning.mybatis.mapper&quot; /&gt;&lt;/bean&gt; 通过 MapperScannerConfigurer 类自动扫描所有的 Mapper 接口，使用时可以直接注入接口。MapperScannerConfigurer 中常配置以下两个属性: basePackage:用于配置基本的包路径。可以使用分号或逗号作为分隔符设置多于一个的包路径，每个映射器将会在指定的包路径中递归地被搜索到。 annotationClass:用于过滤被扫描的接口，如果设置了该属性，那么MyBatis的接口只有包含该注解才会被扫描进去 配置事务，让 Mybatis 集成 spring 的事务 1234567&lt;!-- (事务管理)transaction manager --&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt; &lt;qualifier value=&quot;transactionManager&quot; /&gt;&lt;/bean&gt;&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot; /&gt; 这部分和mybatis没关系 MyBatis-Spring 集成原理分析比如我这样配置： 1234567891011&lt;!-- spring和MyBatis完美整合，不需要mybatis的配置映射文件 --&gt;&lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt; &lt;property name=&quot;typeAliasesPackage&quot; value=&quot;com.enjoylearning.mybatis.entity&quot; /&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:sqlmapper/*.xml&quot; /&gt;&lt;/bean&gt;&lt;bean id=&quot;tUserMapper&quot; class=&quot;org.mybatis.spring.mapper.MapperFactoryBean&quot;&gt; &lt;property name=&quot;mapperInterface&quot; value=&quot;com.enjoylearning.mybatis.mapper.TUserMapper&quot;&gt;&lt;/property&gt; &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; &lt;/bean&gt; SqlSessionFactoryBean 源码分析SqlSessionFactoryBean 实现了 InitializingBean 接口，那么Spring bean的初始化阶段的最后 必然会调用afterPropertiesSet()方法buildSqlSessionFactory方法完成了mybatis的初始化（mapper.xml已经扫描了第一阶段完成）并生成SqlSessionFactory对象 其次，SqlSessionFactoryBean 实现了 FactoryBean 接口，当在容器中配置 FactoryBean 的 实现类时，并不是将该 FactoryBean 注入到容器，而是调用 FactoryBean 的 getObject 方法产 生的实例对象注入容器SqlSessionFactoryBean 就是将 sqlSessionFactory 注入容器，IOC 容器中的其他类型能拿到 SqlSession 实例了，就可以进行相关的 SQL 执行任务了; 执行完该类后mapper已经解析完，也就是说mapper.xml文件中的namespace对应的接口已经在MapperRegistry中有一个MapperProxyFactory与之对应了，那么剩下的工作就是生成一个接口的代理对象，并放入到spring容器中。这一步由MapperFactoryBean完成 MapperFactoryBean 源码分析 封装了Mybatis的第二阶段，注入容器的是通过MapperProxyFactory创建的代理对象。 在这里有一点需要注意。如果没有整合spring，通过这里这样生成的SqlSession是一个局部变量这种方式符合mybatis的定义，每次都生成一个新的SqlSession。整合spring后，由于接口的代理对象是单例的并且会被多个线程调用的，而这时的SqlSession也是持久而且要线程安全，所以MyBatis-Spring引入了SqlSessionTemplate类。 MapperFactoryBean的父类是SqlSessionDaoSupport： 12345678910111213141516171819202122232425262728293031323334353637public abstract class SqlSessionDaoSupport extends DaoSupport &#123; private SqlSession sqlSession; private boolean externalSqlSession; public void setSqlSessionFactory(SqlSessionFactory sqlSessionFactory) &#123; if (!this.externalSqlSession) &#123; this.sqlSession = new SqlSessionTemplate(sqlSessionFactory); &#125; &#125; public void setSqlSessionTemplate(SqlSessionTemplate sqlSessionTemplate) &#123; this.sqlSession = sqlSessionTemplate; this.externalSqlSession = true; &#125; /** * Users should use this method to get a SqlSession to call its statement methods * This is SqlSession is managed by spring. Users should not commit/rollback/close it * because it will be automatically done. * * @return Spring managed thread safe SqlSession */ public SqlSession getSqlSession() &#123; return this.sqlSession; &#125; /** * &#123;@inheritDoc&#125; */ @Override protected void checkDaoConfig() &#123; notNull(this.sqlSession, &quot;Property &#x27;sqlSessionFactory&#x27; or &#x27;sqlSessionTemplate&#x27; are required&quot;); &#125;&#125; 可以看到它会对SqlSessionFactory再一次包装一下，变成了SqlSessionTemplate，之所以这么做在上边已经说了。 SqlSessionTemplate源码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138public class SqlSessionTemplate implements SqlSession, DisposableBean &#123; private final SqlSessionFactory sqlSessionFactory; private final ExecutorType executorType; private final SqlSession sqlSessionProxy; private final PersistenceExceptionTranslator exceptionTranslator; /** * Constructs a Spring managed SqlSession with the &#123;@code SqlSessionFactory&#125; * provided as an argument. * * @param sqlSessionFactory */ public SqlSessionTemplate(SqlSessionFactory sqlSessionFactory) &#123; this(sqlSessionFactory, sqlSessionFactory.getConfiguration().getDefaultExecutorType()); &#125; /** * Constructs a Spring managed SqlSession with the &#123;@code SqlSessionFactory&#125; * provided as an argument and the given &#123;@code ExecutorType&#125; * &#123;@code ExecutorType&#125; cannot be changed once the &#123;@code SqlSessionTemplate&#125; * is constructed. * * @param sqlSessionFactory * @param executorType */ public SqlSessionTemplate(SqlSessionFactory sqlSessionFactory, ExecutorType executorType) &#123; this(sqlSessionFactory, executorType, new MyBatisExceptionTranslator( sqlSessionFactory.getConfiguration().getEnvironment().getDataSource(), true)); &#125; /** * Constructs a Spring managed &#123;@code SqlSession&#125; with the given * &#123;@code SqlSessionFactory&#125; and &#123;@code ExecutorType&#125;. * A custom &#123;@code SQLExceptionTranslator&#125; can be provided as an * argument so any &#123;@code PersistenceException&#125; thrown by MyBatis * can be custom translated to a &#123;@code RuntimeException&#125; * The &#123;@code SQLExceptionTranslator&#125; can also be null and thus no * exception translation will be done and MyBatis exceptions will be * thrown * * @param sqlSessionFactory * @param executorType * @param exceptionTranslator */ public SqlSessionTemplate(SqlSessionFactory sqlSessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator) &#123; notNull(sqlSessionFactory, &quot;Property &#x27;sqlSessionFactory&#x27; is required&quot;); notNull(executorType, &quot;Property &#x27;executorType&#x27; is required&quot;); this.sqlSessionFactory = sqlSessionFactory; this.executorType = executorType; this.exceptionTranslator = exceptionTranslator; this.sqlSessionProxy = (SqlSession) newProxyInstance( SqlSessionFactory.class.getClassLoader(), new Class[] &#123; SqlSession.class &#125;, new SqlSessionInterceptor()); &#125; public SqlSessionFactory getSqlSessionFactory() &#123; return this.sqlSessionFactory; &#125; public ExecutorType getExecutorType() &#123; return this.executorType; &#125; public PersistenceExceptionTranslator getPersistenceExceptionTranslator() &#123; return this.exceptionTranslator; &#125; /** * &#123;@inheritDoc&#125; */ @Override public &lt;T&gt; T selectOne(String statement) &#123; return this.sqlSessionProxy.&lt;T&gt; selectOne(statement); &#125; /** * &#123;@inheritDoc&#125; */ @Override public &lt;T&gt; T selectOne(String statement, Object parameter) &#123; return this.sqlSessionProxy.&lt;T&gt; selectOne(statement, parameter); &#125; //去掉一些方法 ...... @Override public List&lt;BatchResult&gt; flushStatements() &#123; return this.sqlSessionProxy.flushStatements(); &#125; @Override public void destroy() throws Exception &#123; &#125; private class SqlSessionInterceptor implements InvocationHandler &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; SqlSession sqlSession = getSqlSession( SqlSessionTemplate.this.sqlSessionFactory, SqlSessionTemplate.this.executorType, SqlSessionTemplate.this.exceptionTranslator); try &#123; Object result = method.invoke(sqlSession, args); if (!isSqlSessionTransactional(sqlSession, SqlSessionTemplate.this.sqlSessionFactory)) &#123; // force commit even on non-dirty sessions because some databases require // a commit/rollback before calling close() sqlSession.commit(true); &#125; return result; &#125; catch (Throwable t) &#123; Throwable unwrapped = unwrapThrowable(t); if (SqlSessionTemplate.this.exceptionTranslator != null &amp;&amp; unwrapped instanceof PersistenceException) &#123; // release the connection to avoid a deadlock if the translator is no loaded. See issue #22 closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); sqlSession = null; Throwable translated = SqlSessionTemplate.this.exceptionTranslator.translateExceptionIfPossible((PersistenceException) unwrapped); if (translated != null) &#123; unwrapped = translated; &#125; &#125; throw unwrapped; &#125; finally &#123; if (sqlSession != null) &#123; closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); &#125; &#125; &#125; &#125;&#125; 源码中可以看到 1234567891011121314151617181920212223public SqlSessionTemplate(SqlSessionFactory sqlSessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator) &#123; notNull(sqlSessionFactory, &quot;Property &#x27;sqlSessionFactory&#x27; is required&quot;); notNull(executorType, &quot;Property &#x27;executorType&#x27; is required&quot;); this.sqlSessionFactory = sqlSessionFactory; this.executorType = executorType; this.exceptionTranslator = exceptionTranslator; this.sqlSessionProxy = (SqlSession) newProxyInstance( SqlSessionFactory.class.getClassLoader(), new Class[] &#123; SqlSession.class &#125;, new SqlSessionInterceptor());&#125;/** * &#123;@inheritDoc&#125; */@Overridepublic &lt;T&gt; T selectOne(String statement) &#123; return this.sqlSessionProxy.&lt;T&gt; selectOne(statement);&#125;.......... 它去执行Sql的方法都交给了sqlSessionProxy执行，而sqlSessionProxy对象又是一个动态代理类。可以看出SqlSessionTemplate其实只是一个静态的代理，目标类是sqlSessionProxy，而目标对象sqlSessionProxy的InvocationHandler是名为SqlSessionInterceptor的内部类。 123456789101112131415161718192021222324252627282930313233343536373839404142 /** * Proxy needed to route MyBatis method calls to the proper SqlSession got * from Spring&#x27;s Transaction Manager * It also unwraps exceptions thrown by &#123;@code Method#invoke(Object, Object...)&#125; to * pass a &#123;@code PersistenceException&#125; to the &#123;@code PersistenceExceptionTranslator&#125;. */ private class SqlSessionInterceptor implements InvocationHandler &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; SqlSession sqlSession = getSqlSession( SqlSessionTemplate.this.sqlSessionFactory, SqlSessionTemplate.this.executorType, SqlSessionTemplate.this.exceptionTranslator); try &#123; Object result = method.invoke(sqlSession, args); if (!isSqlSessionTransactional(sqlSession, SqlSessionTemplate.this.sqlSessionFactory)) &#123; // force commit even on non-dirty sessions because some databases require // a commit/rollback before calling close() sqlSession.commit(true); &#125; return result; &#125; catch (Throwable t) &#123; Throwable unwrapped = unwrapThrowable(t); if (SqlSessionTemplate.this.exceptionTranslator != null &amp;&amp; unwrapped instanceof PersistenceException) &#123; // release the connection to avoid a deadlock if the translator is no loaded. See issue #22 closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); sqlSession = null; Throwable translated = SqlSessionTemplate.this.exceptionTranslator.translateExceptionIfPossible((PersistenceException) unwrapped); if (translated != null) &#123; unwrapped = translated; &#125; &#125; throw unwrapped; &#125; finally &#123; if (sqlSession != null) &#123; closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); &#125; &#125; &#125; &#125;&#125; invoke方法中的getSqlSession源码设计到Spring的源码，在合理就不深入，可以简单的理解为在一个ThreadLocal中拿一个SqlSession（实例获上时一个ConnectionHolder对象），该SqlSession是DefaultSqlSession。也就是说，调用线程都有自己的SqlSession MapperScannerConfigurer 源码分析虽然 MapperFactoryBean 用于帮助 Spring 生成 Mapper 接口，但我们很少直接配置 MapperFactoryBean 而是配置 MapperScannerConfigurer，如下图所示:原因在于又可能工程中的 mapper 接口数量比较多，为每个 mapper 接口都配置 MapperFactoryBean，配置文件会变得非常庞大，所以才会使用 MapperScannerConfigurer 为 每个 mapper 接口一对一的生成 MapperFactoryBean，那 MapperScannerConfigurer 是怎么做 到的呢?先看看其源码:MapperScannerConfigurer 实现了BeanDefinitionRegistryPostProcessor 接口， 因此可以对 Bean 的结构调整之后再注入容器。 MapperScannerConfigurer 在扫描完这些 mapper 接口之后，主要是将 Mapper 接口一个个的转换成 MapperFactoryBean 之后注入容器的，具体转换代码见:org.mybatis.spring.mapper.ClassPathMapperScanner.processBeanDefinitions(Set)由于spring的类的描述为一个BeanDefinition，因此执行了definition.setBeanClass(this.mapperFactoryBean.getClass());后就会定义了一个mapperFactoryBean类了","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"11第三个阶段:数据访问阶段","slug":"mybatis/11第三个阶段:数据访问阶段","date":"2021-11-17T12:00:12.000Z","updated":"2022-03-23T09:03:57.466Z","comments":true,"path":"blog/mybatis/11第三个阶段:数据访问阶段/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/11%E7%AC%AC%E4%B8%89%E4%B8%AA%E9%98%B6%E6%AE%B5:%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE%E9%98%B6%E6%AE%B5/","excerpt":"","text":"前面两个阶段都是准备阶段，还没有涉及到数据库的访问。这一阶段就是SqlSession方法调用的原理，核心类为Executor，它完成了数据库访问到结果转换这两部工作也就是转换成jdbc代码。这是mybatis的核心部分。 Executor在讲第二阶段的时候，提到 Sqlsession 的功能都是基于 Executor 来实现的，Executor 是 MyBaits 核心接口之一，定义了数据库操作最基本的方法，在其内部遵循 JDBC 规范完成对数 据库的访问;Executor 类继承机构如下图所示: Executor:MyBaits核心接口之一，定义了数据库操作最基本的方法; CacheingExecutor:使用装饰器模式，对真正提供数据库查询的Executor增强了二级缓存的能力; BaseExecutor:抽象类，典型的模版类，实现了executor接口的大部分方法，主要提供了缓存管理和事务管理的能力，其他子类需要实现的抽象方为:doUpdate,doQuery 等方法; BatchExecutor: 批量执行所有更新语句，基于 jdbc 的 batch 操作实现批处理; SimpleExecutor: 默认执行器，每次执行都会创建一个 statement，用完后关闭。 ReuseExecutor: 可重用执行器，将 statement 存入 map 中，操作 map中的 statement 而不会重复创建 statement; 在 BaseExecutor 中进行一次数据库查询操作的流程如下:如上图所示，doQuery 方法是查询数据的结果的子步骤，doQuery 方法有 SIMPLE、REUSER、 BATCH 三种实现，这三种不同的实现是在子类中定义的; SimpleExecutor:默认配置，在 doQuery 方法中使用 PrepareStatement 对象访问数据库，每次访问都要创建新的 PrepareStatement 对象; ReuseExecutor:在 doQuery 方法中，使用预编译 PrepareStatement 对象访问数据库，访问时，会重用缓存中的 statement 对象; BatchExecutor:在 doQuery 方法中，实现批量执行多条 SQL 语句的能力; 创建ExecutorExecutor是在DefaultSqlSession创建之前创建的，并把Executor对象传入了DefaultSqlSession中 123456789101112131415161718192021222324252627282930313233343536373839404142//从数据源获取数据库连接private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; //获取mybatis配置文件中的environment对象 final Environment environment = configuration.getEnvironment(); //从environment获取transactionFactory对象 final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); //创建事务对象 tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); //根据配置创建executor final Executor executor = configuration.newExecutor(tx, execType); //创建DefaultSqlSession return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123; closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(&quot;Error opening session. Cause: &quot; + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125;&#125;//Configuration.newExecutorpublic Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123; executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; if (ExecutorType.BATCH == executorType) &#123; executor = new BatchExecutor(this, transaction); &#125; else if (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125; //如果有&lt;cache&gt;节点，通过装饰器，添加二级缓存的能力 if (cacheEnabled) &#123; executor = new CachingExecutor(executor); &#125; //通过interceptorChain遍历所有的插件为executor增强，添加插件的功能 executor = (Executor) interceptorChain.pluginAll(executor); return executor;&#125; 从newExecutor源码可以了解到 默认的Executor是SIMPLE 如果开启了二级缓存，就会使用装饰器CachingExecutor添加二级缓存的功能executor = new CachingExecutor(executor); Executor还支持插件扩展，通过插件形成一条责任链executor = (Executor) interceptorChain.pluginAll(executor);插件的添加核心还是动态代理 Executor核心源码在第二阶段中知道，获取到的接口对象实际上是一个动态代理对象，该代理对象的InvocationHandler的实现是MapperProxy，所以所有的方法都会在MapperProxy.invoke中执行。而对于接口的方法最后会交给MapperMethod对象来执行。看MapperMethod的execute源码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//三步翻译在此完成public Object execute(SqlSession sqlSession, Object[] args) &#123; Object result; //第一步 根据sql语句类型以及接口返回的参数选择调用不同的方法 switch (command.getType()) &#123; case INSERT: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.insert(command.getName(), param)); break; &#125; case UPDATE: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.update(command.getName(), param)); break; &#125; case DELETE: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.delete(command.getName(), param)); break; &#125; case SELECT: if (method.returnsVoid() &amp;&amp; method.hasResultHandler()) &#123;//返回值为void executeWithResultHandler(sqlSession, args); result = null; &#125; else if (method.returnsMany()) &#123;//返回值为集合或者数组 result = executeForMany(sqlSession, args); &#125; else if (method.returnsMap()) &#123;//返回值为map result = executeForMap(sqlSession, args); &#125; else if (method.returnsCursor()) &#123;//返回值为游标 result = executeForCursor(sqlSession, args); &#125; else &#123;//处理返回为单一对象的情况 //通过参数解析器解析解析参数 Object param = method.convertArgsToSqlCommandParam(args);//第三步翻译，讲入参转化成Map result = sqlSession.selectOne(command.getName(), param); if (method.returnsOptional() &amp;&amp; (result == null || !method.getReturnType().equals(result.getClass()))) &#123; result = OptionalUtil.ofNullable(result); &#125; &#125; break; case FLUSH: result = sqlSession.flushStatements(); break; default: throw new BindingException(&quot;Unknown execution method for: &quot; + command.getName()); &#125; if (result == null &amp;&amp; method.getReturnType().isPrimitive() &amp;&amp; !method.returnsVoid()) &#123; throw new BindingException(&quot;Mapper method &#x27;&quot; + command.getName() + &quot; attempted to return null from a method with a primitive return type (&quot; + method.getReturnType() + &quot;).&quot;); &#125; return result;&#125; 比如最后执行了 executeForMany(sqlSession, args): 12345678910111213141516171819private &lt;E&gt; Object executeForMany(SqlSession sqlSession, Object[] args) &#123; List&lt;E&gt; result; Object param = method.convertArgsToSqlCommandParam(args); if (method.hasRowBounds()) &#123; RowBounds rowBounds = method.extractRowBounds(args); result = sqlSession.&lt;E&gt;selectList(command.getName(), param, rowBounds); &#125; else &#123; result = sqlSession.&lt;E&gt;selectList(command.getName(), param); &#125; // issue #510 Collections &amp; arrays support if (!method.getReturnType().isAssignableFrom(result.getClass())) &#123; if (method.getReturnType().isArray()) &#123; return convertToArray(result); &#125; else &#123; return convertToDeclaredCollection(sqlSession.getConfiguration(), result); &#125; &#125; return result;&#125; 可与看到MapperMethod最后将接口的方法调用转换成了ibatis版本时的调用所以接下来看SqlSession对象的方法就可以了，比如： 12345678910111213@Overridepublic &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) &#123; try &#123; //从configuration中获取要执行的sql语句的配置信息 MappedStatement ms = configuration.getMappedStatement(statement); //通过executor执行语句，并返回指定的结果集 return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException(&quot;Error querying database. Cause: &quot; + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125;&#125; 会先从configuration对象的MappedStatementMap中取到对应的MappedStatement，然后交给executor去执行了，这里也验证了之前说的 Sqlsession 的功能都是基于 Executor 来实现的,所有的数据库操作都是在Executor中完成的。接下来看Executor就好了。前面在newExecutor时说过，会对最原始的executor（SimpleExecutor）做功能的增强扩展，比如添加二级缓存（CachingExecutor）或者通过interceptor（拦截器）形成连接器链（责任链）。这里假设开启了二级缓存，没有拦截器的情况，也就是上边的executor的真实对象的类型是CachingExecutor。看CachingExecutor： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273//假设开启了二级缓存，那么在创建Executor的时候//CachingExecutor.query@Overridepublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException &#123;//获取sql语句信息，包括占位符，参数等信息 BoundSql boundSql = ms.getBoundSql(parameterObject);//拼装缓存的key值 CacheKey key = createCacheKey(ms, parameterObject, rowBounds, boundSql); return query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);&#125;@Overridepublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123;//从MappedStatement中获取二级缓存 //如果开启了二级缓存，MappedStatement都会保存对应二级缓存的对应引用 Cache cache = ms.getCache(); if (cache != null) &#123; flushCacheIfRequired(ms); if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123; ensureNoOutParams(ms, boundSql); @SuppressWarnings(&quot;unchecked&quot;) List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key);//从二级缓存中获取数据 if (list == null) &#123; //二级缓存为空，才会调用BaseExecutor.query list = delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); tcm.putObject(cache, key, list); // issue #578 and #116 &#125; return list; &#125; &#125; return delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);&#125;//如果在二级缓存中找不到数据//就会执行SimpleExecutor（BaseExecutor）的query方法@Overridepublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; ErrorContext.instance().resource(ms.getResource()).activity(&quot;executing a query&quot;).object(ms.getId()); if (closed) &#123;//检查当前executor是否关闭 throw new ExecutorException(&quot;Executor was closed.&quot;); &#125; if (queryStack == 0 &amp;&amp; ms.isFlushCacheRequired()) &#123;//非嵌套查询，并且FlushCache配置为true，则需要清空一级缓存 clearLocalCache(); &#125; List&lt;E&gt; list; try &#123; queryStack++;//查询层次加一 list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null;//查询以及缓存 if (list != null) &#123; //针对调用存储过程的结果处理 handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); &#125; else &#123; //缓存未命中，从数据库加载数据 list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql); &#125; &#125; finally &#123; queryStack--; &#125; if (queryStack == 0) &#123; for (DeferredLoad deferredLoad : deferredLoads) &#123;//延迟加载处理 deferredLoad.load(); &#125; // issue #601 deferredLoads.clear(); if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) &#123;//如果当前sql的一级缓存配置为STATEMENT，查询完既清空一集缓存 // issue #482 clearLocalCache(); &#125; &#125; return list;&#125; 如果是非select的操作就会执行清空一级缓存的操作可以看到挺简单粗暴的，全部清空。 从最后的BaseExecutor.query可以看到，如果一级缓存不存在就会执行queryFromDatabase，在这里才开始真正涉及到jdbc规范。 12345678910111213141516171819202122232425262728293031323334//真正访问数据库获取结果的方法private &lt;E&gt; List&lt;E&gt; queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; List&lt;E&gt; list; localCache.putObject(key, EXECUTION_PLACEHOLDER);//在缓存中添加占位符 try &#123; //调用抽象方法doQuery，方法查询数据库并返回结果，可选的实现包括：simple、reuse、batch list = doQuery(ms, parameter, rowBounds, resultHandler, boundSql); &#125; finally &#123; localCache.removeObject(key);//在缓存中删除占位符 &#125; localCache.putObject(key, list);//将真正的结果对象添加到一级缓存 if (ms.getStatementType() == StatementType.CALLABLE) &#123;//如果是调用存储过程 localOutputParameterCache.putObject(key, parameter);//缓存输出类型结果参数 &#125; return list;&#125;//默认Executor--SimpleExecutor@Override//查询的实现public &lt;E&gt; List&lt;E&gt; doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException &#123; Statement stmt = null; try &#123; Configuration configuration = ms.getConfiguration();//获取configuration对象 //创建StatementHandler对象， StatementHandler handler = configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql); //StatementHandler对象创建stmt,并使用parameterHandler对占位符进行处理 stmt = prepareStatement(handler, ms.getStatementLog()); //通过statementHandler对象调用ResultSetHandler将结果集转化为指定对象返回 return handler.&lt;E&gt;query(stmt, resultHandler); &#125; finally &#123; closeStatement(stmt); &#125;&#125; 在解析doQuery之前，先说下三个重要接口。 在Executor中，涉及到三个重要的接口StatementHandler、ParameterHandler和ResultSetHandler这三个接口的实现都是由Configuration对应的new..Handler方法创建的，都支持插件扩展（还有Executor）。 StatementHandler:它的作用是使用数据库的Statement或PrepareStatement执行操作，启承上启下作用; ParameterHandler:对预编译的SQL语句进行参数设置，SQL语句中的的占位符“?”都对应BoundSql.parameterMappings集合中的一个元素，在该对象中记录了对应的参数名称以及该参数的相关属性 ResultSetHandler:对数据库返回的结果集(ResultSet)进行封装，返回用户指定的实体类型; 这三个接口和Executor的执行示意图： 在doQuery中，执行了StatementHandler handler = configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql); 1234567891011121314151617181920212223 public StatementHandler newStatementHandler(Executor executor, MappedStatement mappedStatement, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) &#123;//创建RoutingStatementHandler对象，实际由statmentType来指定真实的StatementHandler来实现StatementHandler statementHandler = new RoutingStatementHandler(executor, mappedStatement, parameterObject, rowBounds, resultHandler, boundSql); statementHandler = (StatementHandler) interceptorChain.pluginAll(statementHandler);//加入插件 return statementHandler; &#125;0 public RoutingStatementHandler(Executor executor, MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) &#123; //RoutingStatementHandler最主要的功能就是根据mappedStatment的配置，生成一个对应的StatementHandler对象并赋值给delegate switch (ms.getStatementType()) &#123; case STATEMENT: delegate = new SimpleStatementHandler(executor, ms, parameter, rowBounds, resultHandler, boundSql); break; case PREPARED: delegate = new PreparedStatementHandler(executor, ms, parameter, rowBounds, resultHandler, boundSql); break; case CALLABLE: delegate = new CallableStatementHandler(executor, ms, parameter, rowBounds, resultHandler, boundSql); break; default: throw new ExecutorException(&quot;Unknown statement type: &quot; + ms.getStatementType()); &#125; &#125; 后生成了一个RoutingStatementHandler，这是一个静态代理类，根据MappedStatement.statementType来选择到底使用哪个StatementHandler。 SimpleStatmentHandler :使用 statement 对象访问数据库，无须参数化; PreparedStatmentHandler :使用预编译 PrepareStatement 对象访问数据库; CallableStatmentHandler :调用存储过程; 默认情况下是是使用PreparedStatmentHandler。 123456789101112131415161718192021222324252627public class PreparedStatementHandler extends BaseStatementHandler &#123; public PreparedStatementHandler(Executor executor, MappedStatement mappedStatement, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) &#123; super(executor, mappedStatement, parameter, rowBounds, resultHandler, boundSql); &#125;&#125;public abstract class BaseStatementHandler implements StatementHandler &#123; protected BaseStatementHandler(Executor executor, MappedStatement mappedStatement, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) &#123; this.configuration = mappedStatement.getConfiguration(); this.executor = executor; this.mappedStatement = mappedStatement; this.rowBounds = rowBounds; this.typeHandlerRegistry = configuration.getTypeHandlerRegistry(); this.objectFactory = configuration.getObjectFactory(); if (boundSql == null) &#123; // issue #435, get the key before calculating the statement generateKeys(parameterObject); boundSql = mappedStatement.getBoundSql(parameterObject); &#125; this.boundSql = boundSql; this.parameterHandler = configuration.newParameterHandler(mappedStatement, parameterObject, boundSql); this.resultSetHandler = configuration.newResultSetHandler(executor, mappedStatement, rowBounds, parameterHandler, resultHandler, boundSql); &#125;&#125; 可以看到最后执行了: 12this.parameterHandler = configuration.newParameterHandler(mappedStatement, parameterObject, boundSql);this.resultSetHandler = configuration.newResultSetHandler(executor, mappedStatement, rowBounds, parameterHandler, resultHandler, boundSql); 也就是说StatmentHandler和ResultSetHandler、ParameterHandler的关系是依赖的关系，StatmentHandler依赖ResultSetHandler、ParameterHandler。在代码上的体现就是ResultSetHandler、ParameterHandler是StatmentHandler的成员变量。 new完StatementHandler后接着执行了stmt = prepareStatement(handler, ms.getStatementLog());，该方法会去生成一个prepareStatement 123456789101112//创建Statementprivate Statement prepareStatement(StatementHandler handler, Log statementLog) throws SQLException &#123; Statement stmt; //获取connection对象的动态代理，添加日志能力； Connection connection = getConnection(statementLog); //通过不同的StatementHandler，利用connection创建（prepare）Statement stmt = handler.prepare(connection, transaction.getTimeout()); //使用parameterHandler处理占位符 //就是prepareStatement.set handler.parameterize(stmt); return stmt;&#125; 随便看看ReuseExecutor中的prepareStatement方法: 12345678910111213141516private Statement prepareStatement(StatementHandler handler, Log statementLog) throws SQLException &#123; Statement stmt; BoundSql boundSql = handler.getBoundSql(); String sql = boundSql.getSql();//获取sql语句 if (hasStatementFor(sql)) &#123;//根据sql语句检查是否缓存了对应的Statement stmt = getStatement(sql);//获取缓存的Statement applyTransactionTimeout(stmt);//设置新的超时时间 &#125; else &#123;//缓存中没有statment，创建statment过程和SimpleExecutor类似 Connection connection = getConnection(statementLog); stmt = handler.prepare(connection, transaction.getTimeout()); putStatement(sql, stmt);//放入缓存中 &#125;//使用parameterHandler处理占位符 handler.parameterize(stmt); return stmt;&#125; getConnection(statementLog)回去生成一个支持打印日志的动态代理对象。 最后执行了PreparedStatementHandler.query方法，真正的去查询数据库了 123456@Overridepublic &lt;E&gt; List&lt;E&gt; query(Statement statement, ResultHandler resultHandler) throws SQLException &#123; PreparedStatement ps = (PreparedStatement) statement; ps.execute(); return resultSetHandler.&lt;E&gt; handleResultSets(ps);&#125; ps.execute()真正的操作数据库了，handleResultSets就不看了，会涉及到很多对象，大概流程就是获取到ResultSet后通过规则并且使用mybatis的反射模块生成目标结果集就行了。 额外DefaultParameterHandler源码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class DefaultParameterHandler implements ParameterHandler &#123;//typeHandler注册中心 private final TypeHandlerRegistry typeHandlerRegistry; //对应的sql节点的信息 private final MappedStatement mappedStatement; //用户传入的参数 private final Object parameterObject; //SQL语句信息，其中还包括占位符和参数名称信息 private final BoundSql boundSql; private final Configuration configuration; public DefaultParameterHandler(MappedStatement mappedStatement, Object parameterObject, BoundSql boundSql) &#123; this.mappedStatement = mappedStatement; this.configuration = mappedStatement.getConfiguration(); this.typeHandlerRegistry = mappedStatement.getConfiguration().getTypeHandlerRegistry(); this.parameterObject = parameterObject; this.boundSql = boundSql; &#125; @Override public Object getParameterObject() &#123; return parameterObject; &#125; @Override public void setParameters(PreparedStatement ps) &#123; ErrorContext.instance().activity(&quot;setting parameters&quot;).object(mappedStatement.getParameterMap().getId()); //从boundSql中获取sql语句的占位符对应的参数信息 List&lt;ParameterMapping&gt; parameterMappings = boundSql.getParameterMappings(); //遍历这个参数列表，把参数设置到PreparedStatement中 if (parameterMappings != null) &#123; for (int i = 0; i &lt; parameterMappings.size(); i++) &#123; ParameterMapping parameterMapping = parameterMappings.get(i);//获得入参的形参名字，javatype，jdbctype等信息 if (parameterMapping.getMode() != ParameterMode.OUT) &#123;//对于存储过程中的参数不处理 Object value;//绑定的实参值 String propertyName = parameterMapping.getProperty();//参数的名字 // 获取对应的实参值 if (boundSql.hasAdditionalParameter(propertyName)) &#123; //是否为附加参数 value = boundSql.getAdditionalParameter(propertyName); &#125; else if (parameterObject == null) &#123;//是否为空 value = null; &#125; else if (typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) &#123;//是否要进行指定的类型转换 value = parameterObject; &#125; else &#123;//大部分情况走这段 MetaObject metaObject = configuration.newMetaObject(parameterObject); value = metaObject.getValue(propertyName); &#125; TypeHandler typeHandler = parameterMapping.getTypeHandler();//从parameterMapping中获取typeHandler对象 JdbcType jdbcType = parameterMapping.getJdbcType();//获取参数对应的jdbcType if (value == null &amp;&amp; jdbcType == null) &#123; jdbcType = configuration.getJdbcTypeForNull(); &#125; try &#123; //为statment中的占位符绑定参数 typeHandler.setParameter(ps, i + 1, value, jdbcType); &#125; catch (TypeException e) &#123; throw new TypeException(&quot;Could not set parameters for mapping: &quot; + parameterMapping + &quot;. Cause: &quot; + e, e); &#125; catch (SQLException e) &#123; throw new TypeException(&quot;Could not set parameters for mapping: &quot; + parameterMapping + &quot;. Cause: &quot; + e, e); &#125; &#125; &#125; &#125; &#125;&#125; DefaultResultSetHandler源码看DefaultResultSetHandler类，作用是将从数据库查询得到的结果按照映射配置文件的映射规则，映射成相应的结果集对象。 在 ResultSetHandler 内部实际是做三个步骤: 找到映射匹配规则 反射实例化目标对象 根据规则填充属性值 为完成这三部 ResultHandler 内部调用的流程图如下:","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"10第二阶段:代理封装阶段","slug":"mybatis/10第二阶段:代理封装阶段","date":"2021-11-17T12:00:11.000Z","updated":"2022-03-23T09:03:57.459Z","comments":true,"path":"blog/mybatis/10第二阶段:代理封装阶段/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/10%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5:%E4%BB%A3%E7%90%86%E5%B0%81%E8%A3%85%E9%98%B6%E6%AE%B5/","excerpt":"","text":"先看一测试代码 123456789101112131415@Testpublic void quickStart() throws IOException &#123; //--------------------第二阶段--------------------------- // 2.获取sqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); // 3.获取对应mapper TUserMapper mapper = sqlSession.getMapper(TUserMapper.class); //--------------------第三阶段--------------------------- // 4.执行查询语句并返回单条数据 TUser user = mapper.selectByPrimaryKey(2, &quot;fe&quot;); System.out.println(user); System.out.println(&quot;----------------------------------&quot;);&#125; 第二个阶段使用到的第一个对象就是 SqlSession，SqlSession 是 MyBaits 对外提供的最关 键的核心接口，通过它可以执行数据库读写命令、获取映射器、管理事务等；SqlSession 也意味着客户端与数据库的一次连接session，客户端对数据库的访问请求都是由 SqlSession 来处理的， SqlSession 由 SqlSessionFactory 创建，每个 SqlSession 都会引用 SqlSessionFactory 中全局唯一 单例存在的 configuration 对象;如下图所示: SqlSession是由SqlSessionFactory创建的，SqlSessionFactory的默认实现为DefaultSqlSessionFactory，他通过openSessionFromDataSource方法创建SqlSession 123456789101112131415161718192021//从数据源获取数据库连接private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; //获取mybatis配置文件中的environment对象 final Environment environment = configuration.getEnvironment(); //从environment获取transactionFactory对象 final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); //创建事务对象 tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); //根据配置创建executor final Executor executor = configuration.newExecutor(tx, execType); //创建DefaultSqlSession return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123; closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(&quot;Error opening session. Cause: &quot; + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125;&#125; 可以看到SqlSession的默认实现为DefaultSqlSession，在创建它时都会传入一个Executor。SqlSession是MyBatis的门面，是MyBatis对外提供数据访问的主要API；实际上 Sqlsession 的功能都是基于 Executor 来实现的，遵循了单一职责原则。 上面提到 SqlSession 是 MyBatis 对外提供数据库访问最主要的 API，但是因为直接使用 SqlSession 进行数据库开发存在代码可读性差、可维护性差的问题，所以我们很少使用，而 是使用 Mapper 接口的方式进行数据库的开发。表面上我们在使用 Mapper 接口编程，实际 上 MyBatis 的内部，将对 Mapper 接口的调用转发给了 SqlSession，这个请求的转发是建立在 配置文件解读、动态代理增强的基础之上实现的。实现这一需求的核心模块时binding模块 MapperRegistry:mapper接口和对应的代理对象工厂的注册中心; MapperProxyFactory:用于生成mapper接口动态代理的实例对象;保证Mapper实例对象是局部变量，对象保存在MapperRegistry中 MapperProxy:实现了InvocationHandler接口，它是增强mapper接口的实现; MapperMethod:封装了Mapper接口中对应方法的信息，以及对应的sql语句的信息;它是 mapper 接口与映射配置文件中 sql 语句的桥梁; MapperMethod 内几个关键数据结构: SqlCommand:从 configuration 中获取方法的命名空间.方法名以及 SQL 语句的类型; MethodSignature:封装mapper接口方法的相关信息(入参，返回类型); 在XmlMapperBuilder扫描完mapper后都会把接口放入MapperRegistry中的map中，并创建一个MapperProxyFactory： 12345678910111213141516171819202122232425262728293031323334353637/** * * 用于生成mapper接口动态代理的实例对象； * @author Lasse Voss */public class MapperProxyFactory&lt;T&gt; &#123; //mapper接口的class对象 private final Class&lt;T&gt; mapperInterface; //key是mapper接口中的某个方法的method对象，value是对应的MapperMethod，MapperMethod对象不记录任何状态信息，所以它可以在多个代理对象之间共享 private final Map&lt;Method, MapperMethod&gt; methodCache = new ConcurrentHashMap&lt;&gt;(); public MapperProxyFactory(Class&lt;T&gt; mapperInterface) &#123; this.mapperInterface = mapperInterface; &#125; public Class&lt;T&gt; getMapperInterface() &#123; return mapperInterface; &#125; public Map&lt;Method, MapperMethod&gt; getMethodCache() &#123; return methodCache; &#125; @SuppressWarnings(&quot;unchecked&quot;) protected T newInstance(MapperProxy&lt;T&gt; mapperProxy) &#123; //创建实现了mapper接口的动态代理对象 return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] &#123; mapperInterface &#125;, mapperProxy); &#125; public T newInstance(SqlSession sqlSession) &#123; //每次调用都会创建新的MapperProxy对象 final MapperProxy&lt;T&gt; mapperProxy = new MapperProxy&lt;&gt;(sqlSession, mapperInterface, methodCache); return newInstance(mapperProxy); &#125;&#125; 该类的作用有两个。第一创建对应接口的动态代理对象，第二缓存MapperMethod对象。 所以sqlSession.getMapper就是在MapperRegistry中拿到对应的MapperProxyFactory，然后执行newInstance就可以了 1234567891011public &lt;T&gt; T getMapper(Class&lt;T&gt; type, SqlSession sqlSession) &#123; final MapperProxyFactory&lt;T&gt; mapperProxyFactory = (MapperProxyFactory&lt;T&gt;) knownMappers.get(type); if (mapperProxyFactory == null) &#123; throw new BindingException(&quot;Type &quot; + type + &quot; is not known to the MapperRegistry.&quot;); &#125; try &#123; return mapperProxyFactory.newInstance(sqlSession); &#125; catch (Exception e) &#123; throw new BindingException(&quot;Error getting mapper instance. Cause: &quot; + e, e); &#125;&#125; 而动态代理类的InvocationHandler是MapperProxy，而MapperProxy的核心代码为： 1234//从缓存中获取mapperMethod对象，如果缓存中没有，则创建一个，并添加到缓存中final MapperMethod mapperMethod = cachedMapperMethod(method);//调用execute方法执行sqlreturn mapperMethod.execute(sqlSession, args); 也就是说方法最终是交给了MapperMethod来执行，MapperMethod中有个MethodSignature参数，封装mapper接口方法的相关信息（入参，返回类型），比如有这样一个方法： 123456789&lt;mapper namespace=&quot;com.enjoylearning.mybatis.mapper.TUserMapper&quot;&gt; &lt;select id=&quot;selectByPrimaryKey&quot; resultType=&quot;TUser&quot;&gt; select id, userName, realName, sex, mobile, email, note from t_user where id = #&#123;id,jdbcType=INTEGER&#125; and userName=$&#123;userName&#125; &lt;/select&gt;&lt;/mapper&gt; 那在MethodSignature.paramNameResolver中保存了这样一种关系 每次在MapperMethod中执行method.convertArgsToSqlCommandParam(args);方法获取参数对应关系时就通过paramNameResolver生成了下列这种关系：这也就是为什么可以使用param的原因 MapperMethod的核心方法是execute，这也是所有sql方法执行的方法，该方法的作用就是把接口的方法调用转换成通过sqlSession调用的方式MapperMethod源码的一部分： 12345678910111213141516171819private &lt;E&gt; Object executeForMany(SqlSession sqlSession, Object[] args) &#123; List&lt;E&gt; result; Object param = method.convertArgsToSqlCommandParam(args); if (method.hasRowBounds()) &#123; RowBounds rowBounds = method.extractRowBounds(args); result = sqlSession.&lt;E&gt;selectList(command.getName(), param, rowBounds); &#125; else &#123; result = sqlSession.&lt;E&gt;selectList(command.getName(), param); &#125; // issue #510 Collections &amp; arrays support if (!method.getReturnType().isAssignableFrom(result.getClass())) &#123; if (method.getReturnType().isArray()) &#123; return convertToArray(result); &#125; else &#123; return convertToDeclaredCollection(sqlSession.getConfiguration(), result); &#125; &#125; return result;&#125; 从 SqlSession.getMapper(Class)方法开始跟踪，画出 binding 模块的时序图如下所示: 中的来说，第二部分主要完成了三部分的工作。 通过sqlSessionFactory对象new一个SqlSession对象 mapper接口的动态代理对象生成。 把mapper接口的方法调用在底层转换成ibatis的SqlSession方法调用。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"09第一阶段:配置加载阶段","slug":"mybatis/09第一阶段:配置加载阶段","date":"2021-11-17T12:00:10.000Z","updated":"2022-03-23T09:03:57.456Z","comments":true,"path":"blog/mybatis/09第一阶段:配置加载阶段/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/09%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5:%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD%E9%98%B6%E6%AE%B5/","excerpt":"","text":"前置知识《建造器模式》 入口： 在 MyBatis 中负责加载配置文件的核心类有三个，类图如下: XMLConfigBuilder:主要负责解析mybatis-config.xml； XMLMapperBuilder:主要负责解析映射配置Mapper.xml文件； MapperAnnotationBuilder:用来解析接口的注解的 XMLStatementBuilder:主要负责解析映射配置文件中的SQL节点； XMLConfigBuilder、XMLMapperBuilder、XMLStatementBuilder 这三个类在配置文件加载 过程中非常重要，具体分工如下图所示:这三个类使用了建造者模式对configuration对象进行初始化，但是没有使用建造者模式 的“肉体”(流式编程风格)，只用了灵魂(屏蔽复杂对象的创建过程)，把建造者模式演绎 成了工厂模式；后面还会对这三个类源码进行分析； 实例化并初始化 Configuration 对象是第一个阶段的最终目的，所以熟悉 configuration 对象是理解第一个阶段代码的心；这类在源码解析时再分析。 需要特别注意的是 Configuration 对象在 MyBatis 中是单例的，生命周期是应用级的，只有在new一个XMLConfigBuilder对象的时候才会去创建一个Configuration 配置加载的补助可以把第一个阶段配置加载过程分解为四个步骤，四个步骤如下图: 第一步：通过SqlSessionFactoryBuilder.build创建SqlSessionFactory，并且创建XMLConfigBuilder进行配置解析 第二部：XMLConfigBuilder通过parseConfiguration方法，进行配置解析。先看mybatis的配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;!-- 参数设置 --&gt; &lt;settings&gt; &lt;!-- 这个配置使全局的映射器启用或禁用缓存 --&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot; /&gt; &lt;!-- 全局启用或禁用延迟加载。当禁用时，所有关联对象都会即时加载 --&gt; &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot; /&gt; &lt;!-- 当启用时，有延迟加载属性的对象在被调用时将会完全加载任意属性。否则，每种属性将会按需要加载 --&gt; &lt;setting name=&quot;aggressiveLazyLoading&quot; value=&quot;true&quot; /&gt; &lt;!-- 允许或不允许多种结果集从一个单独的语句中返回（需要适合的驱动） --&gt; &lt;setting name=&quot;multipleResultSetsEnabled&quot; value=&quot;true&quot; /&gt; &lt;!-- 使用列标签代替列名。不同的驱动在这方便表现不同。参考驱动文档或充分测试两种方法来决定所使用的驱动 --&gt; &lt;setting name=&quot;useColumnLabel&quot; value=&quot;true&quot; /&gt; &lt;!-- 允许JDBC支持生成的键。需要适合的驱动。如果设置为true则这个设置强制生成的键被使用，尽管一些驱动拒绝兼容但仍然有效（比如Derby） --&gt; &lt;setting name=&quot;useGeneratedKeys&quot; value=&quot;true&quot; /&gt; &lt;!-- 指定MyBatis如何自动映射列到字段/属性。PARTIAL只会自动映射简单，没有嵌套的结果。FULL会自动映射任意复杂的结果（嵌套的或其他情况） --&gt; &lt;setting name=&quot;autoMappingBehavior&quot; value=&quot;PARTIAL&quot; /&gt; &lt;!--当检测出未知列（或未知属性）时，如何处理，默认情况下没有任何提示，这在测试的时候很不方便，不容易找到错误。 NONE : 不做任何处理 (默认值) WARNING : 警告日志形式的详细信息 FAILING : 映射失败，抛出异常和详细信息 --&gt; &lt;setting name=&quot;autoMappingUnknownColumnBehavior&quot; value=&quot;WARNING&quot; /&gt; &lt;!-- 配置默认的执行器。SIMPLE执行器没有什么特别之处。REUSE执行器重用预处理语句。BATCH执行器重用语句和批量更新 --&gt; &lt;setting name=&quot;defaultExecutorType&quot; value=&quot;REUSE&quot; /&gt; &lt;!-- 设置超时时间，它决定驱动等待一个数据库响应的时间 --&gt; &lt;setting name=&quot;defaultStatementTimeout&quot; value=&quot;25000&quot; /&gt; &lt;!--设置查询返回值数量，可以被查询数值覆盖 --&gt; &lt;setting name=&quot;defaultFetchSize&quot; value=&quot;100&quot; /&gt; &lt;!-- 允许在嵌套语句中使用分页 --&gt; &lt;setting name=&quot;safeRowBoundsEnabled&quot; value=&quot;false&quot; /&gt; &lt;!--是否开启自动驼峰命名规则（camel case）映射，即从经典数据库列名 A_COLUMN 到经典 Java 属性名 aColumn 的类似映射。 --&gt; &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;false&quot; /&gt; &lt;!--MyBatis 利用本地缓存机制（Local Cache）防止循环引用（circular references）和加速重复嵌套查询。 默认值为 SESSION，这种情况下会缓存一个会话中执行的所有查询。 若设置值为 STATEMENT，本地会话仅用在语句执行上，对相同 SqlSession 的不同调用将不会共享数据。 --&gt; &lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot; /&gt; &lt;!-- 当没有为参数提供特定的 JDBC 类型时，为空值指定 JDBC 类型。 某些驱动需要指定列的 JDBC 类型，多数情况直接用一般类型即可，比如 NULL、VARCHAR OTHER。 --&gt; &lt;setting name=&quot;jdbcTypeForNull&quot; value=&quot;OTHER&quot; /&gt; &lt;!-- 指定哪个对象的方法触发一次延迟加载。 --&gt; &lt;setting name=&quot;lazyLoadTriggerMethods&quot; value=&quot;equals,clone,hashCode,toString&quot; /&gt; &lt;/settings&gt; &lt;!-- 别名定义 --&gt; &lt;typeAliases&gt; &lt;typeAlias alias=&quot;pageAccessURL&quot; type=&quot;com.lgm.mybatis.model.PageAccessURL&quot; /&gt; &lt;/typeAliases&gt; &lt;!--自定义类型处理器 --&gt; &lt;typeHandlers&gt; &lt;!-- &lt;typeHandler handler=&quot;com.xhm.util.BooleanTypeHandlder&quot; /&gt; --&gt; &lt;!--扫描整个包下的自定义类型处理器 --&gt; &lt;package name=&quot;com.xhm.util&quot; /&gt; &lt;/typeHandlers&gt; &lt;!--plugins插件之 分页拦截器 --&gt; &lt;plugins&gt; &lt;plugin interceptor=&quot;com.xhm.util.PageInterceptor&quot;&gt;&lt;/plugin&gt; &lt;/plugins&gt; &lt;!--配置environment环境 --&gt; &lt;environments default=&quot;development&quot;&gt; &lt;!-- 环境配置1，每个SqlSessionFactory对应一个环境 --&gt; &lt;environment id=&quot;development1&quot;&gt; &lt;!-- 事务配置 type= JDBC、MANAGED 1.JDBC:这个配置直接简单使用了JDBC的提交和回滚设置。它依赖于从数据源得到的连接来管理事务范围。 2.MANAGED:这个配置几乎没做什么。它从来不提交或回滚一个连接。而它会让容器来管理事务的整个生命周期（比如Spring或JEE应用服务器的上下文）。 默认情况下它会关闭连接。然而一些容器并不希望这样，因此如果你需要从连接中停止它，将closeConnection属性设置为false --&gt; &lt;transactionManager type=&quot;JDBC&quot; /&gt; &lt;!-- &lt;transactionManager type=&quot;MANAGED&quot;&gt; &lt;property name=&quot;closeConnection&quot; value=&quot;false&quot;/&gt; &lt;/transactionManager&gt; --&gt; &lt;!-- 数据源类型：type = UNPOOLED、POOLED、JNDI 1.UNPOOLED：这个数据源的实现是每次被请求时简单打开和关闭连接。它有一点慢，这是对简单应用程序的一个很好的选择，因为它不需要及时的可用连接。 不同的数据库对这个的表现也是不一样的，所以对某些数据库来说配置数据源并不重要，这个配置也是闲置的 2.POOLED：这是JDBC连接对象的数据源连接池的实现，用来避免创建新的连接实例时必要的初始连接和认证时间。 这是一种当前Web应用程序用来快速响应请求很流行的方法。 3.JNDI：这个数据源的实现是为了使用如Spring或应用服务器这类的容器，容器可以集中或在外部配置数据源，然后放置一个JNDI上下文的引用 --&gt; &lt;dataSource type=&quot;UNPOOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/xhm&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot; /&gt; &lt;!-- 默认连接事务隔离级别 &lt;property name=&quot;defaultTransactionIsolationLevel&quot; value=&quot;&quot; /&gt; --&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;!-- 环境配置2 --&gt; &lt;environment id=&quot;development2&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot; /&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/xhm&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot; /&gt; &lt;!-- 在任意时间存在的活动（也就是正在使用）连接的数量 --&gt; &lt;property name=&quot;poolMaximumActiveConnections&quot; value=&quot;10&quot; /&gt; &lt;!-- 任意时间存在的空闲连接数 --&gt; &lt;property name=&quot;poolMaximumIdleConnections&quot; value=&quot;5&quot; /&gt; &lt;!-- 在被强制返回之前，池中连接被检查的时间 --&gt; &lt;property name=&quot;poolMaximumCheckoutTime&quot; value=&quot;20000&quot; /&gt; &lt;!-- 这是给连接池一个打印日志状态机会的低层次设置，还有重新尝试获得连接，这些情况下往往需要很长时间（为了避免连接池没有配置时静默失败） --&gt; &lt;property name=&quot;poolTimeToWait&quot; value=&quot;20000&quot; /&gt; &lt;!-- 发送到数据的侦测查询，用来验证连接是否正常工作，并且准备接受请求。 --&gt; &lt;property name=&quot;poolPingQuery&quot; value=&quot;NO PING QUERY SET&quot; /&gt; &lt;!-- 这是开启或禁用侦测查询。如果开启，你必须用一个合法的SQL语句（最好是很快速的）设置poolPingQuery属性 --&gt; &lt;property name=&quot;poolPingEnabled&quot; value=&quot;false&quot; /&gt; &lt;!-- 这是用来配置poolPingQuery多次时间被用一次。这可以被设置匹配标准的数据库连接超时时间，来避免不必要的侦测 --&gt; &lt;property name=&quot;poolPingConnectionsNotUsedFor&quot; value=&quot;0&quot; /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;!-- 环境配置3 --&gt; &lt;environment id=&quot;development3&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot; /&gt; &lt;dataSource type=&quot;JNDI&quot;&gt; &lt;property name=&quot;data_source&quot; value=&quot;java:comp/env/jndi/mybatis&quot; /&gt; &lt;property name=&quot;env.encoding&quot; value=&quot;UTF8&quot; /&gt; &lt;!-- &lt;property name=&quot;initial_context&quot; value=&quot;&quot;/&gt; &lt;property name=&quot;env.encoding&quot; value=&quot;UTF8&quot;/&gt; --&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 映射文件，mapper的配置文件 --&gt; &lt;mappers&gt; &lt;!--直接映射到相应的mapper文件 --&gt; &lt;mapper resource=&quot;com/xhm/mapper/UserMapper.xml&quot; /&gt; &lt;!--扫描包路径下所有xxMapper.xml文件 --&gt; &lt;package name=&quot;com.xhm.mapper&quot; /&gt; &lt;/mappers&gt;&lt;/configuration&gt; XMLConfigBuilder的任务就是解析上述xml文件，把配置文件的信息都解析道Configuration对象中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177private void parseConfiguration(XNode root) &#123; try &#123; //issue #117 read properties first //解析&lt;properties&gt;节点 propertiesElement(root.evalNode(&quot;properties&quot;)); //解析&lt;settings&gt;节点 Properties settings = settingsAsProperties(root.evalNode(&quot;settings&quot;)); loadCustomVfs(settings); //解析&lt;typeAliases&gt;节点 typeAliasesElement(root.evalNode(&quot;typeAliases&quot;)); //解析&lt;plugins&gt;节点 pluginElement(root.evalNode(&quot;plugins&quot;)); //解析&lt;objectFactory&gt;节点 objectFactoryElement(root.evalNode(&quot;objectFactory&quot;)); //解析&lt;objectWrapperFactory&gt;节点 objectWrapperFactoryElement(root.evalNode(&quot;objectWrapperFactory&quot;)); //解析&lt;reflectorFactory&gt;节点 reflectorFactoryElement(root.evalNode(&quot;reflectorFactory&quot;)); settingsElement(settings);//将settings填充到configuration // read it after objectFactory and objectWrapperFactory issue #631 //解析&lt;environments&gt;节点 environmentsElement(root.evalNode(&quot;environments&quot;)); //解析&lt;databaseIdProvider&gt;节点 databaseIdProviderElement(root.evalNode(&quot;databaseIdProvider&quot;)); //解析&lt;typeHandlers&gt;节点 typeHandlerElement(root.evalNode(&quot;typeHandlers&quot;)); //解析&lt;mappers&gt;节点 mapperElement(root.evalNode(&quot;mappers&quot;)); &#125; catch (Exception e) &#123; throw new BuilderException(&quot;Error parsing SQL Mapper Configuration. Cause: &quot; + e, e); &#125;&#125;//解析settings属性的信息private void settingsElement(Properties props) throws Exception &#123; configuration.setAutoMappingBehavior(AutoMappingBehavior.valueOf(props.getProperty(&quot;autoMappingBehavior&quot;, &quot;PARTIAL&quot;))); configuration.setAutoMappingUnknownColumnBehavior(AutoMappingUnknownColumnBehavior.valueOf(props.getProperty(&quot;autoMappingUnknownColumnBehavior&quot;, &quot;NONE&quot;))); configuration.setCacheEnabled(booleanValueOf(props.getProperty(&quot;cacheEnabled&quot;), true)); configuration.setProxyFactory((ProxyFactory) createInstance(props.getProperty(&quot;proxyFactory&quot;))); configuration.setLazyLoadingEnabled(booleanValueOf(props.getProperty(&quot;lazyLoadingEnabled&quot;), false)); configuration.setAggressiveLazyLoading(booleanValueOf(props.getProperty(&quot;aggressiveLazyLoading&quot;), false)); configuration.setMultipleResultSetsEnabled(booleanValueOf(props.getProperty(&quot;multipleResultSetsEnabled&quot;), true)); configuration.setUseColumnLabel(booleanValueOf(props.getProperty(&quot;useColumnLabel&quot;), true)); configuration.setUseGeneratedKeys(booleanValueOf(props.getProperty(&quot;useGeneratedKeys&quot;), false)); configuration.setDefaultExecutorType(ExecutorType.valueOf(props.getProperty(&quot;defaultExecutorType&quot;, &quot;SIMPLE&quot;))); configuration.setDefaultStatementTimeout(integerValueOf(props.getProperty(&quot;defaultStatementTimeout&quot;), null)); configuration.setDefaultFetchSize(integerValueOf(props.getProperty(&quot;defaultFetchSize&quot;), null)); configuration.setMapUnderscoreToCamelCase(booleanValueOf(props.getProperty(&quot;mapUnderscoreToCamelCase&quot;), false)); configuration.setSafeRowBoundsEnabled(booleanValueOf(props.getProperty(&quot;safeRowBoundsEnabled&quot;), false)); configuration.setLocalCacheScope(LocalCacheScope.valueOf(props.getProperty(&quot;localCacheScope&quot;, &quot;SESSION&quot;))); configuration.setJdbcTypeForNull(JdbcType.valueOf(props.getProperty(&quot;jdbcTypeForNull&quot;, &quot;OTHER&quot;))); configuration.setLazyLoadTriggerMethods(stringSetValueOf(props.getProperty(&quot;lazyLoadTriggerMethods&quot;), &quot;equals,clone,hashCode,toString&quot;)); configuration.setSafeResultHandlerEnabled(booleanValueOf(props.getProperty(&quot;safeResultHandlerEnabled&quot;), true)); configuration.setDefaultScriptingLanguage(resolveClass(props.getProperty(&quot;defaultScriptingLanguage&quot;))); @SuppressWarnings(&quot;unchecked&quot;) Class&lt;? extends TypeHandler&gt; typeHandler = (Class&lt;? extends TypeHandler&gt;)resolveClass(props.getProperty(&quot;defaultEnumTypeHandler&quot;)); configuration.setDefaultEnumTypeHandler(typeHandler); configuration.setCallSettersOnNulls(booleanValueOf(props.getProperty(&quot;callSettersOnNulls&quot;), false)); configuration.setUseActualParamName(booleanValueOf(props.getProperty(&quot;useActualParamName&quot;), true)); configuration.setReturnInstanceForEmptyRow(booleanValueOf(props.getProperty(&quot;returnInstanceForEmptyRow&quot;), false)); configuration.setLogPrefix(props.getProperty(&quot;logPrefix&quot;)); @SuppressWarnings(&quot;unchecked&quot;) Class&lt;? extends Log&gt; logImpl = (Class&lt;? extends Log&gt;)resolveClass(props.getProperty(&quot;logImpl&quot;)); configuration.setLogImpl(logImpl); configuration.setConfigurationFactory(resolveClass(props.getProperty(&quot;configurationFactory&quot;)));&#125;//解析typeAliases，也就是别名注册，别名和类名的对应关系放到了typeAliasRegistry对象中的map中 //private final Map&lt;String, Class&lt;?&gt;&gt; TYPE_ALIASES = new HashMap&lt;&gt;();//typeAliasRegistry在创建的时候也会把注册一些别名，绝体可以看TypeAliasRegistry类private void typeAliasesElement(XNode parent) &#123; if (parent != null) &#123; for (XNode child : parent.getChildren()) &#123; if (&quot;package&quot;.equals(child.getName())) &#123; String typeAliasPackage = child.getStringAttribute(&quot;name&quot;); configuration.getTypeAliasRegistry().registerAliases(typeAliasPackage); &#125; else &#123; String alias = child.getStringAttribute(&quot;alias&quot;); String type = child.getStringAttribute(&quot;type&quot;); try &#123; Class&lt;?&gt; clazz = Resources.classForName(type); if (alias == null) &#123; typeAliasRegistry.registerAlias(clazz); &#125; else &#123; typeAliasRegistry.registerAlias(alias, clazz); &#125; &#125; catch (ClassNotFoundException e) &#123; throw new BuilderException(&quot;Error registering typeAlias for &#x27;&quot; + alias + &quot;&#x27;. Cause: &quot; + e, e); &#125; &#125; &#125; &#125;&#125;/** * * 解析自定义类型处理器，类型处理器的作用就是用来完成javaType和jdbcType之间的转换。 * 自定义的话有两种方法： * 1.实现TypeHandler接口并加上MappedTypes（必须）注解指明对应的jdbcType * 2.继承BaseTypeHandler类， * * 从重写的方法可以看该自定义类型处理器是为了实现PreparedStatement.set和ResultSet.get的细节 * * 解析后的对应关系存储在TypeHandlerRegistry类的map中 * * @param parent * @throws Exception */private void typeHandlerElement(XNode parent) throws Exception &#123; if (parent != null) &#123; for (XNode child : parent.getChildren()) &#123; //包解析，自定义的typeHandler必须按上面的方式实现 if (&quot;package&quot;.equals(child.getName())) &#123; String typeHandlerPackage = child.getStringAttribute(&quot;name&quot;); typeHandlerRegistry.register(typeHandlerPackage); &#125; else &#123; String javaTypeName = child.getStringAttribute(&quot;javaType&quot;); String jdbcTypeName = child.getStringAttribute(&quot;jdbcType&quot;); String handlerTypeName = child.getStringAttribute(&quot;handler&quot;); Class&lt;?&gt; javaTypeClass = resolveClass(javaTypeName); JdbcType jdbcType = resolveJdbcType(jdbcTypeName); Class&lt;?&gt; typeHandlerClass = resolveClass(handlerTypeName); if (javaTypeClass != null) &#123; if (jdbcType == null) &#123; typeHandlerRegistry.register(javaTypeClass, typeHandlerClass); &#125; else &#123; typeHandlerRegistry.register(javaTypeClass, jdbcType, typeHandlerClass); &#125; &#125; else &#123; typeHandlerRegistry.register(typeHandlerClass); &#125; &#125; &#125; &#125;&#125;//加载插件，把类放到Configuration的InterceptorChain中，插件单独一章再讲private void pluginElement(XNode parent) throws Exception &#123; if (parent != null) &#123; //遍历所有的插件配置 for (XNode child : parent.getChildren()) &#123; //获取插件的类名 String interceptor = child.getStringAttribute(&quot;interceptor&quot;); //获取插件的配置 Properties properties = child.getChildrenAsProperties(); //实例化插件对象 Interceptor interceptorInstance = (Interceptor) resolveClass(interceptor).newInstance(); //设置插件属性 interceptorInstance.setProperties(properties); //将插件添加到configuration对象，底层使用list保存所有的插件并记录顺序 configuration.addInterceptor(interceptorInstance); &#125; &#125;&#125;//解析环境，说明了就是解析数据源配置，把结果放到了Configuration中的Environment对象中//Environment这对象中//了解即可，大多数情况下都不会使用mybatis的数据库链接池的private void environmentsElement(XNode context) throws Exception &#123; if (context != null) &#123; if (environment == null) &#123; environment = context.getStringAttribute(&quot;default&quot;); &#125; for (XNode child : context.getChildren()) &#123; String id = child.getStringAttribute(&quot;id&quot;); if (isSpecifiedEnvironment(id)) &#123; TransactionFactory txFactory = transactionManagerElement(child.evalNode(&quot;transactionManager&quot;)); DataSourceFactory dsFactory = dataSourceElement(child.evalNode(&quot;dataSource&quot;)); DataSource dataSource = dsFactory.getDataSource(); Environment.Builder environmentBuilder = new Environment.Builder(id) .transactionFactory(txFactory) .dataSource(dataSource); configuration.setEnvironment(environmentBuilder.build()); &#125; &#125; &#125;&#125; 第三步 通过XMLMapperBuilder和MapperAnnotationBuilder解析mapper12345678910111213141516171819202122232425262728293031323334353637/** * 这方法是重点，解析mapper.xml文件、和接口中的的@Select注解 * @param parent * @throws Exception */private void mapperElement(XNode parent) throws Exception &#123; if (parent != null) &#123; for (XNode child : parent.getChildren()) &#123;//处理mapper子节点 if (&quot;package&quot;.equals(child.getName())) &#123;//package子节点 String mapperPackage = child.getStringAttribute(&quot;name&quot;); configuration.addMappers(mapperPackage); &#125; else &#123;//获取&lt;mapper&gt;节点的resource、url或mClass属性这三个属性互斥 String resource = child.getStringAttribute(&quot;resource&quot;); String url = child.getStringAttribute(&quot;url&quot;); String mapperClass = child.getStringAttribute(&quot;class&quot;); if (resource != null &amp;&amp; url == null &amp;&amp; mapperClass == null) &#123;//如果resource不为空 ErrorContext.instance().resource(resource); InputStream inputStream = Resources.getResourceAsStream(resource);//加载mapper文件 //实例化XMLMapperBuilder解析mapper映射文件 XMLMapperBuilder mapperParser = new XMLMapperBuilder(inputStream, configuration, resource, configuration.getSqlFragments()); mapperParser.parse(); &#125; else if (resource == null &amp;&amp; url != null &amp;&amp; mapperClass == null) &#123;//如果url不为空 ErrorContext.instance().resource(url); InputStream inputStream = Resources.getUrlAsStream(url);//加载mapper文件 //实例化XMLMapperBuilder解析mapper映射文件 XMLMapperBuilder mapperParser = new XMLMapperBuilder(inputStream, configuration, url, configuration.getSqlFragments()); mapperParser.parse(); &#125; else if (resource == null &amp;&amp; url == null &amp;&amp; mapperClass != null) &#123;//如果class不为空 Class&lt;?&gt; mapperInterface = Resources.classForName(mapperClass);//加载class对象 configuration.addMapper(mapperInterface);//向代理中心注册mapper &#125; else &#123; throw new BuilderException(&quot;A mapper element may only specify a url, resource or class, but not more than one.&quot;); &#125; &#125; &#125; &#125;&#125; 可以看到有两种解析类型，一种是直接解析xml文件的，一种是解析接口文件（包扫描就是解析接口） 通过MapperAnnotationBuilder解析接口文件核心方法就是configuration.addMapper 123456789101112131415161718192021222324//将mapper接口的工厂类添加到mapper注册中心 public &lt;T&gt; void addMapper(Class&lt;T&gt; type) &#123; if (type.isInterface()) &#123; if (hasMapper(type)) &#123; throw new BindingException(&quot;Type &quot; + type + &quot; is already known to the MapperRegistry.&quot;); &#125; boolean loadCompleted = false; try &#123; //实例化Mapper接口的代理工程类，并将信息添加至knownMappers knownMappers.put(type, new MapperProxyFactory&lt;T&gt;(type)); // It&#x27;s important that the type is added before the parser is run // otherwise the binding may automatically be attempted by the // mapper parser. If the type is already known, it won&#x27;t try. //解析接口上的注解信息，并添加至configuration对象 MapperAnnotationBuilder parser = new MapperAnnotationBuilder(config, type); parser.parse(); loadCompleted = true; &#125; finally &#123; if (!loadCompleted) &#123; knownMappers.remove(type); &#125; &#125; &#125; &#125; 该方法是用来加载接口的，先为接口创建一个Mapper接口的代理工厂类。MapperProxyFactory。这个工厂类是用来创建该接口类的代理对象，代理对象的InvocationHandler是MapperProxy。然后创建了一个MapperAnnotationBuilder类用来解析接口的注解。MapperAnnotationBuilder.parse源码如下 123456789101112131415161718192021222324public void parse() &#123; String resource = type.toString(); if (!configuration.isResourceLoaded(resource)) &#123; loadXmlResource(); //把接口放到已加载集合中 configuration.addLoadedResource(resource); assistant.setCurrentNamespace(type.getName()); parseCache(); parseCacheRef(); Method[] methods = type.getMethods(); //解析注解 for (Method method : methods) &#123; try &#123; // issue #237 if (!method.isBridge()) &#123; parseStatement(method); &#125; &#125; catch (IncompleteElementException e) &#123; configuration.addIncompleteMethod(new MethodResolver(this, method)); &#125; &#125; &#125; parsePendingMethods();&#125; 可以看到是先加载对应的xml文件然后再去解析接口的注解的 1234567891011121314151617181920private void loadXmlResource() &#123; // Spring may not know the real resource name so we check a flag // to prevent loading again a resource twice // this flag is set at XMLMapperBuilder#bindMapperForNamespace //检查xml文件是否加载过了 if (!configuration.isResourceLoaded(&quot;namespace:&quot; + type.getName())) &#123; //获取xml文件路径，与接口同一个目录，并且名字和接口名相同 String xmlResource = type.getName().replace(&#x27;.&#x27;, &#x27;/&#x27;) + &quot;.xml&quot;; InputStream inputStream = null; try &#123; inputStream = Resources.getResourceAsStream(type.getClassLoader(), xmlResource); &#125; catch (IOException e) &#123; // ignore, resource is not required &#125; if (inputStream != null) &#123; XMLMapperBuilder xmlParser = new XMLMapperBuilder(inputStream, assistant.getConfiguration(), xmlResource, configuration.getSqlFragments(), type.getName()); xmlParser.parse(); &#125; &#125;&#125; 该方法加载的xml文件正常情况下是不存在，因为正常项目是不会把xml文件和接口文件放到同一个，所以都是扫描xml文件 parseCache()和parseCacheRef()其实就是初始化初始化二级缓存和解析缓存Ref，而parseStatement()是把注解解析成MappedStatement然后放到Configuration.mappedStatements中，具体在XMLMapperBuilder再说。 通过XMLMapperBuilder解析xml文件XMLMapperBuilder.parse源码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 public void parse() &#123;//判断是否已经加载该配置文件 if (!configuration.isResourceLoaded(resource)) &#123; configurationElement(parser.evalNode(&quot;/mapper&quot;));//处理mapper节点 configuration.addLoadedResource(resource);//将mapper文件添加到configuration.loadedResources中 bindMapperForNamespace();//注册mapper接口 &#125; /* * 这里是为了处理嵌套的情况 * * 比如 A 引用了 B，而Mapper文件的是解析是按顺序来的 * 也就是说解析A的时候发现B还没解析，这是就报错放入Pending列表中 * */ //处理解析失败的ResultMap节点 parsePendingResultMaps(); //处理解析失败的CacheRef节点 parsePendingCacheRefs(); //处理解析失败的Sql语句节点 parsePendingStatements(); &#125; private void configurationElement(XNode context) &#123; try &#123; //获取mapper节点的namespace属性 String namespace = context.getStringAttribute(&quot;namespace&quot;); if (namespace == null || namespace.equals(&quot;&quot;)) &#123; throw new BuilderException(&quot;Mapper&#x27;s namespace cannot be empty&quot;); &#125; //设置builderAssistant的namespace属性 builderAssistant.setCurrentNamespace(namespace); //解析cache-ref节点 cacheRefElement(context.evalNode(&quot;cache-ref&quot;)); //重点分析 ：解析cache节点----------------1------------------- cacheElement(context.evalNode(&quot;cache&quot;)); //解析parameterMap节点（已废弃） parameterMapElement(context.evalNodes(&quot;/mapper/parameterMap&quot;)); //重点分析 ：解析resultMap节点（基于数据结果去理解）----------------2------------------- resultMapElements(context.evalNodes(&quot;/mapper/resultMap&quot;)); //解析sql节点 sqlElement(context.evalNodes(&quot;/mapper/sql&quot;)); //重点分析 ：解析select、insert、update、delete节点 ----------------3------------------- buildStatementFromContext(context.evalNodes(&quot;select|insert|update|delete&quot;)); &#125; catch (Exception e) &#123; throw new BuilderException(&quot;Error parsing Mapper XML. The XML location is &#x27;&quot; + resource + &quot;&#x27;. Cause: &quot; + e, e); &#125; &#125; 解析cache节点方法cacheElement(context.evalNode(“cache”)); 123456789101112131415161718192021222324//创建二级缓存private void cacheElement(XNode context) throws Exception &#123; if (context != null) &#123; //获取cache节点的type属性，默认为PERPETUAL String type = context.getStringAttribute(&quot;type&quot;, &quot;PERPETUAL&quot;); //根据type对应的cache接口的实现 Class&lt;? extends Cache&gt; typeClass = typeAliasRegistry.resolveAlias(type); //读取eviction属性，既缓存的淘汰策略，默认LRU String eviction = context.getStringAttribute(&quot;eviction&quot;, &quot;LRU&quot;); //根据eviction属性，找到装饰器 Class&lt;? extends Cache&gt; evictionClass = typeAliasRegistry.resolveAlias(eviction); //读取flushInterval属性，既缓存的刷新周期 Long flushInterval = context.getLongAttribute(&quot;flushInterval&quot;); //读取size属性，既缓存的容量大小 Integer size = context.getIntAttribute(&quot;size&quot;); //读取readOnly属性，既缓存的是否只读 boolean readWrite = !context.getBooleanAttribute(&quot;readOnly&quot;, false); //读取blocking属性，既缓存的是否阻塞 boolean blocking = context.getBooleanAttribute(&quot;blocking&quot;, false); Properties props = context.getChildrenAsProperties(); //通过builderAssistant创建缓存对象，并添加至configuration builderAssistant.useNewCache(typeClass, evictionClass, flushInterval, size, readWrite, blocking, props); &#125;&#125; 解析完cache节点后，把解析后的数据交给了MapperBuilderAssistant对象去创建二级缓存Cache. builderAssistant.useNewCache: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 //通过builderAssistant创建缓存对象，并添加至configuration public Cache useNewCache(Class&lt;? extends Cache&gt; typeClass, Class&lt;? extends Cache&gt; evictionClass, Long flushInterval, Integer size, boolean readWrite, boolean blocking, Properties props) &#123; //经典的建造者模式，创建一个cache对象 Cache cache = new CacheBuilder(currentNamespace) .implementation(valueOrDefault(typeClass, PerpetualCache.class)) .addDecorator(valueOrDefault(evictionClass, LruCache.class)) .clearInterval(flushInterval) .size(size) .readWrite(readWrite) .blocking(blocking) .properties(props) .build(); //将缓存添加至configuration，注意二级缓存以命名空间为单位进行划分 configuration.addCache(cache); currentCache = cache; return cache; &#125; //CacheBuilder.build public Cache build() &#123; //设置缓存的主实现类为PerpetualCache setDefaultImplementations(); //通过反射实例化PerpetualCache对象 Cache cache = newBaseCacheInstance(implementation, id); setCacheProperties(cache);//根据cache节点下的&lt;property&gt;信息，初始化cache // issue #352, do not apply decorators to custom caches if (PerpetualCache.class.equals(cache.getClass())) &#123;//如果cache是PerpetualCache的实现，则为其添加标准的装饰器 for (Class&lt;? extends Cache&gt; decorator : decorators) &#123;//为cache对象添加装饰器，这里主要处理缓存清空策略的装饰器 cache = newCacheDecoratorInstance(decorator, cache); setCacheProperties(cache); &#125; //通过一些属性为cache对象添加装饰器,这里主要logger、Synchronized、定时调度等功能 cache = setStandardDecorators(cache); &#125; else if (!LoggingCache.class.isAssignableFrom(cache.getClass())) &#123; //如果cache不是PerpetualCache的实现，则为其添加日志的能力 cache = new LoggingCache(cache); &#125; return cache; &#125;// CacheBuilder.setStandardDecorators private Cache setStandardDecorators(Cache cache) &#123; try &#123; MetaObject metaCache = SystemMetaObject.forObject(cache); if (size != null &amp;&amp; metaCache.hasSetter(&quot;size&quot;)) &#123;//如果有size属性设置最大元素 metaCache.setValue(&quot;size&quot;, size); &#125; if (clearInterval != null) &#123;//设置定时清空 cache = new ScheduledCache(cache); ((ScheduledCache) cache).setClearInterval(clearInterval); &#125; if (readWrite) &#123;//设置读写属性 cache = new SerializedCache(cache); &#125; cache = new LoggingCache(cache);//默认加上日志能力 cache = new SynchronizedCache(cache);//默认加上同步能力 if (blocking) &#123;//根据配置加上阻塞能力 cache = new BlockingCache(cache); &#125; return cache; &#125; catch (Exception e) &#123; throw new CacheException(&quot;Error building standard cache decorators. Cause: &quot; + e, e); &#125; &#125; 可以看到这里面涉及到了两种设计模式，分别是建造者设计模式，用来隐藏Cache创建的细节；装饰器模式，对PerpetualCache做了层层包装，扩展了PerpetualCache的功能，最后可以看到为缓存添加了同步功能来保证二级缓存的线程安全。和添加了日志打印功能。 解析resultMap节点方法resultMapElements(context.evalNodes(“&#x2F;mapper&#x2F;resultMap”)); 核心方法是 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private ResultMap resultMapElement(XNode resultMapNode, List&lt;ResultMapping&gt; additionalResultMappings) throws Exception &#123; ErrorContext.instance().activity(&quot;processing &quot; + resultMapNode.getValueBasedIdentifier()); //获取resultmap节点的id属性 String id = resultMapNode.getStringAttribute(&quot;id&quot;, resultMapNode.getValueBasedIdentifier()); //获取resultmap节点的type属性 String type = resultMapNode.getStringAttribute(&quot;type&quot;, resultMapNode.getStringAttribute(&quot;ofType&quot;, resultMapNode.getStringAttribute(&quot;resultType&quot;, resultMapNode.getStringAttribute(&quot;javaType&quot;)))); //获取resultmap节点的extends属性，描述继承关系 String extend = resultMapNode.getStringAttribute(&quot;extends&quot;); //获取resultmap节点的autoMapping属性，是否开启自动映射 Boolean autoMapping = resultMapNode.getBooleanAttribute(&quot;autoMapping&quot;); //从别名注册中心获取entity的class对象 Class&lt;?&gt; typeClass = resolveClass(type); Discriminator discriminator = null; //记录子节点中的映射结果集合 List&lt;ResultMapping&gt; resultMappings = new ArrayList&lt;&gt;(); resultMappings.addAll(additionalResultMappings); //从xml文件中获取当前resultmap中的所有子节点，并开始遍历 List&lt;XNode&gt; resultChildren = resultMapNode.getChildren(); for (XNode resultChild : resultChildren) &#123; if (&quot;constructor&quot;.equals(resultChild.getName())) &#123;//处理&lt;constructor&gt;节点 processConstructorElement(resultChild, typeClass, resultMappings); &#125; else if (&quot;discriminator&quot;.equals(resultChild.getName())) &#123;//处理&lt;discriminator&gt;节点 discriminator = processDiscriminatorElement(resultChild, typeClass, resultMappings); &#125; else &#123;//处理&lt;id&gt; &lt;result&gt; &lt;association&gt; &lt;collection&gt;节点 List&lt;ResultFlag&gt; flags = new ArrayList&lt;&gt;(); if (&quot;id&quot;.equals(resultChild.getName())) &#123; flags.add(ResultFlag.ID);//如果是id节点，向flags中添加元素 &#125; //创建ResultMapping对象并加入resultMappings集合中 resultMappings.add(buildResultMappingFromContext(resultChild, typeClass, flags)); &#125; &#125; //实例化resultMap解析器 ResultMapResolver resultMapResolver = new ResultMapResolver(builderAssistant, id, typeClass, extend, discriminator, resultMappings, autoMapping); try &#123; //通过resultMap解析器实例化resultMap并将其注册到configuration对象 return resultMapResolver.resolve(); &#125; catch (IncompleteElementException e) &#123; //这里是为了解决resultMap相互引后被引用的resultMap还未加载导致出错该resultMap出错，出错放到一个Incomplete列表中，等到最后才去加载 configuration.addIncompleteResultMap(resultMapResolver); throw e; &#125;&#125; 看最后resultMapResolver.resolve();方法，其实又是把创建的流程交给了MapperBuilderAssistant。 assistant.addResultMap： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//实例化resultMap并将其注册到configuration对象public ResultMap addResultMap( String id, Class&lt;?&gt; type, String extend, Discriminator discriminator, List&lt;ResultMapping&gt; resultMappings, Boolean autoMapping) &#123;//完善id，id的完整格式是&quot;namespace.id&quot; id = applyCurrentNamespace(id, false); //获得父类resultMap的完整id extend = applyCurrentNamespace(extend, true); //针对extend属性的处理 if (extend != null) &#123; if (!configuration.hasResultMap(extend)) &#123; throw new IncompleteElementException(&quot;Could not find a parent resultmap with id &#x27;&quot; + extend + &quot;&#x27;&quot;); &#125; ResultMap resultMap = configuration.getResultMap(extend); List&lt;ResultMapping&gt; extendedResultMappings = new ArrayList&lt;&gt;(resultMap.getResultMappings()); extendedResultMappings.removeAll(resultMappings); // Remove parent constructor if this resultMap declares a constructor. boolean declaresConstructor = false; for (ResultMapping resultMapping : resultMappings) &#123; if (resultMapping.getFlags().contains(ResultFlag.CONSTRUCTOR)) &#123; declaresConstructor = true; break; &#125; &#125; if (declaresConstructor) &#123; Iterator&lt;ResultMapping&gt; extendedResultMappingsIter = extendedResultMappings.iterator(); while (extendedResultMappingsIter.hasNext()) &#123; if (extendedResultMappingsIter.next().getFlags().contains(ResultFlag.CONSTRUCTOR)) &#123; extendedResultMappingsIter.remove(); &#125; &#125; &#125; //添加需要被继承下来的resultMapping对象结合 resultMappings.addAll(extendedResultMappings); &#125; //通过建造者模式实例化resultMap,并注册到configuration.resultMaps中 ResultMap resultMap = new ResultMap.Builder(configuration, id, type, resultMappings, autoMapping) .discriminator(discriminator) .build(); configuration.addResultMap(resultMap); return resultMap;&#125; 会把resultMap的创建解析成如下效果： 最后以namespace.id为key，ResultMap为value，放入Configuration.resultMaps属性中 解析sql节点方法sqlElement(context.evalNodes(“&#x2F;mapper&#x2F;sql”));这挺简单的，就是把字符串放入XMLMapperBuilder.sqlFragments这个map中，为之后的sql语句解析提供支持 解析select、insert、update、delete节点方法buildStatementFromContext(context.evalNodes(“select|insert|update|delete”)); 核心方法buildStatementFromContext： 12345678910111213//处理所有的sql语句节点并注册至configuration对象private void buildStatementFromContext(List&lt;XNode&gt; list, String requiredDatabaseId) &#123; for (XNode context : list) &#123; //创建XMLStatementBuilder 专门用于解析sql语句节点 final XMLStatementBuilder statementParser = new XMLStatementBuilder(configuration, builderAssistant, context, requiredDatabaseId); try &#123; //解析sql语句节点 statementParser.parseStatementNode(); &#125; catch (IncompleteElementException e) &#123; configuration.addIncompleteStatement(statementParser); &#125; &#125;&#125; 该方法会创建XMLStatementBuilder对象，去解析select、insert、update、delete元素的内容，具体的细节看下一节。 解析完后会执行一个bindMapperForNamespace方法： 12345678910111213141516171819202122232425//注册mapper接口 private void bindMapperForNamespace() &#123; //获取命名空间 String namespace = builderAssistant.getCurrentNamespace(); if (namespace != null) &#123; Class&lt;?&gt; boundType = null; try &#123; //通过命名空间获取mapper接口的class对象 boundType = Resources.classForName(namespace); &#125; catch (ClassNotFoundException e) &#123; //ignore, bound type is not required &#125; if (boundType != null) &#123; if (!configuration.hasMapper(boundType)) &#123;//是否已经注册过该mapper接口？ // Spring may not know the real resource name so we set a flag // to prevent loading again this resource from the mapper interface // look at MapperAnnotationBuilder#loadXmlResource //将命名空间添加至configuration.loadedResource集合中 configuration.addLoadedResource(&quot;namespace:&quot; + namespace); //将mapper接口添加到mapper注册中心 configuration.addMapper(boundType); &#125; &#125; &#125; &#125; 在最后执行了configuration.addMapper(boundType);该方法会把namespace对应的接口添加到Configuration.MapperRegistry中knownMappers.put(type, new MapperProxyFactory&lt;T&gt;(type));然后就创建MapperAnnotationBuilder去解析接口的注解。细节看&gt;&gt; 第四步 XMLStatementBuilder对象解析mapper.xml文件中的select、insert、update、delete元素核心方法parseStatementNode 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 public void parseStatementNode() &#123;//获取sql节点的id String id = context.getStringAttribute(&quot;id&quot;); String databaseId = context.getStringAttribute(&quot;databaseId&quot;); if (!databaseIdMatchesCurrent(id, databaseId, this.requiredDatabaseId)) &#123; return; &#125; /*获取sql节点的各种属性*/ Integer fetchSize = context.getIntAttribute(&quot;fetchSize&quot;); Integer timeout = context.getIntAttribute(&quot;timeout&quot;); String parameterMap = context.getStringAttribute(&quot;parameterMap&quot;); String parameterType = context.getStringAttribute(&quot;parameterType&quot;); Class&lt;?&gt; parameterTypeClass = resolveClass(parameterType); String resultMap = context.getStringAttribute(&quot;resultMap&quot;); String resultType = context.getStringAttribute(&quot;resultType&quot;); String lang = context.getStringAttribute(&quot;lang&quot;); LanguageDriver langDriver = getLanguageDriver(lang); Class&lt;?&gt; resultTypeClass = resolveClass(resultType); String resultSetType = context.getStringAttribute(&quot;resultSetType&quot;); StatementType statementType = StatementType.valueOf(context.getStringAttribute(&quot;statementType&quot;, StatementType.PREPARED.toString())); ResultSetType resultSetTypeEnum = resolveResultSetType(resultSetType); //根据sql节点的名称获取SqlCommandType（INSERT, UPDATE, DELETE, SELECT） String nodeName = context.getNode().getNodeName(); SqlCommandType sqlCommandType = SqlCommandType.valueOf(nodeName.toUpperCase(Locale.ENGLISH)); boolean isSelect = sqlCommandType == SqlCommandType.SELECT; boolean flushCache = context.getBooleanAttribute(&quot;flushCache&quot;, !isSelect); boolean useCache = context.getBooleanAttribute(&quot;useCache&quot;, isSelect); boolean resultOrdered = context.getBooleanAttribute(&quot;resultOrdered&quot;, false); // Include Fragments before parsing //在解析sql语句之前先解析&lt;include&gt;节点 XMLIncludeTransformer includeParser = new XMLIncludeTransformer(configuration, builderAssistant); includeParser.applyIncludes(context.getNode()); // Parse selectKey after includes and remove them. //在解析sql语句之前，处理&lt;selectKey&gt;子节点，并在xml节点中删除 processSelectKeyNodes(id, parameterTypeClass, langDriver); // Parse the SQL (pre: &lt;selectKey&gt; and &lt;include&gt; were parsed and removed) //解析sql语句是解析mapper.xml的核心，实例化sqlSource，使用sqlSource封装sql语句 SqlSource sqlSource = langDriver.createSqlSource(configuration, context, parameterTypeClass); String resultSets = context.getStringAttribute(&quot;resultSets&quot;);//获取resultSets属性 String keyProperty = context.getStringAttribute(&quot;keyProperty&quot;);//获取主键信息keyProperty String keyColumn = context.getStringAttribute(&quot;keyColumn&quot;);///获取主键信息keyColumn //根据&lt;selectKey&gt;获取对应的SelectKeyGenerator的id KeyGenerator keyGenerator; String keyStatementId = id + SelectKeyGenerator.SELECT_KEY_SUFFIX; keyStatementId = builderAssistant.applyCurrentNamespace(keyStatementId, true); //获取keyGenerator对象，如果是insert类型的sql语句，会使用KeyGenerator接口获取数据库生产的id； if (configuration.hasKeyGenerator(keyStatementId)) &#123; keyGenerator = configuration.getKeyGenerator(keyStatementId); &#125; else &#123; keyGenerator = context.getBooleanAttribute(&quot;useGeneratedKeys&quot;, configuration.isUseGeneratedKeys() &amp;&amp; SqlCommandType.INSERT.equals(sqlCommandType)) ? Jdbc3KeyGenerator.INSTANCE : NoKeyGenerator.INSTANCE; &#125; //通过builderAssistant实例化MappedStatement，并注册至configuration对象 builderAssistant.addMappedStatement(id, sqlSource, statementType, sqlCommandType, fetchSize, timeout, parameterMap, parameterTypeClass, resultMap, resultTypeClass, resultSetTypeEnum, flushCache, useCache, resultOrdered, keyGenerator, keyProperty, keyColumn, databaseId, langDriver, resultSets); &#125; 最后委托给了MapperBuilderAssistant去创建builderAssistant.addMappedStatement： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758//添加MappedStatement对象public MappedStatement addMappedStatement( String id, SqlSource sqlSource, StatementType statementType, SqlCommandType sqlCommandType, Integer fetchSize, Integer timeout, String parameterMap, Class&lt;?&gt; parameterType, String resultMap, Class&lt;?&gt; resultType, ResultSetType resultSetType, boolean flushCache, boolean useCache, boolean resultOrdered, KeyGenerator keyGenerator, String keyProperty, String keyColumn, String databaseId, LanguageDriver lang, String resultSets) &#123; if (unresolvedCacheRef) &#123; throw new IncompleteElementException(&quot;Cache-ref not yet resolved&quot;); &#125; id = applyCurrentNamespace(id, false); boolean isSelect = sqlCommandType == SqlCommandType.SELECT; //使用建造者模式创建一个mappedStatment MappedStatement.Builder statementBuilder = new MappedStatement.Builder(configuration, id, sqlSource, sqlCommandType) .resource(resource) .fetchSize(fetchSize) .timeout(timeout) .statementType(statementType) .keyGenerator(keyGenerator) .keyProperty(keyProperty) .keyColumn(keyColumn) .databaseId(databaseId) .lang(lang) .resultOrdered(resultOrdered) .resultSets(resultSets) .resultMaps(getStatementResultMaps(resultMap, resultType, id)) .resultSetType(resultSetType) .flushCacheRequired(valueOrDefault(flushCache, !isSelect)) .useCache(valueOrDefault(useCache, isSelect)) .cache(currentCache); ParameterMap statementParameterMap = getStatementParameterMap(parameterMap, parameterType, id); if (statementParameterMap != null) &#123; statementBuilder.parameterMap(statementParameterMap); &#125; MappedStatement statement = statementBuilder.build();//使用建造者模式创建一个mappedStatment configuration.addMappedStatement(statement);//将mappedStatment注册到configuration return statement;&#125; MappedStatement可以理解为是一个包装类，把必要的信息都包装进该类中。MappedStatement与xml元素的对应关系如下：最后将新建好的MappedStatement为value，以namespace.id为key放入Configuration.mappedStatements中，以后通过代理代理对象执行方法时就是在该map中找到对应的MappedStatement。 至此，一个mapper文件的解析完成了，但由于mapper文件中存在父子关系，而mapper文件的解析是按顺序解析了。有这种情况，a引入c，a先解析，c后解析，最典型的是就resultMap的解析，在源码中如果父为解析，会报错然后该初始化会放到一个Incomplete列表中当全部解析完后在去重新解析","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"08MyBatis流程概述","slug":"mybatis/08MyBatis流程概述","date":"2021-11-17T12:00:09.000Z","updated":"2022-03-23T09:03:57.446Z","comments":true,"path":"blog/mybatis/08MyBatis流程概述/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/08MyBatis%E6%B5%81%E7%A8%8B%E6%A6%82%E8%BF%B0/","excerpt":"","text":"jdbc1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;public class JDBCTest &#123; public static void main(String[] args) throws Exception &#123; Connection connection = null; PreparedStatement prepareStatement = null; ResultSet rs = null; try &#123; // 加载驱动 Class.forName(&quot;com.mysql.jdbc.Driver&quot;); // 获取连接 String url = &quot;jdbc:mysql://127.0.0.1:3306/ssmdemo&quot;; String user = &quot;root&quot;; String password = &quot;123456&quot;; connection = DriverManager.getConnection(url, user, password); // 获取statement，preparedStatement String sql = &quot;select * from tb_user where id=?&quot;; prepareStatement = connection.prepareStatement(sql); // 设置参数 prepareStatement.setLong(1, 1l); // 执行查询 rs = prepareStatement.executeQuery(); // 处理结果集 while (rs.next()) &#123; System.out.println(rs.getString(&quot;userName&quot;)); System.out.println(rs.getString(&quot;name&quot;)); System.out.println(rs.getInt(&quot;age&quot;)); System.out.println(rs.getDate(&quot;birthday&quot;)); &#125; &#125; finally &#123; // 关闭连接，释放资源 if (rs != null) &#123; rs.close(); &#125; if (prepareStatement != null) &#123; prepareStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125; &#125;&#125; JDBC缺点分析 ibatisibatis是mybatis的前一个大版本，在ibatis中 1234567891011121314151617181920212223@Beforepublic void init() throws IOException &#123; //--------------------第一阶段--------------------------- // 1.读取mybatis配置文件创SqlSessionFactory String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); // 1.读取mybatis配置文件创SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); inputStream.close();&#125; @Test// ibatis编程模型 本质分析public void originalOperation() throws IOException &#123; //--------------------第二阶段--------------------------- // 2.获取sqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //--------------------第三阶段--------------------------- // 3.执行查询语句并返回结果 TUser user = sqlSession.selectOne(&quot;com.enjoylearning.mybatis.mapper.TUserMapper.selectByPrimaryKey&quot;, 2); System.out.println(user.toString());&#125; mybatis123456789101112131415161718192021222324252627282930313233 @Before public void init() throws IOException &#123; //--------------------第一阶段--------------------------- // 1.读取mybatis配置文件创SqlSessionFactory String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); // 1.读取mybatis配置文件创SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); inputStream.close(); &#125; @Test // 快速入门 public void quickStart() throws IOException &#123; //--------------------第二阶段--------------------------- // 2.获取sqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); // 3.获取对应mapper TUserMapper mapper = sqlSession.getMapper(TUserMapper.class); //--------------------第三阶段--------------------------- // 4.执行查询语句并返回单条数据 TUser user = mapper.selectByPrimaryKey(2, &quot;fe&quot;); System.out.println(user); System.out.println(&quot;----------------------------------&quot;); // 5.执行查询语句并返回多条数据// List&lt;TUser&gt; users = mapper.selectAll();// for (TUser tUser : users) &#123;// System.out.println(tUser);// &#125; &#125; mybatis本质就是代理接口的方法转换成ibatis中的sqlSession.selectOne(&quot;com.enjoylearning.mybatis.mapper.TUserMapper.selectByPrimaryKey&quot;, 2);然后转换成jdbc的代码 Mybaits整体架构这只是一个简单的图，具体的流程在下节说","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"07反射模块分析","slug":"mybatis/07反射模块分析","date":"2021-11-17T12:00:08.000Z","updated":"2022-03-23T09:03:57.442Z","comments":true,"path":"blog/mybatis/07反射模块分析/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/07%E5%8F%8D%E5%B0%84%E6%A8%A1%E5%9D%97%E5%88%86%E6%9E%90/","excerpt":"","text":"反射是Mybatis模块中类最多的模块，通过反射实现了POJO对象的实例化和 POJO的属性赋值，相对JDK自带的反射功能，MyBatis的反射模块功能更为强大，性能更高； 反射模块关 键的几个类如下: ObjectFactory:MyBatis 每次创建结果对象的新实例时，它都会使用对象工厂(ObjectFactory)去构建 POJO; ObjectWrapper:对对象的包装，抽象了对象的属性信息，他定义了一系列查询对象属性信息的方法，以及更新属性的方法; ObjectWrapperFactory: ObjectWrapper 的工厂类，用于创建ObjectWrapper; ReflectorFactory:创建Reflector的工厂类，Reflector是MyBatis反射模块的基础，每个Reflector对象都对应一个类，在其中缓存了反射操作所需要的类元信息; MetaObject:封装了对象元信息，包装了MyBatis中五个核心的反射类。也是提供给外部使用的反射工具类，可以利用它可以读取或者修改对象的属性信息;MetaObject 的 类结构如下所示: 使用 123456789101112131415161718ObjectFactory objectFactory = new DefaultObjectFactory();TUser user = objectFactory.create(TUser.class);ObjectWrapperFactory objectWrapperFactory = new DefaultObjectWrapperFactory();ReflectorFactory reflectorFactory = new DefaultReflectorFactory();MetaObject metaObject = MetaObject.forObject(user, objectFactory, objectWrapperFactory, reflectorFactory);Reflector findForClass = reflectorFactory.findForClass(TUser.class);//默认的构造函数Constructor&lt;?&gt; defaultConstructor = findForClass.getDefaultConstructor();//获取get属性String[] getablePropertyNames = findForClass.getGetablePropertyNames();//获取set属性String[] setablePropertyNames = findForClass.getSetablePropertyNames();//操作对象ObjectWrapper wrapperForUser = new BeanWrapper(metaObject, user);wrapperForUser.set(new PropertyTokenizer(&quot;realName&quot;), &quot;fwefwe&quot;);wrapperForUser.get(new PropertyTokenizer(&quot;realName&quot;));System.out.println(user);","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"06缓存的唯一标识 CacheKey","slug":"mybatis/06缓存的唯一标识 CacheKey","date":"2021-11-17T12:00:07.000Z","updated":"2022-03-23T09:03:57.441Z","comments":true,"path":"blog/mybatis/06缓存的唯一标识 CacheKey/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/06%E7%BC%93%E5%AD%98%E7%9A%84%E5%94%AF%E4%B8%80%E6%A0%87%E8%AF%86%20CacheKey/","excerpt":"","text":"MyBatis中涉及到动态SQL的原因，缓存项的key不能仅仅通过一个String 来表示，所以通过CacheKey来封装缓存的Key值，CacheKey可以封装多个影响缓存项的因素；判断两个CacheKey是否相同关键是比较两个对象的hash值是否一致；构成 CacheKey 对象的要素包括: mappedStatment 的 id 指定查询结果集的范围(分页信息) 查询所使用的 SQL 语句 用户传递给 SQL 语句的实际参数值 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class CacheKey implements Cloneable, Serializable &#123; private static final long serialVersionUID = 1146682552656046210L; public static final CacheKey NULL_CACHE_KEY = new NullCacheKey(); private static final int DEFAULT_MULTIPLYER = 37; private static final int DEFAULT_HASHCODE = 17; private final int multiplier;//参与hash计算的乘数 private int hashcode;//CacheKey的hash值，在update函数中实时运算出来的 private long checksum;//校验和，hash值的和 private int count;//updateList的中元素个数 // 8/21/2017 - Sonarlint flags this as needing to be marked transient. While true if content is not serializable, this is not always true and thus should not be marked transient. //该集合中的元素觉得两个CacheKey是否相等 private List&lt;Object&gt; updateList; public CacheKey() &#123; this.hashcode = DEFAULT_HASHCODE; this.multiplier = DEFAULT_MULTIPLYER; this.count = 0; this.updateList = new ArrayList&lt;&gt;(); &#125; public CacheKey(Object[] objects) &#123; this(); updateAll(objects); &#125; public int getUpdateCount() &#123; return updateList.size(); &#125; //用于添加构成CacheKey对象的要素，每添加一个元素会对hashcode、checksum、count 以及 updateList 进行更新; public void update(Object object) &#123; //获取object的hash值 int baseHashCode = object == null ? 1 : ArrayUtil.hashCode(object); //更新count、checksum以及hashcode的值 count++; checksum += baseHashCode; baseHashCode *= count; hashcode = multiplier * hashcode + baseHashCode; //将对象添加到updateList中 updateList.add(object); &#125; public void updateAll(Object[] objects) &#123; for (Object o : objects) &#123; update(o); &#125; &#125; //用于比较两个元素是否相等。首先比较hashcode、checksum、count是否 //相等，如果这三个值相等，会循环比较 updateList 中每个元素的 hashCode 是否一致; //按照这种方式判断两个对象是否相等，一方面能很严格的判断是否一致避免出现误判， 另外一方面能提高比较的效率; @Override public boolean equals(Object object) &#123; if (this == object) &#123;//比较是不是同一个对象 return true; &#125; if (!(object instanceof CacheKey)) &#123;//是否类型相同 return false; &#125; final CacheKey cacheKey = (CacheKey) object; if (hashcode != cacheKey.hashcode) &#123;//hashcode是否相同 return false; &#125; if (checksum != cacheKey.checksum) &#123;//checksum是否相同 return false; &#125; if (count != cacheKey.count) &#123;//count是否相同 return false; &#125; //以上都不相同，才按顺序比较updateList中元素的hash值是否一致 for (int i = 0; i &lt; updateList.size(); i++) &#123; Object thisObject = updateList.get(i); Object thatObject = cacheKey.updateList.get(i); if (!ArrayUtil.equals(thisObject, thatObject)) &#123; return false; &#125; &#125; return true; &#125; @Override public int hashCode() &#123; return hashcode; &#125; @Override public String toString() &#123; StringBuilder returnValue = new StringBuilder().append(hashcode).append(&#x27;:&#x27;).append(checksum); for (Object object : updateList) &#123; returnValue.append(&#x27;:&#x27;).append(ArrayUtil.toString(object)); &#125; return returnValue.toString(); &#125; @Override public CacheKey clone() throws CloneNotSupportedException &#123; CacheKey clonedCacheKey = (CacheKey) super.clone(); clonedCacheKey.updateList = new ArrayList&lt;&gt;(updateList); return clonedCacheKey; &#125;&#125;","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"05缓存模块分析","slug":"mybatis/05缓存模块分析","date":"2021-11-17T12:00:06.000Z","updated":"2022-03-23T09:03:57.441Z","comments":true,"path":"blog/mybatis/05缓存模块分析/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/05%E7%BC%93%E5%AD%98%E6%A8%A1%E5%9D%97%E5%88%86%E6%9E%90/","excerpt":"","text":"前置知识&lt;&lt;装饰器模式&gt;&gt; MyBatis 缓存模块是一个经典的使用装饰器实现的模块，类图如下: Cache:Cache接口是缓存模块的核心接口，定义了缓存的基本操作; PerpetualCache:在缓存模块中扮演 ConcreteComponent 角色，用HashMap 来实现 cache 的相关操作; BlockingCache:阻塞版本的缓存装 饰器，保证只有一个线程到数据库去查找指定的 key 对应的数据; BlockingCache 是阻塞版本的缓存装饰器，这个装饰器通过ConcurrentHashMap对锁的粒度进行了控制，提高加锁后系统代码运行的效率(注:缓存雪崩的问题可以使用细粒度锁的方式提升锁性能)，除了BlockingCache之外，缓存模块还有其他的装饰器如: LoggingCache:日志能力的缓存; ScheduledCache:定时清空的缓存; BlockingCache:阻塞式缓存; SerializedCache:序列化能力的缓存; SynchronizedCache:进行同步控制的缓存; 除了PerpetualCache其他都是装饰器类，他们都不实现核心的功能，只是通过组合的方式，对核心功能的扩展。 Mybatis 的缓存功能使用 HashMap 实现会不会出现并发安全的问题?答:MyBatis的缓存分为一级缓存、二级缓存。二级缓存是多个会话共享的缓存，确实会出现并发安全的问题，因此 MyBatis 在初始化二级缓存时，会给二级缓存默认加上SynchronizedCache装饰器的增强，在对共享数据 HashMap操作时进行同步控制，所以二级缓存不会出现并发安全问题；而一级缓存是会话独享的，不会出现多个线程同时操作缓存数 据的场景，因此一级缓存也不会出现并发安全的问题;","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"04数据源模块分析","slug":"mybatis/04数据源模块分析","date":"2021-11-17T12:00:05.000Z","updated":"2022-03-23T09:03:57.440Z","comments":true,"path":"blog/mybatis/04数据源模块分析/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/04%E6%95%B0%E6%8D%AE%E6%BA%90%E6%A8%A1%E5%9D%97%E5%88%86%E6%9E%90/","excerpt":"","text":"这里说的是mybatis实现的数据库连接池前置知识&lt;&lt;工厂模式&gt;&gt; 123456789protected Connection getConnection(Log statementLog) throws SQLException &#123; Connection connection = transaction.getConnection(); if (statementLog.isDebugEnabled()) &#123; //创建了一个Connection的代理对象 return ConnectionLogger.newInstance(connection, statementLog, queryStack); &#125; else &#123; return connection; &#125;&#125; mybatis中Transaction和DataSource是组合关系，实现类是PooledDataSource 拿数据源时：由于返回的逻辑链接是PooledConnection(这不准确，因为返回的是一个动态代理的Connection，而这个代理的InvocationHandler是PooledConnection)，该逻辑链接的唯一作用就是在执行了close方法后执行dataSource.pushConnection(this);方法进行链接回收。 回收数据源：其实逻辑很简单，当空闲置连接池资源是否已经达到上限时，关闭真实链接；当空闲置连接池还没达到上限时，new一个PooledConnection，然就旧的 conn.invalidate();接着state.notifyAll();就行了","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"03优雅的增强日志功能","slug":"mybatis/03优雅的增强日志功能","date":"2021-11-17T12:00:04.000Z","updated":"2022-03-23T09:03:57.434Z","comments":true,"path":"blog/mybatis/03优雅的增强日志功能/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/03%E4%BC%98%E9%9B%85%E7%9A%84%E5%A2%9E%E5%BC%BA%E6%97%A5%E5%BF%97%E5%8A%9F%E8%83%BD/","excerpt":"","text":"前置知识&lt;&lt;Java的三种代理模式&gt;&gt; 使用了动态代理，扩展了日志功能 首先搞清楚那些地方需要打印日志? 在创建 prepareStatement 时，打印执行的 SQL 语句; 访问数据库时，打印参数的类型和值 查询出结构后，打印结果数据条数 因此在日志模块中有 BaseJdbcLogger、ConnectionLogger、PreparedStatementLogger 和 ResultSetLogge 通过动态代理负责在不同的位置打印日志;几个相关类的类图如下:BaseExecutor中的getConnection()方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647protected Connection getConnection(Log statementLog) throws SQLException &#123; Connection connection = transaction.getConnection(); if (statementLog.isDebugEnabled()) &#123; //创建了一个Connection的代理对象 return ConnectionLogger.newInstance(connection, statementLog, queryStack); &#125; else &#123; return connection; &#125;&#125;//ConnectionLogger @Override//对连接的增强public Object invoke(Object proxy, Method method, Object[] params) throws Throwable &#123; try &#123; //如果是从Obeject继承的方法直接忽略 if (Object.class.equals(method.getDeclaringClass())) &#123; return method.invoke(this, params); &#125; //如果是调用prepareStatement、prepareCall、createStatement的方法，打印要执行的sql语句 //并返回prepareStatement的代理对象，让prepareStatement也具备日志能力，打印参数 if (&quot;prepareStatement&quot;.equals(method.getName())) &#123; if (isDebugEnabled()) &#123; debug(&quot; Preparing: &quot; + removeBreakingWhitespace((String) params[0]), true);//打印sql语句 &#125; PreparedStatement stmt = (PreparedStatement) method.invoke(connection, params); stmt = PreparedStatementLogger.newInstance(stmt, statementLog, queryStack);//创建代理对象 return stmt; &#125; else if (&quot;prepareCall&quot;.equals(method.getName())) &#123; if (isDebugEnabled()) &#123; debug(&quot; Preparing: &quot; + removeBreakingWhitespace((String) params[0]), true);//打印sql语句 &#125; PreparedStatement stmt = (PreparedStatement) method.invoke(connection, params);//创建代理对象 stmt = PreparedStatementLogger.newInstance(stmt, statementLog, queryStack); return stmt; &#125; else if (&quot;createStatement&quot;.equals(method.getName())) &#123; Statement stmt = (Statement) method.invoke(connection, params); stmt = StatementLogger.newInstance(stmt, statementLog, queryStack);//创建代理对象 return stmt; &#125; else &#123; return method.invoke(connection, params); &#125; &#125; catch (Throwable t) &#123; throw ExceptionUtil.unwrapThrowable(t); &#125;&#125; 很好理解，就是在原来的功能上扩展了打印日志的功能。","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"02日志模块分析","slug":"mybatis/02日志模块分析","date":"2021-11-17T12:00:03.000Z","updated":"2022-03-23T09:03:57.434Z","comments":true,"path":"blog/mybatis/02日志模块分析/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/02%E6%97%A5%E5%BF%97%E6%A8%A1%E5%9D%97%E5%88%86%E6%9E%90/","excerpt":"","text":"在日志模块中使用了适配器模式。mybatis支持slf4J、commonsLoging、Log4J2、Log4J、JdkLog，由于这些日志框架的底层接口不用，所以mybatis中为了兼容这种不同，使用了适配器模式。提供了一个Log接口 1234567891011121314151617public interface Log &#123; boolean isDebugEnabled(); boolean isTraceEnabled(); void error(String s, Throwable e); void error(String s); void debug(String s); void trace(String s); void warn(String s);&#125; 作为适配器，每种日志框架实现该接口，比如Log4j 1234567891011public class Log4jImpl implements Log &#123; private static final String FQCN = Log4jImpl.class.getName(); private final Logger log; public Log4jImpl(String clazz) &#123; log = Logger.getLogger(clazz); &#125; //.............&#125; 这样，只需要在启动时，根据引入的包选择具体的日志框架，代码在LogFactory类中，核心代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public final class LogFactory &#123; /** * Marker to be used by logging implementations that support markers */ public static final String MARKER = &quot;MYBATIS&quot;; //被选定的第三方日志组件适配器的构造方法 private static Constructor&lt;? extends Log&gt; logConstructor; //自动扫描日志实现，并且第三方日志插件加载优先级如下：slf4J → commonsLoging → Log4J2 → Log4J → JdkLog static &#123; tryImplementation(LogFactory::useSlf4jLogging); tryImplementation(LogFactory::useCommonsLogging); tryImplementation(LogFactory::useLog4J2Logging); tryImplementation(LogFactory::useLog4JLogging); tryImplementation(LogFactory::useJdkLogging); tryImplementation(LogFactory::useNoLogging); &#125; private LogFactory() &#123; // disable construction &#125; public static Log getLog(Class&lt;?&gt; aClass) &#123; return getLog(aClass.getName()); &#125; public static Log getLog(String logger) &#123; try &#123; return logConstructor.newInstance(logger); &#125; catch (Throwable t) &#123; throw new LogException(&quot;Error creating logger for logger &quot; + logger + &quot;. Cause: &quot; + t, t); &#125; &#125; public static synchronized void useCustomLogging(Class&lt;? extends Log&gt; clazz) &#123; setImplementation(clazz); &#125; public static synchronized void useSlf4jLogging() &#123; setImplementation(org.apache.ibatis.logging.slf4j.Slf4jImpl.class); &#125; public static synchronized void useCommonsLogging() &#123; setImplementation(org.apache.ibatis.logging.commons.JakartaCommonsLoggingImpl.class); &#125; public static synchronized void useLog4JLogging() &#123; setImplementation(org.apache.ibatis.logging.log4j.Log4jImpl.class); &#125; public static synchronized void useLog4J2Logging() &#123; setImplementation(org.apache.ibatis.logging.log4j2.Log4j2Impl.class); &#125; public static synchronized void useJdkLogging() &#123; setImplementation(org.apache.ibatis.logging.jdk14.Jdk14LoggingImpl.class); &#125; public static synchronized void useStdOutLogging() &#123; setImplementation(org.apache.ibatis.logging.stdout.StdOutImpl.class); &#125; public static synchronized void useNoLogging() &#123; setImplementation(org.apache.ibatis.logging.nologging.NoLoggingImpl.class); &#125; private static void tryImplementation(Runnable runnable) &#123; if (logConstructor == null) &#123;//当构造方法不为空才执行方法 try &#123; runnable.run(); &#125; catch (Throwable t) &#123; // ignore &#125; &#125; &#125; //通过指定的log类来初始化构造方法 //通过指定的class，尝试实例化对应的对象。成了了结束，失败就下一个class private static void setImplementation(Class&lt;? extends Log&gt; implClass) &#123; try &#123; Constructor&lt;? extends Log&gt; candidate = implClass.getConstructor(String.class); Log log = candidate.newInstance(LogFactory.class.getName()); if (log.isDebugEnabled()) &#123; log.debug(&quot;Logging initialized using &#x27;&quot; + implClass + &quot;&#x27; adapter.&quot;); &#125; logConstructor = candidate; &#125; catch (Throwable t) &#123; throw new LogException(&quot;Error setting Log implementation. Cause: &quot; + t, t); &#125; &#125;&#125; 在类加载阶段执行了static代码块中的代码，可以看到第三方日志插件加载优先级如下:slf4J → commonsLoging → Log4J2 → Log4J → JdkLog 日志模块适配器结构类图:","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"01mybatis源码架构分析","slug":"mybatis/01mybatis源码架构分析","date":"2021-11-17T12:00:02.000Z","updated":"2022-03-23T09:03:57.433Z","comments":true,"path":"blog/mybatis/01mybatis源码架构分析/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/01mybatis%E6%BA%90%E7%A0%81%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90/","excerpt":"","text":"基础支撑层:技术组件专注于底层技术实现，通用性较强无业务含义; 核心处理层:业务组件专注 MyBatis 的业务流程实现，依赖于基础支撑层; 接口层:MyBatis 对外提供的访问接口，面向 SqlSession 编程; 思考题:系统为什么要分层? 代码和系统的可维护性更高。系统分层之后，每个层次都有自己的定位，每个层次内部的组件都有自己的分工，系统就会变得很清晰，维护起来非常明确; 方便开发团队分工和开发效率的提升;举个例子，mybatis 这么大的一个源码框架不可 能是一个人开发的，他需要一个团队，团队之间肯定有分工，既然有了层次的划分，分工也会变得容易，开发人员可以专注于某一层的某一个模块的实现，专注力提升了，开发效率自然也会提升; 提高系统的伸缩性和性能。系统分层之后，我们只要把层次之间的调用接口明确了，那我们就可以从逻辑上的分层变成物理上的分层。当系统并发量吞吐量上来了，怎么办? 为了提高系统伸缩性和性能，我们可以把不同的层部署在不同服务器集群上，不同的组 件放在不同的机器上，用多台机器去抗压力，这就提高了系统的性能。压力大的时候扩 展节点加机器，压力小的时候，压缩节点减机器，系统的伸缩性就是这么来的;","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"00mybatis教程","slug":"mybatis/00mybatis教程","date":"2021-11-17T12:00:01.000Z","updated":"2022-03-23T09:03:57.431Z","comments":true,"path":"blog/mybatis/00mybatis教程/","link":"","permalink":"http://sv.pointcut.cc/blog/mybatis/00mybatis%E6%95%99%E7%A8%8B/","excerpt":"","text":"本文目录 1.从JDBC谈起 2.MyBatis介绍 3.Mybaits整体架构 4.快速入门（quick start） 5.分析 6.完整的CRUD操作 7.动态代理Mapper实现类 8.mybatis-config.xml详解 9.Mapper XML文件详解 10.动态sql 11.缓存 12.高级查询 13.延迟加载 14.如果sql语句中出现’&lt;’的解决方案 15.Spring 集成Mybatis 16.SpringBoot 集成Mybatis 17.Mybatis Generator的使用 18.MyBatis整合分页插件 pageHelper 1.从JDBC谈起1.1.使用IDEA创建maven工程 1.2.引入mysql依赖包12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt;&lt;/dependency&gt; 1.3.准备数据 创建数据库： 1CREATE DATABASE ssmdemo; 创建表： 12345678910111213DROP TABLE IF EXISTS tb_user;CREATE TABLE tb_user (id char(32) NOT NULL,user_name varchar(32) DEFAULT NULL,password varchar(32) DEFAULT NULL,name varchar(32) DEFAULT NULL,age int(10) DEFAULT NULL,sex int(2) DEFAULT NULL,birthday date DEFAULT NULL,created datetime DEFAULT NULL,updated datetime DEFAULT NULL,PRIMARY KEY (id)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 插入数据： 12INSERT INTO ssmdemo.tb_user ( userName, password, name, age, sex, birthday, created, updated) VALUES ( ‘zpc’, ‘123456’, ‘鹏程’, ‘22’, ‘1’, ‘1990-09-02’, sysdate(), sysdate());INSERT INTO ssmdemo.tb_user ( userName, password, name, age, sex, birthday, created, updated) VALUES ( ‘hj’, ‘123456’, ‘静静’, ‘22’, ‘1’, ‘1993-09-05’, sysdate(), sysdate()); 1.4.JDBC基础代码回顾 JDBCTest.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;/** * @author Evan */public class JDBCTest &#123; public static void main(String[] args) throws Exception &#123; Connection connection = null; PreparedStatement prepareStatement = null; ResultSet rs = null; try &#123; // 加载驱动 Class.forName(&quot;com.mysql.jdbc.Driver&quot;); // 获取连接 String url = &quot;jdbc:mysql://127.0.0.1:3306/ssmdemo&quot;; String user = &quot;root&quot;; String password = &quot;123456&quot;; connection = DriverManager.getConnection(url, user, password); // 获取statement，preparedStatement String sql = &quot;select * from tb_user where id=?&quot;; prepareStatement = connection.prepareStatement(sql); // 设置参数 prepareStatement.setLong(1, 1l); // 执行查询 rs = prepareStatement.executeQuery(); // 处理结果集 while (rs.next()) &#123; System.out.println(rs.getString(&quot;userName&quot;)); System.out.println(rs.getString(&quot;name&quot;)); System.out.println(rs.getInt(&quot;age&quot;)); System.out.println(rs.getDate(&quot;birthday&quot;)); &#125; &#125; finally &#123; // 关闭连接，释放资源 if (rs != null) &#123; rs.close(); &#125; if (prepareStatement != null) &#123; prepareStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125; &#125;&#125; 1.5.JDBC缺点分析 2.MyBatis介绍 官方文档 http://www.mybatis.org/mybatis-3/getting-started.html 3.Mybaits整体架构 4.快速入门（quick start）4.1.引入依赖（pom.xml）12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.2.8&lt;/version&gt;&lt;/dependency&gt; 4.2.全局配置文件（mybatis-config.xml）12345678910111213141516171819202122232425262728293031323334353637383940&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;!-- 根标签 --&gt;&lt;configuration&gt;&lt;properties&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://127.0.0.1:3306/mybatis-110?useUnicode=true&amp;amp;characterEncoding=utf-8&amp;amp;allowMultiQueries=true&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot;/&gt; &lt;/properties&gt; &lt;!-- 环境，可以配置多个，default：指定采用哪个环境 --&gt; &lt;environments default=&quot;test&quot;&gt; &lt;!-- id：唯一标识 --&gt; &lt;environment id=&quot;test&quot;&gt; &lt;!-- 事务管理器，JDBC类型的事务管理器 --&gt; &lt;transactionManager type=&quot;JDBC&quot; /&gt; &lt;!-- 数据源，池类型的数据源 --&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://127.0.0.1:3306/mybatis-110&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot; /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;environment id=&quot;development&quot;&gt; &lt;!-- 事务管理器，JDBC类型的事务管理器 --&gt; &lt;transactionManager type=&quot;JDBC&quot; /&gt; &lt;!-- 数据源，池类型的数据源 --&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot; /&gt; &lt;!-- 配置了properties，所以可以直接引用 --&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot; /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;/configuration&gt; 4.3.配置Map.xml（MyMapper.xml）12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;!-- mapper:根标签，namespace：命名空间，随便写，一般保证命名空间唯一 --&gt;&lt;mapper namespace=&quot;MyMapper&quot;&gt; &lt;!-- statement，内容：sql语句。id：唯一标识，随便写，在同一个命名空间下保持唯一 resultType：sql语句查询结果集的封装类型,tb_user即为数据库中的表 --&gt; &lt;select id=&quot;selectUser&quot; resultType=&quot;com.zpc.mybatis.User&quot;&gt; select * from tb_user where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 4.4.修改全局配置文件（mybatis-config.xml）配上MyMapper.xml 1234567891011121314151617181920&lt;configuration&gt; &lt;!-- 环境，可以配置多个，default：指定采用哪个环境 --&gt; &lt;environments default=&quot;test&quot;&gt; &lt;!-- id：唯一标识 --&gt; &lt;environment id=&quot;test&quot;&gt; &lt;!-- 事务管理器，JDBC类型的事务管理器 --&gt; &lt;transactionManager type=&quot;JDBC&quot; /&gt; &lt;!-- 数据源，池类型的数据源 --&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://127.0.0.1:3306/ssmdemo&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot; /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=&quot;mappers/MyMapper.xml&quot; /&gt; &lt;/mappers&gt;&lt;/configuration&gt; 4.5.构建sqlSessionFactory（MybatisTest.java）1234567// 指定全局配置文件String resource = &quot;mybatis-config.xml&quot;;// 读取配置文件InputStream inputStream = Resources.getResourceAsStream(resource);// 构建sqlSessionFactorySqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); 4.6.打开sqlSession会话，并执行sql（MybatisTest.java）123456// 获取sqlSessionSqlSession sqlSession = sqlSessionFactory.openSession();// 操作CRUD，第一个参数：指定statement，规则：命名空间+“.”+statementId// 第二个参数：指定传入sql的参数：这里是用户idUser user = sqlSession.selectOne(&quot;MyMapper.selectUser&quot;, 1);System.out.println(user); 完整代码： MybatisTest.java 12345678910111213141516171819202122232425262728import com.zpc.test.pojo.User;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import java.io.InputStream;public class MybatisTest &#123; public static void main(String[] args) throws Exception &#123; // 指定全局配置文件 String resource = &quot;mybatis-config.xml&quot;; // 读取配置文件 InputStream inputStream = Resources.getResourceAsStream(resource); // 构建sqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); // 获取sqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; // 操作CRUD，第一个参数：指定statement，规则：命名空间+“.”+statementId // 第二个参数：指定传入sql的参数：这里是用户id User user = sqlSession.selectOne(&quot;MyMapper.selectUser&quot;, 1); System.out.println(user); &#125; finally &#123; sqlSession.close(); &#125; &#125;&#125; User.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101import java.text.SimpleDateFormat;import java.util.Date;public class User &#123; private String id; private String userName; private String password; private String name; private Integer age; private Integer sex; private Date birthday; private String created; private String updated; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public Integer getSex() &#123; return sex; &#125; public void setSex(Integer sex) &#123; this.sex = sex; &#125; public Date getBirthday() &#123; return birthday; &#125; public void setBirthday(Date birthday) &#123; this.birthday = birthday; &#125; public String getCreated() &#123; return created; &#125; public void setCreated(String created) &#123; this.created = created; &#125; public String getUpdated() &#123; return updated; &#125; public void setUpdated(String updated) &#123; this.updated = updated; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;id=&#x27;&quot; + id + &#x27;\\&#x27;&#x27; + &quot;, userName=&#x27;&quot; + userName + &#x27;\\&#x27;&#x27; + &quot;, password=&#x27;&quot; + password + &#x27;\\&#x27;&#x27; + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, sex=&quot; + sex + &quot;, birthday=&#x27;&quot; + new SimpleDateFormat(&quot;yyyy-MM-dd&quot;).format(birthday) + &#x27;\\&#x27;&#x27; + &quot;, created=&#x27;&quot; + created + &#x27;\\&#x27;&#x27; + &quot;, updated=&#x27;&quot; + updated + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 4.7.目录结构 5.分析5.1.引入日志依赖包（pom.xml）会自动引入log4j以及slf4j-api 12345&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.5&lt;/version&gt;&lt;/dependency&gt; 5.2.添加log4j.properties12345log4j.rootLogger=DEBUG,A1log4j.logger.org.apache=DEBUGlog4j.appender.A1=org.apache.log4j.ConsoleAppenderlog4j.appender.A1.layout=org.apache.log4j.PatternLayoutlog4j.appender.A1.layout.ConversionPattern=%-d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c]-[%p] %m%n 再次运行程序会打印日志： 123456789102018-06-30 19:53:37,554 [main] [org.apache.ibatis.transaction.jdbc.JdbcTransaction]-[DEBUG] Opening JDBC Connection2018-06-30 19:53:37,818 [main] [org.apache.ibatis.datasource.pooled.PooledDataSource]-[DEBUG] Created connection 2094411587.2018-06-30 19:53:37,818 [main] [org.apache.ibatis.transaction.jdbc.JdbcTransaction]-[DEBUG] Setting autocommit to false on JDBC Connection [com.mysql.jdbc.JDBC4Connection@7cd62f43]2018-06-30 19:53:37,863 [main] [MyMapper.selectUser]-[DEBUG] ==&gt; Preparing: select * from tb_user where id = ? 2018-06-30 19:53:37,931 [main] [MyMapper.selectUser]-[DEBUG] ==&gt; Parameters: 1(Integer)2018-06-30 19:53:37,953 [main] [MyMapper.selectUser]-[DEBUG] &lt;== Total: 1User&#123;id=&#x27;1&#x27;, userName=&#x27;zpc&#x27;, password=&#x27;123456&#x27;, name=&#x27;鹏程&#x27;, age=25, sex=1, birthday=&#x27;1990-09-02&#x27;, created=&#x27;2018-06-30 18:20:18.0&#x27;, updated=&#x27;2018-06-30 18:20:18.0&#x27;&#125;2018-06-30 19:53:37,954 [main] [org.apache.ibatis.transaction.jdbc.JdbcTransaction]-[DEBUG] Resetting autocommit to true on JDBC Connection [com.mysql.jdbc.JDBC4Connection@7cd62f43]2018-06-30 19:53:37,954 [main] [org.apache.ibatis.transaction.jdbc.JdbcTransaction]-[DEBUG] Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@7cd62f43]2018-06-30 19:53:37,955 [main] [org.apache.ibatis.datasource.pooled.PooledDataSource]-[DEBUG] Returned connection 2094411587 to pool. 5.3.MyBatis使用步骤总结 1)配置mybatis-config.xml 全局的配置文件 (1、数据源，2、外部的mapper) 2)创建SqlSessionFactory 3)通过SqlSessionFactory创建SqlSession对象 4)通过SqlSession操作数据库 CRUD 5)调用session.commit()提交事务 6)调用session.close()关闭会话 6.完整的CRUD操作6.1.创建UserDao接口1234567891011121314151617181920212223242526272829303132333435363738394041import com.zpc.mybatis.pojo.User;import java.util.List;public interface UserDao &#123; /** * 根据id查询用户信息 * * @param id * @return */ public User queryUserById(String id); /** * 查询所有用户信息 * * @return */ public List&lt;User&gt; queryUserAll(); /** * 新增用户 * * @param user */ public void insertUser(User user); /** * 更新用户信息 * * @param user */ public void updateUser(User user); /** * 根据id删除用户信息 * * @param id */ public void deleteUser(String id);&#125; 6.2.创建UserDaoImpl1234567891011121314151617181920212223242526272829303132333435363738import com.zpc.mybatis.dao.UserDao;import com.zpc.mybatis.pojo.User;import org.apache.ibatis.session.SqlSession;import java.util.List;public class UserDaoImpl implements UserDao &#123; public SqlSession sqlSession; public UserDaoImpl(SqlSession sqlSession) &#123; this.sqlSession = sqlSession; &#125; @Override public User queryUserById(String id) &#123; return this.sqlSession.selectOne(&quot;UserDao.queryUserById&quot;, id); &#125; @Override public List&lt;User&gt; queryUserAll() &#123; return this.sqlSession.selectList(&quot;UserDao.queryUserAll&quot;); &#125; @Override public void insertUser(User user) &#123; this.sqlSession.insert(&quot;UserDao.insertUser&quot;, user); &#125; @Override public void updateUser(User user) &#123; this.sqlSession.update(&quot;UserDao.updateUser&quot;, user); &#125; @Override public void deleteUser(String id) &#123; this.sqlSession.delete(&quot;UserDao.deleteUser&quot;, id); &#125;&#125; 6.3.编写UserDao对应的UserDaoMapper.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;!-- mapper:根标签，namespace：命名空间，随便写，一般保证命名空间唯一 --&gt;&lt;mapper namespace=&quot;UserDao&quot;&gt; &lt;!-- statement，内容：sql语句。id：唯一标识，随便写，在同一个命名空间下保持唯一 resultType：sql语句查询结果集的封装类型,tb_user即为数据库中的表 --&gt; &lt;!--&lt;select id=&quot;queryUserById&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt;--&gt; &lt;!--select * from tb_user where id = #&#123;id&#125;--&gt; &lt;!--&lt;/select&gt;--&gt; &lt;!--使用别名--&gt; &lt;select id=&quot;queryUserById&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select tuser.id as id, tuser.user_name as userName, tuser.password as password, tuser.name as name, tuser.age as age, tuser.birthday as birthday, tuser.sex as sex, tuser.created as created, tuser.updated as updated from tb_user tuser where tuser.id = #&#123;id&#125;; &lt;/select&gt; &lt;select id=&quot;queryUserAll&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user; &lt;/select&gt; &lt;!--插入数据--&gt; &lt;insert id=&quot;insertUser&quot; parameterType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; INSERT INTO tb_user ( user_name, password, name, age, sex, birthday, created, updated ) VALUES ( #&#123;userName&#125;, #&#123;password&#125;, #&#123;name&#125;, #&#123;age&#125;, #&#123;sex&#125;, #&#123;birthday&#125;, now(), now() ); &lt;/insert&gt; &lt;update id=&quot;updateUser&quot; parameterType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; UPDATE tb_user &lt;trim prefix=&quot;set&quot; suffixOverrides=&quot;,&quot;&gt; &lt;if test=&quot;userName!=null&quot;&gt;user_name = #&#123;userName&#125;,&lt;/if&gt; &lt;if test=&quot;password!=null&quot;&gt;password = #&#123;password&#125;,&lt;/if&gt; &lt;if test=&quot;name!=null&quot;&gt;name = #&#123;name&#125;,&lt;/if&gt; &lt;if test=&quot;age!=null&quot;&gt;age = #&#123;age&#125;,&lt;/if&gt; &lt;if test=&quot;sex!=null&quot;&gt;sex = #&#123;sex&#125;,&lt;/if&gt; &lt;if test=&quot;birthday!=null&quot;&gt;birthday = #&#123;birthday&#125;,&lt;/if&gt; updated = now(), &lt;/trim&gt; WHERE (id = #&#123;id&#125;); &lt;/update&gt; &lt;delete id=&quot;deleteUser&quot;&gt; delete from tb_user where id=#&#123;id&#125; &lt;/delete&gt;&lt;/mapper&gt; 在mybatis-config.xml中添加配置： 1234&lt;mappers&gt; &lt;mapper resource=&quot;mappers/MyMapper.xml&quot;/&gt; &lt;mapper resource=&quot;mappers/UserDaoMapper.xml&quot;/&gt;&lt;/mappers&gt; 6.4.添加UserDao的测试用例Pom文件中添加junit依赖 12345&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt;&lt;/dependency&gt; 按住Alt+Enter,选择create test 6.5.编写UserDao的测试用例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import com.zpc.mybatis.dao.UserDao;import com.zpc.mybatis.dao.impl.UserDaoImpl;import com.zpc.mybatis.pojo.User;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Before;import org.junit.Test;import java.io.InputStream;import java.util.Date;import java.util.List;public class UserDaoTest &#123; public UserDao userDao; public SqlSession sqlSession; @Before public void setUp() throws Exception &#123; // mybatis-config.xml String resource = &quot;mybatis-config.xml&quot;; // 读取配置文件 InputStream is = Resources.getResourceAsStream(resource); // 构建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(is); // 获取sqlSession sqlSession = sqlSessionFactory.openSession(); this.userDao = new UserDaoImpl(sqlSession); &#125; @Test public void queryUserById() throws Exception &#123; System.out.println(this.userDao.queryUserById(&quot;1&quot;)); &#125; @Test public void queryUserAll() throws Exception &#123; List&lt;User&gt; userList = this.userDao.queryUserAll(); for (User user : userList) &#123; System.out.println(user); &#125; &#125; @Test public void insertUser() throws Exception &#123; User user = new User(); user.setAge(16); user.setBirthday(new Date(&quot;1990/09/02&quot;)); user.setName(&quot;大鹏&quot;); user.setPassword(&quot;123456&quot;); user.setSex(1); user.setUserName(&quot;evan&quot;); this.userDao.insertUser(user); this.sqlSession.commit(); &#125; @Test public void updateUser() throws Exception &#123; User user = new User(); user.setBirthday(new Date()); user.setName(&quot;静鹏&quot;); user.setPassword(&quot;654321&quot;); user.setSex(1); user.setUserName(&quot;evanjin&quot;); user.setId(&quot;1&quot;); this.userDao.updateUser(user); this.sqlSession.commit(); &#125; @Test public void deleteUser() throws Exception &#123; this.userDao.deleteUser(&quot;4&quot;); this.sqlSession.commit(); &#125;&#125; 6.6.目录结构 6.7.解决数据库字段名和实体类属性名不一致的问题查询数据的时候，发现查不到userName的信息，User&#123;id=‘2’, userName=‘null’, password=‘123456’, name=‘静静’, age=22, sex=0, birthday=‘1993-09-05’, created=‘2018-06-30 18:22:28.0’, updated=‘2018-06-30 18:22:28.0’&#125;原因：数据库的字段名是user_name，POJO中的属性名字是userName两端不一致，造成mybatis无法填充对应的字段信息。修改方法：在sql语句中使用别名。 解决方案1：在sql语句中使用别名： 123456789101112131415&lt;select id=&quot;queryUserById&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select tuser.id as id, tuser.user_name as userName, tuser.password as password, tuser.name as name, tuser.age as age, tuser.birthday as birthday, tuser.sex as sex, tuser.created as created, tuser.updated as updated from tb_user tuser where tuser.id = #&#123;id&#125;;&lt;/select&gt; 解决方案2： 参考后面的resultMap –mapper具体的配置的时候 解决方案3：参考驼峰匹配 — mybatis-config.xml 的时候 7.动态代理Mapper实现类7.1.思考上述CRUD中的问题 1、接口-&gt;实现类-&gt;mapper.xml 2、实现类中，使用mybatis的方式非常类似 3、xml中的sql statement 硬编码到java代码中。 思考：能否只写接口，不写实现类。只编写接口和Mapper.xml即可？ 因为在dao（mapper）的实现类中对sqlsession的使用方式很类似。因此mybatis提供了接口的动态代理。 7.2.使用动态代理改造CRUD 修改测试用例的setUp方法 执行queryUserAll()方法 12345org.apache.ibatis.binding.BindingException: Type interface com.zpc.mybatis.dao.UserDao is not known to the MapperRegistry. at org.apache.ibatis.binding.MapperRegistry.getMapper(MapperRegistry.java:47) at org.apache.ibatis.session.Configuration.getMapper(Configuration.java:655) at org.apache.ibatis.session.defaults.DefaultSqlSession.getMapper(DefaultSqlSession.java:222)at com.zpc.mybatis.test.UserDaoTest.setUp(UserDaoTest.java:32) 分析原因，在UserMapper.xml中配置接口的全路径 mapper.xml namespace 如果希望使用mybatis通过的动态代理的接口，就需要namespace中的值，和需要对应的Mapper(dao)接口的全路径一致。Mapper中Namespace的定义本身是没有限制的，只要不重复即可，但如果使用Mybatis的DAO接口动态代理，则namespace必须为DAO接口的全路径，例如：com.zpc.mybatis.dao.UserDao 1&lt;mapper namespace=&quot;com.zpc.mybatis.dao.UserDao&quot;&gt; 7.3.完整的例子1、创建UserMapper接口（对应原UserDao） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public interface UserMapper &#123; /** * 登录（直接使用注解指定传入参数名称） * @param userName * @param password * @return */ public User login(@Param(&quot;userName&quot;) String userName, @Param(&quot;password&quot;) String password); /** * 根据表名查询用户信息（直接使用注解指定传入参数名称） * @param tableName * @return */ public List&lt;User&gt; queryUserByTableName(@Param(&quot;tableName&quot;) String tableName); /** * 根据Id查询用户信息 * @param id * @return */ public User queryUserById(Long id); /** * 查询所有用户信息 * @return */ public List&lt;User&gt; queryUserAll(); /** * 新增用户信息 * @param user */ public void insertUser(User user); /** * 根据id更新用户信息 * @param user */ public void updateUser(User user); /** * 根据id删除用户信息 * @param id */ public void deleteUserById(Long id);&#125; 2、创建UserMapper.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;!-- mapper:根标签，namespace：命名空间，随便写，一般保证命名空间唯一 ，为了使用接口动态代理，这里必须是接口的全路径名--&gt;&lt;mapper namespace=&quot;com.zpc.mybatis.dao.UserMapper&quot;&gt; &lt;!-- 1.#&#123;&#125;,预编译的方式preparedstatement，使用占位符替换，防止sql注入，一个参数的时候，任意参数名可以接收 2.$&#123;&#125;,普通的Statement，字符串直接拼接，不可以防止sql注入，一个参数的时候，必须使用$&#123;value&#125;接收参数 --&gt; &lt;select id=&quot;queryUserByTableName&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from $&#123;tableName&#125; &lt;/select&gt; &lt;select id=&quot;login&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user where user_name = #&#123;userName&#125; and password = #&#123;password&#125; &lt;/select&gt; &lt;!-- statement，内容：sql语句。 id：唯一标识，随便写，在同一个命名空间下保持唯一，使用动态代理之后要求和方法名保持一致 resultType：sql语句查询结果集的封装类型，使用动态代理之后和方法的返回类型一致；resultMap：二选一 parameterType：参数的类型，使用动态代理之后和方法的参数类型一致 --&gt; &lt;select id=&quot;queryUserById&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user where id = #&#123;id&#125; &lt;/select&gt; &lt;select id=&quot;queryUserAll&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user &lt;/select&gt; &lt;!-- 新增的Statement id：唯一标识，随便写，在同一个命名空间下保持唯一，使用动态代理之后要求和方法名保持一致 parameterType：参数的类型，使用动态代理之后和方法的参数类型一致 useGeneratedKeys:开启主键回写 keyColumn：指定数据库的主键 keyProperty：主键对应的pojo属性名 --&gt; &lt;insert id=&quot;insertUser&quot; useGeneratedKeys=&quot;true&quot; keyColumn=&quot;id&quot; keyProperty=&quot;id&quot; parameterType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; INSERT INTO tb_user ( id, user_name, password, name, age, sex, birthday, created, updated ) VALUES ( null, #&#123;userName&#125;, #&#123;password&#125;, #&#123;name&#125;, #&#123;age&#125;, #&#123;sex&#125;, #&#123;birthday&#125;, NOW(), NOW() ); &lt;/insert&gt; &lt;!-- 更新的statement id：唯一标识，随便写，在同一个命名空间下保持唯一，使用动态代理之后要求和方法名保持一致 parameterType：参数的类型，使用动态代理之后和方法的参数类型一致 --&gt; &lt;update id=&quot;updateUser&quot; parameterType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; UPDATE tb_user &lt;trim prefix=&quot;set&quot; suffixOverrides=&quot;,&quot;&gt; &lt;if test=&quot;userName!=null&quot;&gt;user_name = #&#123;userName&#125;,&lt;/if&gt; &lt;if test=&quot;password!=null&quot;&gt;password = #&#123;password&#125;,&lt;/if&gt; &lt;if test=&quot;name!=null&quot;&gt;name = #&#123;name&#125;,&lt;/if&gt; &lt;if test=&quot;age!=null&quot;&gt;age = #&#123;age&#125;,&lt;/if&gt; &lt;if test=&quot;sex!=null&quot;&gt;sex = #&#123;sex&#125;,&lt;/if&gt; &lt;if test=&quot;birthday!=null&quot;&gt;birthday = #&#123;birthday&#125;,&lt;/if&gt; updated = now(), &lt;/trim&gt; WHERE (id = #&#123;id&#125;); &lt;/update&gt; &lt;!-- 删除的statement id：唯一标识，随便写，在同一个命名空间下保持唯一，使用动态代理之后要求和方法名保持一致 parameterType：参数的类型，使用动态代理之后和方法的参数类型一致 --&gt; &lt;delete id=&quot;deleteUserById&quot; parameterType=&quot;java.lang.String&quot;&gt; delete from tb_user where id=#&#123;id&#125; &lt;/delete&gt;&lt;/mapper&gt; 3、全局配置文件mybatis-config.xml引入UserMapper.xml 12345&lt;mappers&gt; &lt;mapper resource=&quot;mappers/MyMapper.xml&quot;/&gt; &lt;mapper resource=&quot;mappers/UserDaoMapper.xml&quot;/&gt; &lt;mapper resource=&quot;mappers/UserMapper.xml&quot;/&gt;&lt;/mappers&gt; 4、创建UserMapper测试用例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import com.zpc.mybatis.dao.UserMapper;import com.zpc.mybatis.pojo.User;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Before;import org.junit.Test;import java.io.InputStream;import java.util.Date;import java.util.List;public class UserMapperTest &#123; public UserMapper userMapper; @Before public void setUp() throws Exception &#123; // 指定配置文件 String resource = &quot;mybatis-config.xml&quot;; // 读取配置文件 InputStream inputStream = Resources.getResourceAsStream(resource); // 构建sqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); // 获取sqlSession SqlSession sqlSession = sqlSessionFactory.openSession(true); // 1. 映射文件的命名空间（namespace）必须是mapper接口的全路径 // 2. 映射文件的statement的id必须和mapper接口的方法名保持一致 // 3. Statement的resultType必须和mapper接口方法的返回类型一致 // 4. statement的parameterType必须和mapper接口方法的参数类型一致（不一定） this.userMapper = sqlSession.getMapper(UserMapper.class); &#125; @Test public void testQueryUserByTableName() &#123; List&lt;User&gt; userList = this.userMapper.queryUserByTableName(&quot;tb_user&quot;); for (User user : userList) &#123; System.out.println(user); &#125; &#125; @Test public void testLogin() &#123; System.out.println(this.userMapper.login(&quot;hj&quot;, &quot;123456&quot;)); &#125; @Test public void testQueryUserById() &#123; System.out.println(this.userMapper.queryUserById(&quot;1&quot;)); &#125; @Test public void testQueryUserAll() &#123; List&lt;User&gt; userList = this.userMapper.queryUserAll(); for (User user : userList) &#123; System.out.println(user); &#125; &#125; @Test public void testInsertUser() &#123; User user = new User(); user.setAge(20); user.setBirthday(new Date()); user.setName(&quot;大神&quot;); user.setPassword(&quot;123456&quot;); user.setSex(2); user.setUserName(&quot;bigGod222&quot;); this.userMapper.insertUser(user); System.out.println(user.getId()); &#125; @Test public void testUpdateUser() &#123; User user = new User(); user.setBirthday(new Date()); user.setName(&quot;静静&quot;); user.setPassword(&quot;123456&quot;); user.setSex(0); user.setUserName(&quot;Jinjin&quot;); user.setId(&quot;1&quot;); this.userMapper.updateUser(user); &#125; @Test public void testDeleteUserById() &#123; this.userMapper.deleteUserById(&quot;1&quot;); &#125;&#125; 目录结构： 7.4.动态代理总结 8.mybatis-config.xml详解mybatis-config.xml讲究严格的顺序，具体顺序遵循文档的顺序 8.1.properties属性读取外部资源properties配置的属性都是可外部配置且可动态替换的，既可以在典型的 Java 属性文件中配置，亦可通过 properties 元素的子元素来传递。例如： 1234&lt;properties resource=&quot;org/mybatis/example/config.properties&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;dev_user&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;F2Fa3!33TYyg&quot;/&gt;&lt;/properties&gt; 然后其中的属性就可以在整个配置文件中被用来替换需要动态配置的属性值。比如: 123456&lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt;&lt;/dataSource&gt; 这个例子中的 username 和 password 将会由 properties 元素中设置的相应值来替换。 driver 和 url 属性将会由 config.properties 文件中对应的值来替换。这样就为配置提供了诸多灵活选择。 属性也可以被传递到 SqlSessionFactoryBuilder.build()方法中。例如： 123SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(reader, props);// ... or ...SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(reader, environment, props); 如果属性在不只一个地方进行了配置，那么 MyBatis 将按照下面的顺序来加载： 1）在 properties 元素体内指定的属性首先被读取。 2）然后根据 properties 元素中的 resource 属性读取类路径下属性文件或根据 url 属性指定的路径读取属性文件，并覆盖已读取的同名属性。 3）最后读取作为方法参数传递的属性，并覆盖已读取的同名属性。 因此，通过方法参数传递的属性具有最高优先级，resource&#x2F;url 属性中指定的配置文件次之，最低优先级的是 properties 属性中指定的属性。 8.2.settings设置 123&lt;settings&gt; &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt;&lt;/settings&gt; 测试：没有开启驼峰匹配： 12342018-07-01 13:57:56,486 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Preparing: select * from tb_user where id = ? 2018-07-01 13:57:56,524 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Parameters: 1(String)2018-07-01 13:57:56,568 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] &lt;== Total: 1User&#123;id=&#x27;1&#x27;, userName=&#x27;null&#x27;, password=&#x27;123456&#x27;, name=&#x27;大神&#x27;, age=20, sex=2, birthday=&#x27;2018-07-01&#x27;, created=&#x27;2018-07-01 13:36:09.0&#x27;, updated=&#x27;2018-07-01 13:36:09.0&#x27;&#125; 开启驼峰匹配： 12342018-07-01 13:58:40,599 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Preparing: select * from tb_user where id = ? 2018-07-01 13:58:40,642 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Parameters: 1(String)2018-07-01 13:58:40,661 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] &lt;== Total: 1User&#123;id=&#x27;1&#x27;, userName=&#x27;bigGod222&#x27;, password=&#x27;123456&#x27;, name=&#x27;大神&#x27;, age=20, sex=2, birthday=&#x27;2018-07-01&#x27;, created=&#x27;2018-07-01 13:36:09.0&#x27;, updated=&#x27;2018-07-01 13:36:09.0&#x27;&#125; 8.3.typeAliases类型别名是为 Java 类型命名的一个短的名字。它只和 XML 配置有关，存在的意义仅在于用来减少类完全限定名的冗余。 123&lt;typeAliases&gt; &lt;typeAlias type=&quot;com.zpc.mybatis.pojo.User&quot; alias=&quot;User&quot;/&gt;&lt;/typeAliases&gt; 缺点：每个pojo类都要去配置。解决方案：使用扫描包，扫描指定包下的所有类，扫描之后的别名就是类名（不区分大小写），建议使用的时候和类名一致。 12345&lt;typeAliases&gt; &lt;!--type:实体类的全路径。alias:别名，通常首字母大写--&gt; &lt;!--&lt;typeAlias type=&quot;com.zpc.mybatis.pojo.User&quot; alias=&quot;User&quot;/&gt;--&gt; &lt;package name=&quot;com.zpc.mybatis.pojo&quot;/&gt;&lt;/typeAliases&gt; Mybatis已经为普通的 Java 类型内建了许多相应的类型别名。它们都是大小写不敏感的. 8.4.typeHandlers（类型处理器）无论是 MyBatis 在预处理语句（PreparedStatement）中设置一个参数时，还是从结果集中取出一个值时， 都会用类型处理器将获取的值以合适的方式转换成 Java 类型。可以重写类型处理器或创建你自己的类型处理器来处理不支持的或非标准的类型。 https://www.cnblogs.com/hellowhy/p/9670635.html 使用：第一种，继承BaseTypeHandler类就可以了第二种，实现TypeHandler接口并加上MappedTypes（必须）注解指明对应的jdbcType https://blog.csdn.net/jokemqc/article/details/81326109 8.5.plugins（插件）拦截器MyBatis 允许你在已映射语句执行过程中的某一点进行拦截调用。默认情况下，MyBatis 允许使用插件来拦截的方法调用包括： Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) ParameterHandler (getParameterObject, setParameters) ResultSetHandler (handleResultSets, handleOutputParameters) StatementHandler (prepare, parameterize, batch, update, query) 现在一些MyBatis 插件比如PageHelper都是基于这个原理，有时为了监控sql执行效率，也可以使用插件机制原理： 自定义拦截器： 123456789101112131415// ExamplePlugin.java@Intercepts(&#123;@Signature( type= Executor.class, method = &quot;update&quot;, args = &#123;MappedStatement.class,Object.class&#125;)&#125;)public class ExamplePlugin implements Interceptor &#123; public Object intercept(Invocation invocation) throws Throwable &#123; return invocation.proceed(); &#125; public Object plugin(Object target) &#123; return Plugin.wrap(target, this); &#125; public void setProperties(Properties properties) &#123; &#125;&#125; 配置： 123456&lt;!-- mybatis-config.xml --&gt;&lt;plugins&gt; &lt;plugin interceptor=&quot;org.mybatis.example.ExamplePlugin&quot;&gt; &lt;property name=&quot;someProperty&quot; value=&quot;100&quot;/&gt; &lt;/plugin&gt;&lt;/plugins&gt; 上面的插件将会拦截在 Executor 实例中所有的 “update” 方法调用， 这里的 Executor 是负责执行低层映射语句的内部对象。 8.6.environments(环境)MyBatis 可以配置成适应多种环境，例如，开发、测试和生产环境需要有不同的配置；尽管可以配置多个环境，每个 SqlSessionFactory 实例只能选择其一。虽然，这种方式也可以做到很方便的分离多个环境，但是实际使用场景下，我们更多的是选择使用spring来管理数据源，来做到环境的分离。 8.7.mappers需要告诉 MyBatis 到哪里去找到 SQL 映射语句。即告诉 MyBatis 到哪里去找映射文件。你可以使用相对于类路径的资源引用， 或完全限定资源定位符（包括 file:&#x2F;&#x2F;&#x2F; 的 URL），或类名和包名等。例如： 12345678910111213&lt;!-- 使用相对于类路径的资源引用 --&gt;&lt;mappers&gt; &lt;mapper resource=&quot;org/mybatis/builder/AuthorMapper.xml&quot;/&gt; &lt;mapper resource=&quot;org/mybatis/builder/BlogMapper.xml&quot;/&gt; &lt;mapper resource=&quot;org/mybatis/builder/PostMapper.xml&quot;/&gt;&lt;/mappers&gt;&lt;!-- 使用映射器接口实现类的完全限定类名 --&gt;&lt;mappers&gt; &lt;mapper class=&quot;org.mybatis.builder.AuthorMapper&quot;/&gt; &lt;mapper class=&quot;org.mybatis.builder.BlogMapper&quot;/&gt; &lt;mapper class=&quot;org.mybatis.builder.PostMapper&quot;/&gt;&lt;/mappers&gt; 这里所谓的mapper接口路径。实际上就是dao的接口路径。在mybatis中，通常把dao的包叫做mapper。类名，也叫做mapper 1、定义一个接口。 2、在接口所在的包中定义mapper.xml，并且要求xml文件和interface的名称要相同。 3、在mybatis-config.xml 中通过class路径，引入mapper（注解方式）。要求mapper.xml 中的名称空间是类的接口的全路径。 注解方式： 123456&lt;mappers&gt; &lt;mapper resource=&quot;mappers/MyMapper.xml&quot;/&gt; &lt;mapper resource=&quot;mappers/UserDaoMapper.xml&quot;/&gt; &lt;!--注解方式可以使用如下配置方式--&gt; &lt;mapper class=&quot;com.zpc.mybatis.dao.UserMapper&quot;/&gt;&lt;/mappers&gt; 问题： 1、mapper.xml 和 java文件没有分离。 之后的教程讲述和spring整合之后解决。 2、需要一个一个的去加载mapper。 当然也可以使用包扫描（必须使用注解方式，即在接口方法上使用注解，如@Select(“select * from tb_user “)）：_缺点_： 1、如果包的路径有很多？ 2、mapper.xml和mapper.java没有分离。 spring整合的时候解决。 9.Mapper XML文件详解9.1.CRUD标签9.1.1.selectselect – 书写查询sql语句select中的几个属性说明：id属性：当前名称空间下的statement的唯一标识。必须。要求id和mapper接口中的方法的名字一致。resultType：将结果集映射为java的对象类型。必须（和 resultMap 二选一）parameterType：传入参数类型。可以省略 9.1.2.insertinsert 的几个属性说明：id：唯一标识，随便写，在同一个命名空间下保持唯一，使用动态代理之后要求和方法名保持一致parameterType：参数的类型，使用动态代理之后和方法的参数类型一致useGeneratedKeys:开启主键回写keyColumn：指定数据库的主键keyProperty：主键对应的pojo属性名标签内部：具体的sql语句。 9.1.3.updateid属性：当前名称空间下的statement的唯一标识(必须属性)；parameterType：传入的参数类型，可以省略。标签内部：具体的sql语句。 9.1.4.deletedelete 的几个属性说明：id属性：当前名称空间下的statement的唯一标识(必须属性)；parameterType：传入的参数类型，可以省略。标签内部：具体的sql语句。 9.2.#{}和${}场景：数据库有两个一模一样的表。历史表，当前表查询表中的信息，有时候从历史表中去查询数据，有时候需要去新的表去查询数据。希望使用1个方法来完成操作。 1234567891011&lt;select id=&quot;queryUserByTableName&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from #&#123;tableName&#125;&lt;/select&gt;/** * 根据表名查询用户信息（直接使用注解指定传入参数名称） * * @param tableName * @return */public List&lt;User&gt; queryUserByTableName(String tableName); 测试输出： 有问题,报语法错误：相当于执行了这样一条sql:select * from “tb_user”;显然表名多了引号。 改正： 123&lt;select id=&quot;queryUserByTableName&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from $&#123;tableName&#125;&lt;/select&gt; 注意：#&#123;&#125; 只是替换？，相当于PreparedStatement使用占位符去替换参数，可以防止sql注入。$&#123;&#125; 是进行字符串拼接，相当于sql语句中的Statement，使用字符串去拼接sql；$可以是sql中的任一部分传入到Statement中，不能防止sql注入。 使用$&#123;&#125; 去取出参数值信息，需要使用$&#123;value&#125;#&#123;&#125; 只是表示占位，与参数的名字无关，如果只有一个参数，会自动对应。 推荐： 1234567891011/** * 根据表名查询用户信息（直接使用注解指定传入参数名称） * * @param tableName * @return */public List&lt;User&gt; queryUserByTableName(@Param(&quot;tableName&quot;) String tableName);&lt;select id=&quot;queryUserByTableName&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from $&#123;tableName&#125;&lt;/select&gt; #&#123;&#125;多个参数时： 123456789101112/** * 登录（直接使用注解指定传入参数名称） * * @param userName * @param password * @return */public User login( String userName, String password);&lt;select id=&quot;login&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user where user_name = #&#123;userName&#125; and password = #&#123;password&#125;&lt;/select&gt; 报错： 123org.apache.ibatis.exceptions.PersistenceException: Error querying database. Cause: org.apache.ibatis.binding.BindingException: Parameter &#x27;userName&#x27; not found. Available parameters are [0, 1, param1, param2]Cause: org.apache.ibatis.binding.BindingException: Parameter &#x27;userName&#x27; not found. Available parameters are [0, 1, param1, param2] 解决方案一： 123&lt;select id=&quot;login&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user where user_name = #&#123;0&#125; and password = #&#123;1&#125;&lt;/select&gt; 解决方案二： 123&lt;select id=&quot;login&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user where user_name = #&#123;param1&#125; and password = #&#123;param2&#125;&lt;/select&gt; 最终解决方案： 123456789101112/** * 登录（直接使用注解指定传入参数名称） * * @param userName * @param password * @return */public User login(@Param(&quot;userName&quot;) String userName, @Param(&quot;password&quot;) String password);&lt;select id=&quot;login&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user where user_name = #&#123;userName&#125; and password = #&#123;password&#125;&lt;/select&gt; 通常在方法的参数列表上加上一个注释@Param(“xxxx”) 显式指定参数的名字，然后通${“xxxx”}或#{“xxxx”}sql语句动态生成的时候，使用${};sql语句中某个参数进行占位的时候#{} 9.3.面试题（#、$区别）123456789101112131415161718192021/** * #号 * @param username1 * @return */User queryUserListByName1(@Param(&quot;username1&quot;) String username1);/** * $号 * @param username2 * @return */User queryUserListByName2(@Param(&quot;username2&quot;) String username2);&lt;select id=&quot;queryUserListByName1&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user WHERE user_name=#&#123;username1&#125;&lt;/select&gt;&lt;select id=&quot;queryUserListByName2&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user WHERE user_name=&#x27;$&#123;username2&#125;&#x27;//手动加了引号&lt;/select&gt; 9.4.resultMap 使用： 9.5.sql片段12&lt;sql id=””&gt;&lt;/sql&gt;&lt;include refId=”” /&gt; 例如在UserMapper.xml中定义如下片段： 1234567891011&lt;sql id=&quot;commonSql&quot;&gt; id, user_name, password, name, age, sex, birthday, created, updated &lt;/sql&gt; 则可以在UserMapper.xml中使用它： 1234567&lt;select id=&quot;queryUserById&quot; resultMap=&quot;userResultMap&quot;&gt; select &lt;include refid=&quot;commonSql&quot;&gt;&lt;/include&gt; from tb_user where id = #&#123;id&#125;&lt;/select&gt;&lt;select id=&quot;queryUsersLikeUserName&quot; resultType=&quot;User&quot;&gt; select &lt;include refid=&quot;commonSql&quot;&gt;&lt;/include&gt; from tb_user where user_name like &quot;%&quot;#&#123;userName&#125;&quot;%&quot;&lt;/select&gt; Sql片段也可以定义在单独的.xml文件中如：定义CommonSQL.xml： 1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;CommonSQL&quot;&gt; &lt;sql id=&quot;commonSql&quot;&gt; id, user_name, password, name, age, sex, birthday, created, updated &lt;/sql&gt;&lt;/mapper&gt; 使用： 1234567&lt;select id=&quot;queryUserById&quot; resultMap=&quot;userResultMap&quot;&gt; select &lt;include refid=&quot;CommonSQL.commonSql&quot;&gt;&lt;/include&gt; from tb_user where id = #&#123;id&#125;&lt;/select&gt;&lt;select id=&quot;queryUsersLikeUserName&quot; resultType=&quot;User&quot;&gt; select &lt;include refid=&quot;CommonSQL.commonSql&quot;&gt;&lt;/include&gt; from tb_user where user_name like &quot;%&quot;#&#123;userName&#125;&quot;%&quot;&lt;/select&gt; 当然要完成这个功能还需要在全局配置文件mybatis-config.xml中引入该外部配置文件： 12345&lt;mappers&gt; &lt;mapper resource=&quot;CommonSQL.xml&quot;/&gt; &lt;!-- 开启mapper接口的包扫描，基于class的配置方式 --&gt; &lt;package name=&quot;com.zpc.mybatis.mapper&quot;/&gt;&lt;/mappers&gt; 10.动态sql场景：查询男性用户，如果输入了姓名，按姓名模糊查询 10.1.if场景：查询男性用户，如果输入了姓名，则按姓名查询 定义接口： 123456/** * 查询男性用户，如果输入了姓名，则按姓名查询 * @param name * @return */List&lt;User&gt; queryUserList(@Param(&quot;name&quot;) String name); 编写mapper 123456&lt;select id=&quot;queryUserList&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user WHERE sex=1 &lt;if test=&quot;name!=null and name.trim()!=&#x27;&#x27;&quot;&gt; and name like &#x27;%$&#123;name&#125;%&#x27; &lt;/if&gt;&lt;/select&gt; 测试 1234567@Testpublic void testqueryUserList() &#123; List&lt;User&gt; users = this.userMapper.queryUserList(null); for (User user : users) &#123; System.out.println(user); &#125;&#125; 10.2.choose when otherwise场景：查询男性用户，如果输入了姓名则按照姓名模糊查找，否则如果输入了年龄则按照年龄查找，否则查找姓名为“鹏程”的用户。 定义接口： 1234567/** * 查询男性用户，如果输入了姓名则按照姓名模糊查找，否则如果输入了年龄则按照年龄查找，否则查找姓名为“鹏程”的用户。 * @param name * @param age * @return */List&lt;User&gt; queryUserListByNameOrAge(@Param(&quot;name&quot;) String name,@Param(&quot;age&quot;) Integer age); 编写mapper配置： 123456789101112131415161718&lt;select id=&quot;queryUserListByNameOrAge&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user WHERE sex=1 &lt;!-- 1.一旦有条件成立的when，后续的when则不会执行 2.当所有的when都不执行时,才会执行otherwise --&gt; &lt;choose&gt; &lt;when test=&quot;name!=null and name.trim()!=&#x27;&#x27;&quot;&gt; and name like &#x27;%$&#123;name&#125;%&#x27; &lt;/when&gt; &lt;when test=&quot;age!=null&quot;&gt; and age = #&#123;age&#125; &lt;/when&gt; &lt;otherwise&gt; and name=&#x27;鹏程&#x27; &lt;/otherwise&gt; &lt;/choose&gt;&lt;/select&gt; 测试： 1234567@Testpublic void queryUserListByNameOrAge() throws Exception &#123; List&lt;User&gt; users = this.userMapper.queryUserListByNameOrAge(null, 16); for (User user : users) &#123; System.out.println(user); &#125;&#125; 10.3.where 和set场景一：查询所有用户，如果输入了姓名按照姓名进行模糊查询，如果输入年龄，按照年龄进行查询，如果两者都输入，两个条件都要成立。 接口： 1234567/** * 查询所有用户，如果输入了姓名按照姓名进行模糊查询，如果输入年龄，按照年龄进行查询，如果两者都输入，两个条件都要成立 * @param name * @param age * @return */List&lt;User&gt; queryUserListByNameAndAge(@Param(&quot;name&quot;) String name,@Param(&quot;age&quot;) Integer age); 配置： 123456789101112&lt;select id=&quot;queryUserListByNameAndAge&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user &lt;!--如果多出一个and，会自动去除，如果缺少and或者多出多个and则会报错--&gt; &lt;where&gt; &lt;if test=&quot;name!=null and name.trim()!=&#x27;&#x27;&quot;&gt; and name like &#x27;%$&#123;name&#125;%&#x27; &lt;/if&gt; &lt;if test=&quot;age!=null&quot;&gt; and age = #&#123;age&#125; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 测试： 1234567@Testpublic void queryUserListByNameAndAge() throws Exception &#123; List&lt;User&gt; users = this.userMapper.queryUserListByNameAndAge(&quot;鹏程&quot;, 20); for (User user : users) &#123; System.out.println(user); &#125;&#125; 场景二：修改用户信息，如果参数user中的某个属性为null，则不修改。接口： 123456/** * 根据id更新用户信息 * * @param user */public void updateUser(User user); 配置： 1234567891011121314&lt;update id=&quot;updateUser&quot; parameterType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; UPDATE tb_user &lt;trim prefix=&quot;set&quot; suffixOverrides=&quot;,&quot;&gt; &lt;if test=&quot;userName!=null&quot;&gt;user_name = #&#123;userName&#125;,&lt;/if&gt; &lt;if test=&quot;password!=null&quot;&gt;password = #&#123;password&#125;,&lt;/if&gt; &lt;if test=&quot;name!=null&quot;&gt;name = #&#123;name&#125;,&lt;/if&gt; &lt;if test=&quot;age!=null&quot;&gt;age = #&#123;age&#125;,&lt;/if&gt; &lt;if test=&quot;sex!=null&quot;&gt;sex = #&#123;sex&#125;,&lt;/if&gt; &lt;if test=&quot;birthday!=null&quot;&gt;birthday = #&#123;birthday&#125;,&lt;/if&gt; updated = now(), &lt;/trim&gt; WHERE (id = #&#123;id&#125;);&lt;/update&gt; 测试： 1234567891011@Testpublic void testUpdateUser() &#123; User user = new User(); user.setBirthday(new Date()); user.setName(&quot;静静&quot;); user.setPassword(&quot;123456&quot;); user.setSex(0); user.setUserName(&quot;Jinjin&quot;); user.setId(&quot;1&quot;); this.userMapper.updateUser(user);&#125; 10.4.foreach场景：按照多个id查询用户信息 接口： 123456/** * 按多个Id查询 * @param ids * @return */List&lt;User&gt; queryUserListByIds(@Param(&quot;ids&quot;) String[] ids); 配置： 123456&lt;select id=&quot;queryUserListByIds&quot; resultType=&quot;com.zpc.mybatis.pojo.User&quot;&gt; select * from tb_user where id in &lt;foreach collection=&quot;ids&quot; item=&quot;id&quot; open=&quot;(&quot; close=&quot;)&quot; separator=&quot;,&quot;&gt; #&#123;id&#125; &lt;/foreach&gt;&lt;/select&gt; 测试： 1234567@Testpublic void queryUserListByIds() throws Exception &#123; List&lt;User&gt; users = this.userMapper.queryUserListByIds(new String[]&#123;&quot;1&quot;,&quot;2&quot;&#125;); for (User user : users) &#123; System.out.println(user); &#125;&#125; If：testognl表达式或者简单java代码Choose when otherwise—&gt;相当于if else if elseWhen test参考ifWhere set 都有一定的纠错功能Trim：prefix suffix prefixOverrides suffixOverridesForeach：collection item saparator open close 11.缓存11.1.一级缓存 在mybatis中，一级缓存默认是开启的，并且一直无法关闭 一级缓存满足条件： 1、同一个session中 2、相同的SQL和参数 测试： 12345@Testpublic void testQueryUserById() &#123; System.out.println(this.userMapper.queryUserById(&quot;1&quot;)); System.out.println(this.userMapper.queryUserById(&quot;1&quot;));&#125; 12345672018-07-01 17:08:50,156 [main] [org.apache.ibatis.transaction.jdbc.JdbcTransaction]-[DEBUG] Opening JDBC Connection2018-07-01 17:08:50,421 [main] [org.apache.ibatis.datasource.pooled.PooledDataSource]-[DEBUG] Created connection 242355057.2018-07-01 17:08:50,423 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Preparing: select * from tb_user where id = ? 2018-07-01 17:08:50,476 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Parameters: 1(String)2018-07-01 17:08:50,509 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] &lt;== Total: 1User&#123;id=&#x27;1&#x27;, userName=&#x27;bigGod222&#x27;, password=&#x27;123456&#x27;, name=&#x27;鹏程&#x27;, age=20, sex=1, birthday=&#x27;2018-07-01&#x27;, created=&#x27;2018-07-01 13:35:40.0&#x27;, updated=&#x27;2018-07-01 13:35:40.0&#x27;&#125;User&#123;id=&#x27;1&#x27;, userName=&#x27;bigGod222&#x27;, password=&#x27;123456&#x27;, name=&#x27;鹏程&#x27;, age=20, sex=1, birthday=&#x27;2018-07-01&#x27;, created=&#x27;2018-07-01 13:35:40.0&#x27;, updated=&#x27;2018-07-01 13:35:40.0&#x27;&#125; 使用：sqlSession.clearCache();可以强制清除缓存 测试： 123456@Testpublic void testQueryUserById() &#123; System.out.println(this.userMapper.queryUserById(&quot;1&quot;)); sqlSession.clearCache(); System.out.println(this.userMapper.queryUserById(&quot;1&quot;));&#125; 日志： 123456789102018-07-01 17:10:51,065 [main] [org.apache.ibatis.transaction.jdbc.JdbcTransaction]-[DEBUG] Opening JDBC Connection2018-07-01 17:10:51,359 [main] [org.apache.ibatis.datasource.pooled.PooledDataSource]-[DEBUG] Created connection 242355057.2018-07-01 17:10:51,360 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Preparing: select * from tb_user where id = ? 2018-07-01 17:10:51,408 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Parameters: 1(String)2018-07-01 17:10:51,437 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] &lt;== Total: 1User&#123;id=&#x27;1&#x27;, userName=&#x27;bigGod222&#x27;, password=&#x27;123456&#x27;, name=&#x27;鹏程&#x27;, age=20, sex=1, birthday=&#x27;2018-07-01&#x27;, created=&#x27;2018-07-01 13:35:40.0&#x27;, updated=&#x27;2018-07-01 13:35:40.0&#x27;&#125;2018-07-01 17:10:51,438 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Preparing: select * from tb_user where id = ? 2018-07-01 17:10:51,438 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Parameters: 1(String)2018-07-01 17:10:51,440 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] &lt;== Total: 1User&#123;id=&#x27;1&#x27;, userName=&#x27;bigGod222&#x27;, password=&#x27;123456&#x27;, name=&#x27;鹏程&#x27;, age=20, sex=1, birthday=&#x27;2018-07-01&#x27;, created=&#x27;2018-07-01 13:35:40.0&#x27;, updated=&#x27;2018-07-01 13:35:40.0&#x27;&#125; 执行update、insert、delete的时候，会清空缓存测试： 123456789101112@Testpublic void testQueryUserById() &#123; System.out.println(this.userMapper.queryUserById(&quot;1&quot;)); //sqlSession.clearCache(); User user=new User(); user.setName(&quot;美女&quot;); user.setId(&quot;1&quot;); userMapper.updateUser(user); System.out.println(this.userMapper.queryUserById(&quot;1&quot;));&#125; 日志： 123456789101112132018-07-01 17:18:15,128 [main] [org.apache.ibatis.transaction.jdbc.JdbcTransaction]-[DEBUG] Opening JDBC Connection2018-07-01 17:18:15,399 [main] [org.apache.ibatis.datasource.pooled.PooledDataSource]-[DEBUG] Created connection 242355057.2018-07-01 17:18:15,401 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Preparing: select * from tb_user where id = ? 2018-07-01 17:18:15,466 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Parameters: 1(String)2018-07-01 17:18:15,492 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] &lt;== Total: 1User&#123;id=&#x27;1&#x27;, userName=&#x27;bigGod222&#x27;, password=&#x27;123456&#x27;, name=&#x27;鹏程&#x27;, age=20, sex=1, birthday=&#x27;2018-07-01&#x27;, created=&#x27;2018-07-01 13:35:40.0&#x27;, updated=&#x27;2018-07-01 13:35:40.0&#x27;&#125;2018-07-01 17:18:15,527 [main] [com.zpc.mybatis.dao.UserMapper.updateUser]-[DEBUG] ==&gt; Preparing: UPDATE tb_user set name = ?, updated = now() WHERE (id = ?); 2018-07-01 17:18:15,529 [main] [com.zpc.mybatis.dao.UserMapper.updateUser]-[DEBUG] ==&gt; Parameters: 美女(String), 1(String)2018-07-01 17:18:15,532 [main] [com.zpc.mybatis.dao.UserMapper.updateUser]-[DEBUG] &lt;== Updates: 12018-07-01 17:18:15,532 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Preparing: select * from tb_user where id = ? 2018-07-01 17:18:15,533 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Parameters: 1(String)2018-07-01 17:18:15,538 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] &lt;== Total: 1User&#123;id=&#x27;1&#x27;, userName=&#x27;bigGod222&#x27;, password=&#x27;123456&#x27;, name=&#x27;美女&#x27;, age=20, sex=1, birthday=&#x27;2018-07-01&#x27;, created=&#x27;2018-07-01 13:35:40.0&#x27;, updated=&#x27;2018-07-01 17:18:15.0&#x27;&#125; 11.2.二级缓存mybatis 的二级缓存的作用域是一个mapper的namespace ，同一个namespace中查询sql可以从缓存中命中。 开启二级缓存： 123&lt;mapper namespace=&quot;com.zpc.mybatis.dao.UserMapper&quot;&gt; &lt;cache/&gt;&lt;/mapper&gt; 测试： 12345678910@Testpublic void testCache() &#123; System.out.println(this.userMapper.queryUserById(&quot;1&quot;)); sqlSession.close(); SqlSession sqlSession = sqlSessionFactory.openSession(); UserMapper mapper = sqlSession.getMapper(UserMapper.class); System.out.println(mapper.queryUserById(&quot;1&quot;));&#125; 开启二级缓存，必须序列化： 12public class User implements Serializable&#123; private static final long serialVersionUID = -3330851033429007657L; 日志： 123456789102018-07-01 17:23:39,335 [main] [org.apache.ibatis.transaction.jdbc.JdbcTransaction]-[DEBUG] Opening JDBC Connection2018-07-01 17:23:39,664 [main] [org.apache.ibatis.datasource.pooled.PooledDataSource]-[DEBUG] Created connection 2092769598.2018-07-01 17:23:39,665 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Preparing: select * from tb_user where id = ? 2018-07-01 17:23:39,712 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] ==&gt; Parameters: 1(String)2018-07-01 17:23:39,734 [main] [com.zpc.mybatis.dao.UserMapper.queryUserById]-[DEBUG] &lt;== Total: 1User&#123;id=&#x27;1&#x27;, userName=&#x27;bigGod222&#x27;, password=&#x27;123456&#x27;, name=&#x27;美女&#x27;, age=20, sex=1, birthday=&#x27;2018-07-01&#x27;, created=&#x27;2018-07-01 13:35:40.0&#x27;, updated=&#x27;2018-07-01 17:18:15.0&#x27;&#125;2018-07-01 17:23:39,743 [main] [org.apache.ibatis.transaction.jdbc.JdbcTransaction]-[DEBUG] Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@7cbd213e]2018-07-01 17:23:39,744 [main] [org.apache.ibatis.datasource.pooled.PooledDataSource]-[DEBUG] Returned connection 2092769598 to pool.2018-07-01 17:23:39,746 [main] [com.zpc.mybatis.dao.UserMapper]-[DEBUG] Cache Hit Ratio [com.zpc.mybatis.dao.UserMapper]: 0.5User&#123;id=&#x27;1&#x27;, userName=&#x27;bigGod222&#x27;, password=&#x27;123456&#x27;, name=&#x27;美女&#x27;, age=20, sex=1, birthday=&#x27;2018-07-01&#x27;, created=&#x27;2018-07-01 13:35:40.0&#x27;, updated=&#x27;2018-07-01 17:18:15.0&#x27;&#125; 关闭二级缓存：不开启，或者在全局的mybatis-config.xml 中去关闭二级缓存 123456&lt;settings&gt; &lt;!--开启驼峰匹配--&gt; &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt; &lt;!--开启二级缓存,全局总开关，这里关闭，mapper中开启了也没用--&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;false&quot;/&gt;&lt;/settings&gt; 12.高级查询12.1.表关系说明 创建order表： 12345678CREATE TABLE tb_order (id int(11) NOT NULL AUTO_INCREMENT,user_id int(11) DEFAULT NULL,order_number varchar(255) DEFAULT NULL,create datetime DEFAULT NULL,updated datetime DEFAULT NULL,PRIMARY KEY (id)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8; 1234567public class Order &#123; private Integer id; private Long userId; private String orderNumber; private Date created; private Date updated;&#125; 需求说明： 12.2.一对一查询 方法一：核心思想扩展Order对象，来完成映射 新建OrderUser实体类继承Order： 12345678910public class OrderUser extends Order &#123; private String userName; private String password; private String name; private Integer age; private Integer sex; private Date birthday; private Date created; private Date updated;&#125; OrderMapper接口： 123public interface OrderMapper &#123; OrderUser queryOrderUserByOrderNumber(@Param(&quot;number&quot;) String number);&#125; 配置OrderMapper： 12345 &lt;mapper namespace=&quot;com.zpc.mybatis.dao.OrderMapper&quot;&gt; &lt;select id=&quot;queryOrderUserByOrderNumber&quot; resultType=&quot;com.zpc.mybatis.pojo.OrderUser&quot;&gt; select * from tb_order o left join tb_user u on o.user_id=u.id where o.order_number = #&#123;number&#125; &lt;/select&gt;&lt;/mapper&gt; 测试： 12345@Testpublic void queryOrderUserByOrderNumber() throws Exception &#123; OrderUser orderUser = orderMapper.queryOrderUserByOrderNumber(&quot;201807010001&quot;); System.out.println(orderUser);&#125; 方法二：面向对象的思想，在Order对象中添加User对象。 在Order对象中添加User属性： 12345678public class Order &#123; private Integer id; private Long userId; private String orderNumber; private Date created; private Date updated; private User user;&#125; 接口： 123456/** * 根据订单号查询订单用户的信息 * @param number * @return */Order queryOrderWithUserByOrderNumber(@Param(&quot;number&quot;) String number); 使用resultType不能完成自动映射，需要手动完成结果集映射resultMap： 1234567891011121314 &lt;resultMap id=&quot;OrderUserResultMap&quot; type=&quot;com.zpc.mybatis.pojo.Order&quot; autoMapping=&quot;true&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;!--association:完成子对象的映射--&gt; &lt;!--property:子对象在父对象中的属性名--&gt; &lt;!--javaType:子对象的java类型--&gt; &lt;!--autoMapping:完成子对象的自动映射，若开启驼峰，则按驼峰匹配--&gt; &lt;association property=&quot;user&quot; javaType=&quot;com.zpc.mybatis.pojo.User&quot; autoMapping=&quot;true&quot;&gt; &lt;id column=&quot;user_id&quot; property=&quot;id&quot;/&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;select id=&quot;queryOrderWithUserByOrderNumber&quot; resultMap=&quot;OrderUserResultMap&quot;&gt; select * from tb_order o left join tb_user u on o.user_id=u.id where o.order_number = #&#123;number&#125;&lt;/select&gt; 测试： 12345@Testpublic void queryOrderWithUserByOrderNumber() throws Exception &#123; Order order = orderMapper.queryOrderWithUserByOrderNumber(&quot;201807010001&quot;); System.out.println(order.getUser());&#125; 12.3.一对多查询一对多查询：查询订单，查询出下单人信息并且查询出订单详情。 Order类： 123456789public class Order &#123; private Integer id; private Long userId; private String orderNumber; private Date created; private Date updated; private User user; private List&lt;OrderDetail&gt; detailList;&#125; 123456public class OrderDetail &#123; private Integer id; private Integer orderId; private Double totalPrice; private Integer status;&#125; 接口： 123456/** * 根据订单号查询订单用户的信息及订单详情 * @param number * @return */Order queryOrderWithUserAndDetailByOrderNumber(@Param(&quot;number&quot;) String number); Mapper映射： 123456789101112131415161718192021&lt;resultMap id=&quot;OrderUserDetailResultMap&quot; type=&quot;com.zpc.mybatis.pojo.Order&quot; autoMapping=&quot;true&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;!--collection:定义子对象集合映射--&gt; &lt;!--association:完成子对象的映射--&gt; &lt;!--property:子对象在父对象中的属性名--&gt; &lt;!--javaType:子对象的java类型--&gt; &lt;!--autoMapping:完成子对象的自动映射，若开启驼峰，则按驼峰匹配--&gt; &lt;association property=&quot;user&quot; javaType=&quot;com.zpc.mybatis.pojo.User&quot; autoMapping=&quot;true&quot;&gt; &lt;id column=&quot;user_id&quot; property=&quot;id&quot;/&gt; &lt;/association&gt; &lt;collection property=&quot;detailList&quot; javaType=&quot;List&quot; ofType=&quot;com.zpc.mybatis.pojo.OrderDetail&quot; autoMapping=&quot;true&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;/collection&gt;&lt;/resultMap&gt; &lt;select id=&quot;queryOrderWithUserAndDetailByOrderNumber&quot; resultMap=&quot;OrderUserDetailResultMap&quot;&gt; select * from tb_order o left join tb_user u on o.user_id=u.id left join tb_orderdetail od on o.id=od.order_id where o.order_number = #&#123;number&#125;&lt;/select&gt; 测试： 123456@Testpublic void queryOrderWithUserAndDetailByOrderNumber() throws Exception &#123; Order order = orderMapper.queryOrderWithUserAndDetailByOrderNumber(&quot;201807010001&quot;); System.out.println(order.getUser()); System.out.println(order.getDetailList());&#125; 12.4.多对多查询多对多查询：查询订单，查询出下单人信息并且查询出订单详情中的商品数据。 OrderDetail类 1234567891011121314public class OrderDetail &#123; private Integer id; private Integer orderId; private Double totalPrice; private Integer status; private Item item;&#125;public class Item &#123; private Integer id; private String itemName; private Float itemPrice; private String itemDetail;&#125; 接口： 123456/** * 根据订单号查询订单用户的信息及订单详情及订单详情对应的商品信息 * @param number * @return */Order queryOrderWithUserAndDetailItemByOrderNumber(@Param(&quot;number&quot;) String number); Mapper配置： 1234567891011121314151617181920&lt;resultMap id=&quot;OrderUserDetailItemResultMap&quot; type=&quot;com.zpc.mybatis.pojo.Order&quot; autoMapping=&quot;true&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;association property=&quot;user&quot; javaType=&quot;com.zpc.mybatis.pojo.User&quot; autoMapping=&quot;true&quot;&gt; &lt;id column=&quot;user_id&quot; property=&quot;id&quot;/&gt; &lt;/association&gt; &lt;collection property=&quot;detailList&quot; javaType=&quot;List&quot; ofType=&quot;com.zpc.mybatis.pojo.OrderDetail&quot; autoMapping=&quot;true&quot;&gt; &lt;id column=&quot;detail_id&quot; property=&quot;id&quot;/&gt; &lt;association property=&quot;item&quot; javaType=&quot;com.zpc.mybatis.pojo.Item&quot; autoMapping=&quot;true&quot;&gt; &lt;id column=&quot;item_id&quot; property=&quot;id&quot;/&gt; &lt;/association&gt; &lt;/collection&gt;&lt;/resultMap&gt; &lt;select id=&quot;queryOrderWithUserAndDetailItemByOrderNumber&quot; resultMap=&quot;OrderUserDetailItemResultMap&quot;&gt; select * ,od.id as detail_id from tb_order o left join tb_user u on o.user_id=u.id left join tb_orderdetail od on o.id=od.order_id left join tb_item i on od.item_id=i.id where o.order_number = #&#123;number&#125;&lt;/select&gt; 测试： 1234567@Testpublic void queryOrderWithUserAndDetailItemByOrderNumber() throws Exception &#123; Order order = orderMapper.queryOrderWithUserAndDetailItemByOrderNumber(&quot;201807010001&quot;); System.out.println(order); System.out.println(order.getUser()); System.out.println(order.getDetailList());&#125; 至此，目录结构如下： 数据库脚本： 123456789101112131415161718192021222324252627282930CREATE TABLE tb_order (id int(11) NOT NULL AUTO_INCREMENT,user_id int(11) DEFAULT NULL,order_number varchar(255) DEFAULT NULL,create datetime DEFAULT NULL,updated datetime DEFAULT NULL,PRIMARY KEY (id)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;INSERT INTO tb_order VALUES (‘1’, ‘2’, ‘201807010001’, ‘2018-07-01 19:38:35’, ‘2018-07-01 19:38:40’);CREATE TABLE tb_item (id int(11) NOT NULL,itemName varchar(255) DEFAULT NULL,itemPrice decimal(10,2) DEFAULT NULL,itemDetail varchar(255) DEFAULT NULL,PRIMARY KEY (id)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO tb_item VALUES (‘1’, ‘袜子’, ‘29.90’, ‘香香的袜子’);INSERT INTO tb_item VALUES (‘2’, ‘套子’, ‘99.99’, ‘冈本001’);CREATE TABLE tb_orderdetail (id int(11) NOT NULL AUTO_INCREMENT,order_id int(11) DEFAULT NULL,total_price decimal(10,0) DEFAULT NULL,item_id int(11) DEFAULT NULL,status int(10) unsigned zerofill DEFAULT NULL COMMENT ‘0成功非0失败’,PRIMARY KEY (id)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;INSERT INTO tb_orderdetail VALUES (‘1’, ‘1’, ‘10000’, ‘1’, ‘0000000001’);INSERT INTO tb_orderdetail VALUES (‘2’, ‘1’, ‘2000’, ‘2’, ‘0000000000’); 12.5.resultMap的继承 12.6.高级查询的整理resutlType无法帮助我们自动的去完成映射，所以只有使用resultMap手动的进行映射type 结果集对应的数据类型 id 唯一标识，被引用的时候，进行指定 123456789&lt;resultMap type=&quot;Order&quot; id=&quot;orderUserLazyResultMap&quot;&gt;&lt;!—定义pojo中的单个对象的 property 定义对象的属性名， javaType 属性的类型， &lt;association property=&quot;user&quot; javaType=&quot;User&quot; autoMapping=&quot;true&quot;&gt; &lt;id /&gt; &lt;/association&gt;&lt;!—如果属性是集合使用collection ,javaType 集合的类型，ofType 表示集中的存储的元素类型 &lt;collection property=&quot;details&quot; javaType=&quot;List&quot; ofType=&quot;OrderDetail&quot; autoMapping=&quot;true&quot;&gt; &lt;id /&gt;&lt;/resultMap&gt; 13.延迟加载 编写接口： Mapper配置： 测试： 结果： 开启延迟加载： 修改测试用例： 执行，报错： 添加cglib： 12345&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt;&lt;/dependency&gt; 执行： 14.如果sql语句中出现’&lt;’的解决方案1、使用xml中的字符实体 因为业务，需要在mybatis中，使用到大于号，小于号，所以就在SQL中直接使用了。SELECT * FROM test WHERE 1 = 1 AND start_date &lt;= CURRENT_DATE AND end_date &gt;= CURRENT_DATE可是，在执行时，总报错误： 1Error creating document instance. Cause: org.xml.sax.SAXParseException; lineNumber: 74; columnNumber: 17; 元素内容必须由格式正确的字符数据或标记组成。把AND start_date &gt;= CURRENT_DATE AND end_date &lt;= CURRENT_DATE去掉，就没有问题，所以确定是因为大于号，小于号引起的问题。 于是就想到了特殊符号，于是用了转义字符把&gt;和&lt;替换掉，然后就没有问题了。SELECT * FROM test WHERE 1 = 1 AND start_date &amp;lt;= CURRENT_DATE AND end_date &amp;gt;= CURRENT_DATE案例： 121.&lt;if test=&quot;startDateTime!=null&quot;&gt; and mm.ttime &amp;gt; to_date(#&#123;startDateTime&#125;,&#x27;yyyy-mm-dd hh24:mi:ss&#x27;)&lt;/if&gt; 2.&lt;if test=&quot;endDateTime!=null&quot;&gt; and mm.ttime &amp;lt;= to_date(#&#123;endDateTime&#125;,&#x27;yyyy-mm-dd hh24:mi:ss&#x27;)&lt;/if&gt; 2、使用&lt;![CDATA[ &lt; ]]&gt;案例1： 12341.&lt;![CDATA[ 2. and mm.ttime &gt; to_date(#&#123;startDateTime&#125;,&#x27;yyyy-mm-dd hh24:mi:ss&#x27;) 3. and mm.ttime &lt;= to_date(#&#123;endDateTime&#125;,&#x27;yyyy-mm-dd hh24:mi:ss&#x27;) 4.]]&gt; 案例2： mapper文件示例代码 ： 123and (t1.status &lt;![CDATA[ &gt;= ]]&gt; 1 and t1.status &lt;![CDATA[ &lt;= ]]&gt; 2)上述代码其实对应的sql：and (t1.status &gt; =1 andt1.status &lt;= 2) 注意： 使用标记的sql语句中的 等标签不会被解析。 15.Spring 集成Mybatis15.1引入spring和Mybatis相关依赖pom.xml 1234567891011121314151617181920212223242526272829&lt;!--数据库连接池--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.1.3.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!--spring集成Junit测试--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;4.1.3.RELEASE&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;!--spring容器--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.1.3.RELEASE&lt;/version&gt;&lt;/dependency&gt; 15.2配置spring配置文件applicationContext-dao.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd&quot;&gt; &lt;!-- 加载配置文件 --&gt; &lt;context:property-placeholder location=&quot;classpath:properties/*.properties&quot;/&gt; &lt;!-- 数据库连接池 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot; destroy-method=&quot;close&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://$&#123;jdbc.host&#125;:3306/$&#123;jdbc.database&#125;?useUnicode=true&amp;amp;characterEncoding=utf-8&amp;amp;zeroDateTimeBehavior=convertToNull&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.userName&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.passWord&#125;&quot;/&gt; &lt;!-- 初始化连接大小 --&gt; &lt;property name=&quot;initialSize&quot; value=&quot;$&#123;jdbc.initialSize&#125;&quot;&gt;&lt;/property&gt; &lt;!-- 连接池最大数据库连接数 0 为没有限制 --&gt; &lt;property name=&quot;maxActive&quot; value=&quot;$&#123;jdbc.maxActive&#125;&quot;&gt;&lt;/property&gt; &lt;!-- 连接池最大的空闲连接数，这里取值为20，表示即使没有数据库连接时依然可以保持20空闲的连接，而不被清除，随时处于待命状态 0 为没有限制 --&gt; &lt;property name=&quot;maxIdle&quot; value=&quot;$&#123;jdbc.maxIdle&#125;&quot;&gt;&lt;/property&gt; &lt;!-- 连接池最小空闲 --&gt; &lt;property name=&quot;minIdle&quot; value=&quot;$&#123;jdbc.minIdle&#125;&quot;&gt;&lt;/property&gt; &lt;!--最大建立连接等待时间。如果超过此时间将接到异常。设为-1表示无限制--&gt; &lt;property name=&quot;maxWait&quot; value=&quot;$&#123;jdbc.maxWait&#125;&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- spring和MyBatis完美整合 --&gt; &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;!-- 自动扫描mapping.xml文件 --&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:mappers/*.xml&quot;&gt;&lt;/property&gt; &lt;!--如果mybatis-config.xml没有特殊配置也可以不需要下面的配置--&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis-config.xml&quot; /&gt; &lt;/bean&gt; &lt;!-- DAO接口所在包名，Spring会自动查找其下的类 --&gt; &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.zpc.mybatis.dao&quot;/&gt; &lt;property name=&quot;sqlSessionFactoryBeanName&quot; value=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- (事务管理)transaction manager --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; db.properties 12345678910jdbc.driver=com.mysql.jdbc.Driverjdbc.host=localhostjdbc.database=ssmdemojdbc.userName=rootjdbc.passWord=123456jdbc.initialSize=0jdbc.maxActive=20jdbc.maxIdle=20jdbc.minIdle=1jdbc.maxWait=1000 由于applicationContext-dao.xml中配置了Mapper接口扫描，所以删除mybatis-config.xml中的配置，否则报已映射错误：Caused by: org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Mapped Statements collection already contains value for MyMapper.selectUser删除mybatis-config.xml中的映射配置： 123456&lt;!--&lt;mappers&gt;--&gt; &lt;!--&lt;mapper resource=&quot;mappers/MyMapper.xml&quot;/&gt;--&gt; &lt;!--&lt;mapper resource=&quot;mappers/UserDaoMapper.xml&quot;/&gt;--&gt; &lt;!--&lt;mapper resource=&quot;mappers/UserMapper.xml&quot;/&gt;--&gt; &lt;!--&lt;mapper resource=&quot;mappers/OrderMapper.xml&quot;/&gt;--&gt;&lt;!--&lt;/mappers&gt;--&gt; 或者在构建sqlSessionFactory时不配置mybatis-config.xml也行： 12345678&lt;!-- spring和MyBatis完美整合 --&gt;&lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;!-- 自动扫描mapping.xml文件 --&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:mappers/*.xml&quot;&gt;&lt;/property&gt; &lt;!--如果mybatis-config.xml没有特殊配置也可以不需要下面的配置--&gt; &lt;!--&lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis-config.xml&quot; /&gt;--&gt;&lt;/bean&gt; 15.3 测试UserMapperSpringTest.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117import com.zpc.mybatis.dao.UserMapper;import com.zpc.mybatis.pojo.User;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Before;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import java.io.InputStream;import java.util.Date;import java.util.List;//目标：测试一下spring的bean的某些功能@RunWith(SpringJUnit4ClassRunner.class)//junit整合spring的测试//立马开启了spring的注解@ContextConfiguration(locations=&quot;classpath:spring/applicationContext-*.xml&quot;)//加载核心配置文件，自动构建spring容器public class UserMapperSpringTest &#123; @Autowired private UserMapper userMapper; @Test public void testQueryUserByTableName() &#123; List&lt;User&gt; userList = this.userMapper.queryUserByTableName(&quot;tb_user&quot;); for (User user : userList) &#123; System.out.println(user); &#125; &#125; @Test public void testLogin() &#123; System.out.println(this.userMapper.login(&quot;hj&quot;, &quot;123456&quot;)); &#125; @Test public void testQueryUserById() &#123; System.out.println(this.userMapper.queryUserById(&quot;1&quot;)); User user = new User(); user.setName(&quot;美女&quot;); user.setId(&quot;1&quot;); userMapper.updateUser(user); System.out.println(this.userMapper.queryUserById(&quot;1&quot;)); &#125; @Test public void testQueryUserAll() &#123; List&lt;User&gt; userList = this.userMapper.queryUserAll(); for (User user : userList) &#123; System.out.println(user); &#125; &#125; @Test public void testInsertUser() &#123; User user = new User(); user.setAge(20); user.setBirthday(new Date()); user.setName(&quot;大神&quot;); user.setPassword(&quot;123456&quot;); user.setSex(2); user.setUserName(&quot;bigGod222&quot;); this.userMapper.insertUser(user); System.out.println(user.getId()); &#125; @Test public void testUpdateUser() &#123; User user = new User(); user.setBirthday(new Date()); user.setName(&quot;静静&quot;); user.setPassword(&quot;123456&quot;); user.setSex(0); user.setUserName(&quot;Jinjin&quot;); user.setId(&quot;1&quot;); this.userMapper.updateUser(user); &#125; @Test public void testDeleteUserById() &#123; this.userMapper.deleteUserById(&quot;1&quot;); &#125; @Test public void testqueryUserList() &#123; List&lt;User&gt; users = this.userMapper.queryUserList(null); for (User user : users) &#123; System.out.println(user); &#125; &#125; @Test public void queryUserListByNameAndAge() throws Exception &#123; List&lt;User&gt; users = this.userMapper.queryUserListByNameAndAge(&quot;鹏程&quot;, 20); for (User user : users) &#123; System.out.println(user); &#125; &#125; @Test public void queryUserListByNameOrAge() throws Exception &#123; List&lt;User&gt; users = this.userMapper.queryUserListByNameOrAge(null, 16); for (User user : users) &#123; System.out.println(user); &#125; &#125; @Test public void queryUserListByIds() throws Exception &#123; List&lt;User&gt; users = this.userMapper.queryUserListByIds(new String[]&#123;&quot;5&quot;, &quot;2&quot;&#125;); for (User user : users) &#123; System.out.println(user); &#125; &#125; 目录结构： 16.SpringBoot 集成Mybatis请参见博文：https://blog.csdn.net/iku5200/article/details/82856621 17.Mybatis Generator的使用MyBatis Generator（MBG）是MyBatis 和iBATIS的代码生成器。可以生成简单CRUD操作的XML配置文件、Mapper文件(DAO接口)、实体类。实际开发中能够有效减少程序员的工作量，甚至不用程序员手动写sql。mybatis-generator有多种用法：命令行、maven插件等。命令行方式通常要把相关jar包下载到本地，再使用java -jar 运行。方便起见，本文演示使用maven插件的方式。 1.新建一个Maven项目(可以直接建立一个初始的springboot项目)pom文件引入mybatis-generator-maven-plugin ``` org.mybatis.generator mybatis-generator-maven-plugin 1.3.5 ``` 2.将插件需要的配置文件拷入到resource目录下，并做配置 generator.properties：配置数据库信息,在generatorConfig.xml使用： 12345678910111213141516#generatorConfig Infogenerator.location=D:\\\\software\\\\maven\\\\apache-maven-3.3.9\\\\repository\\\\mysql\\\\mysql-connector-java\\\\5.1.32\\\\mysql-connector-java-5.1.32.jargenerator.targetPackage=com.zpc.videoshow.generated#gererator.schema=oracle-schemagererator.tableName=video_infogererator.domainObjectName=VideoInfojdbc.driver=com.mysql.jdbc.Driverjdbc.host=jdbc:mysql://localhost:3306/videoshowjdbc.userName=rootjdbc.passWord=123456jdbc.initialSize=0jdbc.maxActive=20jdbc.maxIdle=20jdbc.minIdle=1jdbc.maxWait=1000 generatorConfig.xml：配置generator插件运行需要的参数信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC &quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot;&gt;&lt;generatorConfiguration&gt; &lt;!-- 引入配置文件 --&gt; &lt;properties resource=&quot;generator.properties&quot;/&gt; &lt;!-- 数据库驱动包位置,路径请不要有中文--&gt; &lt;!-- &lt;classPathEntry location=&quot;D:\\software\\lib\\mysql-connector-java-5.1.21.jar&quot; /&gt; --&gt; &lt;classPathEntry location=&quot;$&#123;generator.location&#125;&quot;/&gt; &lt;!-- 一个数据库一个context--&gt; &lt;context id=&quot;DB2Tables&quot; targetRuntime=&quot;MyBatis3&quot;&gt; &lt;!-- 生成的pojo，将implements Serializable --&gt; &lt;plugin type=&quot;org.mybatis.generator.plugins.SerializablePlugin&quot;&gt;&lt;/plugin&gt; &lt;!-- 注释 --&gt; &lt;commentGenerator&gt; &lt;property name=&quot;suppressAllComments&quot; value=&quot;true&quot;/&gt;&lt;!-- 是否取消注释 --&gt; &lt;!-- &lt;property name=&quot;suppressDate&quot; value=&quot;true&quot; /&gt; 是否生成注释代时间戳 --&gt; &lt;/commentGenerator&gt; &lt;!-- 数据库链接URL、用户名、密码 --&gt; &lt;!-- &lt;jdbcConnection driverClass=&quot;com.mysql.jdbc.Driver&quot; connectionURL=&quot;jdbc:mysql://localhost:3306/sy&quot; userId=&quot;sypro&quot; password=&quot;sypro&quot;&gt; --&gt; &lt;jdbcConnection driverClass=&quot;$&#123;jdbc.driver&#125;&quot; connectionURL=&quot;$&#123;jdbc.host&#125;&quot; userId=&quot;$&#123;jdbc.userName&#125;&quot; password=&quot;$&#123;jdbc.passWord&#125;&quot;&gt; &lt;/jdbcConnection&gt; &lt;!-- 类型转换 --&gt; &lt;javaTypeResolver&gt; &lt;!-- 默认false，把JDBC DECIMAL 和 NUMERIC 类型解析为 Integer true，把JDBC DECIMAL 和 NUMERIC 类型解析为java.math.BigDecimal --&gt; &lt;property name=&quot;forceBigDecimals&quot; value=&quot;false&quot;/&gt; &lt;/javaTypeResolver&gt; &lt;!-- 生成model模型，设置对应的包名(targetPackage)和存放路径(targetProject)。targetProject可以指定具体的路径,如./src/main/java,也可以使用MAVEN来自动生成,这样生成的代码会在target/generatord-source目录下 --&gt; &lt;javaModelGenerator targetPackage=&quot;$&#123;generator.targetPackage&#125;&quot; targetProject=&quot;./src/main/java&quot;&gt; &lt;!-- 是否在当前路径下新加一层schema,eg：false路径com.oop.eksp.user.model 而true:com.oop.eksp.user.model.[schemaName] --&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot;/&gt; &lt;!-- 从数据库返回的值被清理前后的空格 --&gt; &lt;property name=&quot;trimStrings&quot; value=&quot;true&quot;/&gt; &lt;/javaModelGenerator&gt; &lt;!--对应的mapper.xml文件 --&gt; &lt;sqlMapGenerator targetPackage=&quot;$&#123;generator.targetPackage&#125;&quot; targetProject=&quot;./src/main/java&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot;/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 对应的Mapper接口类文件 --&gt; &lt;javaClientGenerator type=&quot;XMLMAPPER&quot; targetPackage=&quot;$&#123;generator.targetPackage&#125;&quot; targetProject=&quot;./src/main/java&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot;/&gt; &lt;/javaClientGenerator&gt; &lt;!-- 列出要生成代码的所有表，这里配置的是不生成Example文件 --&gt; &lt;!-- schema即为数据库名tableName为对应的数据库表 domainObjectName是要生成的实体类 enable*ByExample是否生成 example类 --&gt; &lt;table tableName=&quot;$&#123;gererator.tableName&#125;&quot; domainObjectName=&quot;$&#123;gererator.domainObjectName&#125;&quot; schema=&quot;$&#123;gererator.schema&#125;&quot; enableCountByExample=&quot;false&quot; enableUpdateByExample=&quot;false&quot; enableDeleteByExample=&quot;false&quot; enableSelectByExample=&quot;false&quot; selectByExampleQueryId=&quot;false&quot;&gt; &lt;!-- 忽略列，不生成bean 字段 &lt;ignoreColumn column=&quot;FRED&quot; /&gt;--&gt; &lt;!-- 指定列的java数据类型 &lt;columnOverride column=&quot;LONG_VARCHAR_FIELD&quot; jdbcType=&quot;VARCHAR&quot; /&gt; --&gt; &lt;!-- 用于指定生成实体类时是否使用实际的列名作为实体类的属性名。false是 Camel Case风格--&gt; &lt;property name=&quot;useActualColumnNames&quot; value=&quot;false&quot;/&gt; &lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 3.运行generator插件（确保数据库已经运行） 方法1：直接找到mybatis-generator的插件，右击运行。 方法2：在运行配置里面添加maven命令 4.查看生成的文件 5.一些小技巧 a) 建表时，字段名称建议用”_“分隔多个单词，比如:AWB_NO、REC_ID…，这样生成的entity，属性名称就会变成漂亮的驼峰命名，即：awbNo、recId b)oracle中，数值形的字段，如果指定精度，比如Number(16,2)，默认生成entity属性是BigDecimal型 ，如果不指定精度，比如:Number(8)，指默认生成的是Long型 c)oracle中的nvarchar&#x2F;nvarchar2，mybatis-generator会识别成Object型，建议不要用nvarchar2，改用varchar2 6.Example文件的使用用过Hibernate的同学一定感叹于其完全不用手动写sql的强大功能，其实Mybatis也可以配置生成Example，省去一些简单的sql编写，实际开发中也会带来方便。 a.修改generatorConfig.xml的配置： 123enableCountByExample=&quot;true&quot; enableUpdateByExample=&quot;true&quot;enableDeleteByExample=&quot;true&quot; enableSelectByExample=&quot;true&quot;selectByExampleQueryId=&quot;true&quot; b.pom中引入mybatis的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.1&lt;/version&gt;&lt;/dependency&gt; c.运行generator 这种情况下多生成了一个Example的文件，Mapper文件的内容也会多很多example相关的： Example的详细使用百度之，参见：https://blog.csdn.net/m0_37795198&#x2F;article&#x2F;details&#x2F;78848045 18.MyBatis整合分页插件 pageHelper请参见博文：https://www.cnblogs.com/helf/p/11098105.htmlhttps://blog.csdn.net/panchang199266/article/details/82890143","categories":[{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"}]},{"title":"java8 快速实现List转map 、分组、过滤等操作","slug":"java/java8/java8 快速实现List转map 、分组、过滤等操作","date":"2021-11-16T12:00:11.000Z","updated":"2022-03-23T09:03:57.688Z","comments":true,"path":"blog/java/java8/java8 快速实现List转map 、分组、过滤等操作/","link":"","permalink":"http://sv.pointcut.cc/blog/java/java8/java8%20%E5%BF%AB%E9%80%9F%E5%AE%9E%E7%8E%B0List%E8%BD%ACmap%20%E3%80%81%E5%88%86%E7%BB%84%E3%80%81%E8%BF%87%E6%BB%A4%E7%AD%89%E6%93%8D%E4%BD%9C/","excerpt":"","text":"本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 利用 java8 新特性，可以用简洁高效的代码来实现一些数据处理。 定义 1 个 Apple 对象： 123456789101112public class Apple &#123; private Integer id; private String name; private BigDecimal money; private Integer num; public Apple(Integer id, String name, BigDecimal money, Integer num) &#123; this.id = id; this.name = name; this.money = money; this.num = num; &#125;&#125; 添加一些测试数据： 1234567891011List&lt;Apple&gt; appleList = new ArrayList&lt;&gt;();//存放apple对象集合 Apple apple1 = new Apple(1,&quot;苹果1&quot;,new BigDecimal(&quot;3.25&quot;),10);Apple apple12 = new Apple(1,&quot;苹果2&quot;,new BigDecimal(&quot;1.35&quot;),20);Apple apple2 = new Apple(2,&quot;香蕉&quot;,new BigDecimal(&quot;2.89&quot;),30);Apple apple3 = new Apple(3,&quot;荔枝&quot;,new BigDecimal(&quot;9.99&quot;),40); appleList.add(apple1);appleList.add(apple12);appleList.add(apple2);appleList.add(apple3); 1、分组 List 里面的对象元素，以某个属性来分组，例如，以 id 分组，将 id 相同的放在一起： 12345//List 以ID分组 Map&lt;Integer,List&lt;Apple&gt;&gt;Map&lt;Integer, List&lt;Apple&gt;&gt; groupBy = appleList.stream().collect(Collectors.groupingBy(Apple::getId)); System.err.println(&quot;groupBy:&quot;+groupBy);&#123;1=[Apple&#123;id=1, name=&#x27;苹果1&#x27;, money=3.25, num=10&#125;, Apple&#123;id=1, name=&#x27;苹果2&#x27;, money=1.35, num=20&#125;], 2=[Apple&#123;id=2, name=&#x27;香蕉&#x27;, money=2.89, num=30&#125;], 3=[Apple&#123;id=3, name=&#x27;荔枝&#x27;, money=9.99, num=40&#125;]&#125; 2、List 转 Map id 为 key，apple 对象为 value，可以这么做： 12345678/** * List -&gt; Map * 需要注意的是： * toMap 如果集合对象有重复的key，会报错Duplicate key .... * apple1,apple12的id都为1。 * 可以用 (k1,k2)-&gt;k1 来设置，如果有重复的key,则保留key1,舍弃key2 */Map&lt;Integer, Apple&gt; appleMap = appleList.stream().collect(Collectors.toMap(Apple::getId, a -&gt; a,(k1,k2)-&gt;k1)); 打印 appleMap 12345//过滤出符合条件的数据List&lt;Apple&gt; filterList = appleList.stream().filter(a -&gt; a.getName().equals(&quot;香蕉&quot;)).collect(Collectors.toList()); System.err.println(&quot;filterList:&quot;+filterList);[Apple&#123;id=2, name=&#x27;香蕉&#x27;, money=2.89, num=30&#125;] 3、过滤 Filter 从集合中过滤出来符合条件的元素： 123//计算 总金额BigDecimal totalMoney = appleList.stream().map(Apple::getMoney).reduce(BigDecimal.ZERO, BigDecimal::add);System.err.println(&quot;totalMoney:&quot;+totalMoney); //totalMoney:17.48 求和 将集合中的数据按照某个属性求和: 1234567Optional&lt;Dish&gt; maxDish = Dish.menu.stream(). collect(Collectors.maxBy(Comparator.comparing(Dish::getCalories)));maxDish.ifPresent(System.out::println); Optional&lt;Dish&gt; minDish = Dish.menu.stream(). collect(Collectors.minBy(Comparator.comparing(Dish::getCalories)));minDish.ifPresent(System.out::println); 查找流中最大 最小值 Collectors.maxBy 和 Collectors.minBy 来计算流中的最大或最小值。 123456789import static java.util.Comparator.comparingLong;import static java.util.stream.Collectors.collectingAndThen;import static java.util.stream.Collectors.toCollection; // 根据id去重 List&lt;Person&gt; unique = appleList.stream().collect( collectingAndThen( toCollection(() -&gt; new TreeSet&lt;&gt;(comparingLong(Apple::getId))), ArrayList::new) ); 去重 123456789import static java.util.Comparator.comparingLong;import static java.util.stream.Collectors.collectingAndThen;import static java.util.stream.Collectors.toCollection; // 根据id去重 List&lt;Person&gt; unique = appleList.stream().collect( collectingAndThen( toCollection(() -&gt; new TreeSet&lt;&gt;(comparingLong(Apple::getId))), ArrayList::new) ); 下表展示 Collectors 类的静态工厂方法。 工厂方法返回类型作用toListList&lt;T&gt;把流中所有项目收集到一个 ListtoSetSet&lt;T&gt;把流中所有项目收集到一个 Set，删除重复项toCollectionCollection&lt;T&gt;把流中所有项目收集到给定的供应源创建的集合menuStream.collect(toCollection(), ArrayList::new)countingLong计算流中元素的个数sumIntInteger对流中项目的一个整数属性求和averagingIntDouble计算流中项目 Integer 属性的平均值summarizingIntIntSummaryStatistics收集关于流中项目 Integer 属性的统计值，例如最大、最小、 总和与平均值joiningString连接对流中每个项目调用 toString 方法所生成的字符串collect(joining(\", \"))maxByOptional&lt;T&gt;一个包裹了流中按照给定比较器选出的最大元素的 Optional， 或如果流为空则为 Optional.empty()minByOptional&lt;T&gt;一个包裹了流中按照给定比较器选出的最小元素的 Optional， 或如果流为空则为 Optional.empty()reducing归约操作产生的类型从一个作为累加器的初始值开始，利用 BinaryOperator 与流 中的元素逐个结合，从而将流归约为单个值累加int totalCalories = menuStream.collect(reducing(0, Dish::getCalories, Integer::sum));collectingAndThen转换函数返回的类型包裹另一个收集器，对其结果应用转换函数int howManyDishes = menuStream.collect(collectingAndThen(toList(), List::size))groupingByMap&lt;K, List&lt;T&gt;&gt;根据项目的一个属性的值对流中的项目作问组，并将属性值作 为结果 Map 的键partitioningByMap&lt;Boolean,List&lt;T&gt;&gt;根据对流中每个项目应用谓词的结果来对项目进行分区","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"Java8中list集合的常用方法","slug":"java/java8/Java8中list集合的常用方法","date":"2021-11-16T12:00:10.000Z","updated":"2022-03-23T09:03:57.687Z","comments":true,"path":"blog/java/java8/Java8中list集合的常用方法/","link":"","permalink":"http://sv.pointcut.cc/blog/java/java8/Java8%E4%B8%ADlist%E9%9B%86%E5%90%88%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/","excerpt":"","text":"本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 利用 java8 新特性，可以用简洁高效的代码来实现一些数据处理。 定义 1 个 Apple 对象： 123456789101112public class Apple &#123; private Integer id; private String name; private BigDecimal money; private Integer num; public Apple(Integer id, String name, BigDecimal money, Integer num) &#123; this.id = id; this.name = name; this.money = money; this.num = num; &#125;&#125; 添加一些测试数据： 1234567891011List&lt;Apple&gt; appleList = new ArrayList&lt;&gt;();//存放apple对象集合 Apple apple1 = new Apple(1,&quot;苹果1&quot;,new BigDecimal(&quot;3.25&quot;),10);Apple apple12 = new Apple(1,&quot;苹果2&quot;,new BigDecimal(&quot;1.35&quot;),20);Apple apple2 = new Apple(2,&quot;香蕉&quot;,new BigDecimal(&quot;2.89&quot;),30);Apple apple3 = new Apple(3,&quot;荔枝&quot;,new BigDecimal(&quot;9.99&quot;),40); appleList.add(apple1);appleList.add(apple12);appleList.add(apple2);appleList.add(apple3); 1、分组 List 里面的对象元素，以某个属性来分组，例如，以 id 分组，将 id 相同的放在一起： 12345//List 以ID分组 Map&lt;Integer,List&lt;Apple&gt;&gt;Map&lt;Integer, List&lt;Apple&gt;&gt; groupBy = appleList.stream().collect(Collectors.groupingBy(Apple::getId)); System.err.println(&quot;groupBy:&quot;+groupBy);&#123;1=[Apple&#123;id=1, name=&#x27;苹果1&#x27;, money=3.25, num=10&#125;, Apple&#123;id=1, name=&#x27;苹果2&#x27;, money=1.35, num=20&#125;], 2=[Apple&#123;id=2, name=&#x27;香蕉&#x27;, money=2.89, num=30&#125;], 3=[Apple&#123;id=3, name=&#x27;荔枝&#x27;, money=9.99, num=40&#125;]&#125; 2、List 转 Map id 为 key，apple 对象为 value，可以这么做： 12345678/** * List -&gt; Map * 需要注意的是： * toMap 如果集合对象有重复的key，会报错Duplicate key .... * apple1,apple12的id都为1。 * 可以用 (k1,k2)-&gt;k1 来设置，如果有重复的key,则保留key1,舍弃key2 */Map&lt;Integer, Apple&gt; appleMap = appleList.stream().collect(Collectors.toMap(Apple::getId, a -&gt; a,(k1,k2)-&gt;k1)); 打印 appleMap 12345//过滤出符合条件的数据List&lt;Apple&gt; filterList = appleList.stream().filter(a -&gt; a.getName().equals(&quot;香蕉&quot;)).collect(Collectors.toList()); System.err.println(&quot;filterList:&quot;+filterList);[Apple&#123;id=2, name=&#x27;香蕉&#x27;, money=2.89, num=30&#125;] 3、过滤 Filter 从集合中过滤出来符合条件的元素： 123//计算 总金额BigDecimal totalMoney = appleList.stream().map(Apple::getMoney).reduce(BigDecimal.ZERO, BigDecimal::add);System.err.println(&quot;totalMoney:&quot;+totalMoney); //totalMoney:17.48 求和 将集合中的数据按照某个属性求和: 1234567Optional&lt;Dish&gt; maxDish = Dish.menu.stream(). collect(Collectors.maxBy(Comparator.comparing(Dish::getCalories)));maxDish.ifPresent(System.out::println); Optional&lt;Dish&gt; minDish = Dish.menu.stream(). collect(Collectors.minBy(Comparator.comparing(Dish::getCalories)));minDish.ifPresent(System.out::println); 查找流中最大 最小值 Collectors.maxBy 和 Collectors.minBy 来计算流中的最大或最小值。 123456789import static java.util.Comparator.comparingLong;import static java.util.stream.Collectors.collectingAndThen;import static java.util.stream.Collectors.toCollection; // 根据id去重 List&lt;Person&gt; unique = appleList.stream().collect( collectingAndThen( toCollection(() -&gt; new TreeSet&lt;&gt;(comparingLong(Apple::getId))), ArrayList::new) ); 去重 123456789import static java.util.Comparator.comparingLong;import static java.util.stream.Collectors.collectingAndThen;import static java.util.stream.Collectors.toCollection; // 根据id去重 List&lt;Person&gt; unique = appleList.stream().collect( collectingAndThen( toCollection(() -&gt; new TreeSet&lt;&gt;(comparingLong(Apple::getId))), ArrayList::new) ); 下表展示 Collectors 类的静态工厂方法。 工厂方法返回类型作用toListList&lt;T&gt;把流中所有项目收集到一个 ListtoSetSet&lt;T&gt;把流中所有项目收集到一个 Set，删除重复项toCollectionCollection&lt;T&gt;把流中所有项目收集到给定的供应源创建的集合menuStream.collect(toCollection(), ArrayList::new)countingLong计算流中元素的个数sumIntInteger对流中项目的一个整数属性求和averagingIntDouble计算流中项目 Integer 属性的平均值summarizingIntIntSummaryStatistics收集关于流中项目 Integer 属性的统计值，例如最大、最小、 总和与平均值joiningString连接对流中每个项目调用 toString 方法所生成的字符串collect(joining(\", \"))maxByOptional&lt;T&gt;一个包裹了流中按照给定比较器选出的最大元素的 Optional， 或如果流为空则为 Optional.empty()minByOptional&lt;T&gt;一个包裹了流中按照给定比较器选出的最小元素的 Optional， 或如果流为空则为 Optional.empty()reducing归约操作产生的类型从一个作为累加器的初始值开始，利用 BinaryOperator 与流 中的元素逐个结合，从而将流归约为单个值累加int totalCalories = menuStream.collect(reducing(0, Dish::getCalories, Integer::sum));collectingAndThen转换函数返回的类型包裹另一个收集器，对其结果应用转换函数int howManyDishes = menuStream.collect(collectingAndThen(toList(), List::size))groupingByMap&lt;K, List&lt;T&gt;&gt;根据项目的一个属性的值对流中的项目作问组，并将属性值作 为结果 Map 的键partitioningByMap&lt;Boolean,List&lt;T&gt;&gt;根据对流中每个项目应用谓词的结果来对项目进行分区","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"篇与面试官和蔼交流的深入了解 JVM（JDK8","slug":"java/篇与面试官和蔼交流的深入了解 JVM（JDK8","date":"2021-11-16T12:00:09.000Z","updated":"2022-03-23T09:03:57.686Z","comments":true,"path":"blog/java/篇与面试官和蔼交流的深入了解 JVM（JDK8/","link":"","permalink":"http://sv.pointcut.cc/blog/java/%E7%AF%87%E4%B8%8E%E9%9D%A2%E8%AF%95%E5%AE%98%E5%92%8C%E8%94%BC%E4%BA%A4%E6%B5%81%E7%9A%84%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3%20JVM%EF%BC%88JDK8/","excerpt":"","text":"文章目录 15.1、100%CPU 的排查 15.2、死锁的检查 13.1、CMS（“标记 - 清除” 算法, -XX:+UseConcMarkSweepGC(old） 13.2、G1 13.3、ZGC 13.1.6.1、增量更新 (Incremental Update)+ 写屏障 13.1.6.2、原始快照（Snapshot At The Beginning，SATB）+ 写屏障 13.1.6.3、并发标记时对漏标的处理方案 13.1.1、运作过程 (5 大步骤) 13.1.2、三色标记法 13.1.3、concurrent model failure（浮动垃圾） 13.1.4、background&#x2F;foreground collector 13.1.5、为什么 G1 用 SATB？CMS 用增量更新？ 13.1.6、漏标 - 读写屏障 (解决方案) 13.1.7、promotion failed 13.1.8、过早提升和提升失败 13.1.9、早提升的原因 13.1.10、提升失败原因 13.2.1、运作流程 13.2.2、Remembered Set（记录集）&#x2F;Card Table（卡表） 13.2.3、Collect Set 13.2.4、young gc 的完整流程 13.2.5、Mixed GC 的完整流程 13.2.6、Full GC 13.2.7、Marking bitmaps&#x2F;TAMS 13.2.8、Pause Prediction Model 13.2.9、G1 收集器参数设置 13.2.10、G1 垃圾收集器优化建议（ -XX:MaxGCPauseMills&#x3D;50ms） 13.2.11、什么场景适合使用 G1 13.3.1、主要目标 13.3.2、color poin（颜色指针） 13.3.3、运作过程 13.3.4、存在的问题，怎么解决 13.3.5、安全点与安全区域 13.3.6、ZGC 参数 13.3.7、ZGC 触发时机 11.1、分代收集理论 11.2、标记 - 复制算法 11.3、标记 - 清除算法 11.4、标记 - 整理算法 10.1、引用计数法 10.2、可达性分析算法（gcroot） 6.1、栈上分配 6.2、对象在 Eden 区分配 (大部分情况，当 Eden 区没有足够空间进行分配时，出现 Young GC) 6.3、大对象直接进入老年代 6.4、长期存活的对象将进入老年代 6.5、对象动态年龄判断 6.6、老年代空间分配担保机制） 5.1.1、对象大小 5.1.2、什么是 java 对象的指针压缩？ 5.1.3、为什么要进行指针压缩？ 5.1、对象大小与指针压缩 4.1、线程私有区域 4.2、线程共享区域 2.1、类加载器 2.2、加载器初始化过程 2.3、双亲委派机制 2.4、为什么要设计双亲委派机制？ 2.5、全盘负责委托机制 2.6、自定义类加载器示例 1、类加载机制 2、双亲委派机制 (先找父亲加载，不行再由儿子自己加载) 3、tomcat 怎么破解类加载机制 4、内存模型 5、对象的创建 6、对象的分配过程 7、如何判断一个类是无用的类 8、finalize() 方法最终判定对象是否存活 9、常见引用类型 (四大引用) 10、对象回收 11、四大垃圾回收算法 12、常见 oom 13、垃圾收集器 14、如何选择垃圾收集器 15、各种命令（例如 100%cpu 的排查、死锁的检查） 16、JIT(即时编译器) 17、逃逸分析 1、类加载机制1类加载过程分为 加载 &gt;&gt; 验证 &gt;&gt; 准备 &gt;&gt; 解析 &gt;&gt; 初始化 &gt;&gt; 使用 &gt;&gt; 卸载 1、加载 在硬盘上查找并通过IO读入字节码文件，使用到类时才会加载，例如调用类的main()方法，new对象 等等，在加载阶段会在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口 2、验证 校验字节码文件的正确性 3、准备 给类的静态变量分配内存，并赋予默认值 4、解析 将符号引用替换为直接引用，该阶段会把一些静态方法(符号引用，比如main()方法)替换为指向数据 所存内存的指针或句柄等(直接引用)，这是所谓的静态链接过程(类加载期间完成)，动态链接是在程 序运行期间完成的将符号引用替换为直接引用，下节课会讲到动态链接 5、初始化 对类的静态变量初始化为指定的值，执行静态代码块 2、双亲委派机制 (先找父亲加载，不行再由儿子自己加载)2.1、类加载器11、根类加载器（**Bootstrap classLoader**）：负责加载支撑JVM运行的位于JRE的lib目录下的核心类库，比如rt.jar、charsets.jar等 2、扩展类加载器（**ExtClassLoader**）：负责加载支撑JVM运行的位于JRE的lib目录下的ext扩展目录中的JAR类包 3、应用加载器（**AppClassLoader**）：负责加载ClassPath路径下的类包，主要就是加载你自己写的那些类,负责加载用户自定义路径下的类包 2.2、加载器初始化过程1类运行加载全过程会创建JVM启动器实例sun.misc.Launcher。sun.misc.Launcher初始化使用了单例模式设计，保证一个JVM虚拟机内只有一个sun.misc.Launcher实例。在Launcher构造方法内部，其创建了两个类加载器，分别是sun.misc.Launcher.ExtClassLoader(扩展类加载器)和sun.misc.Launcher.AppClassLoader(应用类加载器)。 JVM默认使用launcher的`getClassLoader()`方法返回的类加载器`AppClassLoader`的实例来加载我们的应用程序。 2.3、双亲委派机制 应用程序类加载器 AppClassLoader 加载类的双亲委派机制源码，AppClassLoader 的 loadClass 方法最终会调用其父类 ClassLoader 的 loadClass 方法，该方法的大体逻辑如下：首先，检查一下指定名称的类是否已经加载过，如果加载过了，就不需要再加载，直接返回。如果此类没有加载过，那么，再判断一下是否有父加载器；如果有父加载器，则由父加载器加载（即调用 parent.loadClass(name, false);）. 或者是调用 bootstrap 类加载器来加载。如果父加载器及 bootstrap 类加载器都没有找到指定的类，那么调用当前类加载器的 findClass 方法来完成类加载。 2.4、为什么要设计双亲委派机制？ 1、沙箱安全机制：自己写的 java.lang.String.class 类不会被加载，这样便可以防止核心 API 库被随意篡改2、避免类的重复加载：当父亲已经加载了该类时，就没有必要子 ClassLoader 再加载一次，保证被加载类的唯一性 2.5、全盘负责委托机制 “全盘负责” 是指当一个 ClassLoder 装载一个类时，除非显示的使用另外一个 ClassLoder，该类所依赖及引用的类也由这个 ClassLoder 载入 2.6、自定义类加载器示例 自定义类加载器只需要继承 java.lang.ClassLoader 类，该类有两个核心方法，一个是 loadClass(String, boolean)，实现了双亲委派机制，还有一个方法是 findClass，默认实现是空方法，所以我们自定义类加载器主要是重写 findClass 方法。 3、tomcat 怎么破解类加载机制 1、commonLoader：Tomcat 最基本的类加载器，加载路径中的 class 可以被 Tomcat 容器本身以及各个 Webapp 访问； 2、catalinaLoader：Tomcat 容器私有的类加载器，加载路径中的 class 对于 Webapp 不可见； 3、sharedLoader：各个 Webapp 共享的类加载器，加载路径中的 class 对于所有 Webapp 可见，但是对于 Tomcat 容器不可见； 4、WebappClassLoader：各个 Webapp 私有的类加载器，加载路径中的 class 只对当前 Webapp 可见，比如加载 war 包里相关的类， 每个 war 包应用都有自己的 WebappClassLoader，实现相互隔离，比如不同 war 包应用引入了不同的 spring 版本，这样实现就能加载各自的 spring 版本； 5、模拟实现 Tomcat 的 JasperLoader 热加载 原理：后台启动线程监听 jsp 文件变化，如果变化了找到该 jsp 对应的 servlet 类的加载器引用 (gcroot)，重新生成新的 JasperLoader 加载器赋值给引用，然后加载新的 jsp 对应的 servlet 类，之前的那个加载器因为没有 gcroot 引用了，下一次 gc 的时候会被销毁 &#x3D;&gt; 总结：每个 webappClassLoader 加载自己的目录下的 class 文件，不会传递给父类加载器，打破了双亲委派机制。 4、内存模型4.1、线程私有区域 程序计数器：是当前线程所执行的字节码的行号指示器，无 OOM 虚拟机栈：是描述 java 方法执行的内存模型，每个方法在执行的同时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。 栈帧（ Frame）是用来存储数据和部分过程结果的数据结构，同时也被用来处理动态链接 (Dynamic Linking)、 方法返回值和异常分派（ Dispatch Exception）。栈帧随着方法调用而创 建，随着方法结束而销毁——无论方法是正常完成还是异常完成（抛出了在方法内未被捕获的异 常）都算作方法结束。 本地方法栈：和 Java Stack 作用类似, 区别是虚拟机栈为执行 Java 方法服务, 而本地方法栈则为 Native 方法服务, 如果一个 VM 实现使用 C-linkage 模型来支持 Native 调用, 那么该栈将会是一个 C 栈，但 HotSpot VM 直接就把本地方法栈和虚拟机栈合二为一。 4.2、线程共享区域 &#x3D;&#x3D; 堆 - 运行时数据区：&#x3D;&#x3D; 是被线程共享的一块内存区域，创建的对象和数组都保存在 Java 堆内存中，也是垃圾收集器进行垃圾收集的最重要的内存区域。由于现代 VM 采用分代收集算法, 因此 Java 堆从 GC 的角度还可以细分为: 新生代 (Eden 区、From Survivor 区和 To Survivor 区) 和老年代 方法区 &#x2F; 永久代（1.8 之后元空间）：用于存储被 JVM 加载的类信息 、常量、静态变量、 即时编译器编译后的代码等数据. HotSpot VM 把 GC 分代收集扩展至方法区, 即使用 Java 堆的永久代来实现方法区, 这样 HotSpot 的垃圾收集器就可以像管理 Java 堆一样管理这部分内存, 而不必为方法区开发专门的内存管理器 (永久带的内存回收的主要目标是针对常量池的回收和类型的卸载, 因此收益一般很小)。 运行时常量池（Runtime Constant Pool）是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述等信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。 直接内存 jdk1.4 后加入 NIO（New Input&#x2F;Output）类，引入了一种基于通道与缓冲区的 I&#x2F;O 方式，它可以使用 native 函数库直接分配堆外内存，然后通过一个存储在 java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。可以避免在 Java 堆和 Native 堆中来回复制数据 直接内存的分配不会受到 Java 堆大小的限制. 避免大于物理内存的情况 5、对象的创建 1、类加载检查 1虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。new指令对应到语言层面上讲是，new关键词、对象克隆、对象序列化等 2、分配内存 1在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类 加载完成后便可完全确定，为对象分配空间的任务等同于把 一块确定大小的内存从Java堆中划分出来。 //如何划分内存？ 1、“指针碰撞”（Bump the Pointer）(默认用指针碰撞) 如果Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中 间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段 与对象大小相等的距离。 2、“空闲列表”（Free List） 如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简 单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从 列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录。 //解决并发问题的方法 1、CAS（compare and swap） 虚拟机采用CAS配上失败重试的方式保证更新操作的原子性来对分配内存空间的动作进行同 步处理。 2、本地线程分配缓冲（Thread Local Allocation Buffer,TLAB） 把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一 小块内存。通过­XX:+/­UseTLAB参数来设定虚拟机是否使用TLAB(JVM会默认开启 ­XX:+UseTLAB)，­XX:TLABSize指定TLAB大小。 初始化 1内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头）， 如果使用TLAB，这一工作过程也可以提前至TLAB分配时进行。这一步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 设置对象头 1初始化零值之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息存放在对象的对象头Object Header之中。在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、 实例数据（Instance Data）和对齐填充（Padding）。HotSpot虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的运行时数据， 如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时 间戳等。对象头的另外一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 执行方法 1执行&lt;init&gt;方法，即对象按照程序员的意愿进行初始化。对应到语言层面上讲，就是为属性赋值（注意，这与上面的赋零值不同，这是由程序员赋的值），和执行构造方法。 5.1、对象大小与指针压缩5.1.1、对象大小 对象大小可以用 jol­-core 包查看 5.1.2、什么是 java 对象的指针压缩？ jdk1.6 update14 开始，在 64bit 操作系统中，JVM 支持指针压缩 jvm 配置参数: UseCompressedOops，compressed­­ 压缩、oop(ordinary object pointer)­­ 对象指针 启用指针压缩:­XX:+UseCompressedOops(默认开启)，禁止指针压缩:­XX:­UseCompressedOops 5.1.3、为什么要进行指针压缩？ 在 64 位平台的 HotSpot 中使用 32 位指针，内存使用会多出 1.5 倍左右，使用较大指针在主内存和缓存之间移动数据，占用较大宽带，同时 GC 也会承受较大压力 为了减少 64 位平台下内存的消耗，启用指针压缩功能 在 jvm 中，32 位地址最大支持 4G 内存 (2 的 32 次方)，可以通过对对象指针的压缩编码、解码方式进行优化，使得 jvm只用 32 位地址就可以支持更大的内存配置 (小于等于 32G) 堆内存小于 4G 时，不需要启用指针压缩，jvm 会直接去除高 32 位地址，即使用低虚拟地址空间 堆内存大于 32G 时，压缩指针会失效，会强制使用 64 位 (即 8 字节) 来对 java 对象寻址，这就会出现 1 的问题，所以堆内存不要大于 32G 为好 6、对象的分配过程 6.1、栈上分配 我们通过 JVM 内存分配可以知道 JAVA 中的对象都是在堆上进行分配，当对象没有被引用的时候，需要依靠 GC 进行回收内存，如果对象数量较多的时候，会给 GC 带来较大压力，也间接影响了应用的性能。为了减少临时对象在堆内分配的数量，JVM 通过逃逸分析确定该对象不会被外部访问。如果不会逃逸可以将该对象在栈上分配内存，这样该对象所占用的内存空间就可以随栈帧出栈而销毁，就减轻了垃圾回收的压力。 &#x3D;&#x3D; 对象逃逸分析：&#x3D;&#x3D; 就是分析对象动态作用域，当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他地方中。 12345678910public User test1() &#123; User user = new User(); user.setId(1); user.setName(&quot;zhuge&quot;); //TODO 保存到数据库 return user; &#125; public void test2() &#123; User user = new User(); user.setId(1); user.setName(&quot;zhuge&quot;); //TODO 保存到数据库 &#125; 很显然 test1 方法中的 user 对象被返回了，这个对象的作用域范围不确定，test2 方法中的 user 对象我们可以确定当方法结束这个对象就可以认为是无效对象了，对于这样的对象我们其实可以将其分配在栈内存里，让其在方法结束时跟随栈内存一起被回收掉。JVM 对于这种情况可以通过开启逃逸分析参数 (-XX:+DoEscapeAnalysis) 来优化对象内存分配位置，使其通过标量替换优先分配在栈上(栈上分配)，JDK7 之后默认开启逃逸分析，如果要关闭使用参数(-XX:-DoEscapeAnalysis)&#x3D;&#x3D; 标量替换：&#x3D;&#x3D; 通过逃逸分析确定该对象不会被外部访问，并且对象可以被进一步分解时，JVM 不会创建该对象，而是将该对象成员变量分解若干个被这个方法使用的成员变量所代替，这些代替的成员变量在栈帧或寄存器上分配空间，这样就不会因为没有一大块连续空间导致对象内存不够分配。开启标量替换参数 (-XX:+EliminateAllocations)，JDK7 之后默认开启。&#x3D;&#x3D; 标量与聚合量：&#x3D;&#x3D; 标量即不可被进一步分解的量，而 JAVA 的基本数据类型就是标量（如：int，long 等基本数据类型以及 reference 类型等），标量的对立就是可以被进一步分解的量，而这种量称之为聚合量。而在 JAVA 中对象就是可以被进一步分解的聚合量。 结论：栈上分配依赖于逃逸分析和标量替换 6.2、对象在 Eden 区分配 (大部分情况，当 Eden 区没有足够空间进行分配时，出现 Young GC) 大量的对象被分配在 eden 区，eden 区满了后会触发 Young GC，可能会有 99% 以上的对象成为垃圾被回收掉，剩余存活的对象会被挪到 s0 区，下一次 eden 区满了后又会触发 Young GC，把 eden 区和 s0 区垃圾对象回收，把剩余存活的对象一次性挪动到另外一块为空的 s1 区，因为新生代的对象都是朝生夕死的，存活时间很短，所以 JVM 默认的 8:1:1 的比例是很合适的，让 eden 区尽量的大，survivor 区够用即可，JVM 默认有这个参数 - XX:+UseAdaptiveSizePolicy(默认开启)，会导致这个 8:1:1 比例自动变化，如果不想这个比例有变化可以设置参数 - XX:-UseAdaptiveSizePolicy 6.3、大对象直接进入老年代 大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。JVM 参数 -XX:PretenureSizeThreshold 可以设置大对象的大小，如果对象超过设置大小会直接进入老年代，不会进入年轻代，这个参数只在 Serial 和 ParNew 两个收集器下有效。比如设置 JVM 参数：-XX:PretenureSizeThreshold&#x3D;1000000 (单位是字节) -XX:+UseSerialGC ，再执行下上面的第一个程序会发现大对象直接进了老年代 为什么要这样呢？为了避免为大对象分配内存时的复制操作而降低效率。 6.4、长期存活的对象将进入老年代 虚拟机给每个对象一个对象年龄（Age）计数器。如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor空间中，并将对象年龄设为 1。对象在 Survivor 中每熬过一次 MinorGC，年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁，CMS 收集器默认 6 岁，不同的垃圾收集器会略微有点不同），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 6.5、对象动态年龄判断 当前放对象的 Survivor 区域里 (其中一块区域，放对象的那块 s 区)，一批对象的总大小大于这块 Survivor 区域内存大小的 50%(-XX:TargetSurvivorRatio 可以指定)，那么此时大于等于这批对象年龄最大值的对象，就可以直接进入老年代了，例如 Survivor 区域里现在有一批对象，年龄 1 + 年龄 2 + 年龄 n 的多个年龄对象总和超过了 Survivor 区域的 50%，此时就会把年龄 n(含) 以上的对象都放入老年代。这个规则其实是希望那些可能是长期存活的对象，尽早进入老年代。对象动态年龄判断机制一般是在 young gc 之后触发的。 6.6、老年代空间分配担保机制） 年轻代每次 minor gc 之前 JVM 都会计算下老年代剩余可用空间如果这个可用空间小于年轻代里现有的所有对象大小之和 (包括垃圾对象)就会看一个 “-XX:-HandlePromotionFailure”(jdk1.8 默认就设置了) 的参数是否设置了如果有这个参数，就会看看老年代的可用内存大小，是否大于之前每一次 minor gc 后进入老年代的对象的平均大小。如果上一步结果是小于或者之前说的参数没有设置，那么就会触发一次 Full gc，对老年代和年轻代一起回收一次垃圾，如果回收完还是没有足够空间存放新的对象就会发生 “OOM”。 当然，如果 minor gc 之后剩余存活的需要挪动到老年代的对象大小还是大于老年代可用空间，那么也会触发 full gc，full gc 完之后如果还是没有空间放 minor gc 之后的存活对象，则也会发生 “OOM” 7、如何判断一个类是无用的类 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 8、finalize() 方法最终判定对象是否存活 第一次标记并进行一次筛选。 筛选的条件是此对象是否有必要执行 finalize() 方法。 当对象没有覆盖 finalize 方法，对象将直接被回收。 第二次标记 如果这个对象覆盖了 finalize 方法，finalize 方法是对象脱逃死亡命运的最后一次机会，如果对象要在 finalize()中成功拯救 自己，只要重新与引用链上的任何的一个对象建立关联即可，譬如把自己赋值给某个类变量或对象的成员变量，那在第 二次标记时它将移除出 “即将回收” 的集合。如果对象这时候还没逃脱，那基本上它就真的被回收了。 注意：一个对象的 finalize() 方法只会被执行一次，也就是说通过调用 finalize 方法自我救命的机会就一次。 9、常见引用类型 (四大引用) 1、强引用：普通的变量引用 2、软引用（SoftReference）：将对象用 SoftReference 软引用类型的对象包裹，正常情况不会被回收，但是 GC 做完后发现释放不出空间存放新的对象，则会把这些软引用的对象回收掉。软引用可用来实现内存敏感的高速缓存。 使用场景：浏览器的后退按钮 3、弱引用（WeakReference）：将对象用 WeakReference 软引用类型的对象包裹，弱引用跟没引用差不多，GC 会直接回收掉，很少用 4、虚引用：虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系，几乎不用 10、对象回收 什么叫对象回收？ 堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能再被任何途径使用的对象）。 10.1、引用计数法 给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加 1；当引用失效，计数器就减 1；任何时候计数器为 0 的对象就是不可能再被使用的。缺点：循环引用问题 10.2、可达性分析算法（gcroot） 将 “GC Roots” 对象作为起点，从这些节点开始向下搜索引用的对象，找到的对象都标记为非垃圾对象，其余未标记的对象都是垃圾对象GC Roots 根节点：线程栈的本地变量、静态变量、本地方法栈的变量等等 11、四大垃圾回收算法11.1、分代收集理论 当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。比如在新生代中，每次收集都会有大量对象 (近 99%) 死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择 “标记 - 清除” 或“标记 - 整理”算法进行垃圾收集。注意，“标记 - 清除”或 “标记 - 整理” 算法会比复制算法慢 10 倍以上。 11.2、标记 - 复制算法 它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 11.3、标记 - 清除算法 算法分为 “标记” 和“清除”阶段：标记存活的对象， 统一回收所有未被标记的对象(一般选择这种)；也可以反过来，标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象 。它是最基础的收集算法，比较简单，但是会带来两个明显的问题： 效率问题 (如果需要标记的对象太多，效率不高) 空间问题（标记清除后会产生大量不连续的碎片） 11.4、标记 - 整理算法 根据老年代的特点特出的一种标记算法，标记过程仍然与 “标记 - 清除” 算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 12、常见 oom 1、java.lang.StackOverflowError: 报这个错误一般是由于方法深层次的调用，默认的线程栈空间大小一般与具体的硬件平台有关。栈内存为线程私有的空间，每个线程都会创建私有的栈内存。栈空间内存设置过大，创建线程数量较多时会出现栈内存溢出 StackOverflowError。同时，栈内存也决定方法调用的深度，栈内存过小则会导致方法调用的深度较小，如递归调用的次数较少。 2、java.lang.OutOfMemoryError: Java heap space Heap size 设置 JVM 堆的设置是指：java 程序执行过程中 JVM 能够调配使用的内存空间的设置。JVM 在启动的时候会自己主动设置 Heap size 的值，其初始空间 (即 - Xms) 是物理内存的 1&#x2F;64，最大空间 (-Xmx) 是物理内存的 1&#x2F;4。能够利用 JVM 提供的 - Xmn -Xms -Xmx 等选项可进行设置。Heap size 的大小是 Young Generation 和 Tenured Generaion 之和。 3、java.lang.OutOfMemoryError：GC overhead limit exceeded GC 回收时间过长时会抛出的 OutOfMemory。过长是指，超过 98% 的时间都在用来做 GC 并且回收了不到 2% 的堆内存。连续多次的 GC，都回收了不到 2% 的极端情况下才会抛出。假如不抛出 GC overhead limit 错误会发生什么事情呢？那就是 GC 清理出来的一点内存很快又会被再次填满，强迫 GC 再次执行，这样造成恶性循环，CPU 的使用率一直很高，但是 GC 没有任何的进展。 4、java.lang.OutOfMemoryError：Direct buffer memory 写 NIO 程序经常使用到 ByteBuffer 来读取或者写入数据，这是一种基于通道与缓冲区的 I&#x2F;O 方式。它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 java 堆里面的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中提高性能，因为避免了 java 堆和 Native 堆中来回复制数据。 ByteBuffer.allocate(capability) ：这种方式是分配 JVM 堆内存，属于 GC 管辖范围之内。由于需要拷贝，所以速度相对较慢； ByteBuffer.allocateDirect(capability)：这种方式是直接分配 OS 本地内存，不属于 GC 管辖范围之内，由于不需要内存拷贝所以速度相对较快。 但是如果不断分配本地内存，堆内存很少使用，那么 JVM 就不需要执行 GC,DirectByteBuffer 对象就不会被回收。这时候堆内存充足，但是本地内存已经用光了，再次尝试分配的时候就会出现 OutOfMemoryError，那么程序就直接崩溃了。 5、java.lang.OutOfMemoryError：unable to create new native thread 准确的说，这一个异常是和程序运行的平台相关的。导致的原因： 创建了太多的线程，一个应用创建多个线程，超过系统承载极限； 服务器不允许应用程序创建这么多的线程，Linux 系统默认的允许单个进程可以创建的线程数量是 1024 个，当创建多 线程数量多于这个数字的时候就会抛出此异常 如何解决呢？ 想办法减少应用程序创建的线程的数量，分析应用是否真的需要创建这么多的线程。如果不是，改变代码将线程数量降到最低； 对于有的应用，确实需要创建很多的线程，远超过 Linux 限制的 1024 个 限制，那么可以通过修改 Linux 服务器的配置，扩大 Linux 的默认限制。 6、java.lang.OutOfMemoryError：MetaSpace 元空间的本质和永久代类似，都是对 JVM 规范中的方法区的实现。不过元空间与永久代之间最大的区别在于：元空间不在虚拟机中，而是使用的本地内存。因此，默认情况下，元空间的大小仅仅受到本地内存的限制 。 元空间存放了以下的内容： 虚拟机加载的类信息； 常量池； 静态变量； 即时编译后的代码 模拟 MetaSpace 空间溢出，我们不断生成类往元空间里灌，类占据的空间总是会超过 MetaSpace 指定的空间大小的 查看元空间的大小：java -XX:+PrintFlagsInitial 13、垃圾收集器13.1、CMS（“标记 - 清除” 算法, -XX:+UseConcMarkSweepGC(old）定义：以获取最短回收停顿时间为目标的收集器 13.1.1、运作过程 (5 大步骤) 1、初始标记：暂停所有的其他线程 (STW)，并记录下 gc roots 直接能引用的对象，速度很快。2、并发标记：并发标记阶段就是从 GC Roots 的直接关联对象开始遍历整个对象图的过程， 这个过程耗时较长但是不需要停顿用户线程， 可以与垃圾收集线程一起并发运行。因为用户程序继续运行，可能会有导致已经标记过的对象状态发生改变。3、重新标记：重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短。主要用到增量更新算法做重新标记。4、并发清理：开启用户线程，同时 GC 线程开始对未标记的区域做清扫。这个阶段如果有新增对象会被标记为三色标记法里面的黑色不做任何处理5、并发重置：重置本次 GC 过程中的标记数据。 主要优点：并发收集、低停顿。但是它有下面几个明显的缺点： 对 CPU 资源敏感（会和服务抢资源）； 无法处理浮动垃圾 (在并发标记和并发清理阶段又产生垃圾，这种浮动垃圾只能等到下一次 gc 再清理了)； 它使用的回收算法 -“标记 - 清除” 算法会导致收集结束时会有大量空间碎片产生，当然通过参数 &#x3D;&#x3D;-XX:+UseCMSCompactAtFullCollection 可以让 jvm 在执行完标记清除后再做整理 &#x3D;&#x3D; 执行过程中的不确定性，会存在上一次垃圾回收还没执行完，然后垃圾回收又被触发的情况，特别是 在并发标记和并发清理阶段会出现，一边回收，系统一边运行，也许没回收完就再次触发 full gc，也就是 “concurrent mode failure”，此时会进入 stop the world，用 serial old 垃圾收集器来回收 CMS 的相关核心参数 -XX:+UseConcMarkSweepGC：启用 cms -XX:ConcGCThreads：并发的 GC 线程数 -XX:+UseCMSCompactAtFullCollection：FullGC 之后做压缩整理（减少碎片） -XX:CMSFullGCsBeforeCompaction：多少次 FullGC 之后压缩一次，默认是 0，代表每次 FullGC 后都会压缩一次 -XX:CMSInitiatingOccupancyFraction: 当老年代使用达到该比例时会触发 FullGC（默认是 92，这是百分比） -XX:+UseCMSInitiatingOccupancyOnly：只使用设定的回收阈值 (-XX:CMSInitiatingOccupancyFraction 设定的值)，如果不指定，JVM 仅在第一次使用设定值，后续则会自动调整 -XX:+CMSScavengeBeforeRemark：在 CMS GC 前启动一次 minor gc，目的在于减少老年代对年轻代的引用，降低 CMS GC 的标记阶段时的开销，一般 CMS 的 GC 耗时 80% 都在标记阶段 -XX:+CMSParallellnitialMarkEnabled：表示在初始标记的时候多线程执行，缩短 STW -XX:+CMSParallelRemarkEnabled：在重新标记的时候多线程执行，缩短 STW; 13.1.2、三色标记法 黑色：表示对象已经被垃圾收集器访问过， 且这个对象的所有引用都已经扫描过。黑色的对象代表已经扫描过， 它是安全存活的， 如果有其他对象引用指向了黑色对象， 无须重新扫描一遍。黑色对象不可能直接（不经过灰色对象） 指向某个白色对象。 灰色：表示对象已经被垃圾收集器访问过， 但这个对象上至少存在一个引用还没有被扫描过。 白色：表示对象尚未被垃圾收集器访问过。显然在可达性分析刚刚开始的阶段， 所有的对象都是白色的， 若在分析结束的阶段， 仍然是白色的对象， 即代表不可达。 13.1.3、concurrent model failure（浮动垃圾）1在并发标记过程中，如果由于方法运行结束导致部分局部变量(gcroot)被销毁，这个gcroot引用的对象之前又被扫描过(被标记为非垃圾对象)，那么本轮GC不会回收这部分内存。这部分本应该回收但是没有回收到的内存，被称之为“浮动垃圾”。浮动垃圾并不会影响垃圾回收的正确性，只是需要等到下一轮垃圾回收中才被清除。另外，针对并发标记(还有并发清理)开始后产生的新对象，通常的做法是直接全部当成黑色，本轮不会进行清除。这部分对象期间可能也会变为垃圾，这也算是浮动垃圾的一部分。 13.1.4、background&#x2F;foreground collector -XX:ConcGCThreads&#x3D;4 和 - XX:+ExplicitGCInvokesConcurrent 开启 foreground CMS GC，CMS gc 有两种模式，background 和 foreground，正常的 cms gc 使用 background 模式，就是我们平时说的 cms gc；当并发收集失败或者调用了 System.gc() 的时候，就会导致一次 full gc，这个 fullgc 是不是 cms 回收，而是 Serial 单线程回收器，加入了参数 -XX:ConcGCThreads&#x3D;4 后，执行 full gc 的时候，就变成了 CMS foreground gc，它是并行 full gc，只会执行 cms 中 stop the world 阶段的操作，效率比单线程 Serial full GC 要高；需要注意的是它只会回收 old，因为 cms 收集器是老年代收集器；而正常的 Serial 收集是包含整个堆的，加入了参数 &#x3D;&#x3D;-XX:+ExplicitGCInvokesConcurrent&#x3D;&#x3D;, 代表永久带也会被 cms 收集； 13.1.5、为什么 G1 用 SATB？CMS 用增量更新？ SATB 相对增量更新效率会高 (当然 SATB 可能造成更多的浮动垃圾)，因为不需要在重新标记阶段再次深度扫描被删除引用对象，而 CMS 对增量引用的根对象会做深度扫描，G1 因为很多对象都位于不同的 region，CMS 就一块老年代区域，重新深度扫描对象的话 G1 的代价会比 CMS 高，所以 G1 选择 SATB 不深度扫描对象，只是简单标记，等到下一轮 GC 再深度扫描。 13.1.6、漏标 - 读写屏障 (解决方案)13.1.6.1、增量更新 (Incremental Update)+ 写屏障 增量更新就是当黑色对象插入新的指向白色对象的引用关系时， 就将这个新插入的引用记录下来， 等并发扫描结束之后， 再将这些记录过的引用关系中的黑色对象为根， 重新扫描一次。这可以简化理解为， 黑色对象一旦新插入了指向白色对象的引用之后， 它就变回灰色对象了。 写屏障实现增量更新 当对象 A 的成员变量的引用发生变化时，比如新增引用（a.d &#x3D; d），我们可以利用写屏障，将 A 新的成员变量引用对象 D记录下来：void post_write_barrier(oop* field, oop new_value) {remark_set.add(new_value); &#x2F;&#x2F; 记录新引用的对象} 13.1.6.2、原始快照（Snapshot At The Beginning，SATB）+ 写屏障 原始快照就是当灰色对象要删除指向白色对象的引用关系时， 就将这个要删除的引用记录下来， 在并发扫描结束之后，再将这些记录过的引用关系中的灰色对象为根， 重新扫描一次，这样就能扫描到白色的对象，将白色对象直接标记为黑色 (目的就是让这种对象在本轮 gc 清理中能存活下来，待下一轮 gc 的时候重新扫描，这个对象也有可能是浮动垃圾) 以上无论是对引用关系记录的插入还是删除， 虚拟机的记录操作都是通过写屏障实现的。 写屏障实现 SATB 当对象 B 的成员变量的引用发生变化时，比如引用消失（a.b.d &#x3D; null），我们可以利用写屏障，将 B 原来成员变量的引用对象 D 记录下来：void pre_write_barrier(oop* field) {oop old_value &#x3D; *field; &#x2F;&#x2F; 获取旧值remark_set.add(old_value); &#x2F;&#x2F; 记录原来的引用对象} 13.1.6.3、并发标记时对漏标的处理方案 CMS：写屏障 + 增量更新G1，Shenandoah：写屏障 + SATBZGC：读屏障 工程实现中，读写屏障还有其他功能，比如写屏障可以用于记录跨代 &#x2F; 区引用的变化，读屏障可以用于支持移动对象的并发执行等。功能之外，还有性能的考虑，所以对于选择哪种，每款垃圾回收器都有自己的想法。 13.1.7、promotion failed这个异常发生在年轻带回收的时候；在进行 Minor GC 时，Survivor Space 放不下，对象只能放入老年代，而此时老年代也放不下造成的，多数是由于老年带有足够的空闲空间，但是由于碎片较多，新生代要转移到老年带的对象比较大, 找不到一段连续区域存放这个对象导致的， 13.1.8、过早提升和提升失败在 Minor GC 过程中，Survivor Unused 可能不足以容纳 Eden 和另一个 Survivor 中的存活对象， 那么多余的将被移到老年代， 称为过早提升（Premature Promotion）, 这会导致老年代中短期存活对象的增长， 可能会引发严重的性能问题。再进一步， 如果老年代满了， Minor GC 后会进行 Full GC， 这将导致遍历整个堆， 称为提升失败（Promotion Failure）。 13.1.9、早提升的原因 Survivor 空间太小，容纳不下全部的运行时短生命周期的对象，如果是这个原因，可以尝试将 Survivor 调大，否则端生命周期的对象提升过快，导致老年代很快就被占满，从而引起频繁的 full gc； 对象太大，Survivor 和 Eden 没有足够大的空间来存放这些大象； 13.1.10、提升失败原因当提升的时候，发现老年代也没有足够的连续空间来容纳该对象。为什么是没有足够的连续空间而不是空闲空间呢？老年代容纳不下提升的对象有两种情况： 老年代空闲空间不够用了； 老年代虽然空闲空间很多，但是碎片太多，没有连续的空闲空间存放该对象； 解决方法 如果是因为内存碎片导致的大对象提升失败，cms 需要进行空间整理压缩； 如果是因为提升过快导致的，说明 Survivor 空闲空间不足，那么可以尝试调大 Survivor； 如果是因为老年代空间不够导致的，尝试将 CMS 触发的阈值调低 13.2、G1定义：面向服务器的垃圾收集器, 主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC，停顿时间要求的同时, 还具备高吞吐量性能特征 13.2.1、运作流程 G1 将 Java 堆划分为多个大小相等的独立区域（Region），JVM 最多可以有 2048 个 Region。一般 Region 大小等于堆大小除以 2048，比如堆大小为 4096M，则 Region 大小为 2M，当然也可以用参数 “-XX:G1HeapRegionSize” 手动指定 Region 大小，但是推荐默认的计算方式。G1 保留了年轻代和老年代的概念，但不再是物理隔阂了，它们都是（可以不连续）Region 的集合。默认年轻代对堆内存的占比是 5%，如果堆大小为 4096M，那么年轻代占据 200MB 左右的内存，对应大概是 100 个 Region，可以通过 “-XX:G1NewSizePercent” 设置新生代初始占比，在系统运行中，JVM 会不停的给年轻代增加更多的 Region，但是最多新生代的占比不会超过 60%，可以通过 “-XX:G1MaxNewSizePercent” 调整。年轻代中的 Eden 和 Survivor 对应的 region 也跟之前一样，默认 8:1:1，假设年轻代现在有 1000 个 region，eden 区对应 800 个，s0 对应 100 个，s1 对应 100 个。一个 Region 可能之前是年轻代，如果 Region 进行了垃圾回收，之后可能又会变成老年代，也就是说 Region 的区域功能可能会动态变化。G1 垃圾收集器对于对象什么时候会转移到老年代跟之前讲过的原则一样，唯一不同的是对大对象的处理，G1 有专门分配大对象的 Region 叫 Humongous 区，而不是让大对象直接进入老年代的 Region 中。在 G1 中，大对象的判定规则就是一个大对象超过了一个 Region 大小的 50%，比如按照上面算的，每个 Region 是 2M，只要一个大对象超过了 1M，就会被放入 Humongous 中，而且一个大对象如果太大，可能会横跨多个 Region 来存放。 Humongous 区专门存放短期巨型对象，不用直接进老年代，可以节约老年代的空间，避免因为老年代空间不够的 GC 开销。Full GC 的时候除了收集年轻代和老年代之外，也会将 Humongous 区一并回收。 G1 收集器一次 GC 的运作过程大致分为以下 4 个步骤： 初始标记（initial mark，STW）：暂停所有的其他线程，并记录下 gc roots 直接能引用的对象，速度很快 ； 并发标记（Concurrent Marking）：同 CMS 的并发标记 最终标记（Remark，STW）：同 CMS 的重新标记 筛选回收（Cleanup，STW）：筛选回收阶段首先对各个 Region 的 &#x3D;&#x3D; 回收价值和成本进行排序，根据用户所期望的 GC 停顿时间 (可以用 JVM 参数 -XX:MaxGCPauseMillis 指定) 来制定回收计划，&#x3D;&#x3D; 比如说老年代此时有 1000 个 Region 都满了，但是因为根据预期停顿时间，本次垃圾回收可能只能停顿 200 毫秒，那么通过之前回收成本计算得知，可能回收其中 800 个 Region 刚好需要 200ms，那么就只会回收 800 个 Region(Collection Set，要回收的集合)，尽量把 GC 导致的停顿时间控制在我们指定的范围内。这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。不管是年轻代或是老年代，回收算法主要用的是复制算法，将一个 region 中的存活对象复制到另一个 region 中，这种不会像 CMS 那样回收完因为有很多内存碎片还需要整理一次，G1 采用复制算法回收几乎不会有太多内存碎片。(注意：CMS 回收阶段是跟用户线程一起并发执行的，G1 因为内部实现太复杂暂时没实现并发回收，不过到了 Shenandoah 就实现了并发收集，Shenandoah 可以看成是 G1 的升级版本) &#x3D;&#x3D;G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)，比如一个 Region 花 200ms 能回收 10M 垃圾，另外一个 Region 花 50ms 能回收 20M 垃圾，在回收时间有限情况下，G1 当然会优先选择后面这个 Region 回收。&#x3D;&#x3D; 这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率。 被视为 JDK1.7 以上版本 Java 虚拟机的一个重要进化特征。它具备以下特点： 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程来执行 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合：与 CMS 的 “标记–清理” 算法不同，G1 从整体来看是基于 “标记整理” 算法实现的收集器；从局部上来看是基于 “复制” 算法实现的。可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段 (通过参数 “-XX:MaxGCPauseMillis” 指定) 内完成垃圾收集。 毫无疑问， 可以由用户指定期望的停顿时间是 G1 收集器很强大的一个功能， 设置不同的期望停顿时间， 可使得 G1 在不同应用场景中取得关注吞吐量和关注延迟之间的最佳平衡。不过， 这里设置的 “期望值” 必须是符合实际的， 不能异想天开， 毕竟 G1 是要冻结用户线程来复制对象的， 这个停顿时间再怎么低也得有个限度。它默认的停顿目标为两百毫秒， 一般来说， 回收阶段占到几十到一百甚至接近两百毫秒都很正常， 但如果我们把停顿时间调得非常低， 譬如设置为二十毫秒， 很可能出现的结果就是由于停顿目标时间太短， 导致每次选出来的回收集只占堆内存很小的一部分， 收集器收集的速度逐渐跟不上分配器分配的速度， 导致垃圾慢慢堆积。很可能一开始收集器还能从空闲的堆内存中获得一些喘息的时间， 但应用运行时间一长就不行了， 最终占满堆引发 Full GC 反而降低性能， 所以通常把期望停顿时间设置为一两百毫秒或者两三百毫秒会是比较合理的。 13.2.2、Remembered Set（记录集）&#x2F;Card Table（卡表） 在新生代做 GCRoots 可达性扫描过程中可能会碰到跨代引用的对象，这种如果又去对老年代再去扫描效率太低了。为此，在新生代可以引入记录集（Remember Set）的数据结构（记录从非收集区到收集区的指针集合），避免把整个老年代加入 GCRoots 扫描范围。事实上并不只是新生代、 老年代之间才有跨代引用的问题， 所有涉及部分区域收集（Partial GC） 行为的垃圾收集器， 典型的如 G1、 ZGC 和 Shenandoah 收集器， 都会面临相同的问题。垃圾收集场景中，收集器只需通过记忆集判断出某一块非收集区域是否存在指向收集区域的指针即可，无需了解跨代引用指针的全部细节。hotspot 使用一种叫做 “卡表”(cardtable) 的方式实现记忆集，也是目前最常用的一种方式。关于卡表与记忆集的关系，可以类比为 Java 语言中 HashMap 与 Map 的关系。卡表是使用一个字节数组实现：CARD_TABLE[ ]，每个元素对应着其标识的内存区域一块特定大小的内存块，称为 “卡页”。HotSpot 使用的卡页是 2^9 大小，即 512 字节。 一个卡页中可包含多个对象，只要有一个对象的字段存在跨代指针，其对应的卡表的元素标识就变成 1，表示该元素变脏，否则为 0。GC 时，只要筛选本收集区的卡表中变脏的元素加入 GCRoots 里。 卡表如何维护？ 卡表变脏上面已经说了，但是需要知道如何让卡表变脏，即发生引用字段赋值时，如何更新卡表对应的标识为 1。Hotspot 使用写屏障维护卡表状态。 13.2.3、Collect Set Collect Set(CSet) 是指，在 Evacuation 阶段，由 G1 垃圾回收器选择的待回收的 Region 集合。G1 垃圾回收器的软实时的特性就是通过 CSet 的选择来实现的。对应于算法的两种模式 fully-young generational mode 和 partially-young mode，CSet 的选择可以分成两种： 在 fully-young generational mode 下：顾名思义，该模式下 CSet 将只包含 young 的 Region。G1 将调整 young 的 Region 的数量来匹配软实时的目标； 在 partially-young mode 下：该模式会选择所有的 young region，并且选择一部分的 old region。old region 的选择将依据在 Marking cycle phase 中对存活对象的计数。G1 选择存活对象最少的 Region 进行回收。 13.2.4、young gc 的完整流程 YoungGC 并不是说现有的 Eden 区放满了就会马上触发，G1 会计算下现在 Eden 区回收大概要多久时间，如果回收时间远远小于参数 -XX:MaxGCPauseMills 设定的值，那么增加年轻代的 region，继续给新对象存放，不会马上做 Young GC，直到下一次 Eden 区放满，G1 计算回收时间接近参数 -XX:MaxGCPauseMills 设定的值，那么就会触发 Young GC 13.2.5、Mixed GC 的完整流程 不是 FullGC，老年代的堆占有率达到参数 (-XX:InitiatingHeapOccupancyPercent) 设定的值则触发，回收所有的 Young 和部分 Old(根据期望的 GC 停顿时间确定 old 区垃圾收集的优先顺序)以及大对象区，正常情况 G1 的垃圾收集是先做 MixedGC，主要使用复制算法，需要把各个 region 中存活的对象拷贝到别的 region 里去，拷贝过程中如果发现没有足够的空 region 能够承载拷贝对象就会触发一次 Full GC 13.2.6、Full GC 停止系统程序，然后采用单线程进行标记、清理和压缩整理，好空闲出来一批 Region 来供下一次 MixedGC 使用，这个过程是非常耗时的。(Shenandoah 优化成多线程收集了) 13.2.7、Marking bitmaps&#x2F;TAMS Marking bitmap 是一种数据结构，其中的每一个 bit 代表的是一个可用于分配给对象的起始地址。举例来说： bitmap 其中 addrN 代表的是一个对象的起始地址。绿色的块代表的是在该起始地址处的对象是存活对象，而其余白色的块则代表了垃圾对象。G1 使用了两个 bitmap，一个叫做 previous bitmap，另外一个叫做 next bitmap。previous bitmap 记录的是上一次的标记阶段完成之后的构造的 bitmap；next bitmap 则是当前正在标记阶段正在构造的 bitmap。在当前标记阶段结束之后，当前标记的 next bitmap 就变成了下一次标记阶段的 previous bitmap。 TAMS(top at mark start) 变量，是一对用于区分在标记阶段新分配对象的变量，分别被称为 previous TAMS 和 next TAMS。在 previous TAMS 和 next TAMS 之间的对象则是本次标记阶段时候新分配的对象。如图： previous TMAS 和 next TAMS 白色 region 代表的是空闲空间，绿色 region 代表是存活对象，橙色 region 代表的在此次标记阶段新分配的对象。注意的是，在橙色区域的对象，并不能确保它们都事实上是存活的。 13.2.8、Pause Prediction Model 停顿预测模型，通过用户设定的 GC 停顿时间（参数 - XX:MaxGCPauseMillis），G1 以衰减平均值为理论基础，计算需要回收的 Region 数量从而进行满足。 13.2.9、G1 收集器参数设置1-XX:+UseG1GC:使用G1收集器 -XX:ParallelGCThreads:指定GC工作的线程数量 -XX:G1HeapRegionSize:指定分区大小(1MB~32MB，且必须是2的N次幂)，默认将整堆划分为2048个分区 -XX:MaxGCPauseMillis:目标暂停时间(默认200ms) -XX:G1NewSizePercent:新生代内存初始空间(默认整堆5%) -XX:G1MaxNewSizePercent:新生代内存最大空间 -XX:TargetSurvivorRatio:Survivor区的填充容量(默认50%)，Survivor区域里的一批对象(年龄1+年龄2+年龄n的多个年龄对象)总和超过了Survivor区域的50%，此时就会把年龄n(含)以上的对象都放入老年代 -XX:MaxTenuringThreshold:最大年龄阈值(默认15) -XX:InitiatingHeapOccupancyPercent:老年代占用空间达到整堆内存阈值(默认45%)，则执行新生代和老年代的混合收集(MixedGC)，比如我们之前说的堆默认有2048个region，如果有接近1000个region都是老年代的region，则可能就要触发MixedGC了 -XX:G1MixedGCLiveThresholdPercent(默认85%) region中的存活对象低于这个值时才会回收该region，如果超过这个值，存活对象过多，回收的的意义不大。 -XX:G1MixedGCCountTarget:在一次回收过程中指定做几次筛选回收(默认8次)，在最后一个筛选回收阶段可以回收一会，然后暂停回收，恢复系统运行，一会再开始回收，这样可以让系统不至于单次停顿时间过长。 -XX:G1HeapWastePercent(默认5%): gc过程中空出来的region是否充足阈值，在混合回收的时候，对Region回收都是基于复制算法进行的，都是把要回收的Region里的存活对象放入其他Region，然后这个Region中的垃圾对象全部清理掉，这样的话在回收过程就会不断空出来新的Region，一旦空闲出来的Region数量达到了堆内存的5%，此时就会立即停止混合回收，意味着本次混合回收就结束了。 13.2.10、G1 垃圾收集器优化建议（ -XX:MaxGCPauseMills&#x3D;50ms） 假设参数 -XX:MaxGCPauseMills 设置的值很大，导致系统运行很久，年轻代可能都占用了堆内存的 60% 了，此时才触发年轻代 gc。那么存活下来的对象可能就会很多，此时就会导致 Survivor 区域放不下那么多的对象，就会进入老年代中。 或者是你年轻代 gc 过后，存活下来的对象过多，导致进入 Survivor 区域后触发了动态年龄判定规则，达到了 Survivor 区域的 50%，也会快速导致一些对象进入老年代中。所以这里核心还是在于调节 -XX:MaxGCPauseMills 这个参数的值，在保证他的年轻代 gc 别太频繁的同时，还得考虑每次 gc 过后的存活对象有多少, 避免存活对象太多快速进入老年代，频繁触发 mixed gc. 13.2.11、什么场景适合使用 G1 50% 以上的堆被存活对象占用 对象分配和晋升的速度变化非常大 垃圾回收时间特别长，超过 1 秒 8GB 以上的堆内存 (建议值) 停顿时间是 500ms 以内 13.3、ZGC定义：具有实验性质的低延迟垃圾收集器 13.3.1、主要目标 支持 TB 量级的堆。我们生产环境的硬盘还没有上 TB 呢，这应该可以满足未来十年内，所有 JAVA 应用的需求了吧。 最大 GC 停顿时间不超 10ms。目前一般线上环境运行良好的 JAVA 应用 Minor GC 停顿时间在 10ms 左右，Major GC 一般都需要 100ms 以上（G1 可以调节停顿时间，但是如果调的过低的话，反而会适得其反），之所以能做到这一点是因为它的停顿时间主要跟 Root 扫描有关，而 Root 数量和堆大小是没有任何关系的。 奠定未来 GC 特性的基础。 最糟糕的情况下吞吐量会降低 15%。这都不是事，停顿时间足够优秀。至于吞吐量，通过扩容分分钟解决。另外，Oracle 官方提到了它最大的优点是：它的停顿时间不会随着堆的增大而增长！也就是说，几十 G 堆的停顿时间是 10ms 以下，几百 G 甚至上 T 堆的停顿时间也是 10ms 以下。 13.3.2、color poin（颜色指针） Colored Pointers，即颜色指针，如下图所示，ZGC 的核心设计之一。以前的垃圾回收器的 GC 信息都保存在对象头中，而 ZGC 的 GC 信息保存在指针中。 每个对象有一个 64 位指针，这 64 位被分为： 18 位：预留给以后使用； 1 位：Finalizable 标识，此位与并发引用处理有关，它表示这个对象只能通过 finalizer 才能访问； 1 位：Remapped 标识，设置此位的值后，对象未指向 relocation set 中（relocation set 表示需要 GC 的Region 集合）； 1 位：Marked1 标识； 1 位：Marked0 标识，和上面的 Marked1 都是标记对象用于辅助 GC； 42 位：对象的地址（所以它可以支持 2^42&#x3D;4T 内存）： 为什么有 2 个 mark 标记？ 每一个 GC 周期开始时，会交换使用的标记位，使上次 GC 周期中修正的已标记状态失效，所有引用都变成未标记。GC 周期 1：使用 mark0, 则周期结束所有引用 mark 标记都会成为 01。GC 周期 2：使用 mark1, 则期待的 mark 标记 10，所有引用都能被重新标记。通过对配置 ZGC 后对象指针分析我们可知，对象指针必须是 64 位，那么 ZGC 就无法支持 32 位操作系统，同样的也就无法支持压缩指针了（CompressedOops，压缩指针也是 32 位）。 颜色指针的三大优势： 一旦某个 Region 的存活对象被移走之后，这个 Region 立即就能够被释放和重用掉，而不必等待整个堆中所有指向该 Region 的引用都被修正后才能清理，这使得理论上只要还有一个空闲 Region，ZGC 就能完成收集。 颜色指针可以大幅减少在垃圾收集过程中内存屏障的使用数量，ZGC 只使用了读屏障。 颜色指针具备强大的扩展性，它可以作为一种可扩展的存储结构用来记录更多与对象标记、重定位过程相关的数据，以便日后进一步提高性能。 读屏障之前的 GC 都是采用 Write Barrier，这次 ZGC 采用了完全不同的方案读屏障，这个是 ZGC 一个非常重要的特性。在标记和移动对象的阶段，每次「从堆里对象的引用类型中读取一个指针」的时候，都需要加上一个 Load Barriers。那么我们该如何理解它呢？看下面的代码，第一行代码我们尝试读取堆中的一个对象引用 obj.fieldA 并赋给引用 o（fieldA 也是一个对象时才会加上读屏障）。如果这时候对象在 GC 时被移动了，接下来 JVM 就会加上一个读屏障，这个屏障会把读出的指针更新到对象的新地址上，并且把堆里的这个指针 “修正” 到原本的字段里。这样就算 GC 把对象移动了，读屏障也会发现并修正指针，于是应用代码就永远都会持有更新后的有效指针，而且不需要 STW。那么，JVM 是如何判断对象被移动过呢？就是利用上面提到的颜色指针，如果指针是 Bad Color，那么程序还不能往下执行，需要「slow path」，修正指针；如果指针是 Good Color，那么正常往下执行即可： ❝ 这个动作是不是非常像 JDK 并发中用到的 CAS 自旋？读取的值发现已经失效了，需要重新读取。而 ZGC 这里是之前持有的指针由于 GC 后失效了，需要通过读屏障修正指针。❞后面 3 行代码都不需要加读屏障：Object p &#x3D; o 这行代码并没有从堆中读取数据：o.doSomething() 也没有从堆中读取数据；obj.fieldB 不是对象引用，而是原子类型。正是因为 Load Barriers 的存在，所以会导致配置 ZGC 的应用的吞吐量会变低。官方的测试数据是需要多出额外 4% 的开销： 那么，判断对象是 Bad Color 还是 Good Color 的依据是什么呢？就是根据上一段提到的 Colored Pointers 的 4 个颜色位。 当加上读屏障时，根据对象指针中这 4 位的信息，就能知道当前对象是 Bad&#x2F;Good Color 了。 PS：既然低 42 位指针可以支持 4T 内存，那么能否通过预约更多位给对象地址来达到支持更大内存的目的呢？答案肯定是不可以。因为目前主板地址总线最宽只有 48bit，4 位是颜色位，就只剩 44 位了，所以受限于目前的硬件，ZGC 最大只能支持 16T 的内存，JDK13 就把最大支持堆内存从 4T 扩大到了 16T。 13.3.3、运作过程 并发标记（Concurrent Mark）：与 G1 一样，并发标记是遍历对象图做可达性分析的阶段，它的初始标记 (Mark Start) 和最终标记 (Mark End) 也会出现短暂的停顿，与 G1 不同的是， ZGC 的标记是在指针上而不是在对象上进行的， 标记阶段会更新染色指针中的 Marked 0、 Marked 1 标志位。 并发预备重分配（Concurrent Prepare for Relocate）：这个阶段需要根据特定的查询条件统计得出本次收集过程要清理哪些 Region，将这些 Region 组成重分配集（Relocation Set）。ZGC 每次回收都会扫描所有的 Region，用范围更大的扫描成本换取省去 G1 中记忆集的维护成本。 并发重分配（Concurrent Relocate）：重分配是 ZGC 执行过程中的核心阶段，这个过程要把重分配集中的存活对象复制到新的 Region 上，并为重分配集中的每个 Region 维护一个转发表（Forward Table），记录从旧对象到新对象的转向关系。ZGC 收集器能仅从引用上就明确得知一个对象是否处于重分配集之中，如果用户线程此时并发访问了位于重分配集中的对象，这次访问将会被预置的内存屏障 (读屏障) 所截获，然后立即根据 Region 上的转发表记录将访问转发到新复制的对象上，并同时修正更新该引用的值，使其直接指向新对象，ZGC 将这种行为称为指针的“自愈”（Self-Healing）能力。 1 ZGC的颜色指针因为“自愈”（Self‐Healing）能力，所以只有第一次访问旧对象会变慢， 一旦重分配集中某个Region的存活对象都复制完毕后，2 这个Region就可以立即释放用于新对象的分配，但是转发表还得留着不能释放掉， 因为可能还有访问在使用这个转发表。 并发重映射（Concurrent Remap）：重映射所做的就是修正整个堆中指向重分配集中旧对象的所有引用，但是 ZGC 中对象引用存在 “自愈” 功能，所以这个重映射操作并不是很迫切。ZGC 很巧妙地把并发重映射阶段要做的工作，合并到了下一次垃圾收集循环中的并发标记阶段里去完成，反正它们都是要遍历所有对象的，这样合并就节省了一次遍历对象图的开销。一旦所有指针都被修正之后， 原来记录新旧对象关系的转发表就可以释放掉了。 13.3.4、存在的问题，怎么解决 ZGC 最大的问题是浮动垃圾。ZGC 的停顿时间是在 10ms 以下，但是 ZGC 的执行时间还是远远大于这个时间的。假如 ZGC 全过程需要执行 10 分钟，在这个期间由于对象分配速率很高，将创建大量的新对象，这些对象很难进入当次 GC，所以只能在下次 GC 的时候进行回收，这些只能等到下次 GC 才能回收的对象就是浮动垃圾。ZGC 没有分代概念，每次都需要进行全堆扫描，导致一些 “朝生夕死” 的对象没能及时的被回收。 解决方案目前唯一的办法是增大堆的容量，使得程序得到更多的喘息时间，但是这个也是一个治标不治本的方案。如果需要从根本上解决这个问题，还是需要引入分代收集，让新生对象都在一个专门的区域中创建，然后专门针对这个区域进行更频繁、更快的收集。 13.3.5、安全点与安全区域 安全点就是指代码中一些特定的位置, 当线程运行到这些位置时它的状态是确定的, 这样 JVM 就可以安全的进行一些操作, 比如 GC 等，所以 GC 不是想什么时候做就立即触发的，是需要等待所有线程运行到安全点后才能触发。这些特定的安全点位置主要有以下几种: 方法返回之前 调用某个方法之后 抛出异常的位置 循环的末尾 大体实现思想是当垃圾收集需要中断线程的时候， 不直接对线程操作， 仅仅简单地设置一个标志位， 各个线程执行过程时会不停地主动去轮询这个标志， 一旦发现中断标志为真时就自己在最近的安全点上主动中断挂起。轮询标志的地方和安全点是重合的。 安全区域又是什么？ Safe Point 是对正在执行的线程设定的。如果一个线程处于 Sleep 或中断状态，它就不能响应 JVM 的中断请求，再运行到 Safe Point 上。因此 JVM 引入了 Safe Region。Safe Region 是指在一段代码片段中，引用关系不会发生变化。在这个区域内的任意地方开始 GC 都是安全的。 13.3.6、ZGC 参数 13.3.7、ZGC 触发时机ZGC 目前有 4 中机制触发 GC： 1、定时触发，默认为不使用，可通过 ZCollectionInterval 参数配置。2、预热触发，最多三次，在堆内存达到 10%、20%、30% 时触发，主要时统计 GC 时间，为其他 GC 机制使用。3、分配速率，基于正态分布统计，计算内存 99.9% 可能的最大分配速率，以及此速率下内存将要耗尽的时间点，在耗尽之前触发 GC（耗尽时间 - 一次 GC 最大持续时间 - 一次 GC 检测周期时间）。4、主动触发，（默认开启，可通过 ZProactive 参数配置） 距上次 GC 堆内存增长 10%，或超过 5 分钟时，对比距上次 GC 的间隔时间跟（49 * 一次 GC 的最大持续时间），超过则触发。 14、如何选择垃圾收集器 优先调整堆的大小让服务器自己来选择 如果内存小于 100M，使用串行收集器 如果是单核，并且没有停顿时间的要求，串行或 JVM 自己选择 如果允许停顿时间超过 1 秒，选择并行或者 JVM 自己选 如果响应时间最重要，并且不能超过 1 秒，使用并发收集器 4G 以下可以用 parallel，4-8G 可以用 ParNew+CMS，8G 以上可以用 G1，几百 G 以上用 ZGC JDK 1.8 默认使用 Parallel(年轻代和老年代都是)JDK 1.9 默认使用 G1 15、各种命令（例如 100%cpu 的排查、死锁的检查）15.1、100%CPU 的排查 1 、 使用 top 命令查看 cpu 占用资源较高的 PID 2、通过 jps 找到当前用户下的 java 程序 PID（jps -l 能够打印出所有的应用的 PID） 3、使用 pidstat -p 4、找到 cpu 占用较高的线程 TID 5、将 TID 转换为十六进制的表示方式 6、通过 jstack -l（使用 jstack 输出当前 PID 的线程 dunp 信息） 7、 查找 TID 对应的线程 (输出的线程 id 为十六进制)，找到对应的代码 15.2、死锁的检查 方法一、使用 jps + jstack 在 windons 命令窗口，使用 jps -l （找到运行的程序的 PID） 使用 jstack -l PID(上面的) 方法二：使用 jconsole 方法三：使用 Java Visual VM 16、JIT(即时编译器) JIT 是一种提高程序运行效率的方法。通常，程序有两种运行方式：静态编译与动态解释。静态编译的程序在执行前全部被翻译为机器码，而动态解释执行的则是一句一句边运行边翻译。 17、逃逸分析 逃逸分析是指在某个方法之内创建的对象，除了在方法体之内被引用之外，还在方法体之外被其它变量引用到；这样带来的后果是在该方法执行完毕之后，该方法中创建的对象将无法被 GC 回收，由于其被其它变量引用。正常的方法调用中，方法体中创建的对象将在执行完毕之后，将回收其中创建的对象；故由于无法回收，即成为逃逸。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"如何使用JDK自带工具JConsole","slug":"java/如何使用JDK自带工具JConsole","date":"2021-11-16T12:00:08.000Z","updated":"2022-03-23T09:03:57.685Z","comments":true,"path":"blog/java/如何使用JDK自带工具JConsole/","link":"","permalink":"http://sv.pointcut.cc/blog/java/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8JDK%E8%87%AA%E5%B8%A6%E5%B7%A5%E5%85%B7JConsole/","excerpt":"","text":"前言jconsole.exe为jdk自带的监控工具，操作简便，比较容易上手。jconsole可以实时的监控Java程序在运行过程中的内存，cpu，线程的使用情况，并可以对加载的相关类进行分析。下面开始一起看看如何使用jconsole吧！ 打开jconsole步骤： 1、首先进入到jdk的安装目录bin目录下；双击“jconsole.exe”打开jconsole 双击jconsole打开，并进入管理控制页面，如果本地有正在运行的Java进程的话会自动检测出来；这里分为监控本地进程或远程进程 另外可以通过菜单栏对内存、线程、类、VM、MBean进行更一步的监控 下面说下界面中的元素： 概述 ：记录了“堆内存使用情况”、“线程”、“类”、“CPU使用情况”共四个资源的实时情况； 内存 ：可以选择查看“堆内存使用情况”、“非堆内存使用情况”、“内存池”PS Eden Space””等内存占用的实时情况；界面右下角还有图形化的堆一级、二级、三级缓存（从左到右）占用情况，当然，如果三级缓存被全部占用也就是很可能内存溢出啦！这时可以去查看服务器的tomcat日志，应该会有“outofmemory”的异常日志信息。界面右上角处还提供了一个“执行GC”的手动垃圾收集功能，这个也很实用~而且界面下方还有详细的GC信息记录。 线程 ：界面上部显示实时线程数目。下部还能查看到详细的每个进程及相应状态、等待、堆栈追踪等信息； 类 ：显示“已装入类的数目”、“已卸载类的数目”信息； VM摘要 ：显示服务器详细资源信息，包括：线程、类、OS、内存等； MBean : 可在此页进行参数的配置。-——————–","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"java的基本数据类型占据的内存大小","slug":"java/java的基本数据类型占据的内存大小","date":"2021-11-16T12:00:07.000Z","updated":"2022-03-23T09:03:57.682Z","comments":true,"path":"blog/java/java的基本数据类型占据的内存大小/","link":"","permalink":"http://sv.pointcut.cc/blog/java/java%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%8D%A0%E6%8D%AE%E7%9A%84%E5%86%85%E5%AD%98%E5%A4%A7%E5%B0%8F/","excerpt":"","text":"","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"java数字二进制表示","slug":"java/java数字二进制表示","date":"2021-11-16T12:00:06.000Z","updated":"2022-03-23T09:03:57.681Z","comments":true,"path":"blog/java/java数字二进制表示/","link":"","permalink":"http://sv.pointcut.cc/blog/java/java%E6%95%B0%E5%AD%97%E4%BA%8C%E8%BF%9B%E5%88%B6%E8%A1%A8%E7%A4%BA/","excerpt":"","text":"十进制要理解整数的二进制，我们先来看下熟悉的十进制。十进制是如此的熟悉，我们可能已忽略了它的含义。比如123，我们不假思索就知道它的值是多少。 但其实123表示的1*(10^2) + 2*(10^1) + 3*(10^0)，(10^2表示10的二次方)，它表示的是各个位置数字含义之和，每个位置的数字含义与位置有关，从右向左，第一位乘以10的0次方， 即1，第二位乘以10的1次方，即10，第三位乘以10的2次方，即100，依次类推。 换句话说，每个位置都有一个位权，从右到左，第一位为1，然后依次乘以10，即第二位为10，第三位为100，依次类推。 正整数的二进制表示正整数的二进制表示与此类似， 只是在十进制中，每个位置可以有10个数字，从0到9，但在二进制中，每个位置只能是0或1。位权的概念是类似的，从右到左，第一位为1，然后依次乘以2，即第二位为2，第三位为4，依次类推。 看一些数字的例子吧： 二进制十进制1021131117101010 负整数的二进制表示十进制的负数表示就是在前面加一个负数符号-，例如-123。但二进制如何表示负数呢？其实概念是类似的，二进制使用最高位表示符号位，用1表示负数，用0表示正数。但哪个是最高位呢？整数有4种类型byte、short、int、long，分别占1、2、4、8个字节，即分别占8、16、32、64位，每种类型的符号位都是其最左边的一位。为方便举例，下面假定类型是byte，即从右到左的第8位表示符号位。 但负数表示不是简单地将最高位变为1，比如： byte a&#x3D;-1，如果只是将最高位变为1，二进制应该是10000001，但实际上，它应该是11111111。 byte a&#x3D;-127，如果只是将最高位变为1，二进制应该是11111111，但实际上，它却应该是10000001 和我们的直觉正好相反，这是什么表示法？这种表示法称为补码表示法，而符合我们直觉的表示称为原码表示法，补码表示就是在原码表示的基础上取反然后加1。取反就是将0变为1，1变为0。负数的二进制表示就是对应的正数的补码表示，比如： -1：1的原码表示是00000001，取反是11111110，然后再加1，就是11111111。 -2：2的原码表示是00000010，取反是11111101，然后再加1，就是11111110。 -127：127的原码表示是01111111，取反是10000000，然后再加1，就是10000001。 给定一个负数的二进制表示，要想知道它的十进制值，可以采用相同的补码运算。比如：10010010，首先取反，变为01101101，然后加1，结果为01101110，它的十进制值为110，所以原值就是-110。 直觉上，应该是先减1，然后再取反，但计算机只能做加法，而补码的一个良好特性就是，对负数的补码表示做补码运算就可以得到其对应正数的原码，正如十进制运算中负负得正一样。 对于byte类型，正数最大表示是01111111，即127，负数最小表示（绝对值最大）是10000000，即-128，表示范围就是-128～127。其他类型的整数也类似，负数能多表示一个数。 负整数为什么采用补码呢？负整数为什么要采用这种奇怪的表示形式呢？原因是，只有这种形式，计算机才能实现正确的加减法。 计算机其实只能做加法，1-1其实是1+(-1)。如果用原码表示，计算结果是不对的。比如说： 1 -&gt; 00000001-1 -&gt; 10000001+ ——————-2 -&gt; 10000010 用符合直觉的原码表示，1-1的结果是-2。 如果是补码表示： 1 -&gt; 00000001-1 -&gt; 11111111+ ——————0 -&gt; 00000000 结果是正确的。 再比如，5-3： 5 -&gt; 00000101-3 -&gt; 11111101+ ——————2 -&gt; 00000010 结果也是正确的。 就是这样的，看上去可能比较奇怪和难以理解，但这种表示其实是非常严谨和正确的，是不是很奇妙？ 理解了二进制加减法，我们就能理解为什么正数的运算结果可能出现负数了。当计算结果超出表示范围的时候，最高位往往是1，然后就会被看做负数。比如说，127+1： 127 -&gt; 011111111 -&gt; 00000001+ ——————-128 -&gt; 10000000 计算结果超出了byte的表示范围，会被看做-128 十六进制二进制写起来太长，为了简化写法，可以将四个二进制位简化为一个0到15的数，10到15用字符A到F表示，这种表示方法称为16进制，如下所示： 2进制 10进制 16进制 1010 10 A 1011 11 B 1100 12 C 1101 13 D 1110 14 E 1111 15 F 可以用16进制直接写常量数字，在数字前面加0x即可。比如10进制的123，用16进制表示是0x7B，即123 &#x3D; 7*16+11。给整数赋值或者进行运算的时候，都可以直接使用16进制，比如： 1int a = 0x7B; Java中不支持直接写二进制常量，比如，想写二进制形式的11001，Java中不能直接写，可以在前面补0，补足8位，为00011001，然后用16进制表示，即 0x19。 查看整数的二进制和十六进制表示在Java中，可以方便的使用Integer和Long的方法查看整数的二进制和十六进制表示，例如： 12345int a = 25;System.out.println(Integer.toBinaryString(a)); //二进制System.out.println(Integer.toHexString(a)); //十六进制System.out.println(Long.toBinaryString(a)); //二进制System.out.println(Long.toHexString(a)); //十六进制 位运算位运算是将数据看做二进制，进行位级别的操作，Java不能单独表示一个位，但是可以用byte表示8位，可以用16进制写二进制常量。比如： 0010表示成16进制是 0x2, 110110表示成16进制是 0x36。 位运算有移位运算和逻辑运算。 移位有： 左移：操作符为&lt;&lt;，向左移动，右边的低位补0，高位的就舍弃掉了，将二进制看做整数，左移1位就相当于乘以2。 无符号右移：操作符为&gt;&gt;&gt;，向右移动，右边的舍弃掉，左边补0。 有符号右移：操作符为&gt;&gt;，向右移动，右边的舍弃掉，左边补什么取决于原来最高位是什么，原来是1就补1，原来是0就补0，将二进制看做整数，右移1位相当于除以2。 例如： 123int a = 4; // 100a = a &gt;&gt; 2; // 001，等于1a = a &lt;&lt; 3 // 1000，变为8 逻辑运算有： 按位与 &amp;：两位都为1才为1 按位或 |：只要有一位为1，就为1 按位取反 ~： 1变为0，0变为1 按位异或 ^ ：相异为真，相同为假 大部分都比较简单，就不详细说了。具体形式，例如： 123int a = ...; a = a &amp; 0x1 // 返回0或1，就是a最右边一位的值。a = a | 0x1 //不管a原来最右边一位是什么，都将设为1 小数计算为什么会出错？违反直觉的事实计算机之所以叫“计算”机，就是因为发明它主要是用来计算的，“计算”当然是它的特长，在大家的印象中，计算一定是非常准确的。但实际上，即使在一些非常基本的小数运算中，计算的结果也是不精确的，比如： 12float f = 0.1f*0.1f;System.out.println(f); 这个结果看上去，应该是0.01，但实际上，屏幕输出却是0.010000001，后面多了个1。 看上去这么简单的运算，计算机怎么会出错了呢？ 简要答案实际上，不是运算本身会出错，而是计算机根本就不能精确的表示很多数，比如0.1这个数。 计算机是用一种二进制格式存储小数的，这个二进制格式不能精确表示0.1，它只能表示一个非常接近0.1但又不等于0.1的一个数。 数字都不能精确表示，在不精确数字上的运算结果不精确也就不足为奇了。 0.1怎么会不能精确表示呢？在十进制的世界里是可以的，但在二进制的世界里不行。在说二进制之前，我们先来看下熟悉的十进制。 实际上，十进制也只能表示那些可以表述为10的多少次方和的数，比如12.345，实际上表示的：1*10+2*1+3*0.1+4*0.01+5*0.001，与整数的表示类似，小数点后面的每个位置也都有一个位权，从左到右，依次为 0.1,0.01,0.001,…即10^(-1), 10^(-2), 10^(-3)。 很多数，十进制也是不能精确表示的，比如1&#x2F;3, 保留三位小数的话，十进制表示是0.333，但无论后面保留多少位小数，都是不精确的，用0.333进行运算，比如乘以3，期望结果是1，但实际上却是0.999。 二进制是类似的，但二进制只能表示那些可以表述为2的多少次方和的数，来看下2的次方的一些例子： 2的次方 十进制 2^(-1) 0.5 2^(-2) 0.25 2^(-3) 0.125 2^(-4) 0.0625 为什么一定要用二进制呢？为什么就不能用我们熟悉的十进制呢？在最最底层，计算机使用的电子元器件只能表示两个状态，通常是低压和高压，对应0和1，使用二进制容易基于这些电子器件构建硬件设备和进行运算。如果非要使用十进制，则这些硬件就会复杂很多，并且效率低下。 有什么有的小数计算是准确的其实，这只是Java语言给我们造成的假象，计算结果其实也是不精确的，但是由于结果和0.2足够接近，在输出的时候，Java选择了输出0.2这个看上去非常精简的数字，而不是一个中间有很多0的小数。 在误差足够小的时候，结果看上去是精确的，但不精确其实才是常态。 怎么处理计算不精确计算不精确，怎么办呢？大部分情况下，我们不需要那么高的精度，可以四舍五入，或者在输出的时候只保留固定个数的小数位。 如果真的需要比较高的精度，一种方法是将小数转化为整数进行运算，运算结束后再转化为小数，另外的方法一般是使用十进制的数据类型，这个没有统一的规范，在Java中是BigDecimal，运算更准确，但效率比较低。 二进制表示我们之前一直在用”小数”这个词表示float和double类型，其实，这是不严谨的，”小数”是在数学中用的词，在计算机中，我们一般说的是”浮点数”。float和double被称为浮点数据类型，小数运算被称为浮点运算。 为什么要叫浮点数呢？这是由于小数的二进制表示中，表示那个小数点的时候，点不是固定的，而是浮动的。 我们还是用10进制类比，10进制有科学表示法，比如123.45这个数，直接这么写，就是固定表示法，如果用科学表示法，在小数点前只保留一位数字，可以写为1.2345E2即1.2345*(10^2)，即在科学表示法中，小数点向左浮动了两位。 二进制中为表示小数，也采用类似的科学表示法，形如 m*(2^e)。m称为尾数，e称为指数。指数可以为正，也可以为负，负的指数表示那些接近0的比较小的数。在二进制中，单独表示尾数部分和指数部分，另外还有一个符号位表示正负。 几乎所有的硬件和编程语言表示小数的二进制格式都是一样的，这种格式是一个标准，叫做IEEE 754标准，它定义了两种格式，一种是32位的，对应于Java的float，另一种是64位的，对应于Java的double。 32位格式中，1位表示符号，23位表示尾数，8位表示指数。64位格式中，1位表示符号，52位表示尾数，11位表示指数。 在两种格式中，除了表示正常的数，标准还规定了一些特殊的二进制形式表示一些特殊的值，比如负无穷，正无穷，0，NaN (非数值，比如0乘以无穷大)。 如果你想查看浮点数的具体二进制形式，在Java中，可以使用如下代码： 12Integer.toBinaryString(Float.floatToIntBits(value))Long.toBinaryString(Double.doubleToLongBits(value));","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"javaSPI","slug":"java/javaSPI","date":"2021-11-16T12:00:05.000Z","updated":"2022-03-23T09:03:57.680Z","comments":true,"path":"blog/java/javaSPI/","link":"","permalink":"http://sv.pointcut.cc/blog/java/javaSPI/","excerpt":"","text":"在项目中，我们使用一个接口的实例时一般都是通过 1InfoService infoService = new InfoServiceBImpl(); 这种显示的方式实例化一个对象，这有个问题，如果想换个实例，就需要显示的修改。如果在一些分模块的项目中，一个为定义接口模块，一个为实现接口模块，一个为业务模块。如果遇到上面的情况，业务模块就需要显示的引入实例模块，那有没有一种机制，业务模块不需要显示的实例化对象，当业务发生变化时（换一个实现类）使这种变化只局限在实例模块中，业务模块不需要任何的修改？ SPI 全称 Service Provider Interface，是 Java 提供的一套用来被第三方实现或 者扩展的 API，它可以用来启用框架扩展和替换组件。 可以看到，SPI 的本质，其实是帮助程序，为某个特定的接口寻找它的实现类。而且哪些实现类的会加载，是个动态过程(不是提前预定好的)。 有点类似 IOC 的思想，就是将装配的控制权移到程序之外，在模块化设计中这个机制尤其重要。所以 SPI 的核心思想就是解耦。 要使用 Java SPI 服务提供者提供了接口的一种具体实现后，在jar包的META-INF&#x2F;services目录下创建一个以“接口全限定名”为命名的文件，内容为实现类的全限定名; 例如图1.1 SPI 的实现类必须携带一个不带参数的构造方法 接口实现类所在的 jar 包放在主程序的 classpath 中 主程序通过java.util.ServiceLoder动态装载实现模块，它通过扫描META-INF&#x2F;services目录下的配置文件找到实现类的全限定名，把类加载到JVM 以上图1.1为例： 12345678910111213/** * java spi机制验证 */@Testpublic void javaSPI() &#123; //服务加载器，加载实现类 ServiceLoader&lt;InfoService&gt; serviceLoader = ServiceLoader.load(InfoService.class); //serviceLoader是实现了Iterable的迭代器，直接遍历实现类 for (InfoService service : serviceLoader)&#123; Object result = service.sayHello(&quot;james&quot;);//依次调用实现类 &#125;&#125; java spi的局限java spi机制非常简单，就是读取指定的配置文件，将所有的类都加载到程序中。而这种机制，存在很多缺陷，比如： 所有实现类无论是否使用，直接被加载，可能存在浪费 不能够灵活，无法控制什么时候什么时机，匹配什么实现，功能太弱。 为了解决这些问题，dubbo和spring都增强了这套SPI机制。dubbo spispring spi","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"java幂等性","slug":"java/java幂等性","date":"2021-11-16T12:00:05.000Z","updated":"2022-03-23T09:31:57.281Z","comments":true,"path":"blog/java/java幂等性/","link":"","permalink":"http://sv.pointcut.cc/blog/java/java%E5%B9%82%E7%AD%89%E6%80%A7/","excerpt":"","text":"理解HTTP幂等性基于HTTP协议的Web API是时下最为流行的一种分布式服务提供方式。无论是在大型互联网应用还是企业级架构中，我们都见到了越来越多的SOA或RESTful的Web API。为什么Web API如此流行呢？我认为很大程度上应归功于简单有效的HTTP协议。HTTP协议是一种分布式的面向资源的网络应用层协议，无论是服务器端提供Web服务，还是客户端消费Web服务都非常简单。再加上浏览器、Javascript、AJAX、JSON以及HTML5等技术和工具的发展，互联网应用架构设计表现出了从传统的PHP、JSP、ASP.NET等服务器端动态网页向Web API + RIA（富互联网应用）过渡的趋势。Web API专注于提供业务服务，RIA专注于用户界面和交互设计，从此两个领域的分工更加明晰。在这种趋势下，Web API设计将成为服务器端程序员的必修课。然而，正如简单的Java语言并不意味着高质量的Java程序，简单的HTTP协议也不意味着高质量的Web API。要想设计出高质量的Web API，还需要深入理解分布式系统及HTTP协议的特性。 幂等性定义本文所要探讨的正是HTTP协议涉及到的一种重要性质：幂等性(Idempotence)。在HTTP&#x2F;1.1规范中幂等性的定义是： Methods can also have the property of “idempotence” in that (aside from error or expiration issues) the side-effects of N &gt; 0 identical requests is the same as for a single request. 从定义上看，HTTP方法的幂等性是指一次和多次请求某一个资源应该具有同样的副作用。幂等性属于语义范畴，正如编译器只能帮助检查语法错误一样，HTTP规范也没有办法通过消息格式等语法手段来定义它，这可能是它不太受到重视的原因之一。但实际上，幂等性是分布式系统设计中十分重要的概念，而HTTP的分布式本质也决定了它在HTTP中具有重要地位。 分布式事务 vs 幂等设计为什么需要幂等性呢？我们先从一个例子说起，假设有一个从账户取钱的远程API（可以是HTTP的，也可以不是），我们暂时用类函数的方式记为： 1bool withdraw(account_id, amount) withdraw的语义是从account_id对应的账户中扣除amount数额的钱；如果扣除成功则返回true，账户余额减少amount；如果扣除失败则返回false，账户余额不变。值得注意的是：和本地环境相比，我们不能轻易假设分布式环境的可靠性。一种典型的情况是withdraw请求已经被服务器端正确处理，但服务器端的返回结果由于网络等原因被掉丢了，导致客户端无法得知处理结果。如果是在网页上，一些不恰当的设计可能会使用户认为上一次操作失败了，然后刷新页面，这就导致了withdraw被调用两次，账户也被多扣了一次钱。如图1所示： 图1 这个问题的解决方案一是采用分布式事务，通过引入支持分布式事务的中间件来保证withdraw功能的事务性。分布式事务的优点是对于调用者很简单，复杂性都交给了中间件来管理。缺点则是一方面架构太重量级，容易被绑在特定的中间件上，不利于异构系统的集成；另一方面分布式事务虽然能保证事务的ACID性质，而但却无法提供性能和可用性的保证。 另一种更轻量级的解决方案是幂等设计。我们可以通过一些技巧把withdraw变成幂等的，比如： 1int create_ticket() bool idempotent_withdraw(ticket_id, account_id, amount) create_ticket的语义是获取一个服务器端生成的唯一的处理号ticket_id，它将用于标识后续的操作。idempotent_withdraw和withdraw的区别在于关联了一个ticket_id，一个ticket_id表示的操作至多只会被处理一次，每次调用都将返回第一次调用时的处理结果。这样，idempotent_withdraw就符合幂等性了，客户端就可以放心地多次调用。 基于幂等性的解决方案中一个完整的取钱流程被分解成了两个步骤：1.调用create_ticket()获取ticket_id；2.调用idempotent_withdraw(ticket_id, account_id, amount)。虽然create_ticket不是幂等的，但在这种设计下，它对系统状态的影响可以忽略，加上idempotent_withdraw是幂等的，所以任何一步由于网络等原因失败或超时，客户端都可以重试，直到获得结果。如图2所示： 图2 和分布式事务相比，幂等设计的优势在于它的轻量级，容易适应异构环境，以及性能和可用性方面。在某些性能要求比较高的应用，幂等设计往往是唯一的选择。 HTTP的幂等性HTTP协议本身是一种面向资源的应用层协议，但对HTTP协议的使用实际上存在着两种不同的方式：一种是RESTful的，它把HTTP当成应用层协议，比较忠实地遵守了HTTP协议的各种规定；另一种是SOA的，它并没有完全把HTTP当成应用层协议，而是把HTTP协议作为了传输层协议，然后在HTTP之上建立了自己的应用层协议。本文所讨论的HTTP幂等性主要针对RESTful风格的，不过正如上一节所看到的那样，幂等性并不属于特定的协议，它是分布式系统的一种特性；所以，不论是SOA还是RESTful的Web API设计都应该考虑幂等性。下面将介绍HTTP GET、DELETE、PUT、POST四种主要方法的语义和幂等性。 HTTP GET方法用于获取资源，不应有副作用，所以是幂等的。比如：GET http://www.bank.com/account/123456，不会改变资源的状态，不论调用一次还是N次都没有副作用。请注意，这里强调的是一次和N次具有相同的副作用，而不是每次GET的结果相同。GET http://www.news.com/latest-news这个HTTP请求可能会每次得到不同的结果，但它本身并没有产生任何副作用，因而是满足幂等性的。 HTTP DELETE方法用于删除资源，有副作用，但它应该满足幂等性。比如：DELETE http://www.forum.com/article/4231，调用一次和N次对系统产生的副作用是相同的，即删掉id为4231的帖子；因此，调用者可以多次调用或刷新页面而不必担心引起错误。 比较容易混淆的是HTTP POST和PUT。POST和PUT的区别容易被简单地误认为“POST表示创建资源，PUT表示更新资源”；而实际上，二者均可用于创建资源，更为本质的差别是在幂等性方面。在HTTP规范中对POST和PUT是这样定义的： The POST method is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line …… If a resource has been created on the origin server, the response SHOULD be 201 (Created) and contain an entity which describes the status of the request and refers to the new resource, and a Location header. The PUT method requests that the enclosed entity be stored under the supplied Request-URI. If the Request-URI refers to an already existing resource, the enclosed entity SHOULD be considered as a modified version of the one residing on the origin server. If the Request-URI does not point to an existing resource, and that URI is capable of being defined as a new resource by the requesting user agent, the origin server can create the resource with that URI. POST所对应的URI并非创建的资源本身，而是资源的接收者。比如：POST http://www.forum.com/articles的语义是在http://www.forum.com/articles下创建一篇帖子，HTTP响应中应包含帖子的创建状态以及帖子的URI。两次相同的POST请求会在服务器端创建两份资源，它们具有不同的URI；所以，POST方法不具备幂等性。而PUT所对应的URI是要创建或更新的资源本身。比如：PUT http://www.forum/articles/4231的语义是创建或更新ID为4231的帖子。对同一URI进行多次PUT的副作用和一次PUT是相同的；因此，PUT方法具有幂等性。 在介绍了几种操作的语义和幂等性之后，我们来看看如何通过Web API的形式实现前面所提到的取款功能。很简单，用POST &#x2F;tickets来实现create_ticket；用PUT &#x2F;accounts&#x2F;account_id&#x2F;ticket_id&amp;amount&#x3D;xxx来实现idempotent_withdraw。值得注意的是严格来讲amount参数不应该作为URI的一部分，真正的URI应该是&#x2F;accounts&#x2F;account_id&#x2F;ticket_id，而amount应该放在请求的body中。这种模式可以应用于很多场合，比如：论坛网站中防止意外的重复发帖。 总结上面简单介绍了幂等性的概念，用幂等设计取代分布式事务的方法，以及HTTP主要方法的语义和幂等性特征。其实，如果要追根溯源，幂等性是数学中的一个概念，表达的是N次变换与1次变换的结果相同，有兴趣的读者可以从Wikipedia上进一步了解。 上面的概括1、何为幂等性？ 幂等性即为无论请求几次，最后的结果都是一样的。 2、幂等性在什么地方做控制？ 一般在分布式系统中，所谓的分布式，即指同时发起多个并行请求（应该是这样的吧），数据可能会被同时操作同样的操作。这样的话如果不做好控制，数据容易被篡改，变成各种奇怪的脏数据。现在很多的文章例子都用付钱这个例子来举例说明。付钱的时候，客户的请求到服务端，此时是否真的数据被修改我们并不得知，如果系统出现缓慢或者其他情况，可能同一种操作就被请求了多次，这样钱可能被扣了多次。所以我们必须对数据进行控制，防止出现数据丢失或者数据脏乱。 3、怎么做幂等性？ 幂等性可以从数据出发。 A、可以利用数据处理中创建一个唯一识别码，达到每次处理数据时首先去对比这个识别码是否重复。 B、请求时通过redis加锁做控制。 C、数据库里可以加锁，乐观锁，悲观锁（这两个锁后续展开） 如果有其他的方法后续补充。 4、关于http的幂等性？ get为查询，所以为幂等；put为修改，为幂等；delete为删除，为幂等；post为增加或修改等，不为幂等。 两次相同的POST请求会在服务器端创建两份资源，它们具有不同的URI；所以，POST方法不具备幂等性（来自参考文章）。 最后：写的比较仓促，总结就是，在代码设计中需要考虑数据的幂等性等。不能造成数据多次被处理，如果数据不幂等了该怎么处理，可以通过事务或者捕捉错误来进行回滚操作等。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"java 幂等性","slug":"java/java 幂等性","date":"2021-11-16T12:00:04.000Z","updated":"2022-03-23T09:03:57.679Z","comments":true,"path":"blog/java/java 幂等性/","link":"","permalink":"http://sv.pointcut.cc/blog/java/java%20%E5%B9%82%E7%AD%89%E6%80%A7/","excerpt":"","text":"理解HTTP幂等性基于HTTP协议的Web API是时下最为流行的一种分布式服务提供方式。无论是在大型互联网应用还是企业级架构中，我们都见到了越来越多的SOA或RESTful的Web API。为什么Web API如此流行呢？我认为很大程度上应归功于简单有效的HTTP协议。HTTP协议是一种分布式的面向资源的网络应用层协议，无论是服务器端提供Web服务，还是客户端消费Web服务都非常简单。再加上浏览器、Javascript、AJAX、JSON以及HTML5等技术和工具的发展，互联网应用架构设计表现出了从传统的PHP、JSP、ASP.NET等服务器端动态网页向Web API + RIA（富互联网应用）过渡的趋势。Web API专注于提供业务服务，RIA专注于用户界面和交互设计，从此两个领域的分工更加明晰。在这种趋势下，Web API设计将成为服务器端程序员的必修课。然而，正如简单的Java语言并不意味着高质量的Java程序，简单的HTTP协议也不意味着高质量的Web API。要想设计出高质量的Web API，还需要深入理解分布式系统及HTTP协议的特性。 幂等性定义本文所要探讨的正是HTTP协议涉及到的一种重要性质：幂等性(Idempotence)。在HTTP&#x2F;1.1规范中幂等性的定义是： Methods can also have the property of “idempotence” in that (aside from error or expiration issues) the side-effects of N &gt; 0 identical requests is the same as for a single request. 从定义上看，HTTP方法的幂等性是指一次和多次请求某一个资源应该具有同样的副作用。幂等性属于语义范畴，正如编译器只能帮助检查语法错误一样，HTTP规范也没有办法通过消息格式等语法手段来定义它，这可能是它不太受到重视的原因之一。但实际上，幂等性是分布式系统设计中十分重要的概念，而HTTP的分布式本质也决定了它在HTTP中具有重要地位。 分布式事务 vs 幂等设计为什么需要幂等性呢？我们先从一个例子说起，假设有一个从账户取钱的远程API（可以是HTTP的，也可以不是），我们暂时用类函数的方式记为： 1bool withdraw(account_id, amount) withdraw的语义是从account_id对应的账户中扣除amount数额的钱；如果扣除成功则返回true，账户余额减少amount；如果扣除失败则返回false，账户余额不变。值得注意的是：和本地环境相比，我们不能轻易假设分布式环境的可靠性。一种典型的情况是withdraw请求已经被服务器端正确处理，但服务器端的返回结果由于网络等原因被掉丢了，导致客户端无法得知处理结果。如果是在网页上，一些不恰当的设计可能会使用户认为上一次操作失败了，然后刷新页面，这就导致了withdraw被调用两次，账户也被多扣了一次钱。如图1所示： 图1 这个问题的解决方案一是采用分布式事务，通过引入支持分布式事务的中间件来保证withdraw功能的事务性。分布式事务的优点是对于调用者很简单，复杂性都交给了中间件来管理。缺点则是一方面架构太重量级，容易被绑在特定的中间件上，不利于异构系统的集成；另一方面分布式事务虽然能保证事务的ACID性质，而但却无法提供性能和可用性的保证。 另一种更轻量级的解决方案是幂等设计。我们可以通过一些技巧把withdraw变成幂等的，比如： 1int create_ticket() bool idempotent_withdraw(ticket_id, account_id, amount) create_ticket的语义是获取一个服务器端生成的唯一的处理号ticket_id，它将用于标识后续的操作。idempotent_withdraw和withdraw的区别在于关联了一个ticket_id，一个ticket_id表示的操作至多只会被处理一次，每次调用都将返回第一次调用时的处理结果。这样，idempotent_withdraw就符合幂等性了，客户端就可以放心地多次调用。 基于幂等性的解决方案中一个完整的取钱流程被分解成了两个步骤：1.调用create_ticket()获取ticket_id；2.调用idempotent_withdraw(ticket_id, account_id, amount)。虽然create_ticket不是幂等的，但在这种设计下，它对系统状态的影响可以忽略，加上idempotent_withdraw是幂等的，所以任何一步由于网络等原因失败或超时，客户端都可以重试，直到获得结果。如图2所示： 图2 和分布式事务相比，幂等设计的优势在于它的轻量级，容易适应异构环境，以及性能和可用性方面。在某些性能要求比较高的应用，幂等设计往往是唯一的选择。 HTTP的幂等性HTTP协议本身是一种面向资源的应用层协议，但对HTTP协议的使用实际上存在着两种不同的方式：一种是RESTful的，它把HTTP当成应用层协议，比较忠实地遵守了HTTP协议的各种规定；另一种是SOA的，它并没有完全把HTTP当成应用层协议，而是把HTTP协议作为了传输层协议，然后在HTTP之上建立了自己的应用层协议。本文所讨论的HTTP幂等性主要针对RESTful风格的，不过正如上一节所看到的那样，幂等性并不属于特定的协议，它是分布式系统的一种特性；所以，不论是SOA还是RESTful的Web API设计都应该考虑幂等性。下面将介绍HTTP GET、DELETE、PUT、POST四种主要方法的语义和幂等性。 HTTP GET方法用于获取资源，不应有副作用，所以是幂等的。比如：GET http://www.bank.com/account/123456，不会改变资源的状态，不论调用一次还是N次都没有副作用。请注意，这里强调的是一次和N次具有相同的副作用，而不是每次GET的结果相同。GET http://www.news.com/latest-news这个HTTP请求可能会每次得到不同的结果，但它本身并没有产生任何副作用，因而是满足幂等性的。 HTTP DELETE方法用于删除资源，有副作用，但它应该满足幂等性。比如：DELETE http://www.forum.com/article/4231，调用一次和N次对系统产生的副作用是相同的，即删掉id为4231的帖子；因此，调用者可以多次调用或刷新页面而不必担心引起错误。 比较容易混淆的是HTTP POST和PUT。POST和PUT的区别容易被简单地误认为“POST表示创建资源，PUT表示更新资源”；而实际上，二者均可用于创建资源，更为本质的差别是在幂等性方面。在HTTP规范中对POST和PUT是这样定义的： The POST method is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line …… If a resource has been created on the origin server, the response SHOULD be 201 (Created) and contain an entity which describes the status of the request and refers to the new resource, and a Location header. The PUT method requests that the enclosed entity be stored under the supplied Request-URI. If the Request-URI refers to an already existing resource, the enclosed entity SHOULD be considered as a modified version of the one residing on the origin server. If the Request-URI does not point to an existing resource, and that URI is capable of being defined as a new resource by the requesting user agent, the origin server can create the resource with that URI. POST所对应的URI并非创建的资源本身，而是资源的接收者。比如：POST http://www.forum.com/articles的语义是在http://www.forum.com/articles下创建一篇帖子，HTTP响应中应包含帖子的创建状态以及帖子的URI。两次相同的POST请求会在服务器端创建两份资源，它们具有不同的URI；所以，POST方法不具备幂等性。而PUT所对应的URI是要创建或更新的资源本身。比如：PUT http://www.forum/articles/4231的语义是创建或更新ID为4231的帖子。对同一URI进行多次PUT的副作用和一次PUT是相同的；因此，PUT方法具有幂等性。 在介绍了几种操作的语义和幂等性之后，我们来看看如何通过Web API的形式实现前面所提到的取款功能。很简单，用POST &#x2F;tickets来实现create_ticket；用PUT &#x2F;accounts&#x2F;account_id&#x2F;ticket_id&amp;amount&#x3D;xxx来实现idempotent_withdraw。值得注意的是严格来讲amount参数不应该作为URI的一部分，真正的URI应该是&#x2F;accounts&#x2F;account_id&#x2F;ticket_id，而amount应该放在请求的body中。这种模式可以应用于很多场合，比如：论坛网站中防止意外的重复发帖。 总结上面简单介绍了幂等性的概念，用幂等设计取代分布式事务的方法，以及HTTP主要方法的语义和幂等性特征。其实，如果要追根溯源，幂等性是数学中的一个概念，表达的是N次变换与1次变换的结果相同，有兴趣的读者可以从Wikipedia上进一步了解。 上面的概括1、何为幂等性？ 幂等性即为无论请求几次，最后的结果都是一样的。 2、幂等性在什么地方做控制？ 一般在分布式系统中，所谓的分布式，即指同时发起多个并行请求（应该是这样的吧），数据可能会被同时操作同样的操作。这样的话如果不做好控制，数据容易被篡改，变成各种奇怪的脏数据。现在很多的文章例子都用付钱这个例子来举例说明。付钱的时候，客户的请求到服务端，此时是否真的数据被修改我们并不得知，如果系统出现缓慢或者其他情况，可能同一种操作就被请求了多次，这样钱可能被扣了多次。所以我们必须对数据进行控制，防止出现数据丢失或者数据脏乱。 3、怎么做幂等性？ 幂等性可以从数据出发。 A、可以利用数据处理中创建一个唯一识别码，达到每次处理数据时首先去对比这个识别码是否重复。 B、请求时通过redis加锁做控制。 C、数据库里可以加锁，乐观锁，悲观锁（这两个锁后续展开） 如果有其他的方法后续补充。 4、关于http的幂等性？ get为查询，所以为幂等；put为修改，为幂等；delete为删除，为幂等；post为增加或修改等，不为幂等。 两次相同的POST请求会在服务器端创建两份资源，它们具有不同的URI；所以，POST方法不具备幂等性（来自参考文章）。 最后：写的比较仓促，总结就是，在代码设计中需要考虑数据的幂等性等。不能造成数据多次被处理，如果数据不幂等了该怎么处理，可以通过事务或者捕捉错误来进行回滚操作等。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"java反射getDeclaredMethod与getMethod的区别","slug":"java/java反射getDeclaredMethod与getMethod的区别","date":"2021-11-16T12:00:04.000Z","updated":"2022-03-23T09:31:57.271Z","comments":true,"path":"blog/java/java反射getDeclaredMethod与getMethod的区别/","link":"","permalink":"http://sv.pointcut.cc/blog/java/java%E5%8F%8D%E5%B0%84getDeclaredMethod%E4%B8%8EgetMethod%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Method getDeclaredMethod(String name, Class… parameterTypes)d 返回一个 Method 对象，该对象反映此 Class 对象所表示的类或接口的指定已声明方法(可以获取私有)。 Method[] getDeclaredMethods() 返回 Method 对象的一个数组，这些对象反映此 Class 对象表示的类或接口声明的所有方法，包括公共、保护、默认（包）访问和私有方法，但不包括继承的方法(如果继承方法在子类中实现,就可以获得到) Method getMethod(String name, Class… parameterTypes) 返回一个 Method 对象，它反映此 Class 对象所表示的类或接口的指定公共成员方法(不包括私有,但可以获得父类中未被继承的方法)。 Method[] getMethods() 返回一个包含某些 Method 对象的数组，这些对象反映此 Class 对象所表示的类或接口（包括那些由该类或接口声明的以及从超类和超接口继承的那些的类或接口）的公共 member 方法。 Field getDeclaredField(String name) 返回一个 Field 对象，该对象反映此 Class 对象所表示的类或接口的指定已声明字段。 Field[] getDeclaredFields() 返回 Field 对象的一个数组，这些对象反映此 Class 对象所表示的类或接口所声明的所有字段，包括公共、保护、默认（包）访问和私有字段，但不包括继承的字段。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"java 反射getDeclaredMethod与getMethod的区别","slug":"java/java 反射getDeclaredMethod与getMethod的区别","date":"2021-11-16T12:00:03.000Z","updated":"2022-03-23T09:03:57.677Z","comments":true,"path":"blog/java/java 反射getDeclaredMethod与getMethod的区别/","link":"","permalink":"http://sv.pointcut.cc/blog/java/java%20%E5%8F%8D%E5%B0%84getDeclaredMethod%E4%B8%8EgetMethod%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Method getDeclaredMethod(String name, Class… parameterTypes)d 返回一个 Method 对象，该对象反映此 Class 对象所表示的类或接口的指定已声明方法(可以获取私有)。 Method[] getDeclaredMethods() 返回 Method 对象的一个数组，这些对象反映此 Class 对象表示的类或接口声明的所有方法，包括公共、保护、默认（包）访问和私有方法，但不包括继承的方法(如果继承方法在子类中实现,就可以获得到) Method getMethod(String name, Class… parameterTypes) 返回一个 Method 对象，它反映此 Class 对象所表示的类或接口的指定公共成员方法(不包括私有,但可以获得父类中未被继承的方法)。 Method[] getMethods() 返回一个包含某些 Method 对象的数组，这些对象反映此 Class 对象所表示的类或接口（包括那些由该类或接口声明的以及从超类和超接口继承的那些的类或接口）的公共 member 方法。 Field getDeclaredField(String name) 返回一个 Field 对象，该对象反映此 Class 对象所表示的类或接口的指定已声明字段。 Field[] getDeclaredFields() 返回 Field 对象的一个数组，这些对象反映此 Class 对象所表示的类或接口所声明的所有字段，包括公共、保护、默认（包）访问和私有字段，但不包括继承的字段。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"ThreadLocal的内存泄露分析","slug":"java/ThreadLocal的内存泄露分析","date":"2021-11-16T12:00:02.000Z","updated":"2022-03-23T09:03:57.677Z","comments":true,"path":"blog/java/ThreadLocal的内存泄露分析/","link":"","permalink":"http://sv.pointcut.cc/blog/java/ThreadLocal%E7%9A%84%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2%E5%88%86%E6%9E%90/","excerpt":"","text":"ThreadLocal的实现原理，每一个Thread维护一个ThreadLocalMap，key为使用弱引用的ThreadLocal实例，value为线程变量的副本。这些对象之间的引用关系如下 ThreadLocal 是基于 ThreadLocalMap 实现的，这个 Map 的 Entry 继承了 WeakReference，而 Entry 对象中的 key 使用了 WeakReference 封装，也就是说 Entry 中的 key 是一个弱引用类型，而弱引用类型只能存活在下次 GC 之前。 当把 threadlocal 变量置为 null 以后，没有任何强引用指向 threadlocal 实例，所以 threadlocal 将会被 gc 回收。 当发生一次垃圾回收，ThreadLocalMap 中就会出现 key 为 null 的 Entry，就没有办法访问这些 key 为 null 的 Entry 的 value，如果当前线程再迟迟不结束的 话(肯定不会结束)，这些 key 为 null 的 Entry 的 value 就会一直存在一条强引用链:Thread Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value，而这块 value 永远不会被访问到了，所以存在着内存泄露。如下图: 只有当前 thread 结束以后，current thread 就不会存在栈中，强引用断开，Current Thread、Map value 将全部被 GC 回收(但是这 种情况很难)。最好的做法是不在需要使用 ThreadLocal 变量后，都调用它的 remove()方法，清除数据。","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"FileChannel","slug":"java/FileChannel","date":"2021-11-16T12:00:01.000Z","updated":"2022-03-23T09:03:57.667Z","comments":true,"path":"blog/java/FileChannel/","link":"","permalink":"http://sv.pointcut.cc/blog/java/FileChannel/","excerpt":"","text":"顾名思义，FileChannel就是连接到文件的Channel。使用FileChannel，你可以读取文件数据，以及往文件里面写入数据。Java NIO的FileChannel是使用标准Java IO读取文件的一种替代方案。 FileChannel不能被设置非阻塞模式，它总是以阻塞模式运行。 Opening a FileChannel在你使用FileChannel前，你必须先打开它。你不能直接打开一个FileChannel。你必须通过一个InputStream、OutputStream或者RandomAccessFile来获得一个FileChannel。以下是如何通过一个RandomAccessFile来获得FileChannel的示例： 12RandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;);FileChannel inChannel = aFile.getChannel(); Reading Data from a FileChannel为了从FileChannel读取数据，你需要调用其read()方法。如下： 123ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf); 首先分配一个Buffer，从FileChannel读取的数据将被读入Buffer。 然后调用FileChannel.read()方法。这个方法将数据从FileChannel读入Buffer。read()方法返回的int代表着有多少数据被读入了Buffer。如果返回-1，代表着已经读到文件结尾。 Writing Data to a FileChannel要将数据写入FileChannel，你需要调用带有一个Buffer类型参数的FileChannel.write()方法。如下： 1234567891011String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();while (buf.hasRemaining()) &#123; channel.write(buf);&#125; 注意在while-循环里面调用的FileChannel.write()方法。我们无法保证每次write()方法向Channel写了多少字节。因此我们重复调用write()直至没有数据可写为止。 Closing a FileChannel当时使用完FileChannel后，你必须关闭它。如下： 1channel.close(); FileChannel Position当读取或写入FileChannel时，需要在特定position执行。你可以通过调用position()方法来获得FileChannel的当前position。 你还可以通过调用position(long pos)来设置FileChannel的position。 以下是两个例子： 12long pos = channel.position();channel.position(pos + 123); 如果你设置的position超过了文件的大小，并且尝试从Channel读取数据，则会返回-1代表文件结尾。 如果你设置的position超过了文件的大小，并且尝试往Channel写入数据，文件会自动扩张至能放下position以及写入的数据。这个可能导致”file hole”，即磁盘上的物理文件在写入的数据中存在漏洞（即中间有一段完全没有任何数据）。 FileChannel SizeFileChannel的size()方法返回这个Channel连接的文件大小。如下： 1long fileSize = channel.size(); FileChannel Truncate通过调用FileChannel.truncate()方法，你可以truncate一个文件。当你truncate一个文件，你会把其截断为指定的长度。如下： 1channel.truncate(1024); 这个例子将文件截断为1024字节。 FileChannel ForceFileChannel.force()方法会将Channel里面还未写入的数据全部刷新到磁盘。操作系统可能会将数据缓存在内存里以提升性能，因此我们无法保证你写入Channel的数据都被写到了磁盘，直到你调用force()方法。 force()方法有一个boolean类型的参数，代表是否将文件元数据（比如权限等）也刷新到磁盘。 以下是刷新数据以及元数据到磁盘的例子： 1channel.force(true);","categories":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"}]},{"title":"使用acme生成证书","slug":"nginx/使用acme生成证书","date":"2021-11-15T12:00:07.000Z","updated":"2022-03-23T09:03:57.522Z","comments":true,"path":"blog/nginx/使用acme生成证书/","link":"","permalink":"http://sv.pointcut.cc/blog/nginx/%E4%BD%BF%E7%94%A8acme%E7%94%9F%E6%88%90%E8%AF%81%E4%B9%A6/","excerpt":"","text":"安装123curl https://get.acme.sh | sh #或者wget -O - https://freessl.cn/api/get.acme.sh | sh 生成证书acme.sh 实现了 acme 协议支持的所有验证协议. 一般有两种方式验证: http 和 dns 验证。 HTTP 方式http 方式需要在你的网站根目录下放置一个文件, 来验证你的域名所有权,完成验证. 然后就可以生成证书了. 1acme.sh --issue -d clsn.io -d *.clsn.io --webroot /www/wwwroot/clsn.io/ 只需要指定域名, 并指定域名所在的网站根目录. acme.sh 会全自动的生成验证文件, 并放到网站的根目录, 然后自动完成验证. 最后会聪明的删除验证文件. 整个过程没有任何副作用.如果你用的 apache服务器, acme.sh 还可以智能的从 apache的配置中自动完成验证, 你不需要指定网站根目录: 1acme.sh --issue -d clsn.io --clsn.io 如果你用的 nginx服务器, 或者反代, acme.sh 还可以智能的从 nginx的配置中自动完成验证, 你不需要指定网站根目录: 1acme.sh --issue -d clsn.io --nginx 注意, 无论是 apache 还是 nginx 模式, acme.sh在完成验证之后, 会恢复到之前的状态, 都不会私自更改你本身的配置. 好处是你不用担心配置被搞坏。该类型的配置有一个缺点, 你需要自己配置 ssl 的配置, 否则只能成功生成证书, 你的网站还是无法访问https. 但是为了安全, 你还是自己手动改配置吧.如果你还没有运行任何 web 服务, 80 端口是空闲的, 那么 acme.sh 还能假装自己是一个webserver, 临时听在80 端口, 完成验证: 1acme.sh --issue -d clsn.io --standalone 更高级的用法请参考: https://github.com/Neilpang/acme.sh/wiki/How-to-issue-a-cert DNS方式这种方式的好处是, 你不需要任何服务器, 不需要任何公网 ip, 只需要 dns 的解析记录即可完成验证。 这种方式的缺点是，如果不同时配置 Automatic DNS API，使用这种方式 acme.sh 将无法自动更新证书，每次都需要手动再次重新解析验证域名所有权。 下面以NameSilo为例 Namesilo用acme申请免费的证书 NameSilo API 令牌，可从 https://www.namesilo.com/account_api.php 获得 123456789cd /root/acme.shexport Namesilo_Key=&quot;NameSilo API 令牌&quot;# 换成自己的./acme.sh --register-account -m my@example.com./acme.sh --issue --dns dns_namesilo --dnssleep 9000 --log -d pointcut.buzz -d *.pointcut.buzz# 等待900秒，证书生效 其他服务商可以看 https://github.com/acmesh-official/acme.sh/wiki/dnsapi 生成了4个文件，其中 fullchain.cer是证书文件 pointcut.buzz.key是密钥","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://sv.pointcut.cc/tags/nginx/"}]},{"title":"nginx配置注解","slug":"nginx/nginx配置注解","date":"2021-11-15T12:00:06.000Z","updated":"2022-03-23T09:03:57.520Z","comments":true,"path":"blog/nginx/nginx配置注解/","link":"","permalink":"http://sv.pointcut.cc/blog/nginx/nginx%E9%85%8D%E7%BD%AE%E6%B3%A8%E8%A7%A3/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331######Nginx配置文件nginx.conf中文详解######定义Nginx运行的用户和用户组user www www;#nginx进程数，建议设置为等于CPU总核心数。worker_processes 8; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]error_log /usr/local/nginx/logs/error.log info;#进程pid文件pid /usr/local/nginx/logs/nginx.pid;#指定进程可以打开的最大描述符：数目#工作模式与连接数上限#这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。#现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。#这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。worker_rlimit_nofile 65535;events&#123; #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型 #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。 #补充说明： #与apache相类，nginx针对不同的操作系统，有不同的事件模型 #A）标准事件模型 #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll #B）高效事件模型 #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 #Epoll：使用于Linux内核2.6版本及以后的系统。 #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。 #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。 use epoll; #单个进程最大连接数（最大连接数=连接数*进程数） #根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。 worker_connections 65535; #keepalive超时时间。 keepalive_timeout 60; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。 #分页大小可以用命令getconf PAGESIZE 取得。 #[root@web001 ~]# getconf PAGESIZE #4096 #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。 client_header_buffer_size 4k; #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 open_file_cache max=65535 inactive=60s; #这个是指多长时间检查一次缓存的有效信息。 #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息. open_file_cache_valid 80s; #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location 这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态. open_file_cache_min_uses 1; #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件时记录cache错误. open_file_cache_errors on;&#125; #设定http服务器，利用它的反向代理功能提供负载均衡支持http&#123; #文件扩展名与文件类型映射表 include mime.types; #默认文件类型 default_type application/octet-stream; #默认编码 #charset utf-8; #服务器名字的hash表大小 #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. server_names_hash_bucket_size 128; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 client_header_buffer_size 32k; #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。 large_client_header_buffers 4 64k; #设定通过nginx上传文件的大小 client_max_body_size 8m; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。 sendfile on; #开启目录列表访问，合适下载服务器，默认关闭。 autoindex on; #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用 tcp_nopush on; tcp_nodelay on; #长连接超时时间，单位是秒 keepalive_timeout 120; #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; #gzip模块设置 gzip on; #开启gzip压缩输出 gzip_min_length 1k; #最小压缩文件大小 gzip_buffers 4 16k; #压缩缓冲区 gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; #压缩等级 gzip_types text/plain application/x-javascript text/css application/xml; #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 gzip_vary on; #开启限制IP连接数的时候需要使用 #limit_zone crawler $binary_remote_addr 10m; #负载均衡配置 upstream jh.w3cschool.cn &#123; #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 server 192.168.80.121:80 weight=3; server 192.168.80.122:80 weight=2; server 192.168.80.123:80 weight=3; #nginx的upstream目前支持4种方式的分配 #1、轮询（默认） #每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 #2、weight #指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 #例如： #upstream bakend &#123; # server 192.168.0.14 weight=10; # server 192.168.0.15 weight=10; #&#125; #2、ip_hash #每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 #例如： #upstream bakend &#123; # ip_hash; # server 192.168.0.14:88; # server 192.168.0.15:80; #&#125; #3、fair（第三方） #按后端服务器的响应时间来分配请求，响应时间短的优先分配。 #upstream backend &#123; # server server1; # server server2; # fair; #&#125; #4、url_hash（第三方） #按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 #例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 #upstream backend &#123; # server squid1:3128; # server squid2:3128; # hash $request_uri; # hash_method crc32; #&#125; #tips: #upstream bakend&#123;#定义负载均衡设备的Ip及设备状态&#125;&#123; # ip_hash; # server 127.0.0.1:9090 down; # server 127.0.0.1:8080 weight=2; # server 127.0.0.1:6060; # server 127.0.0.1:7070 backup; #&#125; #在需要使用负载均衡的server中增加 proxy_pass http://bakend/; #每个设备的状态设置为: #1.down表示单前的server暂时不参与负载 #2.weight为weight越大，负载的权重就越大。 #3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误 #4.fail_timeout:max_fails次失败后，暂停的时间。 #5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 #nginx支持同时设置多组的负载均衡，用来给不用的server来使用。 #client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug #client_body_temp_path设置记录文件的目录 可以设置最多3层目录 #location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡 &#125; #虚拟主机的配置 server &#123; #监听端口 listen 80; #域名可以有多个，用空格隔开 server_name www.w3cschool.cn w3cschool.cn; index index.html index.htm index.php; root /data/www/w3cschool; #对******进行负载均衡 location ~ .*.(php|php5)?$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; &#125; #图片缓存时间设置 location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$ &#123; expires 10d; &#125; #JS和CSS缓存时间设置 location ~ .*.(js|css)?$ &#123; expires 1h; &#125; #日志格式设定 #$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址； #$remote_user：用来记录客户端用户名称； #$time_local： 用来记录访问时间与时区； #$request： 用来记录请求的url与http协议； #$status： 用来记录请求状态；成功是200， #$body_bytes_sent ：记录发送给客户端文件主体内容大小； #$http_referer：用来记录从那个页面链接访问过来的； #$http_user_agent：记录客户浏览器的相关信息； #通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。 log_format access &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; $http_x_forwarded_for&#x27;; #定义本虚拟主机的访问日志 access_log /usr/local/nginx/logs/host.access.log main; access_log /usr/local/nginx/logs/host.access.404.log log404; #对 &quot;/&quot; 启用反向代理 location / &#123; proxy_pass http://127.0.0.1:88; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #以下是一些反向代理的配置，可选。 proxy_set_header Host $host; #允许客户端请求的最大单文件字节数 client_max_body_size 10m; #缓冲区代理缓冲用户端请求的最大字节数， #如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。 #无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误 client_body_buffer_size 128k; #表示使nginx阻止HTTP应答代码为400或者更高的应答。 proxy_intercept_errors on; #后端服务器连接的超时时间_发起握手等候响应超时时间 #nginx跟后端服务器连接超时时间(代理连接超时) proxy_connect_timeout 90; #后端服务器数据回传时间(代理发送超时) #后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据 proxy_send_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) #连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间） proxy_read_timeout 90; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 #设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小 proxy_buffer_size 4k; #proxy_buffers缓冲区，网页平均在32k以下的设置 #设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k proxy_buffers 4 32k; #高负荷下缓冲大小（proxy_buffers*2） proxy_busy_buffers_size 64k; #设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长 #设定缓存文件夹大小，大于这个值，将从upstream服务器传 proxy_temp_file_write_size 64k; &#125; #设定查看Nginx状态的地址 location /NginxStatus &#123; stub_status on; access_log on; auth_basic &quot;NginxStatus&quot;; auth_basic_user_file confpasswd; #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。 &#125; #本地动静分离反向代理配置 #所有jsp的页面均交由tomcat或resin处理 location ~ .(jsp|jspx|do)?$ &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080; &#125; #所有静态文件由nginx直接读取不经过tomcat或resin location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt| pdf|xls|mp3|wma)$ &#123; expires 15d; &#125; location ~ .*.(js|css)?$ &#123; expires 1h; &#125; &#125;&#125;######Nginx配置文件nginx.conf中文详解#####","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://sv.pointcut.cc/tags/nginx/"}]},{"title":"nginx处理请求的11个阶段","slug":"nginx/nginx处理请求的11个阶段","date":"2021-11-15T12:00:05.000Z","updated":"2022-03-23T09:03:57.520Z","comments":true,"path":"blog/nginx/nginx处理请求的11个阶段/","link":"","permalink":"http://sv.pointcut.cc/blog/nginx/nginx%E5%A4%84%E7%90%86%E8%AF%B7%E6%B1%82%E7%9A%8411%E4%B8%AA%E9%98%B6%E6%AE%B5/","excerpt":"","text":"上图中的死代表的是防止死循环 Nginx 处理请求的全过程一共划分为 11 个阶段（如图），按阶段由上到下依次执行 （上一阶段的所有指令执行完毕，才进入下一阶段） 各阶段的含义如下： post-read: 接收到完整的http头部后处理的阶段，在uri重写之前。一般跳过 server-rewrite: location匹配前，修改uri的阶段，用于重定向，location块外的重写指令（多次执行） find-config: uri寻找匹配的location块配置项（多次执行） rewrite: 找到location块后再修改uri，location级别的uri重写阶段（多次执行） post-rewrite: 防死循环，跳转到对应阶段 preaccess: 权限预处理 access: 判断是否允许这个请求进入 post-access: 向用户发送拒绝服务的错误码，用来响应上一阶段的拒绝 try-files: 访问静态文件资源 content : 内容生成阶段，该阶段产生响应，并发送到客户端 log: 记录访问日志","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://sv.pointcut.cc/tags/nginx/"}]},{"title":"04nginx高可用","slug":"nginx/04nginx高可用","date":"2021-11-15T12:00:04.000Z","updated":"2022-03-23T09:03:57.516Z","comments":true,"path":"blog/nginx/04nginx高可用/","link":"","permalink":"http://sv.pointcut.cc/blog/nginx/04nginx%E9%AB%98%E5%8F%AF%E7%94%A8/","excerpt":"","text":"lvs 思想解决高可用问题 如上图，由服务器集群虚拟出来一台 虚拟网关vip。此vip由两台机器共同协商生成。当有一台机器宕机时，另一台机器一样能维持vip。这保证了，只要两台机器不同时宕机，vip就存在。 keepalived配置LVS过程前提： 1# 关闭selinux，打开/etc/sysconfig/selinux设置其中值 SELINUX=disabled keepalived安装： 123456wget https://keepalived.org/software/keepalived-2.0.20.tar.gztar -zxvf keepalived-2.0.20.tar.gzcd keepalived-2.0.20# 指定安装目录和配置目录，否则文件太散乱./configure --prefix=/usr/local/keepalived --sysconf=/etcmake &amp;&amp; make install keepalived主机配置: 1234567891011121314151617181920212223242526272829vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; router_id LVS_DEVEL ####keepalived的唯一标识&#125;vrrp_script chk_http_port &#123; script &quot;/etc/nginx/chk_nginx.sh&quot; #心跳执行的脚本 interval 2 #（检测脚本执行的间隔，单位是秒） weight 2&#125;vrrp_instance VI_1 &#123; state MASTER interface ens33 ##系统网上名,可以使用ip addr命令查看 virtual_router_id 51 ##组名，参与此虚拟ip的机器配置一样的值 priority 200 ##优先级，数值大的优先级高，组内最高的胜出 advert_int 1 ##心跳检测1s一次 authentication &#123; ##授权认证，一般不用 auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_http_port #（调用检测脚本） &#125; virtual_ipaddress &#123; 192.168.244.200 ##虚拟的ip &#125;&#125; 多台机器配置keepalived，只需要把上面的配置复制过去就好了，改下priority，其他一样就好了。这个priority的意思就是，最大的就先提供服务，当它宕机后，胚胎就转正了。当它恢复后，又从胚胎中夺回服务权利，转正了。 上面还配置了一个检查脚步，用来检查nginx是否存活 123456789#!/bin/bashA=`ps -C nginx --no-header |wc -l` ##统计nginx进程数，若为0，表明nginx被杀if [ $A -eq 0 ];then /usr/local/openresty/nginx/sbin/nginx #重启nginx #nginx重启失败，则停掉keepalived服务，进行VIP转移 if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then killall keepalived #杀掉，vip就漫游到另一台机器 fifi Nginx在mvvm模式中的使用","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://sv.pointcut.cc/tags/nginx/"}]},{"title":"03nginx小功能合集","slug":"nginx/03nginx小功能合集","date":"2021-11-15T12:00:03.000Z","updated":"2022-03-23T09:03:57.510Z","comments":true,"path":"blog/nginx/03nginx小功能合集/","link":"","permalink":"http://sv.pointcut.cc/blog/nginx/03nginx%E5%B0%8F%E5%8A%9F%E8%83%BD%E5%90%88%E9%9B%86/","excerpt":"","text":"跨域处理跨域问题由来：浏览器拒绝执行其它域名下的ajax运作。 如果浏览器在static.enjoy.com对应的html页面内，发起ajax请求偷盗www.enjoy.com域名下的内容来填充自己的页面,整个互联网秩序将混乱. 为了防止这种混乱,W3C组织制定了浏览器安全规范，即html页面发起的ajax请求仅限于同域名后端范围，跨越域名的ajax请求不得执行，此谓跨域问题。 cors方案的解决之道W3C制定跨域限制的本意，是防止页面领域安全混乱，即防止A公司不经B公司同意，使用ajax盗取B公司的服务内容。 出于这个本意，W3C改进了跨域的方案，即：如果B公司是同意将自己的内容分享给A公司的，跨域限制可放开，此方案即CORS方案，如下图： nginx配置跨域操作123456789101112131415161718192021server &#123; listen 80; server_name test.enjoy.com; #是否允许请求带有验证信息 add_header Access-Control-Allow-Credentials true; #允许跨域访问的域名,可以是一个域的列表，也可以是通配符*，多个用空格分开 add_header Access-Control-Allow-Origin http://static.enjoy.com; #允许脚本访问的返回头 add_header Access-Control-Allow-Headers &#x27;x-requested-with,content-type,Cache-Control,Pragma,Date,x-timestamp&#x27;; #允许使用的请求方法，以逗号隔开 add_header Access-Control-Allow-Methods &#x27;POST,GET,OPTIONS,PUT,DELETE&#x27;; #允许自定义的头部，以逗号隔开,大小写不敏感 add_header Access-Control-Expose-Headers &#x27;WWW-Authenticate,Server-Authorization&#x27;; #P3P支持跨域cookie操作 add_header P3P &#x27;policyref=&quot;/w3c/p3p.xml&quot;, CP=&quot;NOI DSP PSAa OUR BUS IND ONL UNI COM NAV INT LOC&quot;&#x27;; if ($request_method = &#x27;OPTIONS&#x27;) &#123;##OPTIONS类的请求，是跨域先验请求 return 204;##204代表ok &#125;&#125; 上面的配置都是通用的，如果真的有需求直接改下域名复制就好了。 防盗链目标： 让资源只能在我的页面内显示，不能被其它页面直接引用。 作用： 防止其他网站把服务压力引到别人的服务上。 解决办法浏览器发起的任何请求，在其request头部，都会标注其请求发起地的URL，如下： 因此，在Nginx服务器上，只要校验此发起地url，就可以对应地拒绝响应它。 nginx配置12345678910server &#123; location ^~ /mall &#123; valid_referers *.enjoy.com;##对referer进行校验 if ($invalid_referer) &#123;##校验不过，拒绝访问 return 404; &#125; root /etc/nginx/html/gzip; &#125;&#125; 压缩浏览器在发送请求时，会附带自己支持的压缩方式： nginx配置1234567891011121314151617server &#123; listen 80; server_name static.enjoy.com; location ~ /(.*)\\.(html|js|css|png)$ &#123; gzip on; # 启用gzip压缩，默认是off，不启用 # 对js、css、jpg、png、gif格式的文件启用gzip压缩功能 gzip_types application/javascript text/css image/jpeg image/png image/gif; gzip_min_length 1024; # 所压缩文件的最小值，小于这个的不会压缩 gzip_buffers 4 1k; # 设置压缩响应的缓冲块的大小和个数，默认是内存一个页的大小 gzip_comp_level 1; # 压缩水平，默认1。取值范围1-9，取值越大压缩比率越大，但越耗cpu时间 root /etc/nginx/html/gzip; &#125;&#125; https配置对称加密 安全隐患：钥匙除我之外，还有多个人拥有。泄露风险较大，钥匙传递的过程风险较大 非对称加密 优缺点：私钥很安全。但是非对称算法开销很大，大批量应用于业务，会导致性能成本过高。 https加密方案综合上述方案优缺点，各取所长，得到自己的方案 业务数据的加密使用对称加密，降低性能开销 传输对称加密的密钥时，采用非对称加密，保驾护航 配置Nginx配置https只需要两个东东。一个是浏览器证书（内含公钥，供浏览器加密使用），一个是私钥（供自己解密使用） server.crt和server.key可以自己去购买商业的。 可以用acme生成一个，前提是有一台服务器或者能进行域名解析 使用acme生成证书 nginx配置： 123456789101112server &#123; listen 443 ssl; server_name lua.enjoy.com; ssl_certificate /etc/nginx/server.crt; ssl_certificate_key /etc/nginx/server_nopass.key; location /hello &#123; ##ngx.say--输出内容print content_by_lua &#x27;ngx.say(&quot;Hello, Peter!&quot;)&#x27;; &#125;&#125;","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://sv.pointcut.cc/tags/nginx/"}]},{"title":"02nginx进阶","slug":"nginx/02nginx进阶","date":"2021-11-15T12:00:02.000Z","updated":"2022-03-23T09:03:57.503Z","comments":true,"path":"blog/nginx/02nginx进阶/","link":"","permalink":"http://sv.pointcut.cc/blog/nginx/02nginx%E8%BF%9B%E9%98%B6/","excerpt":"","text":"对于我这个做后端的人来说，这个还是很难记住的，以后如果真的有需求的话就再看看吧，但是前一节的基础要懂。 先看nginx处理请求的11个阶段 OpenrestyOpenResty是一个全功能的 Web 应用服务器。它打包了标准的 Nginx 核心，常用的第三方模块以及大多数依赖项。 可以把它看成是Nginx附加众多的第三方插件的合集。其主体是嵌入lua脚本的支持，让你能够使用lua灵活地处理运算逻辑。说白了就是nginx的全家桶。 Openresty的安装配置和源码安装nginx一样，不过可以直接使用./configure就好了，默认会帮我们安装了好多的模块 安装前最好删除nginx 123456wget https://openresty.org/download/openresty-1.19.3.1.tar.gztar -zxvf openresty-1.19.3.1.tar.gz##选择需要的插件启用, --with-Components 激活组件，--without 则是禁止组件 ##./configure --without-http_redis2_module --with-http_iconv_module ./configuremake &amp;&amp; make install nix_lua基本常量与api主要帮助对http请求取参、取header头、输出等 api 作用 ngx.arg 指令参数，如跟在content_by_lua_file后面的参数 ngx.var request变量，ngx.var.VARIABLE引用某个变量 ngx.ctx 请求的lua上下文 ngx.header 响应头，ngx.header.HEADER引用某个头 ngx.status 响应码 ngx.log 输出到error.log ngx.send_headers 发送响应头 ngx.headers_sent 响应头是否已发送 ngx.resp.get_headers 获取响应头 ngx.is_subrequest 当前请求是否是子请求 ngx.location.capture 发布一个子请求 ngx.location.capture_multi 发布多个子请求 ngx.print 输出响应 ngx.say 输出响应，自动添加‘\\n‘ ngx.flush 刷新响应 ngx.exit 结束请求 Lua嵌入Nginx的时机阶段Nginx执行lua脚本片断时，需要明确指明执行的nginx阶段时机。主要有以下几种时机： 阶段 说明 set_by_lua* 设置nginx变量，实现复杂的赋值逻辑 rewrite_by_lua* 实现转发、重定向等功能 access_by_lua* IP准入、接口访问权限等情况集中处理 content_by_lua* 接收请求处理并输出响应 header_filter_by_lua* 设置header和cookie body_filter_by_lua* 对响应数据进行过滤，如截断&#x2F;替换等等 Lua基础功能使用介绍nginx处理请求的11个阶段 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869server &#123; listen 80 default_server; server_name lua.enjoy.com; location /hello &#123; ##ngx.say--输出内容print content_by_lua &#x27;ngx.say(&quot;Hello, Peter!&quot;)&#x27;; &#125; location /args &#123; ##ngx.var--取请求参数，arg_a指参数a content_by_lua_block &#123; ngx.say(ngx.var.arg_a) ngx.say(ngx.var.arg_b) &#125; &#125; location /args_read &#123; ##执行lua文件脚本 content_by_lua_file /etc/nginx/lua/lua_args.lua; &#125; location /header &#123; ##执行lua文件脚本 content_by_lua_file /etc/nginx/lua/lua_req.lua; &#125; location /setfile &#123; ##给lua脚本传递参数 set_by_lua_file $val &quot;/etc/nginx/lua/set.lua&quot; $arg_a $arg_b; echo $val; &#125; location /access &#123; ##权限控制 access_by_lua_file &quot;/etc/nginx/lua/access.lua&quot;; echo &quot;welcome $arg_name !&quot;; &#125; location /rew &#123; ##重定向 rewrite_by_lua &#x27;ngx.exec(&quot;/hello&quot;)&#x27;; echo &quot;I am rew&quot;; &#125; location /redirect &#123; ##页面重定向，是要生成内容的，因此使用content阶段 content_by_lua_block &#123; ngx.redirect(&quot;http://www.baidu.com&quot;, 302) &#125; &#125; location /filter &#123; echo &#x27;hello Peter&#x27;; echo &#x27;you are welcome!&#x27;; ##内容过滤 body_filter_by_lua_file &quot;/etc/nginx/lua/filter.lua&quot;; &#125; location /redis &#123; ##读redis值 content_by_lua_file &quot;/etc/nginx/lua/redis.lua&quot;; &#125; location /mysql &#123; ##读mysql库 content_by_lua_file &quot;/etc/nginx/lua/mysql.lua&quot;; &#125;&#125;","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://sv.pointcut.cc/tags/nginx/"}]},{"title":"01nginx基础","slug":"nginx/01nginx基础","date":"2021-11-15T12:00:01.000Z","updated":"2022-03-23T09:03:57.502Z","comments":true,"path":"blog/nginx/01nginx基础/","link":"","permalink":"http://sv.pointcut.cc/blog/nginx/01nginx%E5%9F%BA%E7%A1%80/","excerpt":"","text":"nginx在架构体系中的位置和作用 网关面向客户的总入口。 虚拟主机一台机器为不同的域名&#x2F;ip&#x2F;端口提供服务 路由使用反向代理，整合后续服务为一个完整业务 静态服务器mvvm模式中，用来发布前端html&#x2F;css&#x2F;js&#x2F;img 负载集群使用upstream，负载多个tomcat 高度模块化的设计是 Nginx 的架构基础。Nginx 服务器被分解为多个模块，每个模块就是一个功能模块，只负责自身的功能，模块之间严格遵循“高内聚，低耦合”的原则。 Nginx使用epoll模式来处理请求。 反向代理： Nginx多进程模型： 服务器每当收到一个客户端时。就有服务器主进程（master process）生成一个子进程（worker process）出来和客户端建立连接进行交互，直到连接断开，该子进程结束。 Nginx会按需同时运行多个进程，一个主进程(master)和几个工作进程(worker)，配置了缓存时还会有缓存加载器进程(cache loader)和缓存管理器进程(cache manager)等。 所有进程均是仅含有一个线程，并主要通过“共享内存”的机制实现进程间通信。 主进程以root用户身份运行，而worker、cache loader和cache manager均应以非特权用户身份（user配置项）运行。 主进程主要完成如下工作： 读取并验正配置信息； 创建、绑定及关闭套接字； 启动、终止及维护worker进程的个数； 无须中止服务而重新配置工作特性； 重新打开日志文件； worker进程主要完成的任务包括： 接收、传入并处理来自客户端的连接； 提供反向代理及过滤功能； nginx任何能完成的其它任务； 安装docker: http://hub.daocloud.io/repos/2b7310fb-1a50-48f2-9586-44622a2d1771 Mac: 12brew updatebrew install nginx ubuntu: 12apt updateapt install nginx Linux通用： 123456789101112wget http://nginx.org/download/nginx-1.15.8.tar.gz #自己改下版本tar -zxvf nginx-1.15.8.tar.gzcd nginx-1.15.8#--prefix指定安装目录#--with-http_ssl_module安装https模块#creating objs/Makefile 代表编译成功./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module #make编译#make install安装make &amp;&amp; make install nginx安装第三方模块12./configure --add-module=模块的目录make &amp;&amp; make install Nginx目录结构 conf 配置文件 html 网页文件 logs 日志文件 sbin 二进制程序 Nginx常用命令123456789nginx -c nginx.conf # 指定配置文件。如果不指定，默认为NGINX_HOME/conf/nginx.confnginx -s stop # 停止nginx -s quit # 退出nginx -s reload # 重新加载nginx.confnginx -V # 查看安装的第三方插件 nginx.conf配置文件结构nginx.conf 默认情况下只支持http应用。 从1.9.0开始，新增加了一个stream模块，用来实现四层协议的转发、代理或者负载均衡等。能tcp负载均衡、udp(dns)负载均衡、redis代理或zk服务代理等。 需要安装--with-stream 配置如下： 12345678910111213141516171819202122232425262728293031323334353637383940stream &#123; # stream 模块配置和 http 模块在相同级别 upstream redis &#123; server 127.0.0.1:6379 max_fails=3 fail_timeout=30s weight=5; &#125; server &#123; listen 16379; proxy_connect_timeout 1s; proxy_timeout 3s; proxy_pass redis; &#125;&#125;stream &#123; upstream backend &#123; hash $remote_addr consistent; server 127.0.0.1:12346 weight=5; server 127.0.0.1:12347 max_fails=3 fail_timeout=30s; server 127.0.0.1:12348 max_fails=3 fail_timeout=30s; &#125; server &#123; listen 12345; proxy_connect_timeout 1s; proxy_timeout 3s; proxy_pass backend; &#125; upstream dns &#123; server 17.61.29.79:53; server 17.61.29.80:53; server 17.61.29.81:53; server 17.61.29.82:53; &#125; server &#123; listen 127.0.0.1:53 udp; proxy_responses 1; proxy_timeout 20s; proxy_pass dns; &#125;&#125; Nginx配置demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#user nobody;worker_processes 1;error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; # default_type application/octet-stream; default_type text/plain; log_format peter &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;; log_format access &#x27;$remote_addr &#x27;; access_log logs/access.log peter; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 8080; server_name enjoy.com; location / &#123; root /etc/nginx/html; index c.html; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 带注解的配置nginx配置注解 Nginx日志Nginx日志对于统计、系统服务排错很有用。 Nginx日志主要分为两种：access_log(访问日志)和error_log(错误日志)。 通过访问日志我们可以得到用户的IP地址、浏览器的信息，请求的处理时间等信息。 错误日志记录了访问出错的信息，可以帮助我们定位错误的原因。 因此，将日志好好利用，可以得到很多有价值的信息。 设置access_log访问日志主要记录客户端的请求。客户端向Nginx服务器发起的每一次请求都记录在这里。客户端IP，浏览器信息，referer，请求处理时间，请求URL等都可以在访问日志中得到。当然具体要记录哪些信息，你可以通过log_format指令定义。 access_log path [format [buffer&#x3D;size] [gzip[&#x3D;level]] [flush&#x3D;time] [if&#x3D;condition]]; # 设置访问日志 access_log off; #关闭访问日志 path 指定日志的存放位置。 format 指定日志的格式。默认使用预定义的combined。 buffer 用来指定日志写入时的缓存大小。默认是64k。 gzip 日志写入前先进行压缩。压缩率可以指定，从1到9数值越大压缩比越高，同时压缩的速度也越慢。默认是1。 flush 设置缓存的有效时间。如果超过flush指定的时间，缓存中的内容将被清空。 if 条件判断。如果指定的条件计算为0或空字符串，那么该请求不会写入日志。 另外，还有一个特殊的值off。如果指定了该值，当前作用域下的所有的请求日志都被关闭。 事例 1access_log /var/logs/nginx-access.log buffer=32k gzip flush=1m; log_format自定义格式各参数明细表： $remote_addr 客户端的ip地址(代理服务器，显示代理服务ip) $remote_user 用于记录远程客户端的用户名称（一般为“-”） $time_local 用于记录访问时间和时区 $request 用于记录请求的url以及请求方法 $status 响应状态码，例如：200成功、404页面找不到等。 $body_bytes_sent 给客户端发送的文件主体内容字节数 $http_user_agent 用户所使用的代理（一般为浏览器） $http_x_forwarded_for 可以记录客户端IP，通过代理服务器来记录客户端的ip地址 $http_referer 可以记录用户是从哪个链接访问过来的 事例 123log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; 设置error_log错误日志在Nginx中是通过error_log指令实现的。该指令记录服务器和请求处理过程中的错误信息。 错误日志不支持自定义。 语法： error_log path [level]; path参数指定日志的写入位置。 level参数指定日志的级别（不写为全部）。level可以是debug, info, notice, warn, error, crit, alert,emerg中的任意值（等级从低到高排列）。只有日志的错误级别等于或高于level指定的值才会写入错误日志中。默认值是error。 事例 123error_log logs/error.log; error_log logs/error_notice.log notice;error_log logs/error_info.log info; 日志配置和及切割12/etc/init.d/rsyslog start #系统日志，如不开启，看不到定时任务日志/etc/rc.d/init.d/crond start #定时任务开启 编写sh： 12345678910#!/bin/bash#设置日志文件存放目录LOG_HOME=&quot;/usr/local/nginx/logs/&quot;#备分文件名称LOG_PATH_BAK=&quot;$(date -d yesterday +%Y%m%d%H%M)&quot;#重命名日志文件mv $&#123;LOG_HOME&#125;/access.log $&#123;LOG_HOME&#125;/access.$&#123;LOG_PATH_BAK&#125;.logmv $&#123;LOG_HOME&#125;/error.log $&#123;LOG_HOME&#125;/error.$&#123;LOG_PATH_BAK&#125;.log#向nginx主进程发信号重新打开日志kill -USR1 `cat $&#123;LOG_HOME&#125;/nginx.pid` 配置cron： 执行命令 1crontab -e 然后在里面添加下面的命令 1*/1 * * * * /usr/local/nginx/sbin/logcut.sh 虚拟主机主配置文件 123456789101112131415161718192021222324252627282930313233343536373839#user nobody;worker_processes 1;error_log /Users/xyz/markdown/mweb/nginx/demo/logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;pid /Users/xyz/markdown/mweb/nginx/demo/logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; # include mime.types; # default_type application/octet-stream; default_type text/plain; #日志匹配 log_format peter &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;; log_format access &#x27;$remote_addr &#x27;; #输出peter的日志 access_log /Users/xyz/markdown/mweb/nginx/demo/logs/access.log peter; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; include /Users/xyz/markdown/mweb/nginx/demo/conf/server/*.conf;&#125; 看最后，有一行include ...../*.conf;，这就表示引入该目录下的.conf文件。这些文件可以是一个个的server配置文件。 比如a.conf 1234567891011121314151617server &#123; listen 9669; server_name xyza.com; access_log /Users/xyz/markdown/mweb/nginx/demo/logs/access_a.log peter; location / &#123; root /Users/xyz/markdown/mweb/nginx/demo/html; index a.html; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; b.conf 1234567891011121314151617server &#123; listen 9669; server_name xyzb.com; access_log /Users/xyz/markdown/mweb/nginx/demo/logs/access_b.log peter; location / &#123; root /Users/xyz/markdown/mweb/nginx/demo/html; index b.html; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; 路由——Location的使用 语法规则location [&#x3D;|~|~*|^~|空格] &#x2F;uri&#x2F; {… } 符号 含义 &#x3D; &#x3D; 开头表示精确匹配 ^~ ^~开头表示uri以某个常规字符串开头，理解为匹配 url路径即可（禁止正则匹配）。 空格 空格开头表示uri以某个常规字符串开头。 ~ ~ 开头表示区分大小写的正则匹配 ~* ~* 开头表示不区分大小写的正则匹配 !~和!~* !~和!~*分别为区分大小写不匹配及不区分大小写不匹配的正则 &#x2F; 用户所使用的代理（一般为浏览器） location指令优先级验证： &#x3D;精准匹配，最优，匹配就直接返回 ^~，空 为一般匹配，收集所有匹配，取最长匹配执行（此时不返回，还会执行下面的正则匹配） *~，~* 为正则匹配，按顺序依次匹配，命中即返回（可能会覆盖普通匹配），没命中就放回普通匹配中匹配最长的。 语法规则总结先总结下在做测试，用下面的伪代码做总结 1234567891011121314151617181920212223242526比如去掉服务名后的字符串为urlif url == .. &#123; return location1&#125; else &#123; temp if url.startWith(str1) &#123; temp = location2 &#125; if url.startWith(str2) &#123; if temp == null || str2.len &gt; str1.len &#123; temp = location3 &#125; &#125; ... if temp != null &amp;&amp; temp.contains(^~) &#123; return temp; &#125; else &#123; if (pattern1.metcher(url).find()) &#123; return location4 &#125; if (pattern2.metcher(url).find()) &#123; return location5 &#125; &#125; return temp;&#125; location测试下面是用于测试location.conf文件。其中echo需要引入第三方的模块ngx_echo。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#location指令优先级验证：# =精准匹配，最优，匹配就直接返回# ^~,空 为一般匹配，收集所有匹配，取最长匹配执行（此时不返回，还会执行下面的正则匹配）# ~,~* 为正则匹配，按顺序依次匹配，命中即返回（可能会覆盖普通匹配）server &#123; listen 80; server_name loc.enjoy.com; #精准匹配测试 #第1，2条虽然匹配，但第三条是精准匹配，出第三条结果 #测试路径/equal/a/b/c location ~ /equal/* &#123;#被命中，但被下面的推断：location = /equal/a/b/c echo &#x27;/equal/*&#x27;; &#125; location /equal/a/b &#123;#被命中，但被下面的推断：location = /equal/a/b/c echo &#x27;/equal/a/b&#x27;; &#125; location = /equal/a/b/c &#123;#被命中，直接执行，不等待 echo &#x27;/equal/a/b/c&#x27;; &#125; #普通匹配测试 #第1，2条虽然匹配，第三条匹配更长，出第三条结果 #测试路径/match/a/b/c location /match/a &#123;#被命中，但不是最长 return 200 &quot;/match/a&quot;; &#125; location /match/a/b &#123;#被命中，但不是最长 return 200 &quot;/match/a/b&quot;; &#125; location /match/a/b/c &#123;#被命中，且最长 return 200 &quot;/match/a/b/c&quot;; &#125; location /match/a/b/c/d &#123;#不命中 return 200 &quot;/match/a/b/c/d&quot;; &#125; #正则匹配覆盖普通匹配测试 #会覆盖普通匹配，不会覆盖=和^~ location =/re/a.js &#123;#访问/re/a.js，不会被后面的正则覆盖 echo &#x27;match =&#x27;; &#125; location ^~ /re/a/b &#123;#访问/re/a/b开头的路径，不会被后面的正则覆盖 echo &#x27;math ^~/re/a/b*&#x27;; &#125; location /re/a.htm &#123;#访问/re/a.htm，会被后面的正则覆盖 echo &#x27;match /re/a.htm&#x27;; &#125; location ~ /re/(.*)\\.(htm|js|css)$ &#123;#覆盖/re/a.htm路径 echo &quot;cover /re/$1.$2&quot;; &#125; #正则匹配成功一条后，便不再走其它正则 #最长正则匹配是第三个，但匹配第一个后便不往下走 #测试路径/rex/a/b/c.htm location ~ /rex/.*\\.(htm|js|css)$ &#123;#覆盖/re/a.htm路径 echo &quot;match first&quot;; &#125; location ~ /rex/a/(.*)\\.(htm|js|css)$ &#123;#覆盖/re/a.htm路径 echo &quot;match second&quot;; &#125; location ~ /rex/a/b/(.*)\\.(htm|js|css)$ &#123;#覆盖/re/a.htm路径 echo &quot;match third&quot;; &#125; location / &#123; root /etc/nginx/html; index c.html; &#125;&#125; 测试112http://loc.enjoy.com/equal/a/b/c预测结果：页面显示：/equal/a/b/c 结果： 测试212http://loc.enjoy.com/match/a/b/c预测结果：页面显示：/match/a/b/c 结果： 测试312http://loc.enjoy.com/re/a/b/c.htm预测结果：页面显示：math ^~/re/a/b* 结果: 测试412http://loc.enjoy.com/rex/a/b/c.htm预测结果：页面显示：match first 结果： 反向代理12345678910111213141516171819server &#123; listen 80; server_name pxy.enjoy.com; #后台服务原始路径：http://192.168.0.132:8088/mvc/index?id=2 #无/，访问路径：http://pxy.enjoy.com/mvc/index?id=2 location /mvc &#123; #此处未关闭，传递整个路径/nginx/enjoy/getInfo到目标ip:port proxy_pass http://192.168.0.132:8088; &#125; #有/，访问路径 ：http://pxy.enjoy.com/nginx/mvc/index?id=2 location /nginx/mvc &#123;#匹配路径/nginx/mvc，剩余路径/index?id=2 proxy_pass http://192.168.0.132:8088/mvc; &#125; &#125; 会把匹配到的内容/mvc/index?id=2和代理的地址进行拼接，最会回去调用 1http://192.168.0.132:8088/mvc/index?id=2 这个服务。 这种情况如果还是按上面的逻辑处理的话最终地址变成了 1http://192.168.0.132:8088/mvc/nginx/mvc/index?id=2 这不是我们希望的，所以nginx会有一些规则 以上图为例，ip:prot是用来定位server的而path是用来做location匹配的，在location匹配时path可以看出有两部分，第一部分是匹配的，第二部分是未匹配的。上图以path1和path2分别表示。 这种时候有两种处理逻辑： 如果代理的地址是ip:port时，转发的地址就变成了ip:port+path1+path2 如果代理的地址是ip:port&#x2F;时，转发的地址就变成了ip:port+path2 所以，对于上边的，最终转发地址为： 1http://192.168.0.132:8088/mvc/index?id=2 静态代理1234567891011121314server &#123; listen 80; server_name pxy.enjoy.com; #访问路径：http://pxy.enjoy.com/static/d.html location /static &#123;#匹配路径/static，剩余路径/a.html root /etc/nginx/html/;#root声明，在html文件夹，查找/static/a.html文件 &#125; #访问路径：http://pxy.enjoy.com/target/d.html location /target &#123;#匹配路径/target，剩余路径/a.html alias /etc/nginx/html/static/;##alias声明，在static/文件夹，查找a.html文件 &#125; &#125; 上面对两种声名，root和alias root很简单粗暴，就是匹配后，把整个path去指定的目录下查找，对应上边的就是找这个文件 1/etc/nginx/html/static/d.html alias意识是别名，就是把匹配到的字符作为一个路径的别名，这样的话只需要找别名后边的路径的资源就好了，对应上边的是找这个文件 1/etc/nginx/html/static/d.html rewrite使用语法rewrite regex replacement [flag] regex 是正则表达式 replacement 是替换值，新值 flag – 后续处理标识，flag&#x3D;[break|last|redirect|permanent] flag各个参数说明 flag&#x3D;break发生nginx内部重定向，path值被更新，rewrite层面的命令会中断。原控制流程逻辑不变往下走 flag&#x3D;last发生nginx内部重定向，path值被更新，rewrite层面的命令会中断。控制流程刷新，重新进行整个location层的逻辑流程。 flag&#x3D; redirect&#x2F;permanent发生页面重定向（301永久重定向&#x2F;302临时重定向），nginx流程结束，返回http响应到浏览器，页面url更新 flag为空发生nginx内部重定向，path值被更新，rewrite层面的命令继续。最后一个rewrite完毕，刷新控制流程，重新进行location重匹配 使用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647server &#123; listen 80; server_name rew.enjoy.com; location /a.html &#123; echo &#x27;I am a.html&#x27;; &#125; location /b.html &#123; echo &#x27;I am b.html&#x27;; &#125; #此路径请求：http://rew.enjoy.com/aa.html location /aa.html &#123;##内部重定向 rewrite ^/ /a.html break;##停止指令，流程不变往下走 rewrite ^/ /b.html break; root /etc/nginx/html/; &#125; #此路径请求：http://rew.enjoy.com/ab.html location /ab.html &#123; rewrite ^/ /a.html last;##停止指令，但重新location匹配 rewrite ^/ /b.html last; rewrite ^/ /c.html; root /etc/nginx/html/; &#125; #此路径请求：http://rew.enjoy.com/bb location /bb &#123; rewrite ^/ /b.html redirect;##302临时重定向 set $aa 12; root html/; &#125; #此路径请求：http://rew.enjoy.com/ba location /ba &#123; rewrite ^/ /b.html permanent;##301永久重定向 root html/; &#125; #此路径请求：http://rew.enjoy.com/cc.html location /cc.html &#123; rewrite ^/ /c.html;##指令不停，继续往下 rewrite ^/ /b.html; rewrite ^/ /a.html;##最后一条，生效的是这条 root /etc/nginx/html/; &#125;&#125; 负载均衡——upstream语法放在server外，语法： upstream 负载名 { ​ [ip_hash;] ​ server ip:port [weight&#x3D;数字] [down]; ​ server ip:port [weight&#x3D;数字]; } 使用123456789101112131415161718upstream order &#123; ip_hash; server 192.168.0.128:8383 weight=3; server 192.168.244.1:8383 weight=1;&#125;server &#123; listen 80; server_name test.enjoy.com; location /order/enjoy &#123; ##后台请求为：http://192.168.0.128:8383/enjoy/getPage ##调整后请求：http://test.enjoy.com/order/enjoy/getPage ##故代理需要关闭/order/enjoy的传递 proxy_pass http://order/enjoy; &#125;&#125; 上边的配置中，server的proxy_pass 地址中的order是指upstream的别名，可以认为就是指ip:port。剩下的对于真实地址的生成的规则还是和proxy_pass的一样 ip_hash每个请求按访问ip的hash结果分配，这样同一客户端的请求总是发往同一个后端服务器。 场景：可以解决session的登陆问题（不想用缓存保存seesion的情况） weight权重，如果不填写，默认值即为1。 每个请求按照顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 如果填写后，weight&#x2F;总weight的概率访问。 down在后面加上down，表示不参与负载 场景：服务器升级、程序更新","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://sv.pointcut.cc/tags/nginx/"}]},{"title":"缓存雪崩、击穿、穿透","slug":"redis/缓存雪崩、击穿、穿透","date":"2021-11-14T12:00:25.000Z","updated":"2022-03-23T09:03:57.665Z","comments":true,"path":"blog/redis/缓存雪崩、击穿、穿透/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E3%80%81%E5%87%BB%E7%A9%BF%E3%80%81%E7%A9%BF%E9%80%8F/","excerpt":"","text":"缓存雪崩缓存雪崩是指大量的应用请求无法在 Redis 缓存中进行处理，紧接着，应用将大量请求发送到数据库层，导致数据库层的压力激增。 缓存雪崩一般是由两个原因导致的： 缓存中有大量数据同时过期，导致大量请求无法得到处理。具体来说，当数据保存在缓存中，并且设置了过期时间时，如果在某一个时刻，大量数据同时过期，此时，应用再访问这些数据的话，就会发生缓存缺失。紧接着，应用就会把请求发送给数据库，从数据库中读取数据。如果应用的并发请求量很大，那么数据库的压力也就很大，这会进一步影响到数据库的其他正常业务请求处理。针对大量数据同时失效带来的缓存雪崩问题，提供两种解决方案： 首先，我们可以避免给大量的数据设置相同的过期时间。如果业务层的确要求有些数据同时失效，你可以在用 EXPIRE 命令给每个数据设置过期时间时，给这些数据的过期时间增加一个较小的随机数（例如，随机增加 1~3 分钟），这样一来，不同数据的过期时间有所差别，但差别又不会太大，既避免了大量数据同时过期，同时也保证了这些数据基本在相近的时间失效，仍然能满足业务需求。 通过服务降级，来应对缓存雪。 当业务应用访问的是非核心数据（例如电商商品属性）时，暂时停止从缓存中查询这些数据，而是直接返回预定义信息、空值或是错误信息； 当业务应用访问的是核心数据（例如电商商品库存）时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取。 Redis 缓存实例发生故障宕机了，无法处理请求，这就会导致大量请求一下子积压到数据库层，从而发生缓存雪崩。提供3个建议： 务请求可以直接返回错误，没必要再去请求数据库了，这样就不会导致数据库层压力变大。 第一个建议，是在业务系统中实现服务熔断或请求限流机制。 所谓的服务熔断，是指在发生缓存雪崩时，为了防止引发连锁的数据库雪崩，甚至是整个系统的崩溃，我们暂停业务应用对缓存系统的接口访问。再具体点说，就是业务应用调用缓存接口时，缓存客户端并不把请求发给 Redis 缓存实例，而是直接返回，等到 Redis 缓存实例重新恢复服务后，再允许应用请求发送到缓存系统。 可以进行请求限流。这里说的请求限流，就是指，我们在业务系统的请求入口前端控制每秒进入系统的请求数，避免过多的请求被发送到数据库。 对于宕机的问题，我们可以使用主从复制的方式来解决的，主节点故障宕机了，从节点还可以切换成为主节点，继续提供缓存服务，避免了由于缓存实例宕机而导致的缓存雪崩问题。 缓存击穿缓存击穿是指，针对某个访问非常频繁的热点数据的请求，无法在缓存中进行处理，紧接着，访问该数据的大量请求，一下子都发送到了后端数据库，导致了数据库压力激增，会影响数据库处理其他请求。缓存击穿的情况，经常发生在热点数据过期失效时，如图： 为了避免缓存击穿给数据库带来的激增压力，我们的解决方法也比较直接，对于访问特别频繁的热点数据，我们就不设置过期时间了 缓存穿透缓存穿透是指要访问的数据既不在 Redis 缓存中，也不在数据库中，导致请求在访问缓存时，发生缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据。此时，应用也无法从数据库中读取数据再写入缓存，来服务后续请求，这样一来，缓存也就成了“摆设”，如果应用持续有大量请求访问数据，就会同时给缓存和数据库带来巨大压力，如下图所示： 缓存穿透一般发生在恶意攻击，专门访问数据库中没有的数据。 对于这种情况，使用布隆过滤器快速判断数据是否存在，避免从数据库中查询数据是否存在，减轻数据库压力。 布隆过滤器的特点就是，说它存在它可能不存在，说它不存在它肯定不存在","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"如何查看redis内存使用情况","slug":"redis/如何查看redis内存使用情况","date":"2021-11-14T12:00:24.000Z","updated":"2022-03-23T09:03:57.646Z","comments":true,"path":"blog/redis/如何查看redis内存使用情况/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8Bredis%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/","excerpt":"","text":"redis-cli info memory used_memory : 由 Redis 分配器分配的内存总量，以字节（byte）为单位 used_memory_human : 以人类可读的格式返回 Redis 分配的内存总量 used_memory_rss : 从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和 top 、 ps 等命令的输出一致。 used_memory_peak : Redis 的内存消耗峰值（以字节为单位） used_memory_peak_human : 以人类可读的格式返回 Redis 的内存消耗峰值 used_memory_lua : Lua 引擎所使用的内存大小（以字节为单位） mem_fragmentation_ratio : used_memory_rss 和 used_memory 之间的比率 mem_allocator : 在编译时指定的， Redis 所使用的内存分配器。可以是 libc 、 jemalloc 或者 tcmalloc 。 对比几个值 1）当 rss &gt; used ，且两者的值相差较大时，表示存在（内部或外部的）内存碎片。 内存碎片的比率可以通过 mem_fragmentation_ratio 的值看出。 2）当 used &gt; rss 时，表示 Redis 的部分内存被操作系统换出到交换空间了，在这种情况下，操作可能会产生明显的延迟。 当 Redis 释放内存时，分配器可能会，也可能不会，将内存返还给操作系统。 如果 Redis 释放了内存，却没有将内存返还给操作系统，那么 used_memory 的值可能和操作系统显示的 Redis 内存占用并不一致。查看 used_memory_peak 的值可以验证这种情况是否发生。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"redis6.0特性","slug":"redis/redis6.0特性","date":"2021-11-14T12:00:23.000Z","updated":"2022-03-23T09:03:57.645Z","comments":true,"path":"blog/redis/redis6.0特性/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/redis6.0%E7%89%B9%E6%80%A7/","excerpt":"","text":"Redis 一直被大家熟知的就是它的单线程架构，虽然有些命令操作可以用后台线程或子进程执行（比如数据删除、快照生成、AOF 重写），但是，从网络 IO 处理到实际的读写命令处理，都是由单个线程完成的。 随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 IO 的处理上，也就是说，单个主线程处理网络请求的速度跟不上底层网络硬件的速度。 Redis 的多 IO 线程只是用来处理网络请求的，对于读写命令，Redis 仍然使用单线程来处理。这是因为，Redis 处理请求时，网络处理经常是瓶颈，通过多个 IO 线程并行处理网络操作，可以提升实例的整体处理性能。而继续使用单线程执行命令操作，就不用为了保证 Lua 脚本、事务的原子性，额外开发多线程互斥机制了。这样一来，Redis 线程模型实现就简单了。 阶段一：服务端和客户端建立 Socket 连接，并分配处理线程首先，主线程负责接收建立连接请求。当有客户端请求和实例建立 Socket 连接时，主线程会创建和客户端的连接，并把 Socket 放入全局等待队列中。紧接着，主线程通过轮询方法把 Socket 连接分配给 IO 线程。 阶段二：IO 线程读取并解析请求主线程一旦把 Socket 分配给 IO 线程，就会进入阻塞状态，等待 IO 线程完成客户端请求读取和解析。因为有多个 IO 线程在并行处理，所以，这个过程很快就可以完成。 阶段三：主线程执行请求操作等到 IO 线程解析完请求，主线程还是会以单线程的方式执行这些命令操作。下面这张图显示了刚才介绍的这三个阶段，你可以看下，加深理解。 阶段四：IO 线程回写 Socket 和主线程清空全局队列当主线程执行完请求操作后，会把需要返回的结果写入缓冲区，然后，主线程会阻塞等待 IO 线程把这些结果回写到 Socket 中，并返回给客户端。 和 IO 线程读取和解析请求一样，IO 线程回写 Socket 时，也是有多个线程在并发执行，所以回写 Socket 的速度也很快。等到 IO 线程回写 Socket 完毕，主线程会清空全局队列，等待客户端的后续请求。 开启redis6.0多线程设置 io-thread-do-reads 配置项为 yes，表示启用多线程 1io-threads-do-reads yes 设置线程个数。一般来说，线程个数要小于 Redis 实例所在机器的 CPU 核个数，例如，对于一个 8 核的机器来说，Redis 官方建议配置 6 个 IO 线程。 1io-threads 6 如果你在实际应用中，发现 Redis 实例的 CPU 开销不大，吞吐量却没有提升，可以考虑使用 Redis 6.0 的多线程机制，加速网络处理，进而提升实例的吞吐量。 实现服务端协助的客户端缓存和之前的版本相比，Redis 6.0 新增了一个重要的特性，就是实现了服务端协助的客户端缓存功能，也称为跟踪（Tracking）功能。有了这个功能，业务应用中的 Redis 客户端就可以把读取的数据缓存在业务应用本地了，应用就可以直接在本地快速读取数据了。 不过，当把数据缓存在客户端本地时，我们会面临一个问题：如果数据被修改了或是失效了，如何通知客户端对缓存的数据做失效处理？ 6.0 实现的 Tracking 功能实现了两种模式，来解决这个问题。 第一种模式是普通模式。在这个模式下，实例会在服务端记录客户端读取过的 key，并监测 key 是否有修改。一旦 key 的值发生变化，服务端会给客户端发送 invalidate 消息，通知客户端缓存失效了。 在使用普通模式时，有一点你需要注意一下，服务端对于记录的 key 只会报告一次 invalidate 消息，也就是说，服务端在给客户端发送过一次 invalidate 消息后，如果 key 再被修改，此时，服务端就不会再次给客户端发送 invalidate 消息。 只有当客户端再次执行读命令时，服务端才会再次监测被读取的 key，并在 key 修改时发送 invalidate 消息。这样设计的考虑是节省有限的内存空间。毕竟，如果客户端不再访问这个 key 了，而服务端仍然记录 key 的修改情况，就会浪费内存资源。 我们可以通过执行下面的命令，打开或关闭普通模式下的 Tracking 功能。 1CLIENT TRACKING ON|OFF 第二种模式是广播模式。在这个模式下，服务端会给客户端广播所有 key 的失效情况，不过，这样做了之后，如果 key 被频繁修改，服务端会发送大量的失效广播消息，这就会消耗大量的网络带宽资源。 所以，在实际应用时，我们会让客户端注册希望跟踪的 key 的前缀，当带有注册前缀的 key 被修改时，服务端会把失效消息广播给所有注册的客户端。和普通模式不同，在广播模式下，即使客户端还没有读取过 key，但只要它注册了要跟踪的 key，服务端都会把 key 失效消息通知给这个客户端。 我给你举个例子，带你看一下客户端如何使用广播模式接收 key 失效消息。当我们在客户端执行下面的命令后，如果服务端更新了 user:\\id:1003 这个 key，那么，客户端就会收到 invalidate 消息。 1CLIENT TRACKING ON BCAST PREFIX user 这种监测带有前缀的 key 的广播模式，和我们对 key 的命名规范非常匹配。我们在实际应用时，会给同一业务下的 key 设置相同的业务名前缀，所以，我们就可以非常方便地使用广播模式。 不过，刚才介绍的普通模式和广播模式，需要客户端使用 RESP 3 协议，RESP 3 协议是 6.0 新启用的通信协议. 对于使用 RESP 2 协议的客户端来说，就需要使用另一种模式，也就是重定向模式（redirect）。在重定向模式下，想要获得失效消息通知的客户端，就需要执行订阅命令 SUBSCRIBE，专门订阅用于发送失效消息的频道 redis:invalidate。同时，再使用另外一个客户端，执行 CLIENT TRACKING 命令，设置服务端将失效消息转发给使用 RESP 2 协议的客户端。 我再给你举个例子，带你了解下如何让使用 RESP 2 协议的客户端也能接受失效消息。假设客户端 B 想要获取失效消息，但是客户端 B 只支持 RESP 2 协议，客户端 A 支持 RESP 3 协议。我们可以分别在客户端 B 和 A 上执行 SUBSCRIBE 和 CLIENT TRACKING，如下所示： 12345//客户端B执行，客户端B的ID号是303SUBSCRIBE _redis_:invalidate//客户端A执行CLIENT TRACKING ON BCAST REDIRECT 303 这样设置以后，如果有键值对被修改了，客户端 B 就可以通过 redis:invalidate 频道，获得失效消息了。 启用 RESP 3 协议Redis 6.0 实现了 RESP 3 通信协议，而之前都是使用的 RESP 2。在 RESP 2 中，客户端和服务器端的通信内容都是以字节数组形式进行编码的，客户端需要根据操作的命令或是数据类型自行对传输的数据进行解码，增加了客户端开发复杂度。 而 RESP 3 直接支持多种数据类型的区分编码，包括空值、浮点数、布尔值、有序的字典集合、无序的集合等。 所谓区分编码，就是指直接通过不同的开头字符，区分不同的数据类型，这样一来，客户端就可以直接通过判断传递消息的开头字符，来实现数据转换操作了，提升了客户端的效率。除此之外，RESP 3 协议还可以支持客户端以普通模式和广播模式实现客户端缓存。 总结","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"Redis中如何发现并优化big key？","slug":"redis/Redis中如何发现并优化big key？","date":"2021-11-14T12:00:22.000Z","updated":"2022-03-23T09:03:57.615Z","comments":true,"path":"blog/redis/Redis中如何发现并优化big key？/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/Redis%E4%B8%AD%E5%A6%82%E4%BD%95%E5%8F%91%E7%8E%B0%E5%B9%B6%E4%BC%98%E5%8C%96big%20key%EF%BC%9F/","excerpt":"","text":"寻找big key redis-cli自带–bigkeys，例如：redis-cli -h -a –bigkeys 获取生产Redis的rdb文件，通过rdbtools分析rdb生成csv文件，再导入MySQL或其他数据库中进行分析统计，根据size_in_bytes统计bigkey 通过python脚本，迭代scan key，每次scan 1000，对扫描出来的key进行类型判断，例如：string长度大于10K，list长度大于10240认为是big bigkeys 其他第三方工具，例如：redis-rdb-cli。地址：https://github.com/leonchen83/redis-rdb-cli 优化big key 优化big key的原则就是string减少字符串长度，list、hash、set、zset等减少成员数 以hash类型举例来说，对于field过多的场景，可以根据field进行hash取模，生成一个新的key，例如原来的hash_key:{filed1:value, filed2:value, filed3:value …}，可以hash取模后形成如下key:value形式hash_key:mod1:{filed1:value}hash_key:mod2:{filed2:value}hash_key:mod3:{filed3:value}…取模后，将原先单个key分成多个key，每个key filed个数为原先的1&#x2F;N string类型的big key，如文章正文，建议不要存入redis，用文档型数据库MongoDB代替或者直接缓存到CDN上等方式优化。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"通信协议","slug":"redis/原理/通信协议","date":"2021-11-14T12:00:21.000Z","updated":"2022-03-23T09:03:57.614Z","comments":true,"path":"blog/redis/原理/通信协议/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%8E%9F%E7%90%86/%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"RESP是Redis序列化协议。它是种直观的文本协议，优势在于实现过程异常简单，解析性能极好。规则： 单行字符串以“+”符号开头。 多行字符串以“$”符号开头，后跟字符串长度。（冒号扩起来） 整数值以“:”符号开头，后跟整数的字符串形式。 错误消息以“-”符号开头。 数组以“*”号开头，后跟数组的长度 客户端→服务器客户端向服务器发送的指令只有一种格式，多行字符串数组。比如一个简单的set指令set author codehole会被序列化成下面的字符串 1*3\\r\\n$3\\r\\nset\\r\\n$6\\r\\nauthor\\r\\n$8\\r\\ncodehole\\r\\n 更加直观的形式： 1234567*3$3set$6author$8codehole 服务器→客户端单行字待串响应 1+OK 错误晌应 1-ERR value is not an integer or out of range 整数晌应 1:1 多行字待串晌应使用双引号括起来的字符串就是多行字符串晌应 12$8codehole 数组晌应 1234567*3$4name$6laoqian$3age 嵌套 12345678910*2$10*3$4info$5books$6author","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"线程IO模型","slug":"redis/原理/线程IO模型","date":"2021-11-14T12:00:20.000Z","updated":"2022-03-23T09:03:57.613Z","comments":true,"path":"blog/redis/原理/线程IO模型/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%8E%9F%E7%90%86/%E7%BA%BF%E7%A8%8BIO%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"Redis是个单线程程序（这里的单线程指一个线程处理客户端的请求，redis内部设计也会用多线来完成一些操作的，比如RDB备份）。因为它的所有数据都在内存中,所有的运算都是内存级别的运算。 redis使用了高效的数据结构，比如hash表跳表等 多路复用: 详情 指令队列: Redis会将每个客户端套接字都关联个指令队列。客户端的指令通过队列来排队进行顺序处理，先到先服务。 晌应队列：Redis同样也会为每个客户端套接字关联一个晌应队列。Redis服务器通过响应队列来将指令的返回结果回复给客户端。如果队列为空，那么意昧着连接暂时处于空闲状态，不需要去获取写事件，也就是可以将当前的客户端描述符从write_fds里面移出来。等到队列有数据了，再将描述符放进去，避免select系统调用立即返回写事件，结果发现没什么数据可以写，出现这种情况的线程会令CPU消耗飘升。 定时任务：Redis的定时任务会记录在一个被称为“最小堆”的数据结构中。在这个堆中，最快要执行的任务排在堆的最上方。在每个循环周期里，Redis都会对最小堆里面已经到时间点的任务进行处理。处理完毕后，将最快要执行的任务还需要的时间记录下来，这个时间就是select系统调用的timeout参数。因为Redis知道未来timeout的值的时间内，没有其他定时任务需要处理，所以可以安心睡眠timeout的值的时间。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"管道","slug":"redis/原理/管道","date":"2021-11-14T12:00:19.000Z","updated":"2022-03-23T09:03:57.612Z","comments":true,"path":"blog/redis/原理/管道/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%8E%9F%E7%90%86/%E7%AE%A1%E9%81%93/","excerpt":"","text":"管道不是redis服务器提供的一种技术，而是由客户端提供的。实际是客户端把多条命令，写入到了链接的输出缓冲区中，服务器将结果一起写入到了输出缓冲区中。这样可以充分的利用缓冲区。socket套接字网络通信模型：","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"持久化","slug":"redis/原理/持久化","date":"2021-11-14T12:00:18.000Z","updated":"2022-03-23T09:03:57.611Z","comments":true,"path":"blog/redis/原理/持久化/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%8E%9F%E7%90%86/%E6%8C%81%E4%B9%85%E5%8C%96/","excerpt":"","text":"Redis的持久化机制有两种，第一种是快照（RDB），第二种是AOF日志。 快照（RDB）是一次全量备份，是内存数据的二进制序列化形式,在存储上非常紧凑。 AOF日志是连续的增量备份，是内存数据修改的指令记录文本。AOF日志在长期的运行过程中会变得无比庞大，数据库重启时需要加载AOF日志进行指令重放，这个时间就会无比漫长，所以需要定期进行AOF重写，给AOF日志进行瘦身。 快照原理(一般自动触发)我们知道Redis是单线程程序，这个线程要同时负责多个客户端套接字的并发读写操作和内存数据结构的逻辑读写。 在服务线上请求的同时，Redis还需要进行内存快照，内存快照要求Redis必须进行文件IO操作，可文件IO操作不能使用多路复用API。 这意昧着单线程在服务线上请求的同时，还要进行文件IO操作，而文件IO操作会严重拖累服务器的性能。 还有个重要的问题，为了不阻塞线上的业务，Redis就需要一边持久化，一边响应客户端的请求。 Redis使用操作系统的多进程COW(Copy On Write），写时复制)机制来实现快照持久化。（Java的juc包的CopyOnWriteArrayList也是使用了这种原理，当写操作时把原数组复制一份，写操作在新的数组上进行，所以该类的读操作是天然的线程安全，但写操作不是需要加锁） Redis在持久化时会调用glibc的函数fork产生一个子进程，快照持久化完全交给子进程来处理，父进程继续处理客户端请求。子进程刚刚产生时，它和父进程共享内存里面的代码段和数据段。这是Linux操作系统的机制，为了节约内存资源。 子进程做数据持久化，不会修改现有的内存数据结构，它只是对数据结构进行遍历读取，然后序列化写到碰盘中。但是父进程不一样，它必须持续服务客户端请求，然后对内存数据结构进行不间断的修改。 这个时候就会使用操作系统的cow机制来进行数据段页面的分离，如图数据段是由很多操作系统的页面组合而成，当父进程对其中一个页面的数据进行修改时，会将被共享的页面复制一份分离出来，然后对这个复制的页面进行修改。这时子进程相应的页面是没有变化的，还是进程产生时那一瞬间的数据。随着父进程修改操作的持续进行，越来越多的共享页面被分离出来，内存就会持续增长，但是也不会超过原有数据内存的2 倍大小。另外，Redis实例里冷数据占的比例往往是比较高的，所以很少会出现所有的页面都被分离的情况，被分离的往往只有其中一部分页面。子进程因为数据没有变化，它能看到的内存里的数据在进程产生的一瞬间就凝固了，再也不会改变，这也是为什么Redis的持久化叫“快照”的原因。接下来子进程就可以非常安心地遍历数据，进行序列化写磁盘了。流程： 缺点 无法做到实时持久化,每次都要创建子进程,频繁操作成本过高 保存后的二进制文件,存在老版本不兼容新版本 rdb 文件的问题 AOF原理Redis会在收到客户端修改指令后，进行参数校验、逻辑处理，如果没问题，就立即将该指令文本存储到AOF曰志中，也就是说，先执行指令才将曰志存盘。 redis 的 AOF 配置详解: appendonly yes 启用 aof 持久化方式# appendfsync always 每收到写命令就立即强制写入磁盘,最慢的,但是保证完全的持久化,不推荐使 appendfsync everysec 每秒强制写入磁盘一次,性能和持久化方面做了折中,推荐 appendfsync no 完全依赖 os, 性能最好,持久化没保证 (操作系统自身的同步) no-appendfsync-on-rewrite yes 正在导出 rdb 快照的过程中,要不要停止同步aof auto-aof-rewrite-percentage 100 aof 文件大小比起上次重写时的大小,增长率100%时,重写 auto-aof-rewrite-min-size 64mb aof 文件,至少超过 64M 时,重写 AOF重写Redis提供了bgrewriteaof指令用于对AOF曰志进行瘦身，其原理就是开辟一个子进程对内存进行遍历，转换成一系列Redis的操作指令，序列化到一个新的AOF曰志文件中。序列化完毕后再将操作期间发生的增量AOF日志追加到这个新的AOF日志文件中，追加完毕后就立即替代旧的AOF曰志文件了，瘦身工作就完成了。 fsyncfsync函数是Linux提供的，可以将指定文件的内容强制从内核缓存刷到磁盘。 AOF日志是以文件的形式存在的，当程序对AOF日志文件进行写操作时，实际上是将内容写到了内核为文件描述符分配的一个内存缓存中，然后内核会异步将脏数据刷回到磁盘的。 所以在生产环境的服务器中，Redis通常是每隔1s左右执行一次fsync操作，这个1s的周期是可以配置的。这是在数据安全性和性能之间做的一个折中，在保持高性能的同时，尽可能使数据少丢失。 AOF 的 fsync 是一个耗时的 IO 操作， 它会降低 Redis 性能， 同时也会增加系 统 IO 负担。 所以通常 Redis 的主节点不会进行持久化操作， 持久化操作主要在从节点进行。 从节点是备份节点， 没有来自客户端请求的压力， 它的操作系统资源往往比较充沛。 但是如果出现网络分区， 从节点长期连不上主节点， 就会出现数据不一致的问 题，特别是在网络分区出现的情况下， 主节点一旦不小心看机了， 那么数据就会丢 失，所以在生产环境下要做好实时监控工作， 保证网络畅通或者能快速修复。 另外 还应该再增加一个从节点以降低网络分区的概率， 只要有一个从节点数据同步正常， 数据也就不会轻易丢失。 redis 重启时恢复加载 AOF 与 RDB 顺序及流程 当 AOF 和 RDB 文件同时存在时,优先加载AOF 若关闭了 AOF,加载 RDB 文件 加载 AOF&#x2F;RDB 成功,redis 重启成功 AOF&#x2F;RDB 存在错误,redis 启动失败并打印错误信息 Redis4 混合持久化重启Redis时，我们很少使用rdb来恢复内存状态，因为会丢失大量数据。我们通常使用AOF日志重放，但是重放AOF日志相对于使用rdb来说要慢很多，这样在Redis实例很大的时候，启动需要花费很长的时间。 Redis4 提供了一种混合持久化。将rdb文件的内容和增量的AOF日志文件存在一起。这里的AOF日志不再是全量的曰志，而是自持久化开始到持久化结束的这段时间发生的增量AOF曰志，通常这部分AOF日志很小。于是在Redis重启的时候，可以先加载rdb的内窑，然后再重放增量AOF日志，就可以完全替代之前的AOF全量文件重放，重启效率因此得到大幅提升。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"小对象压缩","slug":"redis/原理/小对象压缩","date":"2021-11-14T12:00:17.000Z","updated":"2022-03-23T09:03:57.607Z","comments":true,"path":"blog/redis/原理/小对象压缩/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%8E%9F%E7%90%86/%E5%B0%8F%E5%AF%B9%E8%B1%A1%E5%8E%8B%E7%BC%A9/","excerpt":"","text":"Red is 如果使用 32bit 进行编译， 内部所有数据结构所使用的指针空间占用会少 一半， 如果你的 Redis 使用内存不超过 4GB ，可以考虑使用 32bit 进行编译， 能够节 约大量内存。 4GB 的容量作为一些小型站点的缓存数据库是绰绰有余的， 如果不足 还可以通过增加实例的方式来解决。 如果Redis内部管理的集合数据结构很小，它会使用紧凑存储形式压缩存储(压缩列表)。这就好比 Hashmap本来是二维结构，但是如果内部元素比较少，使用二维结构反而浪费空间，还不如使用一维数组进行存储，需要查找时，因为元素少，进行遍历也很快，甚至可以比 Hashmap本身的査找还要快。 如果它存储的是hash结构，那么key和 value会作为两个 entry被相邻存储。 如果它存储的是zset结构，那么 value和 score会作为两个 entry被相邻存储。 如图所示： Redis的intset是一个紧凑的整数数组结构，用于存放元素都是整数且元素个数较少的set集合。 如果整数可以用uint16表示，那么intset的元素就是16位的数组，如果新加入的整数超过了uint16的表示范围，那么就使用uint32表示，如果新加入的元素超过了uint32的表示范围，那么就使用uint64表示。 Redis支持set集合动态从uint16升级到uint32，再升级到uint64 如果set 里存储的是字符串，那么sadd立即升级为hashtable 结构。 存储界限 ：当集合对象的元素不断增加，或者某个 value 值过大，这种小对象存储也会被升级为标准结构。Redis 规定在小对象存储结构的限制条件如下: hash-max-zipmap-entries 512 # hash 的元素个数超过 512 就必须用标准结构存储 hash-max-zipmap-value 64 # hash 的任意元素的 key&#x2F;value 的长度超过 64 就必须用标准结构存储 list-max-ziplist-entries 512 # list 的元素个数超过 512 就必须用标准结构存储 list-max-ziplist-value 64 # list 的任意元素的长度超过 64 就必须用标准结构存储 zset-max-ziplist-entries 128 # zset 的元素个数超过 128 就必须用标准结构存储 zset-max-ziplist-value 64 # zset 的任意元素的长度超过 64 就必须用标准结构存储 set-max-intset-entries 512 # set 的整数元素个数超过 512 就必须用标准结构存储","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"内存回收机制","slug":"redis/原理/内存回收机制","date":"2021-11-14T12:00:16.000Z","updated":"2022-03-23T09:03:57.601Z","comments":true,"path":"blog/redis/原理/内存回收机制/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%8E%9F%E7%90%86/%E5%86%85%E5%AD%98%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/","excerpt":"","text":"Redis 并不总是可以将空闲内存立即归还给操作系统。 如果当前 Redis 内存有 10G，当你删除了 1GB 的 key 后，再去观察内存，你会发现 内存变化不会太大。原因是操作系统回收内存是以页为单位，如果这个页上只要有一个 key 还在使用，那么它就不能被回收。Redis 虽然删除了 1GB 的 key，但是这些 key 分散到了 很多页面中，每个页面都还有其它 key 存在，这就导致了内存不会立即被回收。 不过，如果你执行 flushdb，然后再观察内存会发现内存确实被回收了。原因是所有的 key 都干掉了，大部分之前使用的页面都完全干净了，会立即被操作系统回收。 Redis 虽然无法保证立即回收已经删除的 key 的内存，但是它会重用那些尚未回收的空 闲内存。这就好比电影院里虽然人走了，但是座位还在，下一波观众来了，直接坐就行。而操作系统回收内存就好比把座位都给搬走了。 什么是内存碎片？ 也就是说明明有空间，但由于连续的空间不足，不能分配出去使用。 内存碎片是如何形成的？内存分配算法Redis 为了保持自身结构的简单性，在内存分配这里直接做了甩手掌柜，将内存分配的 细节丢给了第三方内存分配库去实现。目前 Redis 可以使用 jemalloc(facebook) 库来管理内 存，也可以切换到 tcmalloc(google)。因为 jemalloc 相比 tcmalloc 的性能要稍好一些，所以 Redis 默认使用了 jemalloc。127.0.0.1:6379&gt; info memory jemalloc 的分配策略之一，是按照一系列固定的大小划分内存空间，例如 8 字节、16 字节、32 字节、48 字节，…, 2KB、4KB、8KB 等。当程序申请的内存最接近某个固定值时，jemalloc 会给它分配相应大小的空间。 这样的分配方式本身是为了减少分配次数。例如，Redis 申请一个 20 字节的空间保存数据，jemalloc 就会分配 32 字节，此时，如果应用还要写入 10 字节的数据，Redis 就不用再向操作系统申请空间了，因为刚才分配的 32 字节已经够用了，这就避免了一次分配操作。 但是，如果 Redis 每次向分配器申请的内存空间大小不一样，这种分配方式就会有形成碎片的风险. 键值对大小不一样和删改操作第一键值对大小不一样： 删改操作： 如何判断是否有内存碎片？Redis 是内存数据库，内存利用率的高低直接关系到 Redis 运行效率的高低。为了让用户能监控到实时的内存使用情况，Redis 自身提供了 INFO 命令，可以用来查询内存使用的详细信息，命令如下： 12345678INFO memory# Memoryused_memory:1073741736used_memory_human:1024.00Mused_memory_rss:1997159792used_memory_rss_human:1.86G…mem_fragmentation_ratio:1.86 这里有一个 mem_fragmentation_ratio 的指标，它表示的就是 Redis 当前的内存碎片率。它是两个指标used_memory_rss和 used_memory相除的结果。 1mem_fragmentation_ratio = used_memory_rss/ used_memory used_memory_rss：是操作系统实际分配给 Redis 的物理内存空间，里面就包含了碎片 used_memory：是 Redis 为了保存数据实际申请使用的空间 一些经验阈值： mem_fragmentation_ratio 大于 1 但小于 1.5。这种情况是合理的。这是因为，刚才我介绍的那些因素是难以避免的。毕竟，内因的内存分配器是一定要使用的，分配策略都是通用的，不会轻易修改；而外因由 Redis 负载决定，也无法限制。所以，存在内存碎片也是正常的。 mem_fragmentation_ratio 大于 1.5 。这表明内存碎片率已经超过了 50%。一般情况下，这个时候，我们就需要采取一些措施来降低内存碎片率了。 如何清理内存碎片？一个“简单粗暴”的方法就是重启 Redis 实例。当然，这并不是一个“优雅”的方法，毕竟，重启 Redis 会带来两个后果： 如果 Redis 中的数据没有持久化，那么，数据就会丢失； 即使 Redis 数据持久化了，我们还需要通过 AOF 或 RDB 进行恢复，恢复时长取决于 AOF 或 RDB 的大小，如果只有一个 Redis 实例，恢复阶段无法提供服务。 从 4.0-RC3 版本以后，Redis 自身提供了一种内存碎片自动清理的方法： 内存碎片清理，简单来说，就是“搬家让位，合并空间”。 不过，需要注意的是：碎片清理是有代价的，操作系统需要把多份数据拷贝到新位置，把原有空间释放出来，这会带来时间开销。因为 Redis 是单线程，在数据拷贝时，Redis 只能等着，这就导致 Redis 无法及时处理请求，性能就会降低。而且，有的时候，数据拷贝还需要注意顺序，就像刚刚说的清理内存碎片的例子，操作系统需要先拷贝 D，并释放 D 的空间后，才能拷贝 B。这种对顺序性的要求，会进一步增加 Redis 的等待时间，导致性能降低。 所幸的是，Redis提供了自动内存碎片整理功能：activedefrag 配置项设置为 yes。比如： 1config set activedefrag yes 这个命令只是启用了自动清理功能，但是，具体什么时候清理，会受到下面这两个参数的控制。这两个参数分别设置了触发内存清理的一个条件，如果同时满足这两个条件，就开始清理。在清理的过程中，只要有一个条件不满足了，就停止自动清理。 active-defrag-ignore-bytes 100mb：表示内存碎片的字节数达到 100MB 时，开始清理； active-defrag-threshold-lower 10：表示内存碎片空间占操作系统分配给 Redis 的总空间比例达到 10% 时，开始清理。 也提供了连个参数分别用于控制清理操作占用的 CPU 时间比例的上、下限： active-defrag-cycle-min 25： 表示自动清理过程所用 CPU 时间的比例不低于 25%，保证清理能正常开展； active-defrag-cycle-max 75：表示自动清理过程所用 CPU 时间的比例不高于 75%，一旦超过，就停止清理，从而避免在清理时，大量的内存拷贝阻塞 Redis，导致响应延迟升高。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"过期策略","slug":"redis/拓展/过期策略","date":"2021-11-14T12:00:15.000Z","updated":"2022-03-23T09:03:57.589Z","comments":true,"path":"blog/redis/拓展/过期策略/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E6%8B%93%E5%B1%95/%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5/","excerpt":"","text":"过期的key集合Redis会将每个设置了过期时间的key放入一个独立的字典中，以后会定时遍历这个字典来删除到期的key。 定时扫描策略Redis默认每秒进行10次过期扫描，过期扫描不会遍历过期字典中所有的key，而是采用了一种简单的贪心策略： 从过期字典中随机选出20个key。 删除这20个key中已经过期的key。 如果过期的key的比例超过1&#x2F;4，那就重复步骤1 同时，为了保证过期扫描不会出现循环过度，导致结程卡死的现象，算法还增加了扫描时间的上限，默认不会超过25ms 假设一个大型的 Redis实例中所有的key在同一时过期了，会出现怎样的结果呢? 毫无疑问， Redis会持续扫描过期字典(循环多次)，直到过期字典中过期的key变得稀疏，オ会停止(循环次数明显下降)。这就会导致线上读写请求出现明显的卡顿现象。导致这种卡顿的另外一种原因是内存管理器需要频繁回收内存页，这也会产生一定的CPU消耗。 当客户端请求到来时，服务器如果正好进入过期扫描状态，客户端的请求将会等待至少25ms后オ会进行处理，如果客户端将超时时间设置得比较短，比如10ms，那么就会出现大量的链接因为超时而关闭，业务端就会出现很多异常，而且这时你还无法从 Redis的 slowing中看到慢查询记录，因为慢查询指的是逻辑处理过程慢，不包含等待时间。 所以业务开发人员一定要注意过期时间，如果有大批量的key过期，要给过期时间设置一个随机范围，而不能全部在同一时间过期。 惰性策略在客户端访问这个key的时候，Redis对key的过期时间进行检查，如果过期了就立即删除。如果说定时删除是集中处理，那么惰性删除就是零散处理。 建议如果key的过期时间过于集中可能会导致缓存击穿或者卡顿，通过将过期时间随机化总是能很好地解决这个问题。 从节点的过期策略从节点不会进行过期扫描，从节点对过期的处理是被动的。主节点在key到期时，会在AOF文件里增加一条del指令，同步到所有的从节点，从节点通过执行这条del指令来删除过期的key。 因为指令同步是异步进行的，所以如果主节点过期的key的del指令没有及时同步到从节点的话，就会出现主从数据的不一致，主节点没有的数据在从节点里还存在，集群环境分布式锁的算法漏洞就是因为这个同步延迟产生的。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"redis5的新特性Stream","slug":"redis/拓展/redis5的新特性Stream","date":"2021-11-14T12:00:14.000Z","updated":"2022-03-23T09:03:57.588Z","comments":true,"path":"blog/redis/拓展/redis5的新特性Stream/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E6%8B%93%E5%B1%95/redis5%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7Stream/","excerpt":"","text":"Redis Stream的结构如图所示，它有一个消息链表，将所有加入的消息都串起来， 每个消息都有一个唯一的D和对应的内容。消息是持久化的， Redis重启后，内容还在。 每个 Stream都有唯一的名称，它就是 Redis I的key，在我们首次使用xadd指令追加消息时自动创建。 每个Stream都可以挂多个消费组( Consumer Group)，每个消费组会有个游标last_delivered_id在Stream数组之上往前移动，表示当前消费组已经消费到哪条消息了。每个消费组都有一个Stream内唯一的名称，消费组不会自动创建，它需要单独的指令 group create进行创建，需要指定 Stream的某个消息ID开始消费，这个ID用来初始化last_delivered_id变量。而且每个消费者内都有一个pending_ids变量，用来记录组内消费者读取了，但还没ack的消息。 每个消费组的状态都是独立的，相互不受影响。也就是说同一份 Stream内部的消息会被每个消费组都消费到。 同一个消费组可以挂接多个消费者( Consumer)，这些消费者之间是竞争关系任意一个消费者读取了消息都会使游标last_delivered_id往前移动。每个消费者有个组内唯一名称。 消费者内部会有一个状态变量 pending_ids，它记录了当前已经被客户端读取， 但是还没有ack的消息。如果客户端没有ack，这个变量里面的消息ID就会越来越多，一旦某个消息被ack，它就开始减少。这个 pending_ids变量在 Redis官方被称为PEL，也就是 Pending Entries List，这是一个核心的数据结构，它用来确保客户端至少消费了消息一次，而不会在网络传输的中途丢失了而没被处理。 消息的ID消息ID的形式是 timestampinmillis-sequence，例如1527846880572-5，它表示当前的消息在毫米时间戳1527846880572时产生，并且是该毫秒内产生的第5条消息。消息ID可以由服务器自动生成，也可以由客户端自己指定，但是形式必须是“整数整数”，而且后面加入的消息的1D必须要大于前面的消息ID 消息内容消息内容就是键值对，形如hash 结构的键值对 Stream操作XADD：插入消息，保证有序，可以自动生成全局唯一 ID； XDEL：从Stream中删除消息，这里的删除仅仅是设置标志位。需要执行del命令活限制stream的长度才会真正的删除数据回收内存。 XRANGE：获取Stream 中的消息列表，会自动过滤已经删除的消息。 XLEN：获取Stream消息长度。 XREAD：用于读取消息，可以按 ID 读取数据； XREADGROUP：按消费组形式读取消息； XPENDING：XPENDING 命令可以用来查询每个消费组内所有消费者已读取但尚未确认的消息。 XACK： XACK 命令用于向消息队列确认消息处理已完成。 12345# * 号表示服务器自动生成凹，后面顺序跟着key、valuexadd codehole * name xyz age 27xlen codehole# - 表示最小值 +表示最大值xrange codehole - + 1xdel codehole 1527849609889-0 独立消费我们可以在不定义消费组的情况下进行 Stream消息的独立消费，当Stream没有新消息时，甚至可以阻塞等待。 Redis设计了一个单独的消费指令 xread，可以将Stream当成普通的消息队列(list)来使用。使用 xread时，我们可以完全忽略消费组的存在，就好像 Stream是一个普通的列表一样。 123456# 从Stream头部读取两条消息，0-0表示头部xread count 2 streams codehole 0-0# 从尾部阻塞等待新消息到来，下面的指令会堵住，直到新消息到来# block 表示没消息时堵塞，block 0 表示没消息时一直堵塞下去直到有消费为止xread block 0 count 1 streams codehole $ 客户端如果想要使用 xread进行顺序消费，那么一定要记住当前消费到哪里了，也就是返回的消息ID。下次继续调用 xread时，将上次返回的最后一个消息ID作为参数传递进去，就可以继续消费后续的消息。 block0表示永远阻塞，直到消息到来; block1000表示阻塞1s，如果1s内没有任何消息到来，就返回nil 创建消费组12345# 表示从头部开始消费xgroup create codehole cgl 0-0# $表示从尾部开始消费，只接受新消息，当前Stream消息会全部忽略xgroup create codehole cg2 $ 消费组消费 1234567# &gt; 号表示从当前消费组的last_delivered_id后面开始读# 每当消费者读取一条消息，last_delivered_id变量就会前进xreadgroup GROUP cgl cl count 1 streams codehole &gt;# 堵塞等待消息# block 0 的语意和上面的一样xreadgroup GROUP cgl cl block 0 count 1 streams 查看Stream的信息 ack一条消息123xack codehole cgl 1527851486781-0# ack多小消息xack codehole cgl 1527851493405-0 1527851498956-0 1527852774092-0 获取指定消费组、消费者中，待确认（ACK）的消息12345678127.0.0.1:6379&gt; XPENDING my_stream my_group1) (integer) 2 # 消费组中，所有消费者的pending消息数量2) &quot;1605524657157-0&quot; # pending消息中的，最小消息ID3) &quot;1605524665215-0&quot; # pending消息中的，最大消息ID4) 1) 1) &quot;my_consumer1&quot; # 消费者1 2) &quot;1&quot; # 有1条待确认消息 2) 1) &quot;my_consumer2&quot; # 消费者2 2) &quot;1&quot; # 有2条待确认消息 12345127.0.0.1:6379&gt; XPENDING my_stream my_group 0 + 10 my_consumer11) 1) &quot;1605524665215-0&quot; # 待ACK消息ID 2) &quot;my_consumer1&quot; # 所属消费者 3) (integer) 847437 # 消息自从被消费者获取后到现在过去的时间(毫秒) - idle time 4) (integer) 1 # 消息被获取的次数 - delivery counter Stream 消息太多怎么办要是消息积累太多，Stream的链表岂不是很长，内容会不会爆掉？xdel 指令又不会删除消息，它只是给消息做了个标志位。 在xadd 的指令中提供个定长长度参数maxlen ，就可以将老的消息干掉，确保链表不超过指定长度。 分区PartitionRedis的服务器没有原生支持分区能力，如果想要使用分区，那就需要分配多个Stream，然后在客户端使用一定的策略来生产消息到不同的 Stream。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"Redis-del-懒惰删除","slug":"redis/拓展/Redis-del-懒惰删除","date":"2021-11-14T12:00:13.000Z","updated":"2022-03-23T09:03:57.564Z","comments":true,"path":"blog/redis/拓展/Redis-del-懒惰删除/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E6%8B%93%E5%B1%95/Redis-del-%E6%87%92%E6%83%B0%E5%88%A0%E9%99%A4/","excerpt":"","text":"删除指令del会直接释放对象的内存，大部分情况下，这个指令非常快，没有明显延迟。不过如果被删除的key是一个非常大的对象，比如一个包含了上千万个元素的hash，那么删除操作就会导致单线程卡顿。Redis为了解决这个卡顿问题，在4.0版本里引入了unlink指令，它能对删除操作进行懒处理，丢给后台线程来异步回收内存。 不是所有的unlink操作都会延后处理，如果对应key所占用的内存很小，延后处理就没有必要了，这时候Redis会将对应key的内存立即回收，跟del指令一样。 当unlink指令发出时，它只是把大树中的一个树枝剪断了，然后扔到旁边的火堆(异步线程池)里焚烧。在树枝离开大树的瞬间，它就再也无法被主线程中的其他指令访问到了，因为主线程只会沿着这棵大树来访问。 异步队列","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"LRU","slug":"redis/拓展/LRU","date":"2021-11-14T12:00:12.000Z","updated":"2022-03-23T09:03:57.562Z","comments":true,"path":"blog/redis/拓展/LRU/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E6%8B%93%E5%B1%95/LRU/","excerpt":"","text":"在生产环境中我们是不允许Redis出现交换行为的，为了限制最大使用内存，Redis提供了配置参数maxmemory来限制内存超出期望大小。 当实际内存超出maxmemory时，Redis提供了几种可选策略(maxmemory-policy)来让用户自己决定该如何腾出新的空间以继续提供读写服务。 noeviction 不会继续服务写请求 (DEL 请求可以继续服务)，读请求可以继续进行。这样可以保证不会丢失数据，但是会让线上的业务不能持续进行。这是默认的淘汰策略。 volatile-lru 尝试淘汰设置了过期时间的 key，最少使用的 key 优先被淘汰。没有设置过期时间的 key 不会被淘汰，这样可以保证需要持久化的数据不会突然丢失。 volatile-ttl 跟上面一样，除了淘汰的策略不是 LRU，而是 key 的剩余寿命 ttl 的值，ttl越小越优先被淘汰。 volatile-random 跟上面一样，不过淘汰的 key 是过期 key 集合中随机的 key。 allkeys-lru 区别于 volatile-lru，这个策略要淘汰的 key 对象是全体的 key 集合，而不只是过期的 key 集合。这意味着没有设置过期时间的 key 也会被淘汰。 allkeys-random 跟上面一样，不过淘汰的策略是随机的 key。 volatile-xxx 策略只会针对带过期时间的 key 进行淘汰，allkeys-xxx 策略会对所有的key 进行淘汰。如果你只是拿 Redis 做缓存，那应该使用 allkeys-xxx，客户端写缓存时 不必携带过期时间。如果你还想同时使用 Redis 的持久化功能，那就使用 volatile-xxx 策略，这样可以保留没有设置过期时间的 key，它们是永久的 key 不会被 LRU 算法淘 汰。 实现LRU算法除了需要 key&#x2F;value字典外，还需要附加一个链表，链表中的元素按照一定的顺序进行排列。当空间满的时侯，会踢掉链表尾部的元素。当字典的某个元素被访问时，它在链表中的位置会被移动到表头，所以链表的元素排列顺序就是元素最近被访问的时间顺序。 位于链表尾部的元素就是不被重用的元素，所以会被踢掉。位于表头的元素就是最近刚刚被人用过的元素，所以暂时不会被踢。 Redis使用的是一种近似LRU算法，它跟LRU算法还不太一样。之所以不使用LRU算法，是因为其需要消耗大量的额外内存，需要对现有的数据结构进行较大的改造。近似LRU算法很简单，在现有数据结构的基础上使用随机采样法来淘汰元素， 能达到和LRU算法非常近似的效果。 Redis为实现近似LRU算法，给每个key增加了一个额外的小字段，这个字段的长度是24个bit，也就是最后一次被访问的时间戳。 Redis处理key过期方式分为集中处理（定时任务）和懒惰处理，而LRU淘汰不一样，它只有懒惰处理。当redis执行写操作是，发现内存超出了maxmemory，就会执行一次LRU淘汰算法。这个算法也很简单，就是随机采样出5(该数量可以设置〉个key，然后淘汰最旧的key，如果淘汰后内存还是超出maxmemory，那就继续随机采样淘汰，直到内存低于maxmemory可为止。 如何采样要看 maxmemory-policy I的设置，如果是 allkeys，就从所有的key字典中随机采样，如果是 volatile，就从带过期时间的key字典中随机采样。每次采样多少个key取决于 maxmemory-samples的设置，默认为5。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"03Redis集群","slug":"redis/集群/03Redis集群","date":"2021-11-14T12:00:11.000Z","updated":"2022-03-23T09:03:57.561Z","comments":true,"path":"blog/redis/集群/03Redis集群/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E9%9B%86%E7%BE%A4/03Redis%E9%9B%86%E7%BE%A4/","excerpt":"","text":"Redis Cluster是去中心化的，Redis Cluster将所有数据划分为16384个槽位，每个节点分配不同的槽位，也就是说每个节点负责整个集群的一部分数据，每个节点负责的数据多少可能不一样。这三个节点相互连接组成一个对等的集群。（使用了一致性哈希算法） 当Redis Cluster的客户端来连接集群时，也会得到一份集群的槽位配置信息。这样当客户端要查找某个key时，可以直接定位到目标节点。 客户端为了可以直接定位某个具体的key所在的节点，需要缓存槽位相关信息，这样才可以准确快速地定位到相应的节点。同时因为可能会存在客户端与服务器存储槽位的信息不一致的情况，还需要纠正机制来实现槽位信息的校验调整。 槽位定位算法Redis Cluster默认会对key值使用crcl6算法进行hash，得到一个整数值，然后用这个整数值对16384进行取模来得到具体槽位。 Redis Cluster还允许用户强制把某个key挂在特定槽位上。通过在key字符串里面嵌入tag标记，这就可以强制key所挂的槽位等于tag所在的槽位。 跳转当客户端向个错误的节点发出了指令后，该节点会发现指令的key所在的槽位并不归自己管理，这时它会向客户端发送一个特殊的跳转指令携带目标操作的节点地址，告诉客户端去连接这个节点以获取数据。MOVED指令的第一个参数3999是key对应的槽位编号，后面是目标节点地址。MOVED指令前面有-个减号，表示该指令是一个错误消息。 客户端在收到MOVED指令后，要立即纠正本地的槽位映射表。后续所有key将使用新的槽位映射表。 迁移添加结点或者使用官方提供的redis-trib工具，都会使槽位调整的情况。 Redis迁移的单位是槽，Redis一个槽一个槽地进行迁移，当一个槽正在迁移时，这个槽就处于中间过渡状态。如图所示这个槽在源节点的状态为migrating，在目标节点的状态为importing，表示数据正在从源节点流向目标节点。 大致流程如下：从源节点获取内容–&gt;存到目标节点–&gt;从源节点删除内容。注意这里的迁移过程是同步的。 在迁移过程中，客户端访问的流程会有很大的变化 首先新旧两个节点对应的槽位都存在部分key数据。客户端先尝试访问旧节点，如果对应的数据还在旧节点里面，那么旧节点正常处理。如果对应的数据不在旧节点里面，那么有两种可能，要么该数据在新节点里，要么根本就不存在。旧节点不知道是哪种情况，所以它会向客户端返回一个-ASK targetNodeAddr的重定向指令。客户端收到这个重定向指令后，先去目标节点执行一个不带任何参数的ASKING指令，然后在目标节点再重新执行原先的操作指令。 为什么需要执行一个不带参数的ASKING指令呢? 因为在迁移没有完成之前，按理说这个槽位还是不归新节点管理的，如果这个时候向目标节点发送该槽位的指令，节点是不认的，它会向客户端返回一个-MOVED重定向指令告诉它去源节点去执行。如此就会形成重定向循环。ASKING指令的目标就是打开目标节点的选项，告诉它下一条指令不能不理，而要当成自己的槽位来处理。 容错Redis Cluster可以为每个主节点设置若干个从节点，当主节点发生故障时，集群会自动将其中某个从节点提升为主节点。如果某个主节点没有从节点，那么当它发生故障时，集群将完全处于不可用状态。不过Redis也提供了一个参数cluster­-require-full-coverage可以允许部分节点发生故障，其他节点还可以继续提供对外访问。 故障转移（可能下线(PFail)与确定下线(Fail)）因为Redis Cluster是去中心化的，一个节点认为某个节点失联了并不代表所有的节点都认为它失联了，所以集群还得经过一次协商的过程，只有当大多数节点都认定某个节点失联了，集群才认为该节点需要进行主从切换来容错。 Redis集群节点采用Gossip协议来广播自己的状态以及改变对整个集群的认知。比如个节点发现某个节点失联了(PFail，即Possibly Fail)，它会将这条信息向整个集群广播，其他节点就可以收到这点的失联信息。如果收到了某个节点失联的节点数量(PFail Count)已经达到了集群的大多数，就可以标记该失联节点为确定下线状态(Fail)，然后向整个集群广播，强迫其他节点也接受该节点已经下线的事实，并立即对该失联节点进行主从切换。 结点重新上线结点重新启动时，会作为主结点的从节点 网络抖动真实世界的机房网络往往不是风平浪静的，它们经常会发生各种各样的小问题。比如网络抖动就是非常常见的一种现象，突然之间部分连接变得不可访问，然后很快又恢复正常。 为解决这种问题，Redis Cluster提供了一种选项cluster-node-timeout，表示当某个节点持续timeout的时间失联时，才可以认定该节点出现故障，需要进行主从切换。如果没有这个选项，网络抖动会导致主从频繁切换(数据的重新复制）。 还有另外一个选项cluster-slave-validity-factor作为倍乘系数放大这个超时时间来宽松容错的紧急程度。如果这个系数为零，那么主从切换是不会抗拒网络抖动的。如果这个系数大于1，它就成了主从切换的松弛系数。 槽位迁移感知如果 Cluster中某个槽位正在迁移或者已经迁移完毕，那么客户端如何能感知到槽位的变化呢？客户端保存了槽位和节点的映射关系表，它需要及时得到更新，才可以正常地将某条指令发到正确的节点中。 我们前面提到 Cluster有两个特殊的eror指令，一个是 MOVED，一个是 ASKING。 MOVED指令是用来纠正槽位的。如果我们将指令发送到了错误的节点，该节点发现对应的指令槽位不归自己管理，就会将目标节点的地址随同 MOVED指令回复给客户端通知客户端去目标节点去访问。这个时候客户端就会刷新自己的槽位关系表，然后重试指令，后续所有打在该槽位的指令都会转到目标节点。. ASKING指令和 MOVED不一样，它是用来临时纠正槽位的。如果当前槽位正处于迁移中，指令会先被发送到槽位所在的旧节点。如果旧节点存在数据，那就直接返回结果了，如果不存在数据，那么数据可能真的不存在，也可能在迁移目标节点上，所以旧节点会通知客户端去新节点尝试拿数据，看看新节点有没有。这时就会给客户端返回一个 asking error携带上目标节点的地址。客户端收到这个 asking eror后，就会去目标节点尝试。客户端不会刷新槽位映射关系表，因为它只是临时纠正该指令的槽位信息，不影响后续指令。 在某些特殊情况下，客户端甚至会重试多次，大家可以打开自己的脑洞，想想什么情况下客户端会重试多次。 正是因为存在多次重试的情况，所以客户端的源码里在执行指令时都会有一个循环，然后会设置一个最大重试次数，Java和 Python都有这个参数，只是设置的值不一样。当重试次数超过这个值时，客户端会直接向业务层抛出异常。 集群变更感知当服务器节点变更时，客户端应该立即得到通知以实时刷新自己的节点关系表。那么客户端是如何得到通知的呢？这要分为两种情况。 目标节点挂掉了，客户端会抛出一个 Connectionerror，紧接着会随机挑一个节点来重试，这时被重试的节点会通过 MOVED指令告知目标槽位被分配到的新的节点地址。 运维手动修改了集群信息，将主节点切换到其他节点，并将旧的主节点移除出集群。这时打在旧的主节点上的指令会收到一个 Clusterdown的错误，告知当前节点所在集群不可用(当前节点已经被孤立了，它不再属于之前的集群)。这时客户端就会关闭所有的连接，清空槽位映射关系表，然后向上层抛错。待下一条指令过来时，就会重新尝试初始化节点信息。 部署每个节点都必须有个从节点，在每个配置文件中添加 12345port 6379cluster-enabled yescluster-node-timeout 15000cluster-config-file /usrlocalbin/cluster/data/nodes-6379.conf 集群内部配置文件其它节点的配置和这个一致，改端口即可 配置完成后启动redis服务 启动集群5.0以下12wget http://download.redis.io/redis-stable/src/redis-trib.rbruby redis-trib.rb create --replicas 1 XX.XXX.XX.XX:9001 XX.XXX.XX.XX:9002 XX.XXX.XX.XX:9003 XX.XXX.XX.XX:9004 XX.XXX.XX.XX:9005 XX.XXX.XX.XX:9006 //--replicas 1 代表从节点1个 5.0及以上1redis-cli --cluster create --cluster-replicas 1 XX.XXX.XX.XX:9001 XX.XXX.XX.XX:9002 XX.XXX.XX.XX:9003 XX.XXX.XX.XX:9004 XX.XXX.XX.XX:9005 XX.XXX.XX.XX:9006 –cluster-replicas 1 表示集群主节点需要多少个从节点 主从节点数量MSN为：-replicas 的数量 + 1节点数NN为：连接数 &#x2F; 主从节点数量连接从后开始，NN个为主节点，后NN个为从节点，后NN个也为从节点…… 比如：上面的，主从节点数量MSN为：1+1&#x3D;2节点数NN为：6&#x2F;2&#x3D;3 个节点 docker只讲5.0以上的，可以看这篇文章https://www.jianshu.com/p/908a306a1cb3 redis配置文件 123456789port $&#123;PORT&#125;protected-mode nocluster-enabled yescluster-config-file nodes.confcluster-node-timeout 15000cluster-announce-ip XX.XXX.XX.XX //自己服务器内网IPcluster-announce-port $&#123;PORT&#125;cluster-announce-bus-port 1$&#123;PORT&#125;appendonly yes 12345678910111213141516docker network create redis-netfor port in `seq 9001 9003`; do \\ docker run -d -ti -p $&#123;port&#125;:$&#123;port&#125; -p 1$&#123;port&#125;:1$&#123;port&#125; \\ -v /mnt/redis-cluster/$&#123;port&#125;/conf/redis.conf:/usr/local/etc/redis/redis.conf:rw \\ -v /mnt/redis-cluster/$&#123;port&#125;/data:/data:rw \\ --restart always --name redis-$&#123;port&#125; --net redis-net \\ --sysctl net.core.somaxconn=1024 redis redis-server /usr/local/etc/redis/redis.conf; \\done// 5.0.2(最新版好像不能直接exec -it。)docker run -it --link redis-9001:redis --net redis-net --rm redis redis-cli --cluster create XX.XXX.XX.XX:9001 XX.XXX.XX.XX:9002 XX.XXX.XX.XX:9003 XX.XXX.XX.XX:9004 XX.XXX.XX.XX:9005 XX.XXX.XX.XX:9006 --cluster-replicas 1// 5.0docker exec -it redis容器ID redis-cli --cluster create XX.XXX.XX.XX:9001 XX.XXX.XX.XX:9002 XX.XXX.XX.XX:9003 XX.XXX.XX.XX:9004 XX.XXX.XX.XX:9005 XX.XXX.XX.XX:9006 --cluster-replicas 1//或者直接在局域网的一台机器执行redis-cli --cluster create XX.XXX.XX.XX:9001 XX.XXX.XX.XX:9002 XX.XXX.XX.XX:9003 XX.XXX.XX.XX:9004 XX.XXX.XX.XX:9005 XX.XXX.XX.XX:9006 --cluster-replicas 1","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"02Sentinel-哨兵模式","slug":"redis/集群/02Sentinel-哨兵模式","date":"2021-11-14T12:00:10.000Z","updated":"2022-03-23T09:03:57.557Z","comments":true,"path":"blog/redis/集群/02Sentinel-哨兵模式/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E9%9B%86%E7%BE%A4/02Sentinel-%E5%93%A8%E5%85%B5%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"为什么要讲哨兵机制?由于主从复制机制下，主节出现问题就不能提供服务了，需要人工重新把从节点设为主节点。也就是故障不能转移，不能实现高可用 哨兵机制(sentinel)的高可用我们可以将Redis Sentinel集群看成是-个zookeeper集群，它是集群高可用的心脏，一般由3~5个节点组成，这样即使个别节点挂了，集群还可以正常运转。哨兵模式图Sentinel负责持续监控主从节点的健康，当主节点挂掉时，自动选择最优的从节点切换成为主节点。客户通过Sentinel来查询主节点的地址，然后再连接主节点进行数据交互。当主节点发生故障时，Sentinel集群使用Raft 算法实现选举机制，选出一个哨兵节点来完成转移和通知，客户端会重新向这个Sentinel要地址，Sentinel会将最新的主节点地址告诉客户端。如此应用程序将无须重启即可自动完成节点切换。如图，主节点挂掉后，集群将可能自动调整为图所示结构。待它恢复后如图 部署建议 sentinel 节点应部署在多台物理机(线上环境) 至少三个且奇数个 sentinel 节点 通过以上我们知道，3 个 sentinel 可同时监控一个主节点或多个主节点。监听主节点较多时，如果 sentinel 出现异常，会对多个主节点有影响，同时还会造成 sentinel 节点产生过多的网络连接， 一般线上建议还是， 3 个 sentinel 监听一个主节点 部署先搭好一主两从 redis 的主从复制。准备哨兵的配置文件，核心是添加sentinel monitor mymaster 190.168.1.111 6379 2 监控主节点的 IP 地址端口，sentinel 监控的 master 的名字叫做 mymaster 2代表，当集群中有 2 个 sentinel 认为 master 死了时，才能真正认为该 master已经不可用了 IP 不要写成 127.0.0.1 如果主节点需要密码，加上sentinel auth-pass mymaster 12345678 其他配置：sentinel config-epoch mymaster 2 故障转移时最多可以有 2 从节点同时对新主节点进行数据同步 entinel down-after-milliseconds mymasterA 300000 sentinel节点定期向主节点ping 命令，当超过了 300S 时间后没有回复，可能就认定为此主节点出现故障了 sentinel parallel-syncs mymasterA 1 故障转移后，1代表每个从节点按顺序排队一个一个复制主节点数据，如果为3，指3个从节点同时并发复制主节点数据，不会影响阻塞，但存在网络和 IO 开销 启动redis-sentinel sentinel的配置文件，例如redis-sentinel conf/sentinel_26379.conf 关闭redis-cli -h 192.168.42.111 -p 26379 shutdown 个人理解redis的读写分离比较鸡肋。首先有些业务需求可能对redis数据要求有较高的实时性，毕竟读写分离有读延迟。其次如果redis真的有访问压力，完全可以用redis cluster模式。另外有的客户端不支持redis读写分离。redis的从节点的意义主要还是为了高可用而存在的。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"01主从复制","slug":"redis/集群/01主从复制","date":"2021-11-14T12:00:09.000Z","updated":"2022-03-23T09:03:57.553Z","comments":true,"path":"blog/redis/集群/01主从复制/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E9%9B%86%E7%BE%A4/01%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","excerpt":"","text":"集群理论知识先了解一些集群理论知识 最终一致Redis的主从数据是异步同步的，所以分布式的Redis系统并不满足一致性要求。当客户端在Redis的主节点修改了数据后，立即返回，即使在主从网络断开的情况下，主节点依旧可以正常对外提供修改服务，所以Redis满足可用性。 Redis保证最终一致性，´从节点会努力追赶主节点，最终从节点的状态会和主节点的状态保持一致。如果网络断开了，主从节点的数据将会出现大量不一致，但一旦网络恢复，从节点会采用多种策略努力追赶，继续尽力保持和主节点一致。 主从同步与从从同步 增量同步Redis同步的是指令流，主节点会将那些对自己的状态产生修改性影响的指令记录在本地的内存buffer中，然后异步将buffer中的指令同步到从节点，从节点一边执行同步的指令流来达到和主节点一样的状态，一边向主节点反馈自己同步到哪里了(偏移量）。 因为内存的buffer是有限的，所以Redis主节点不能将所有的指令都记录在内存buffer中(repl_backlog_buffer)。Redis的复制内存buffer是一个定长的环形数组，如图所示，如果数组内容满了，就会从头开始覆盖前面的内容。 repl_backlog_buffer 如果因为网络状况不好，从节点在短时间内无法和主节点进行同步，那么当网络状况恢复时，Redis的主节点中那些没有同步的指令在buffer中有可能已经被后续的指令覆盖掉了，从节点将无法直接通过指令流来进行同步，这个时候就需要用到更加复杂的同步机制一一快照同步。 快照同步（全量同步）快照同步是个非常耗费资源的操作，如图所示，它首先需要在主节点上进行一次bgsave，将当前内存的数据全部快照到磁盘文件中，然后再将快照文件的内容全部传送到从节点。从节点将快照文件接受完毕后，立即执行一次全量加载，加载之前先要将当前内存的数据清空，加载完毕后通知主节点继续进行增量同步。在整个快照同步进行的过程中，主节点的复制buffer还在不停地往前移动，如果快照同步的时间过长或者复制buffer太小，都会导致同步期间的增量指令在复制buffer中被覆盖，这样就会导致快照同步完成后无法进行增量复制，然后会再次发起快照同步，如此极有可能会陷入快照同步的死循环。 所以务必配置一个合适的复制buffer大小参数，避免快照复制的死循环。 因此，我们要想办法避免这一情况，一般而言，我们可以调整 repl_backlog_size 这个参数。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：缓冲空间大小 &#x3D; 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 repl_backlog_size &#x3D; 缓冲空间大小 * 2，这也就是 repl_backlog_size 的最终值。 举个例子，如果主库每秒写入 2000 个操作，每个操作的大小为 2KB，网络每秒能传输 1000 个操作，那么，有 1000 个操作需要缓冲起来，这就至少需要 2MB 的缓冲空间。否则，新写的命令就会覆盖掉旧操作了。为了应对可能的突发压力，我们最终把 repl_backlog_size 设为 4MB。 全量同步的条件 备节点第一次连到主节点。 节点重启，主节点runid会变，备节点runid会丢失，redis &lt;4.0 会发生全量复制。 复制积压未同步数据被部分冲掉，导致全量同步。 增加从节点当从节点刚刚加入到集群时，它必须先进行一次快照同步，同步完成后再继续进行增量同步。 无盘复制从Redis2.8.18版本开始，Redis支持无盘复制。所谓无盘复制是指主服务器直接通过套接字将快照内容发送到从节点，生成快照是一个遍历的过程，主节点会一边遍历内存，一边将序列化的内容发送到从节点，从节点还是跟之前一样，先将接收到的内容存储到磁盘文件中，再进行一次性加载。 启动主从方式一从节配置添加slaveof 主节点ip:port 。在主节点 启动完后再启动从节点 方式二redis-server –slaveof 主节点ip:port 方式三启动主节点后，用客户端连接从节点，然后在节点上执行slaveof 主节点ip:port 断开在 slave 节点，执行 6380:&gt;slaveof no one 主从建议 数据较重要的节点，主从复制时使用密码验证: requirepass 从节点建议用只读模式 slave-read-only&#x3D;yes, 若从节点修改数据，主从数据不一致 影响从节数据同步的参数repl-disable-tcp-nodelay该参数有两个值 yes，no 如果选择“是”，则Redis将使用较少数量的TCP数据包和较少的带宽将数据发送到从属设备。但这会增加数据显示在从属端的延迟，对于使用默认配置的Linux内核，此延迟最多可达40毫秒。适用于网络环境复杂或带宽紧张，如跨机房 如果选择“否”，将减少数据出现在从属端的延迟，但是将使用更多带宽进行复制。适用于主从网络好的场景 影响主从复制的两个参数client-output-buffer-limit有client请求redis数据的时候，redis要返回给client的数据都会先被存储在output-buffer中，等所有信息都被传送完毕之后，再清除output-buffer中的数据。 既然有这个内存buffer存在，那么这个buffer有没有限制呢？如果主从在传播命令时，因为某些原因从库处理得非常慢，那么主库上的这个buffer就会持续增长，消耗大量的内存资源，甚至OOM。redis为了保护做了限制，提供了client-output-buffer-limit参数限制这个buffer的大小，这个配置可以查看redis的配置文件，或者通过config命令查看。如果超过限制，主库会强制断开这个client的连接，也就是说从库处理慢导致主库内存buffer的积压达到限制后，主库会强制断开从库的连接，此时主从复制会中断，中断后如果从库再次发起复制请求，那么此时可能会导致恶性循环，引发复制风暴，这种情况需要格外注意。 例如：client-output-buffer-limit slave 256mb 64mb 60这里对是客户端是slave的做限制 256mb 是一个硬性限制，当output-buffer的大小大于256mb之后就会断开连接 64mb 60 是一个软限制，当output-buffer的大小大于64mb并且超过了60秒的时候就会断开连接 除了slave，还有normal，pubsub repl-backlog-size(复制积压大小)这个参数用来设置增量同步时，主节点的复制缓冲区的大小","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"限流","slug":"redis/基础和应用/限流","date":"2021-11-14T12:00:08.000Z","updated":"2022-03-23T09:03:57.550Z","comments":true,"path":"blog/redis/基础和应用/限流/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%9F%BA%E7%A1%80%E5%92%8C%E5%BA%94%E7%94%A8/%E9%99%90%E6%B5%81/","excerpt":"","text":"这个限流需求中存在一个滑动时间窗口(定宽〉，想想zset数据结构的score值，是不是可以通过score来圈出这个时间窗口来。我们只需要保留这个时间窗口，窗口之外的数据都可以砍掉。那这个zset的value填什么比较合适呢？它只需要保证唯一性即可，用uuid会比较浪费空间，那就改用毫秒时间戳吧。如图，用一个zset结构记录用户的行为历史，每一个行为都会作为zset中的一个key保存下来。同一个用户的同一种行为用一个zset记录。为节省内存，我们只需要保留时间窗口内的行为记录，同时如果用户是冷用户，滑动时间窗口内的行为是空记录，那么这个zset就可以从内存中移除，不再占用空间。通过统计滑动窗口内的行为数量与阙值max_count进行比较就可以得出当前的行为是否被允许。用代码表示如下。 漏斗限流漏斗的容量是有限的，如果将漏嘴堵住，然后一直往里面灌水，它就会变满，直至再也装不进去。如果将漏嘴放开，水就会往下流，流走一部分之后，就又可以继续往里面灌水。如果漏嘴流水的速率大于灌水的速率，那么漏斗永远都装不满。如果漏嘴流水速率小于灌水的速率，那么一旦漏斗满了，灌水就需要暂停并等待漏斗腾出一部分空间。所以，漏斗的剩余空间就代表着当前行为可以持续进行的数量，漏嘴的流水速率代表着系统允许该行为的最大频率。下面我们使用代码来描述单机漏斗算法。代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.util.HashMap;import java.util.Map;public class FunnelRateLimiter &#123; private Map&lt;String, Funnel&gt; funnels = new HashMap&lt;&gt;(); private static class Funnel &#123; int capacity; int leftQuota; float leakingRate; long leakingTs; public Funnel(int capacity, float leakingRate) &#123; this.capacity = capacity; this.leakingRate = leakingRate; leftQuota = capacity; leakingTs = System.currentTimeMillis(); &#125; void makeSpace() &#123; long nowTs = System.currentTimeMillis(); //距离上一次漏水过去了多久 long deltaTs = nowTs - leakingTs; //又可以腾出不少空间了 int deltaQuota = (int) (deltaTs * leakingRate); //间隔时间太长了，整数过大溢出 if (deltaTs &lt; 0) &#123; this.leftQuota = capacity; this.leakingTs = nowTs; return; &#125; //间隔空间太小，最小单位是1 if (deltaQuota &lt; 1) &#123; return; &#125; this.leftQuota += deltaQuota; this.leakingTs = nowTs; if (this.leftQuota &gt; this.capacity) &#123; this.leftQuota = this.capacity; &#125; &#125; boolean watering(int quota) &#123; makeSpace(); if (this.leftQuota &gt;= quota) &#123; this.leftQuota -= quota; return true; &#125; return false; &#125; &#125; public boolean isActionAllowed(String userId, String actionKey, int capacity, float leakingRate) &#123; String key = String.format(&quot;%s:%s&quot;, userId, actionKey); Funnel funnel = funnels.computeIfAbsent(key, k -&gt; new Funnel(capacity, leakingRate)); return funnel.watering(1); //需要一个quota &#125;&#125; make_space方法是漏斗算法的核心，其在每次灌水前都会被调用以触发漏水，给漏斗腾出空间来。能腾出多少空间取决于过去了多久以及流水的速率。Funnel对象占据的空间大小不再和行为的频率成正比，它的空间占用是一个常量。 redis中的漏斗限流模块 Redis-Cell加载模块看(redis导入模块.md)cl.throttle该命令的意思是，每60s最多30次(漏水速率〉，漏斗的初始窑量为15，也就是说一开始可以连续回复15个帖子，然后才开始受漏水速率的影响。返回值：","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"布隆过滤器","slug":"redis/基础和应用/布隆过滤器","date":"2021-11-14T12:00:07.000Z","updated":"2022-03-23T09:03:57.543Z","comments":true,"path":"blog/redis/基础和应用/布隆过滤器/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%9F%BA%E7%A1%80%E5%92%8C%E5%BA%94%E7%94%A8/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/","excerpt":"","text":"专门用来解决这种去重问题，在空间上还能节省90%以上，只是稍微有那么点不精确，也就是有定的误判概率。 布隆过滤器说某个值存在时，这个值可能不存在；当它说某个值不存在时，那就肯定不存在。 使用方法bf.add 添加元素bf.exists 查询元素是否存在bf.madd 批量添加元素bf.mexists 检查多个元素是否存在 自定义参数的bloom-filter使用bf.reserver显示创建。如果对应的key存在，会报错。有三个参数： key：bf的key error_rate：错误率，error_rate越低，需要的空间越大。 initial_size：表示预计放入的元素数量，当实际数量超出这个数值时，误判率会上升，所以需要提前设置一个较大的数值避免超出导致误判率升高。 不指定的情况下，默认error_rate是0.01，initial_size是100。 initial_size设置得过大，会浪费存储空间，设置得过小，就会影响准确率，用户在使用之前一定要尽可能地精确估计元素数量，还需要加上一定的冗余空间以避免实际元素可能会意外高出估计值很多。 error_rate越小，需要的存储空间就越大，对于不需要过于精确的场合，error_rate设置稍大一点也无伤大雅。 bloom-filter的原理每个布隆过滤器对应到Redis的数据结构里面就是一个大型的位数组和几个不一样的无偏（无偏就是能够把元素的bash值算得比较均匀，让元素被hash映射到位数组中的位置比较随机）hash函数，如图中f，g，h就是对应的hash函数。向布隆过滤器中添加key时，会使用多个hash函数对key进行hash，算得一个整数索引值，然后对位数组长度进行取模运算得到一个位置，每个hash函数都会算得一个不同的位置。再把位数组的这几个位置都置为1，就完成了add操作。向布隆过滤器询问key是否存在时，跟add一样，也会把hash的几个位置都算出来，看看位数组中这几个位置是否都为1。只要有一个位为0，那么说明布隆过滤器中这个key不存在。如果这几个位置都是1，并不能说明这个key就一定存在，只是极有可能存在，因为这些位被置为1可能是因为其他的key存在所致。如果这个位数组比较稀疏，判断正确的概率就会很大，如果这个位数组比较拥挤，判断正确的概率就会降低。 空间占用估计计算空间占用网站 加载模块redis导入模块.md","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"基础数据结构","slug":"redis/基础和应用/基础数据结构","date":"2021-11-14T12:00:06.000Z","updated":"2022-03-23T09:03:57.539Z","comments":true,"path":"blog/redis/基础和应用/基础数据结构/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%9F%BA%E7%A1%80%E5%92%8C%E5%BA%94%E7%94%A8/%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"redis的string字符串是一个名为SDS的结构， 123456//sds.hstruct sdshdr &#123; unsigned int len; unsigned int free; char buf[];&#125;; 可以看到底层是一个字符数组 类似ArrayList，由于redis的内存分配机制，也就是采用预分配冗余空间的方式来减少内存的频繁分配。当字符串长度小于1MB时,扩容都是加倍现有的空间。如果字符串长度超过1MB ,扩容时一次只会多扩1MB的空间。需要注意的是字符串最大长度为512MB。 字符串由多个字节组成,每个字节又由8个bit组成,如此便可以将一个字符串看成很多bit的组合,这便是bitmap(位图)数据结构 所以通过这样的设计就有一个问题了，就是会有额外的占用，数据少时没什么，但如果数据一多，这额外的占用就很大了，这时可以考虑下采用一定的策略，使用集合类型，因为集合类型有非常节省内存空间的底层实现结构。 list类似于LinkedList，是双向链表。当列表弹出了最后一个元素之后,该数据结构被自动删除,内存被回收。根据需求可以实现队列或栈 底层存储使用了一种“快速链表”（quicklist）结构： 首先在列表元素较少的情况下,会使用一块连续的内存存储,这个结构是ziplist，即压缩列表。它将所有的元素彼此紧挨着一起存储,分配的是-块连续的内存。 当数据量比较多的时候才会改成quicklist。因为普通的链表需要的附加指针空间太大,会浪费空间,还会加重内存的碎片化,比如某普通链表里存的只是int类型的数据,结构上还需要两个额外的指针prev和next。所以Redis 将链表和ziplist 结合起来组成了quicklist ,也就是将多个ziplist使用双向指针串起来使用。 hash（字典）类似于HashMap，结构如图和HashMap一样，都是使用数组+链表的二维结构。如图：与HashMap不同的是值只能是字符串，还有在rehash时，Redis使用渐进式rehash策略，不会堵塞；HashMap会（concurrentHashMap不会）。在redis的hash发生rehash时，保留新旧两个hash结构，如图：查询时会同时查询两个hash结构，然后在后续的定时任务以及hash操作指令中，循序渐进地将旧hash的内容一点点地迁移到新的hash结构中。迁移完成后，会使用新的hash结构取而代之。当hash移除了最后一个元素之后，该数据结构被自动删除，内存被回收。 set（集合）类似HashSet，它内部是一个特殊的字典，value值为NULL。当集合中最后一个元素被移除之后，数据结构被自动删除，内存被回收。 zset（有序集合(列表)）它是一个set，保证了value的唯一性，另一方面它可以为每个value赋予一个score，代表这个value的排序权重。内部使用了“跳跃列表（跳表）”的数据结构。一方面它需要一个hash结构来存储value和score的对应关系,另一方面需要提供按照score排序的功能,还需要能够指定score的范围来获取valu巳列表的功能zset中最后一个value被移除后，数据结构被自动删除，内存被回收。 跳表 定位插入点时，先在顶层进行定位，然后下潜到下一级定位，一直下潜到最底层找到合适的位置，将新元素插进去。你也许会问，那新插入的元素如何才有机会“身兼数职”呢?跳跃列表采取一个随机策略来决定新元素可以兼职到第几层。首先其位于LO层的概率肯定是100%,而兼职到Ll层只有50%的概率,至IJL2层只有25%的概率,到L3层只有12.5%的概率,以此类推,一直随机到最顶层L31层。绝大多数元素都过不了几层,只有极少数元素可以深入到顶层。列表中的元素越多,能够深入的层次就越深,元素能进入到顶层的可能性就会越大。跳跃表的原理或者跳跃表的原理(有道笔记) 容器型数据结袍的通用规则list、set、hash、zset这四种数据结构是容器型数据结构,它们共享下面两条通用规则。 create if not exists: 如果容器不存在,那就创建一个,再进行操作。比如rpush操作刚开始是没有列表的,Redis就会自动创建一个,然后再rpush进去新元素。 drop if no elements :如果容器里的元素没有了,那么立即删除容器,释放内存。这意昧着lpop操作到最后一个元素,列表就消失了。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"位图","slug":"redis/基础和应用/位图","date":"2021-11-14T12:00:05.000Z","updated":"2022-03-23T09:03:57.531Z","comments":true,"path":"blog/redis/基础和应用/位图/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%9F%BA%E7%A1%80%E5%92%8C%E5%BA%94%E7%94%A8/%E4%BD%8D%E5%9B%BE/","excerpt":"","text":"介绍位图不是特殊的数据结构，它的内容其实就是普通的字符串，也就是byte数组。我们可以使用普通的ge的et直接获取和设置整个位图的内窑，也可以使用位图操作getbit/setbit等将byte数组看成“位数组”来处理。使用：127.0.0.1:6379&gt; setbit s 1 1 127.0.0.1:6379&gt; get s上面这个例子可以理解为“零存整取”，同样我们还可以“零存零取”或者“整存零取”。“零存”就是使用setbit对位值进行逐个设置，“整存”就是使用字符串一次性填充所有位数组，覆盖掉旧值。 零存零取使用单个位操作设置位值，使用单个位操作获取具体位值。 整存零取使用字符串操作批量设置位值，使用单个位操作获取具体位值。如果对应位的字节是不可打印字符,redis-cli会显示该字符的十六进制形式。 统计和查找Redis提供了位图统计指令bitcount和位图查找指令bitpos。bitcount用来统计指定位置范围内1的个数，bitpos用来查找指定范围内出现的第一个0或1。比如我们可以通过bitcount统计用户一共签到了多少天。通过bitpos指令查找用户从哪天开始第一次签到。如果指定了范围参数[start，end]，就可以统计在某个时间范围内用户签到了多少天，用户自某天以后的哪天开始签到。遗憾的是,start和end参数是字节索引，也就是说指定的位范围必须是8 的倍数，而不能任意指定 逻辑操作使用bitop 进行与，或，非，异或操作。BITOP operation destkey key [key …]对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。operation 可以是 AND 、 OR 、 NOT 、 XOR 这四种操作中的任意一种： BITOP AND destkey key [key …] ，对一个或多个 key 求逻辑并，并将结果保存到 destkey 。 BITOP OR destkey key [key …] ，对一个或多个 key 求逻辑或，并将结果保存到 destkey 。 BITOP XOR destkey key [key …] ，对一个或多个 key 求逻辑异或，并将结果保存到 destkey 。 BITOP NOT destkey key ，对给定 key 求逻辑非，并将结果保存到 destkey 。 除了 NOT 操作之外，其他操作都可以接受一个或多个 key 作为输入。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"scan-搜索","slug":"redis/基础和应用/scan-搜索","date":"2021-11-14T12:00:04.000Z","updated":"2022-03-23T09:03:57.529Z","comments":true,"path":"blog/redis/基础和应用/scan-搜索/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%9F%BA%E7%A1%80%E5%92%8C%E5%BA%94%E7%94%A8/scan-%E6%90%9C%E7%B4%A2/","excerpt":"","text":"scan具备一下特点。 复杂度虽然也是O(n)，但它是通过游标分步进行的，不会阻塞线程。 limit参数，可以控制每次返回结果的最大条数，limit只是个hint（暗示），返回的结果可多可少。 同keys一样，它也提供模式匹配功能。 服务器不需要为游标保存状态，游标的唯一状态就是scan返回给客户端的游标整数。 返回的结果可能会有重复，需要客户端去重，这点非常重要。 遍历的过程中如果有数据修改，改动后的数据能不能遍历到是不确定的。 单次返回的结果是空的并不意昧着遍历结束，而要看返回的游标值是否为零。 用法scan 0 match key99* count 1000scan 13976 match key99* count 10000时结束。虽然提供的limit是1000，但是返回的结果却只有10个左右。因为这个limit不是限定返回结果的数量，而是限定服务器单次遍历的字典槽位数量(约等于)。如果将limit设置为10，你会发现返回结果是空的，但是游标值不为零，意昧着遍历还没结束。 原理解析在Redis中所有的key都存储在一个很大的字典中，这个字典的结构和Java中的HashMap一样。scan指令返回的游标就是第一维数组的位置索引，我们将这个位置索引称为槽(slot)。如果不考虑字典的扩容，直接按数组下标按个遍历就行了。limit参数就表示需要遍历的槽位数，之所以返回的结果可能多可能少，是因为不是所有的槽位上都会挂接链表，有些槽位可能是空的，还有些槽位上挂接的链表上的元素可能会有多个。每一次遍历都会将limit数量的槽位上挂接的所有链表元素进行模式匹配过滤后，一次性返回给客户端。 scan遍历顺序scan的遍历顺序非常特别。它不是从第一维数组的第0位直遍历到末尾，而是采用了高位进位加法来遍历。之所以使用这样特殊的方式进行遍历，是考虑到字典的扩容和缩容时避免槽位的遍历重复和遗漏。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"redis导入模块","slug":"redis/基础和应用/redis导入模块","date":"2021-11-14T12:00:03.000Z","updated":"2022-03-23T09:03:57.526Z","comments":true,"path":"blog/redis/基础和应用/redis导入模块/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%9F%BA%E7%A1%80%E5%92%8C%E5%BA%94%E7%94%A8/redis%E5%AF%BC%E5%85%A5%E6%A8%A1%E5%9D%97/","excerpt":"","text":"在配置redis.conf中加上loadmodule modulePath比如：loadmodule /path/to/mymodule.so也可以redis-server loadmodule &#x2F;path&#x2F;to&#x2F;mymodule.so 配置 使用客户端使用命令：加载：MODULE LOAD modulePath例子：MODULE LOAD /path/to/panda.so 卸载：MODULE UNLOAD moduleName例子：MODULE UNLOAD panda 列出加载了的模块Sshdobbf返回模块名称列表","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"HyperLogLog-不精确的去重计数方案","slug":"redis/基础和应用/HyperLogLog-不精确的去重计数方案","date":"2021-11-14T12:00:02.000Z","updated":"2022-03-23T09:03:57.525Z","comments":true,"path":"blog/redis/基础和应用/HyperLogLog-不精确的去重计数方案/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%9F%BA%E7%A1%80%E5%92%8C%E5%BA%94%E7%94%A8/HyperLogLog-%E4%B8%8D%E7%B2%BE%E7%A1%AE%E7%9A%84%E5%8E%BB%E9%87%8D%E8%AE%A1%E6%95%B0%E6%96%B9%E6%A1%88/","excerpt":"","text":"有一些统计需求如果要精确统计的话会很浩资源，比如UV：用户访问统计，它要去重，同一个用户一天之内的多次访问请求只能计数一次。一个简单的方案，那就是为每一个页面设置一个独立的set集合来存储所有当天访问过此页面的用户囚。当一个请求过来时，我们使用sadd将用户ID塞进去就可以了。通过scard可以取出这个集合的大小，这个数字就是这个页面的UV数据。有个问题，如果访问量大了后，就太消耗内存了！而且如果不需要太精确，105万和104万没多大区别。对于这种情况，可以使用HyperLogLog。HyperLogLog提供不精确的去重计数方案，虽然不精确，但是也不是非常离谱，标准误差是0.81%。 使用方法pfadd 增加计数pfcount 获取计数 合并统计PFMERGE destkey sourcekey [sourcekey …] 将多个 HyperLogLog 合并（merge）为一个 HyperLogLog ， 合并后的 HyperLogLog 的基数接近于所有输入 HyperLogLog 的可见集合（observed set）的并集。 合并得出的 HyperLogLog 会被储存在 destkey 键里面， 如果该键并不存在，那么命令在执行之前，会先为该键创建一个空的 HyperLogLog。 HyperLogLog这个数据结构不是免费的。这倒不是说使用这个数据结构要花钱，而是因为它需要占据12阻的存储空间，所以不适合统计单个用户相关的数据。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"GeoHash-地理位置Geo","slug":"redis/基础和应用/GeoHash-地理位置Geo","date":"2021-11-14T12:00:01.000Z","updated":"2022-03-23T09:03:57.525Z","comments":true,"path":"blog/redis/基础和应用/GeoHash-地理位置Geo/","link":"","permalink":"http://sv.pointcut.cc/blog/redis/%E5%9F%BA%E7%A1%80%E5%92%8C%E5%BA%94%E7%94%A8/GeoHash-%E5%9C%B0%E7%90%86%E4%BD%8D%E7%BD%AEGeo/","excerpt":"","text":"可以使用Redis来实现类似摩拜单车的“附近的Mobike”、美团和饿了么的“附近的餐馆”这样的功能了 GeoHash算法GeoHash算法将二维的经纬度数据映射到一维的整数，这样所有的元素都将挂载到一条线上，距离靠近的二维坐标映射到一维后的点之间距离也会很接近。当我们想要计算“附近的人”时，首先将目标位置映射到这条线上，然后在这个一维的线上获取附近的点就行了。它将整个地球看成一个二维平面，然后划分成了一系列正方形的方格，就好比围棋棋盘。所有的地图元素坐标都将被放置于唯一的方格中。方格越小，坐标越精确。然后对这些方格进行整数编码，越是靠近的方格编码越是接近。那如何编码呢？个最简单的方案就是切蛋糕法。如图所示，设想一个正方形的蛋糕、摆在你面前，二刀下去均分分成四块小正方形，这四个小正方形可以分别标记为00，01，10，11四个二进制整数。然后对每一个小正方形继续用二刀法切割，这时每个小小正方形就可以使用4bit的二进制整数表示。然后继续切下去，正方形就会越来越小，二进制整数也会越来越长，精确度就会越来越高。编码之后，每个地图元素的坐标都将变成一个整数，通过这个整数可以还原出元素的坐标，整数越长，还原出来的坐标值的损失程度就越小。对于“附近的人”这个功能而言，损失的一点精确度可以忽略不计。 redis的GeoHashGeoHash算法会继续对这个整数做一次base32编码。变成一个字符串。在Redis里面，经纬度使用52位的整数进行编码，放进了zset里面，zset的value是元素的key，score是GeoHash的52位整数值。在使用Redis进行Geo查询时，我们要时刻想到它的内部结构实际上只是一个zset(skiplist)。通过zset的score排序就可以得到坐标附近的其他元素(实际情况要复杂一些，不过这样理解足够了)，通过将score还原成坐标值就可以得到元素的原始坐标。 使用增加geoadd 指令携带集合名称以及多个经纬度名称三元组。例如：geoadd company 116.48105 39.996794 juejin 删除Geo存储结构上使用的是zset，意昧着我们可以使用zset相关的指令来操作Geo数据，所以元素删除指令可以直接使用zrem指令即可。 距离geodist 指令可以用来计算两个元素之间的距离,携带集合名称、两个名称和距离单位。GEODIST company juejin meituan 获取元素位置geopos指令可以获取集合中任意元素的经纬度坐标，可以次获取多个。GEOPOS company juejin 获取元素的hash值GEOHASH可以获取元素的经纬度编码字符串,GEOHASH company juejin可以到http://geohash.org/{hash}进行直接定位。 附近的公司georadiusbymember它可以用来查询指定元素附近的其他元素。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"}]},{"title":"34REST支持","slug":"dubbo/2.7/34REST支持","date":"2021-11-14T03:00:35.000Z","updated":"2022-03-23T09:33:55.654Z","comments":true,"path":"blog/dubbo/2.7/34REST支持/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/34REST%E6%94%AF%E6%8C%81/","excerpt":"","text":"代码123dubbo.protocols.RESTTEST.id=RESTTESTdubbo.protocol.RESTTEST.name=restdubbo.protocols.RESTTEST.port=8083 Provider： 123456789101112131415161718192021222324252627282930313233public interface DemoService &#123; // 同步调用方法 String sayHello(String name); // 异步调用方法 default CompletableFuture&lt;String&gt; sayHelloAsync(String name) &#123; return null; &#125;; // 添加回调 default String sayHello(String name, String key, DemoServiceListener listener) &#123; return null; &#125;;&#125;@Service(version = &quot;rest&quot;, protocol = &quot;RESTTEST&quot;)@Path(&quot;demo&quot;)public class RestDemoService implements DemoService &#123; @GET @Path(&quot;say&quot;) @Produces(&#123;ContentType.APPLICATION_JSON_UTF_8, ContentType.TEXT_XML_UTF_8&#125;) @Override public String sayHello(@QueryParam(&quot;name&quot;) String name) &#123; System.out.println(&quot;执行了rest服务&quot; + name); URL url = RpcContext.getContext().getUrl(); return String.format(&quot;%s: %s, Hello, %s&quot;, url.getProtocol(), url.getPort(), name); // 正常访问 &#125;&#125; 消费者该怎么写就怎么写。也可以通过http方式调用 1http://127.0.0.1:8083/demo/say?name=xyz","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"33ReferenceConfig 缓存","slug":"dubbo/2.7/33ReferenceConfig 缓存","date":"2021-11-14T03:00:34.000Z","updated":"2022-03-23T09:33:55.653Z","comments":true,"path":"blog/dubbo/2.7/33ReferenceConfig 缓存/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/33ReferenceConfig%20%E7%BC%93%E5%AD%98/","excerpt":"","text":"ReferenceConfig 实例很重，封装了与注册中心的连接以及与提供者的连接，需要缓存。否则重复生成 ReferenceConfig 可能造成性能问题并且会有内存和连接泄漏。在 API 方式编程时，容易忽略此问题。 因此，自 2.4.0 版本开始， dubbo 提供了简单的工具类 ReferenceConfigCache用于缓存 ReferenceConfig 实例。 使用方式如下： 12345678910ReferenceConfig&lt;XxxService&gt; reference = new ReferenceConfig&lt;XxxService&gt;();reference.setInterface(XxxService.class);reference.setVersion(&quot;1.0.0&quot;);......ReferenceConfigCache cache = ReferenceConfigCache.getCache();// cache.get方法中会缓存 Reference对象，并且调用ReferenceConfig.get方法启动ReferenceConfigXxxService xxxService = cache.get(reference);// 注意！ Cache会持有ReferenceConfig，不要在外部再调用ReferenceConfig的destroy方法，导致Cache内的ReferenceConfig失效！// 使用xxxService对象xxxService.sayHello(); 消除 Cache 中的 ReferenceConfig，将销毁 ReferenceConfig 并释放对应的资源。 12ReferenceConfigCache cache = ReferenceConfigCache.getCache();cache.destroy(reference); 缺省 ReferenceConfigCache 把相同服务 Group、接口、版本的 ReferenceConfig 认为是相同，缓存一份。即以服务 Group、接口、版本为缓存的 Key。 可以修改这个策略，在 ReferenceConfigCache.getCache 时，传一个 KeyGenerator。详见 ReferenceConfigCache 类的方法。 12KeyGenerator keyGenerator = new ...ReferenceConfigCache cache = ReferenceConfigCache.getCache(keyGenerator );","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"32服务容器","slug":"dubbo/2.7/32服务容器","date":"2021-11-14T03:00:33.000Z","updated":"2022-03-23T09:33:55.652Z","comments":true,"path":"blog/dubbo/2.7/32服务容器/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/32%E6%9C%8D%E5%8A%A1%E5%AE%B9%E5%99%A8/","excerpt":"","text":"使用 Dubbo 中的服务容器 服务容器是一个 standalone 的启动程序，因为后台服务不需要 Tomcat 或 JBoss 等 Web 容器的功能，如果硬要用 Web 容器去加载服务提供方，增加复杂性，也浪费资源。 服务容器只是一个简单的 Main 方法，并加载一个简单的 Spring 容器，用于暴露服务。 服务容器的加载内容可以扩展，内置了 spring, jetty, log4j 等加载，可通过容器扩展点进行扩展。配置配在 java 命令的 -D 参数或者 dubbo.properties 中。 容器类型Spring Container 自动加载 META-INF/spring 目录下的所有 Spring 配置。 配置 spring 配置加载位置： 1dubbo.spring.config=classpath*:META-INF/spring/*.xml Jetty Container 启动一个内嵌 Jetty，用于汇报状态。 配置： dubbo.jetty.port=8080：配置 jetty 启动端口 dubbo.jetty.directory=/foo/bar：配置可通过 jetty 直接访问的目录，用于存放静态文件 dubbo.jetty.page=log,status,system：配置显示的页面，缺省加载所有页面 Log4j Container 自动配置 log4j 的配置，在多进程启动时，自动给日志文件按进程分目录。 配置： dubbo.log4j.file=/foo/bar.log：配置日志文件路径 dubbo.log4j.level=WARN：配置日志级别 dubbo.log4j.subdirectory=20880：配置日志子目录，用于多进程启动，避免冲突 容器启动缺省只加载 spring 1java org.apache.dubbo.container.Main 通过 main 函数参数传入要加载的容器 1java org.apache.dubbo.container.Main spring jetty log4j 通过 JVM 启动参数传入要加载的容器 1java org.apache.dubbo.container.Main -Ddubbo.container=spring,jetty,log4j 通过 classpath 下的 dubbo.properties 配置传入要加载的容器 1dubbo.container=spring,jetty,log4j","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"31日志适配和访问日志","slug":"dubbo/2.7/31日志适配和访问日志","date":"2021-11-14T03:00:32.000Z","updated":"2022-03-23T09:33:55.652Z","comments":true,"path":"blog/dubbo/2.7/31日志适配和访问日志/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/31%E6%97%A5%E5%BF%97%E9%80%82%E9%85%8D%E5%92%8C%E8%AE%BF%E9%97%AE%E6%97%A5%E5%BF%97/","excerpt":"","text":"日志适配自 2.2.1 开始，dubbo 开始内置 log4j、slf4j、jcl、jdk 这些日志框架的适配[1]，也可以通过以下方式显式配置日志输出策略： 命令行 1java -Ddubbo.application.logger=log4j 在 dubbo.properties 中指定 1dubbo.application.logger=log4j 在 dubbo.xml 中配置 1&lt;dubbo:application logger=&quot;log4j&quot; /&gt; 访问日志如果你想记录每一次请求信息，可开启访问日志，类似于apache的访问日志。注意：此日志量比较大，请注意磁盘容量。 将访问日志输出到当前应用的log4j日志： 1&lt;dubbo:protocol accesslog=&quot;true&quot; /&gt; 将访问日志输出到指定文件： 1&lt;dubbo:protocol accesslog=&quot;http://10.20.160.198/wiki/display/dubbo/foo/bar.log&quot; /&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"30注册信息简化","slug":"dubbo/2.7/30注册信息简化","date":"2021-11-14T03:00:31.000Z","updated":"2022-03-23T09:33:55.651Z","comments":true,"path":"blog/dubbo/2.7/30注册信息简化/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/30%E6%B3%A8%E5%86%8C%E4%BF%A1%E6%81%AF%E7%AE%80%E5%8C%96/","excerpt":"","text":"背景Dubbo provider 中的服务配置项有接近 30 个配置项。 排除注册中心服务治理需要之外，很大一部分配置项是 provider 自己使用，不需要透传给消费者。这部分数据不需要进入注册中心，而只需要以 key-value 形式持久化存储。 Dubbo consumer 中的配置项也有 20+个配置项。在注册中心之中，服务消费者列表中只需要关注 application，version，group，ip，dubbo 版本等少量配置，其他配置也可以以 key-value 形式持久化存储。 这些数据是以服务为维度注册进入注册中心，导致了数据量的膨胀，进而引发注册中心(如 zookeeper)的网络开销增大，性能降低。 现有功能 sample当前现状一个简单展示。通过这个展示，分析下为什么需要做简化配置。 参考 sample 子工程： dubbo-samples-simplified-registry&#x2F;dubbo-samples-simplified-registry-nosimple （跑 sample 前，先跑下 ZKClean 进行配置项清理） dubbo-provider.xml配置 123456&lt;dubbo:application name=&quot;simplified-registry-nosimple-provider&quot;/&gt;&lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot;/&gt;&lt;bean id=&quot;demoService&quot; class=&quot;org.apache.dubbo.samples.simplified.registry.nosimple.impl.DemoServiceImpl&quot;/&gt;&lt;dubbo:service async=&quot;true&quot; interface=&quot;org.apache.dubbo.samples.simplified.registry.nosimple.api.DemoService&quot; version=&quot;1.2.3&quot; group=&quot;dubbo-simple&quot; ref=&quot;demoService&quot; executes=&quot;4500&quot; retries=&quot;7&quot; owner=&quot;vict&quot; timeout=&quot;5300&quot;/&gt; 启动 provider 的 main 方法之后，查看 zookeeper 的叶子节点（路径为：&#x2F;dubbo&#x2F;org.apache.dubbo.samples.simplified.registry.nosimple.api.DemoService&#x2F;providers 目录下）的内容如下： 123456dubbo%3A%2F%2F30.5.124.158%3A20880%2Forg.apache.dubbo.samples.simplified.registry.nosimple.api.DemoService%3Fanyhost%3Dtrue%26application%3Dsimplified-registry-xml-provider%26async%3Dtrue%26dubbo%3D2.0.2%26**executes**%3D4500%26generic%3Dfalse%26group%3Ddubbo-simple%26interface%3Dorg.apache.dubbo.samples.simplified.registry.nosimple.api.DemoService%26methods%3DsayHello%26**owner**%3Dvict%26pid%3D2767%26**retries**%3D7%26revision%3D1.2.3%26side%3Dprovider%26**timeout**%3D5300%26timestamp%3D1542361152795%26valid%3Dtrue%26version%3D1.2.3 从加粗字体中能看到有：executes, retries, owner, timeout。但是这些字段不是每个都需要传递给 dubbo ops 或者 dubbo consumer。 同样的，consumer 也有这个问题，可以在例子中启动 Consumer 的 main 方法进行查看。 设计目标和宗旨期望简化进入注册中心的 provider 和 consumer 配置数量。 期望将部分配置项以其他形式存储。这些配置项需要满足：不在服务调用链路上，同时这些配置项不在注册中心的核心链路上(服务查询，服务列表)。 配置简化注册中心的配置，只在 2.7 之后的版本中进行支持。 开启 provider 或者 consumer 简化配置之后，默认保留的配置项如下： provider： Constant Key Key remark APPLICATION_KEY application CODEC_KEY codec EXCHANGER_KEY exchanger SERIALIZATION_KEY serialization CLUSTER_KEY cluster CONNECTIONS_KEY connections DEPRECATED_KEY deprecated GROUP_KEY group LOADBALANCE_KEY loadbalance MOCK_KEY mock PATH_KEY path TIMEOUT_KEY timeout TOKEN_KEY token VERSION_KEY version WARMUP_KEY warmup WEIGHT_KEY weight TIMESTAMP_KEY timestamp DUBBO_VERSION_KEY dubbo SPECIFICATION_VERSION_KEY specVersion 新增，用于表述dubbo版本，如2.7.0 consumer： Constant Key Key remark APPLICATION_KEY application VERSION_KEY version GROUP_KEY group DUBBO_VERSION_KEY dubbo SPECIFICATION_VERSION_KEY specVersion 新增，用于表述dubbo版本，如2.7.0 Constant Key 表示来自于类 org.apache.dubbo.common.Constants 的字段。 下面介绍几种常用的使用方式。所有的 sample，都可以查看sample-2.7 方式1. 配置dubbo.propertiesdubbo.properties 12dubbo.registry.simplified=truedubbo.registry.extra-keys=retries,owner 方式2. 声明spring beanProvider配置privide 端 bean 配置： 123456789// 等同于dubbo.properties配置，用@Bean形式进行配置@Beanpublic RegistryConfig registryConfig() &#123; RegistryConfig registryConfig = new RegistryConfig(); registryConfig.setAddress(&quot;zookeeper://127.0.0.1:2181&quot;); registryConfig.setSimplified(true); registryConfig.setExtraKeys(&quot;retries,owner&quot;); return registryConfig;&#125;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"29主机配置","slug":"dubbo/2.7/29主机配置","date":"2021-11-14T03:00:30.000Z","updated":"2022-03-23T09:33:55.650Z","comments":true,"path":"blog/dubbo/2.7/29主机配置/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/29%E4%B8%BB%E6%9C%BA%E9%85%8D%E7%BD%AE/","excerpt":"","text":"自定义 Dubbo 服务对外暴露的主机地址 背景在 Dubbo 中， Provider 启动时主要做两个事情，一是启动 server，二是向注册中心注册服务。启动 server 时需要绑定 socket，向注册中心注册服务时也需要发送 socket 唯一标识服务地址。 dubbo中不设置host时默认host是什么? 那在dubbo中如何指定服务的host,我们是否可以用hostname或domain代替IP地址作为host? 在使用docker时,有时需要设置端口映射,此时,启动server时绑定的socket和向注册中心注册的socket使用不同的端口号,此时又该如何设置? dubbo 中不设置 host 时默认 host 是什么一般的 dubbo 协议配置如下: 123...&lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20890&quot; /&gt;... 可以看到,只配置了端口号,没有配置 host，此时设置的 host 又是什么呢? 查看代码发现,在 org.apache.dubbo.config.ServiceConfig#findConfigedHosts() 中,通过 InetAddress.getLocalHost().getHostAddress() 获取默认 host。其返回值如下： 未联网时，返回 127.0.0.1 在阿里云服务器中，返回私有地址,如: 172.18.46.234 在本机测试时，返回公有地址，如: 30.5.10.11 那在 dubbo 中如何指定服务的 socket?除此之外,可以通过 dubbo.protocol 或 dubbo.provider 的 host 属性对 host 进行配置,支持IP地址和域名,如下: 123...&lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20890&quot; host=&quot;www.example.com&quot;/&gt;... 在使用 docker 时，有时需要设置端口映射，此时，启动 server 时绑定的 socket 和向注册中心注册的 socket 使用不同的端口号，此时又该如何设置？有些部署场景需要动态指定服务注册的地址，如 docker bridge 网络模式下要指定注册宿主机 ip 以实现外网通信。dubbo 提供了两对启动阶段的系统属性，用于设置对外通信的ip、port地址。 DUBBO_IP_TO_REGISTRY — 注册到注册中心的ip地址 DUBBO_PORT_TO_REGISTRY — 注册到注册中心的port端口 DUBBO_IP_TO_BIND — 监听ip地址 DUBBO_PORT_TO_BIND — 监听port端口 以上四个配置项均为可选项，如不配置 dubbo 会自动获取 ip 与端口，请根据具体的部署场景灵活选择配置。 dubbo 支持多协议，如果一个应用同时暴露多个不同协议服务，且需要为每个服务单独指定 ip 或 port，请分别在以上属性前加协议前缀。 如： HESSIAN_DUBBO_PORT_TO_BIND hessian协议绑定的port DUBBO_DUBBO_PORT_TO_BIND dubbo协议绑定的port HESSIAN_DUBBO_IP_TO_REGISTRY hessian协议注册的ip DUBBO_DUBBO_PORT_TO_BIND dubbo协议注册的ip PORT_TO_REGISTRY 或 IP_TO_REGISTRY 不会用作默认 PORT_TO_BIND 或 IP_TO_BIND，但是反过来是成立的 如设置 PORT_TO_REGISTRY&#x3D;20881 IP_TO_REGISTRY&#x3D;30.5.97.6，则 PORT_TO_BIND IP_TO_BIND 不受影响 如果设置 PORT_TO_BIND&#x3D;20881 IP_TO_BIND&#x3D;30.5.97.6，则默认 PORT_TO_REGISTRY&#x3D;20881 IP_TO_REGISTRY&#x3D;30.5.97.6","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"28优雅停机","slug":"dubbo/2.7/28优雅停机","date":"2021-11-14T03:00:29.000Z","updated":"2022-03-23T09:33:55.650Z","comments":true,"path":"blog/dubbo/2.7/28优雅停机/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/28%E4%BC%98%E9%9B%85%E5%81%9C%E6%9C%BA/","excerpt":"","text":"让 Dubbo 服务完成优雅停机 Dubbo 是通过 JDK 的 ShutdownHook 来完成优雅停机的，所以如果用户使用 kill -9 PID 等强制关闭指令，是不会执行优雅停机的，只有通过 kill PID 时，才会执行。 原理服务提供方 停止时，先标记为不接收新请求，新请求过来时直接报错，让客户端重试其它机器。 然后，检测线程池中的线程是否正在运行，如果有，等待所有线程执行完成，除非超时，则强制关闭。 服务消费方 停止时，不再发起新的调用请求，所有新的调用在客户端即报错。 然后，检测有没有请求的响应还没有返回，等待响应返回，除非超时，则强制关闭。 设置方式设置优雅停机超时时间，缺省超时时间是 10 秒，如果超时则强制关闭。 12# dubbo.propertiesdubbo.service.shutdown.wait=15000 如果 ShutdownHook 不能生效，可以自行调用： 1DubboShutdownHook.destroyAll(); 建议 使用 tomcat 等容器部署的场景，建议通过扩展 ContextListener 等自行调用以下代码实现优雅停机","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"27服务降级","slug":"dubbo/2.7/27服务降级","date":"2021-11-14T03:00:28.000Z","updated":"2022-03-23T09:33:55.649Z","comments":true,"path":"blog/dubbo/2.7/27服务降级/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/27%E6%9C%8D%E5%8A%A1%E9%99%8D%E7%BA%A7/","excerpt":"","text":"可以通过服务降级功能临时屏蔽某个出错的非关键服务，并定义降级后的返回策略。 向注册中心写入动态配置覆盖规则： 123RegistryFactory registryFactory = ExtensionLoader.getExtensionLoader(RegistryFactory.class).getAdaptiveExtension();Registry registry = registryFactory.getRegistry(URL.valueOf(&quot;zookeeper://10.20.153.10:2181&quot;));registry.register(URL.valueOf(&quot;override://0.0.0.0/com.foo.BarService?category=configurators&amp;dynamic=false&amp;application=foo&amp;mock=force:return+null&quot;)); 其中： mock=force:return+null 表示消费方对该服务的方法调用都直接返回 null 值，不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 还可以改为 mock=fail:return+null 表示消费方对该服务的方法调用在失败后，再返回 null 值，不抛异常。用来容忍不重要服务不稳定时对调用方的影响。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"26路由规则","slug":"dubbo/2.7/26路由规则","date":"2021-11-14T03:00:27.000Z","updated":"2022-03-23T09:33:55.649Z","comments":true,"path":"blog/dubbo/2.7/26路由规则/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/26%E8%B7%AF%E7%94%B1%E8%A7%84%E5%88%99/","excerpt":"","text":"路由规则在发起一次RPC调用前起到过滤目标服务器地址的作用，过滤后的地址列表，将作为消费端最终发起RPC调用的备选地址。 条件路由。支持以服务或 Consumer 应用为粒度配置路由规则。 标签路由。以 Provider 应用为粒度配置路由规则。 详情","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"25令牌验证","slug":"dubbo/2.7/25令牌验证","date":"2021-11-14T03:00:26.000Z","updated":"2022-03-23T09:33:55.648Z","comments":true,"path":"blog/dubbo/2.7/25令牌验证/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/25%E4%BB%A4%E7%89%8C%E9%AA%8C%E8%AF%81/","excerpt":"","text":"通过令牌验证在注册中心控制权限，以决定要不要下发令牌给消费者，可以防止消费者绕过注册中心访问提供者，另外通过注册中心可灵活改变授权方式，而不需修改或升级提供者 可以全局设置开启令牌验证： 12&lt;!--随机token令牌，使用UUID生成--&gt;&lt;dubbo:provider interface=&quot;com.foo.BarService&quot; token=&quot;true&quot; /&gt; 或 12&lt;!--固定token令牌，相当于密码--&gt;&lt;dubbo:provider interface=&quot;com.foo.BarService&quot; token=&quot;123456&quot; /&gt; 也可在服务级别设置： 12&lt;!--随机token令牌，使用UUID生成--&gt;&lt;dubbo:service interface=&quot;com.foo.BarService&quot; token=&quot;true&quot; /&gt; 或 12&lt;!--固定token令牌，相当于密码--&gt;&lt;dubbo:service interface=&quot;com.foo.BarService&quot; token=&quot;123456&quot; /&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"24TLS","slug":"dubbo/2.7/24TLS","date":"2021-11-14T03:00:25.000Z","updated":"2022-03-23T09:33:54.819Z","comments":true,"path":"blog/dubbo/2.7/24TLS/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/24TLS/","excerpt":"","text":"通过 TLS 保证传输安全 2.7.5 版本在传输链路的安全性上做了很多工作，对于内置的 Dubbo Netty Server 和新引入的 gRPC 协议都提供了基于 TLS 的安全链路传输机制。 TLS 的配置都有统一的入口，如下所示： Provider 端12345678910SslConfig sslConfig = new SslConfig();sslConfig.setServerKeyCertChainPath(&quot;path to cert&quot;);sslConfig.setServerPrivateKeyPath(args[1]);// 如果开启双向 cert 认证if (mutualTls) &#123; sslConfig.setServerTrustCertCollectionPath(args[2]);&#125;ProtocolConfig protocolConfig = new ProtocolConfig(&quot;dubbo/grpc&quot;);protocolConfig.setSslEnabled(true); Consumer 端1234567if (!mutualTls) &#123;&#125; sslConfig.setClientTrustCertCollectionPath(args[0]);&#125; else &#123; sslConfig.setClientTrustCertCollectionPath(args[0]); sslConfig.setClientKeyCertChainPath(args[1]); sslConfig.setClientPrivateKeyPath(args[2]);&#125; 为尽可能保证应用启动的灵活性，TLS Cert 的指定还能通过 -D 参数或环境变量等方式来在启动阶段根据部署环境动态指定，具体请参见 Dubbo 配置读取规则与 TLS 示例","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"23粘滞连接","slug":"dubbo/2.7/23粘滞连接","date":"2021-11-14T03:00:24.000Z","updated":"2022-03-23T09:33:54.818Z","comments":true,"path":"blog/dubbo/2.7/23粘滞连接/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/23%E7%B2%98%E6%BB%9E%E8%BF%9E%E6%8E%A5/","excerpt":"","text":"粘滞连接用于有状态服务，尽可能让客户端总是向同一提供者发起调用，除非该提供者挂了，再连另一台 粘滞连接将自动开启延迟连接，以减少长连接数。 1&lt;dubbo:reference id=&quot;xxxService&quot; interface=&quot;com.xxx.XxxService&quot; sticky=&quot;true&quot; /&gt; Dubbo 支持方法级别的粘滞连接，如果你想进行更细粒度的控制，还可以这样配置。 123&lt;dubbo:reference id=&quot;xxxService&quot; interface=&quot;com.xxx.XxxService&quot;&gt; &lt;dubbo:mothod name=&quot;sayHello&quot; sticky=&quot;true&quot; /&gt;&lt;/dubbo:reference&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"22连接控制","slug":"dubbo/2.7/22连接控制","date":"2021-11-14T03:00:23.000Z","updated":"2022-03-23T09:33:54.818Z","comments":true,"path":"blog/dubbo/2.7/22连接控制/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/22%E8%BF%9E%E6%8E%A5%E6%8E%A7%E5%88%B6/","excerpt":"","text":"Dubbo 中服务端和客户端的连接控制 服务端连接控制限制服务器端接受的连接不能超过 10 个 因为连接在 Server上，所以配置在 Provider 上 1&lt;dubbo:provider protocol=&quot;dubbo&quot; accepts=&quot;10&quot; /&gt; 或 1&lt;dubbo:protocol name=&quot;dubbo&quot; accepts=&quot;10&quot; /&gt; 客户端连接控制限制客户端服务使用连接不能超过 10 个 如果是长连接，比如 Dubbo 协议，connections 表示该服务对每个提供者建立的长连接数 1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; connections=&quot;10&quot; /&gt; 或 1&lt;dubbo:service interface=&quot;com.foo.BarService&quot; connections=&quot;10&quot; /&gt; 如果 &lt;dubbo:service&gt; 和 &lt;dubbo:reference&gt; 都配了 connections，&lt;dubbo:reference&gt; 优先","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"21并发控制","slug":"dubbo/2.7/21并发控制","date":"2021-11-14T03:00:22.000Z","updated":"2022-03-23T09:33:54.817Z","comments":true,"path":"blog/dubbo/2.7/21并发控制/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/21%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/","excerpt":"","text":"Dubbo 中的并发控制 配置样例样例 1限制 com.foo.BarService 的每个方法，服务器端并发执行（或占用线程池线程数）不能超过 10 个： 1&lt;dubbo:service interface=&quot;com.foo.BarService&quot; executes=&quot;10&quot; /&gt; 样例 2限制 com.foo.BarService 的 sayHello 方法，服务器端并发执行（或占用线程池线程数）不能超过 10 个： 123&lt;dubbo:service interface=&quot;com.foo.BarService&quot;&gt; &lt;dubbo:method name=&quot;sayHello&quot; executes=&quot;10&quot; /&gt;&lt;/dubbo:service&gt; 样例 3限制 com.foo.BarService 的每个方法，每客户端并发执行（或占用连接的请求数）不能超过 10 个： 1&lt;dubbo:service interface=&quot;com.foo.BarService&quot; actives=&quot;10&quot; /&gt; 或 1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; actives=&quot;10&quot; /&gt; 样例 4限制 com.foo.BarService 的 sayHello 方法，每客户端并发执行（或占用连接的请求数）不能超过 10 个： 123&lt;dubbo:service interface=&quot;com.foo.BarService&quot;&gt; &lt;dubbo:method name=&quot;sayHello&quot; actives=&quot;10&quot; /&gt;&lt;/dubbo:service&gt; 或 123&lt;dubbo:reference interface=&quot;com.foo.BarService&quot;&gt; &lt;dubbo:method name=&quot;sayHello&quot; actives=&quot;10&quot; /&gt;&lt;/dubbo:service&gt; 如果 &lt;dubbo:service&gt; 和 &lt;dubbo:reference&gt; 都配了actives，&lt;dubbo:reference&gt; 优先 Load Balance 均衡配置服务的客户端的 loadbalance 属性为 leastactive，此 Loadbalance 会调用并发数最小的 Provider（Consumer端并发数）。 1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; loadbalance=&quot;leastactive&quot; /&gt; 或 1&lt;dubbo:service interface=&quot;com.foo.BarService&quot; loadbalance=&quot;leastactive&quot; /&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"20本地伪装（mock）","slug":"dubbo/2.7/20本地伪装（mock）","date":"2021-11-14T03:00:21.000Z","updated":"2022-03-23T09:33:54.817Z","comments":true,"path":"blog/dubbo/2.7/20本地伪装（mock）/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/20%E6%9C%AC%E5%9C%B0%E4%BC%AA%E8%A3%85%EF%BC%88mock%EF%BC%89/","excerpt":"","text":"如何在 Dubbo 中利用本地伪装实现服务降级 本地伪装通常用于服务降级，比如某验权服务，当服务提供方全部挂掉后，客户端不抛出异常，而是通过 Mock 数据返回授权失败。 在 spring 配置文件中按以下方式配置： 1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; mock=&quot;true&quot; /&gt; 或 1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; mock=&quot;com.foo.BarServiceMock&quot; /&gt; 在工程中提供 Mock 实现 ： 1234567package com.foo;public class BarServiceMock implements BarService &#123; public String sayHello(String name) &#123; // 你可以伪造容错数据，此方法只在出现RpcException时被执行 return &quot;容错数据&quot;; &#125;&#125; 如果服务的消费方经常需要 try-catch 捕获异常，如： 123456Offer offer = null;try &#123; offer = offerService.findOffer(offerId);&#125; catch (RpcException e) &#123; logger.error(e);&#125; 请考虑改为 Mock 实现，并在 Mock 实现中 return null。如果只是想简单的忽略异常，在 2.0.11 以上版本可用： 1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; mock=&quot;return null&quot; /&gt; 进阶用法return使用 return 来返回一个字符串表示的对象，作为 Mock 的返回值。合法的字符串可以是： empty: 代表空，基本类型的默认值，或者集合类的空值 null: null true: true false: false JSON 格式: 反序列化 JSON 所得到的对象 throw使用 throw 来返回一个 Exception 对象，作为 Mock 的返回值。 当调用出错时，抛出一个默认的 RPCException: 1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; mock=&quot;throw&quot; /&gt; 当调用出错时，抛出指定的 Exception： 1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; mock=&quot;throw com.foo.MockException&quot; /&gt; force 和 fail在 2.6.6 以上的版本，可以开始在 Spring XML 配置文件中使用 fail: 和 force:。force: 代表强制使用 Mock 行为，在这种情况下不会走远程调用。fail: 与默认行为一致，只有当远程调用发生错误时才使用 Mock 行为。force: 和 fail: 都支持与 throw 或者 return 组合使用。 强制返回指定值： 1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; mock=&quot;force:return fake&quot; /&gt; 强制抛出指定异常： 1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; mock=&quot;force:throw com.foo.MockException&quot; /&gt; 在方法级别配置 MockMock 可以在方法级别上指定，假定 com.foo.BarService 上有好几个方法，我们可以单独为 sayHello() 方法指定 Mock 行为。具体配置如下所示，在本例中，只要 sayHello() 被调用到时，强制返回 “fake”: 123&lt;dubbo:reference id=&quot;demoService&quot; check=&quot;false&quot; interface=&quot;com.foo.BarService&quot;&gt; &lt;dubbo:parameter key=&quot;sayHello.mock&quot; value=&quot;force:return fake&quot;/&gt;&lt;/dubbo:reference&gt; 总结 Mock 是 Stub 的一个子集，便于服务提供方在客户端执行容错逻辑，因经常需要在出现 RpcException (比如网络失败，超时等)时进行容错，而在出现业务异常(比如登录用户名密码错误)时不需要容错，如果用 Stub，可能就需要捕获并依赖 RpcException 类，而用 Mock 就可以不依赖 RpcException，因为它的约定就是只有出现 RpcException 时才执行。 在 interface 旁放一个 Mock 实现，它实现 BarService 接口，并有一个无参构造函数","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"19本地存根（stub）","slug":"dubbo/2.7/19本地存根（stub）","date":"2021-11-14T03:00:20.000Z","updated":"2022-03-23T09:33:54.816Z","comments":true,"path":"blog/dubbo/2.7/19本地存根（stub）/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/19%E6%9C%AC%E5%9C%B0%E5%AD%98%E6%A0%B9%EF%BC%88stub%EF%BC%89/","excerpt":"","text":"在 Dubbo 中利用本地存根在客户端执行部分逻辑。 远程服务后，客户端通常只剩下接口，而实现全在服务器端，但提供方有些时候想在客户端也执行部分逻辑，比如：做 ThreadLocal 缓存，提前验证参数，调用失败后伪造容错数据等等，此时就需要在 API 中带上 Stub，客户端生成 Proxy 实例，会把 Proxy 通过构造函数传给 Stub，然后把 Stub 暴露给用户，Stub 可以决定要不要去调 Proxy。 在 spring 配置文件中按以下方式配置： 1&lt;dubbo:service interface=&quot;com.foo.BarService&quot; stub=&quot;true&quot; /&gt; 或 1&lt;dubbo:service interface=&quot;com.foo.BarService&quot; stub=&quot;com.foo.BarServiceStub&quot; /&gt; 提供 Stub 的实现: 12345678910111213141516171819package com.foo;public class BarServiceStub implements BarService &#123; private final BarService barService; // 构造函数传入真正的远程代理对象 public BarServiceStub(BarService barService)&#123; this.barService = barService; &#125; public String sayHello(String name) &#123; // 此代码在客户端执行, 你可以在客户端做ThreadLocal本地缓存，或预先验证参数是否合法，等等 try &#123; return barService.sayHello(name); &#125; catch (Exception e) &#123; // 你可以容错，可以做任何AOP拦截事项 return &quot;容错数据&quot;; &#125; &#125;&#125; Stub 必须有可传入 Proxy 的构造函数。 在 interface 旁边放一个 Stub 实现，它实现 BarService 接口，并有一个传入远程 BarService 实例的构造函数","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"18事件通知","slug":"dubbo/2.7/18事件通知","date":"2021-11-14T03:00:19.000Z","updated":"2022-03-23T09:33:54.003Z","comments":true,"path":"blog/dubbo/2.7/18事件通知/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/18%E4%BA%8B%E4%BB%B6%E9%80%9A%E7%9F%A5/","excerpt":"","text":"在调用之前、调用之后、出现异常时，会触发 oninvoke、onreturn、onthrow 三个事件，可以配置当事件发生时，通知哪个类的哪个方法。 服务提供者与消费者共享服务接口123interface IDemoService &#123; public Person get(int id);&#125; 服务提供者实现12345class NormalDemoService implements IDemoService &#123; public Person get(int id) &#123; return new Person(id, &quot;charles`son&quot;, 4); &#125;&#125; 服务提供者配置1234&lt;dubbo:application name=&quot;rpc-callback-demo&quot; /&gt;&lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot;/&gt;&lt;bean id=&quot;demoService&quot; class=&quot;org.apache.dubbo.callback.implicit.NormalDemoService&quot; /&gt;&lt;dubbo:service interface=&quot;org.apache.dubbo.callback.implicit.IDemoService&quot; ref=&quot;demoService&quot; version=&quot;1.0.0&quot; group=&quot;cn&quot;/&gt; 服务消费者 Callback 接口1234interface Notify &#123; public void onreturn(Person msg, Integer id); public void onthrow(Throwable ex, Integer id);&#125; 服务消费者 Callback 实现12345678910111213class NotifyImpl implements Notify &#123; public Map&lt;Integer, Person&gt; ret = new HashMap&lt;Integer, Person&gt;(); public Map&lt;Integer, Throwable&gt; errors = new HashMap&lt;Integer, Throwable&gt;(); public void onreturn(Person msg, Integer id) &#123; System.out.println(&quot;onreturn:&quot; + msg); ret.put(id, msg); &#125; public void onthrow(Throwable ex, Integer id) &#123; errors.put(id, ex); &#125;&#125; 服务消费者 Callback 配置1234&lt;bean id =&quot;demoCallback&quot; class = &quot;org.apache.dubbo.callback.implicit.NotifyImpl&quot; /&gt;&lt;dubbo:reference id=&quot;demoService&quot; interface=&quot;org.apache.dubbo.callback.implicit.IDemoService&quot; version=&quot;1.0.0&quot; group=&quot;cn&quot; &gt; &lt;dubbo:method name=&quot;get&quot; async=&quot;true&quot; onreturn = &quot;demoCallback.onreturn&quot; onthrow=&quot;demoCallback.onthrow&quot; /&gt;&lt;/dubbo:reference&gt; callback 与 async 功能正交分解，async=true 表示结果是否马上返回，onreturn 表示是否需要回调。 两者叠加存在以下几种组合情况： 异步回调模式：async=true onreturn=&quot;xxx&quot; 同步回调模式：async=false onreturn=&quot;xxx&quot; 异步无回调 ：async=true 同步无回调 ：async=false 测试代码1234567891011121314IDemoService demoService = (IDemoService) context.getBean(&quot;demoService&quot;);NotifyImpl notify = (NotifyImpl) context.getBean(&quot;demoCallback&quot;);int requestId = 2;Person ret = demoService.get(requestId);Assert.assertEquals(null, ret);//for Test：只是用来说明callback正常被调用，业务具体实现自行决定.for (int i = 0; i &lt; 10; i++) &#123; if (!notify.ret.containsKey(requestId)) &#123; Thread.sleep(200); &#125; else &#123; break; &#125;&#125;Assert.assertEquals(requestId, notify.ret.get(requestId).getId());","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"17参数回调","slug":"dubbo/2.7/17参数回调","date":"2021-11-14T03:00:18.000Z","updated":"2022-03-23T09:33:54.003Z","comments":true,"path":"blog/dubbo/2.7/17参数回调/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/17%E5%8F%82%E6%95%B0%E5%9B%9E%E8%B0%83/","excerpt":"","text":"参数回调方式与调用本地 callback 或 listener 相同，只需要在 Spring 的配置文件中声明哪个参数是 callback 类型即可。Dubbo 将基于长连接生成反向代理，这样就可以从服务器端调用客户端逻辑。 服务接口示例CallbackService.java12345package com.callback; public interface CallbackService &#123; void addListener(String key, CallbackListener listener);&#125; CallbackListener.java12345package com.callback; public interface CallbackListener &#123; void changed(String msg);&#125; 服务提供者接口实现示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.callback.impl; import java.text.SimpleDateFormat;import java.util.Date;import java.util.Map;import java.util.concurrent.ConcurrentHashMap; import com.callback.CallbackListener;import com.callback.CallbackService; public class CallbackServiceImpl implements CallbackService &#123; private final Map&lt;String, CallbackListener&gt; listeners = new ConcurrentHashMap&lt;String, CallbackListener&gt;(); public CallbackServiceImpl() &#123; Thread t = new Thread(new Runnable() &#123; public void run() &#123; while(true) &#123; try &#123; for(Map.Entry&lt;String, CallbackListener&gt; entry : listeners.entrySet())&#123; try &#123; entry.getValue().changed(getChanged(entry.getKey())); &#125; catch (Throwable t) &#123; listeners.remove(entry.getKey()); &#125; &#125; Thread.sleep(5000); // 定时触发变更通知 &#125; catch (Throwable t) &#123; // 防御容错 t.printStackTrace(); &#125; &#125; &#125; &#125;); t.setDaemon(true); t.start(); &#125; public void addListener(String key, CallbackListener listener) &#123; System.out.println(&quot;执行了回调服务&quot; + name); callback.changed(&quot;xxxx&quot;); listeners.put(key, callback); URL url = RpcContext.getContext().getUrl(); return String.format(&quot;%s：%s, Hello, %s&quot;, url.getProtocol(), url.getPort(), name); // 正常访问 &#125; private String getChanged(String key) &#123; return &quot;Changed: &quot; + new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).format(new Date()); &#125;&#125; 服务提供者配置示例 通过xml 12345678&lt;bean id=&quot;callbackService&quot; class=&quot;com.callback.impl.CallbackServiceImpl&quot; /&gt;&lt;dubbo:service interface=&quot;com.callback.CallbackService&quot; ref=&quot;callbackService&quot; connections=&quot;1&quot; callbacks=&quot;1000&quot;&gt; &lt;dubbo:method name=&quot;addListener&quot;&gt; &lt;dubbo:argument index=&quot;1&quot; callback=&quot;true&quot; /&gt; &lt;!--也可以通过指定类型的方式--&gt; &lt;!--&lt;dubbo:argument type=&quot;com.demo.CallbackListener&quot; callback=&quot;true&quot; /&gt;--&gt; &lt;/dubbo:method&gt;&lt;/dubbo:service&gt; 通过注解 1@Service(version = &quot;callback&quot;, methods = &#123;@Method(name = &quot;sayHello&quot;, arguments = &#123;@Argument(index = 2, callback = true)&#125;)&#125;, callbacks = 1000) 上面的配置中，name指的是方法名;index指的是参数的下标（在动态代理中，参数就是一个数组）;callback=&quot;true&quot;指的是开发参数回调；callbacks=&quot;1000&quot;指的是同一时刻，支持最大的回调的数量。 服务消费者配置示例1&lt;dubbo:reference id=&quot;callbackService&quot; interface=&quot;com.callback.CallbackService&quot; /&gt; 服务消费者调用示例12345678910ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(&quot;classpath:consumer.xml&quot;);context.start(); CallbackService callbackService = (CallbackService) context.getBean(&quot;callbackService&quot;); callbackService.addListener(&quot;foo.bar&quot;, new CallbackListener()&#123; public void changed(String msg) &#123; System.out.println(&quot;callback1:&quot; + msg); &#125;&#125;);","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"16本地调用","slug":"dubbo/2.7/16本地调用","date":"2021-11-14T03:00:17.000Z","updated":"2022-03-23T09:33:54.002Z","comments":true,"path":"blog/dubbo/2.7/16本地调用/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/16%E6%9C%AC%E5%9C%B0%E8%B0%83%E7%94%A8/","excerpt":"","text":"本地调用使用了 injvm 协议，是一个伪协议，它不开启端口，不发起远程调用，只在 JVM 内直接关联，但执行 Dubbo 的 Filter 链。 配置定义 injvm 协议 1&lt;dubbo:protocol name=&quot;injvm&quot; /&gt; 设置默认协议 1&lt;dubbo:provider protocol=&quot;injvm&quot; /&gt; 设置服务协议 1&lt;dubbo:service protocol=&quot;injvm&quot; /&gt; 优先使用 injvm 12&lt;dubbo:consumer injvm=&quot;true&quot; .../&gt;&lt;dubbo:provider injvm=&quot;true&quot; .../&gt; 或 12&lt;dubbo:reference injvm=&quot;true&quot; .../&gt;&lt;dubbo:service injvm=&quot;true&quot; .../&gt; 自动暴露、引用本地服务从 2.2.0 开始，每个服务默认都会在本地暴露。在引用服务的时候，默认优先引用本地服务。如果希望引用远程服务可以使用一下配置强制引用远程服务。 1&lt;dubbo:reference ... scope=&quot;remote&quot; /&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"15异步执行和异步调用","slug":"dubbo/2.7/15异步执行和异步调用","date":"2021-11-14T03:00:16.000Z","updated":"2022-03-23T09:33:54.001Z","comments":true,"path":"blog/dubbo/2.7/15异步执行和异步调用/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/15%E5%BC%82%E6%AD%A5%E6%89%A7%E8%A1%8C%E5%92%8C%E5%BC%82%E6%AD%A5%E8%B0%83%E7%94%A8/","excerpt":"","text":"异步执行Provider端异步执行将阻塞的业务从Dubbo内部线程池切换到业务自定义线程，避免Dubbo线程池的过度占用，有助于避免不同服务间的互相影响。异步执行无益于节省资源或提升RPC响应性能，因为如果业务执行需要阻塞，则始终还是要有线程来负责执行。 注意 Provider 端异步执行和 Consumer 端异步调用是相互独立的，你可以任意正交组合两端配置 Consumer同步 - Provider同步 Consumer异步 - Provider同步 Consumer同步 - Provider异步 Consumer异步 - Provider异步 定义 CompletableFuture 签名的接口服务接口定义： 123public interface AsyncService &#123; CompletableFuture&lt;String&gt; sayHello(String name);&#125; 服务实现： 12345678910111213141516public class AsyncServiceImpl implements AsyncService &#123; @Override public CompletableFuture&lt;String&gt; sayHello(String name) &#123; RpcContext savedContext = RpcContext.getContext(); // 建议为supplyAsync提供自定义线程池，避免使用JDK公用线程池 return CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(savedContext.getAttachment(&quot;consumer-key1&quot;)); try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return &quot;async response from provider.&quot;; &#125;); &#125;&#125; 通过 return CompletableFuture.supplyAsync() ，业务执行已从 Dubbo 线程切换到业务线程，避免了对 Dubbo 线程池的阻塞。 使用AsyncContextDubbo 提供了一个类似 Serverlet 3.0 的异步接口AsyncContext，在没有 CompletableFuture 签名接口的情况下，也可以实现 Provider 端的异步执行。 服务接口定义： 123public interface AsyncService &#123; String sayHello(String name);&#125; 服务暴露，和普通服务完全一致： 12&lt;bean id=&quot;asyncService&quot; class=&quot;org.apache.dubbo.samples.governance.impl.AsyncServiceImpl&quot;/&gt;&lt;dubbo:service interface=&quot;org.apache.dubbo.samples.governance.api.AsyncService&quot; ref=&quot;asyncService&quot;/&gt; 服务实现： 123456789101112131415161718public class AsyncServiceImpl implements AsyncService &#123; public String sayHello(String name) &#123; final AsyncContext asyncContext = RpcContext.startAsync(); //可以交给一个线程池来做 new Thread(() -&gt; &#123; // 如果要使用上下文，则必须要放在第一句执行 asyncContext.signalContextSwitch(); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 写回响应 asyncContext.write(&quot;Hello &quot; + name + &quot;, response from provider.&quot;); &#125;).start(); return null; &#125;&#125; 异步调用在 Dubbo 中发起异步调用 从 2.7.0 开始，Dubbo 的所有异步编程接口开始以 CompletableFuture 为基础 基于 NIO 的非阻塞实现并行调用，客户端不需要启动多线程即可完成并行调用多个远程服务，相对多线程开销较小。 使用 CompletableFuture 签名的接口需要服务提供者事先定义 CompletableFuture 签名的服务 123public interface AsyncService &#123; CompletableFuture&lt;String&gt; sayHello(String name);&#125; 注意接口的返回类型是 CompletableFuture&lt;String&gt;。 XML引用服务： 1&lt;dubbo:reference id=&quot;asyncService&quot; timeout=&quot;10000&quot; interface=&quot;com.alibaba.dubbo.samples.async.api.AsyncService&quot;/&gt; 调用远程服务： 123456789101112// 调用直接返回CompletableFutureCompletableFuture&lt;String&gt; future = asyncService.sayHello(&quot;async call request&quot;);// 增加回调future.whenComplete((v, t) -&gt; &#123; if (t != null) &#123; t.printStackTrace(); &#125; else &#123; System.out.println(&quot;Response: &quot; + v); &#125;&#125;);// 早于结果输出System.out.println(&quot;Executed before response return.&quot;); 使用 RpcContext在 consumer.xml 中配置： 123&lt;dubbo:reference id=&quot;asyncService&quot; interface=&quot;org.apache.dubbo.samples.governance.api.AsyncService&quot;&gt; &lt;dubbo:method name=&quot;sayHello&quot; async=&quot;true&quot; /&gt;&lt;/dubbo:reference&gt; 调用代码: 123456789101112// 此调用会立即返回nullasyncService.sayHello(&quot;world&quot;);// 拿到调用的Future引用，当结果返回后，会被通知和设置到此FutureCompletableFuture&lt;String&gt; helloFuture = RpcContext.getContext().getCompletableFuture();// 为Future添加回调helloFuture.whenComplete((retValue, exception) -&gt; &#123; if (exception == null) &#123; System.out.println(retValue); &#125; else &#123; exception.printStackTrace(); &#125;&#125;); 或者，你也可以这样做异步调用: 1234567CompletableFuture&lt;String&gt; future = RpcContext.getContext().asyncCall( () -&gt; &#123; asyncService.sayHello(&quot;oneway call request1&quot;); &#125;);future.get(); 重载服务接口如果你只有这样的同步服务定义，而又不喜欢 RpcContext 的异步使用方式。 123public interface GreetingsService &#123; String sayHi(String name);&#125; 那还有一种方式，就是利用 Java 8 提供的 default 接口实现，重载一个带有 CompletableFuture 签名的方法。 有使用这种方式来实现： 提供方或消费方自己修改接口签名 12345678public interface GreetingsService &#123; String sayHi(String name); // AsyncSignal is totally optional, you can use any parameter type as long as java allows your to do that. default CompletableFuture&lt;String&gt; sayHi(String name, AsyncSignal signal) &#123; return CompletableFuture.completedFuture(sayHi(name)); &#125;&#125; 你也可以设置是否等待消息发出： sent=&quot;true&quot; 等待消息发出，消息发送失败将抛出异常。 sent=&quot;false&quot; 不等待消息发出，将消息放入 IO 队列，即刻返回。 1&lt;dubbo:method name=&quot;findFoo&quot; async=&quot;true&quot; sent=&quot;true&quot; /&gt; 如果你只是想异步，完全忽略返回值，可以配置 return=&quot;false&quot;，以减少 Future 对象的创建和管理成本： 1&lt;dubbo:method name=&quot;findFoo&quot; async=&quot;true&quot; return=&quot;false&quot; /&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"14隐式参数","slug":"dubbo/2.7/14隐式参数","date":"2021-11-14T03:00:15.000Z","updated":"2022-03-23T09:33:53.000Z","comments":true,"path":"blog/dubbo/2.7/14隐式参数/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/14%E9%9A%90%E5%BC%8F%E5%8F%82%E6%95%B0/","excerpt":"","text":"通过 Dubbo 中的 Attachment 在服务消费方和提供方之间隐式传递参数 可以通过 RpcContext 上的 setAttachment 和 getAttachment 在服务消费方和提供方之间进行参数的隐式传递。 注意 path, group, version, dubbo, token, timeout 几个 key 是保留字段，请使用其它值。 在服务消费方端设置隐式参数setAttachment 设置的 KV 对，在完成下面一次远程调用会被清空，即多次远程调用要多次设置。 123RpcContext.getContext().setAttachment(&quot;index&quot;, &quot;1&quot;); // 隐式传参，后面的远程调用都会隐式将这些参数发送到服务器端，类似cookie，用于框架集成，不建议常规业务使用xxxService.xxx(); // 远程调用// ... 在服务提供方端获取隐式参数1234567public class XxxServiceImpl implements XxxService &#123; public void xxx() &#123; // 获取客户端隐式传入的参数，用于框架集成，不建议常规业务使用 String index = RpcContext.getContext().getAttachment(&quot;index&quot;); &#125;&#125;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"13上下文信息","slug":"dubbo/2.7/13上下文信息","date":"2021-11-14T03:00:14.000Z","updated":"2022-03-23T09:33:52.086Z","comments":true,"path":"blog/dubbo/2.7/13上下文信息/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/13%E4%B8%8A%E4%B8%8B%E6%96%87%E4%BF%A1%E6%81%AF/","excerpt":"","text":"通过上下文存放当前调用过程中所需的环境信息。 上下文中存放的是当前调用过程中所需的环境信息。所有配置信息都将转换为 URL 的参数。 RpcContext 是一个 ThreadLocal 的临时状态记录器，当接收到 RPC 请求，或发起 RPC 请求时，RpcContext 的状态都会变化。比如：A 调 B，B 再调 C，则 B 机器上，在 B 调 C 之前，RpcContext 记录的是 A 调 B 的信息，在 B 调 C 之后，RpcContext 记录的是 B 调 C 的信息。 服务消费方12345678910// 远程调用xxxService.xxx();// 本端是否为消费端，这里会返回trueboolean isConsumerSide = RpcContext.getContext().isConsumerSide();// 获取最后一次调用的提供方IP地址String serverIP = RpcContext.getContext().getRemoteHost();// 获取当前服务配置信息，所有配置信息都将转换为URL的参数String application = RpcContext.getContext().getUrl().getParameter(&quot;application&quot;);// 注意：每发起RPC调用，上下文状态会变化yyyService.yyy(); 服务提供方123456789101112131415public class XxxServiceImpl implements XxxService &#123; public void xxx() &#123; // 本端是否为提供端，这里会返回true boolean isProviderSide = RpcContext.getContext().isProviderSide(); // 获取调用方IP地址 String clientIP = RpcContext.getContext().getRemoteHost(); // 获取当前服务配置信息，所有配置信息都将转换为URL的参数 String application = RpcContext.getContext().getUrl().getParameter(&quot;application&quot;); // 注意：每发起RPC调用，上下文状态会变化 yyyService.yyy(); // 此时本端变成消费端，这里会返回false boolean isProviderSide = RpcContext.getContext().isProviderSide(); &#125; &#125;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"12回声测试","slug":"dubbo/2.7/12回声测试","date":"2021-11-14T03:00:13.000Z","updated":"2022-03-23T09:33:52.085Z","comments":true,"path":"blog/dubbo/2.7/12回声测试/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/12%E5%9B%9E%E5%A3%B0%E6%B5%8B%E8%AF%95/","excerpt":"","text":"通过回声测试检测 Dubbo 服务是否可用 回声测试用于检测服务是否可用，回声测试按照正常请求流程执行，能够测试整个调用是否通畅，可用于监控。 所有服务自动实现 EchoService 接口，只需将任意服务引用强制转型为 EchoService，即可使用。 Spring 配置： 1&lt;dubbo:reference id=&quot;memberService&quot; interface=&quot;com.xxx.MemberService&quot; 代码： 123456789// 远程服务引用MemberService memberService = ctx.getBean(&quot;memberService&quot;); EchoService echoService = (EchoService) memberService; // 强制转型为EchoService// 回声测试可用性String status = echoService.$echo(&quot;OK&quot;); assert(status.equals(&quot;OK&quot;));","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"11实现泛化调用","slug":"dubbo/2.7/11实现泛化调用","date":"2021-11-14T03:00:12.000Z","updated":"2022-03-23T09:33:52.084Z","comments":true,"path":"blog/dubbo/2.7/11实现泛化调用/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/11%E5%AE%9E%E7%8E%B0%E6%B3%9B%E5%8C%96%E8%B0%83%E7%94%A8/","excerpt":"","text":"实现一个通用的远程服务 Mock 框架，可通过实现 GenericService 接口处理所有服务请求 泛接口实现方式主要用于服务器端没有 API 接口及模型类元的情况，参数及返回值中的所有 POJO 均用 Map 表示，通常用于框架集成，比如：实现一个通用的远程服务 Mock 框架，可通过实现 GenericService 接口处理所有服务请求。 在 Java 代码中实现 GenericService 接口： 123456789package com.foo;public class MyGenericService implements GenericService &#123; public Object $invoke(String methodName, String[] parameterTypes, Object[] args) throws GenericException &#123; if (&quot;sayHello&quot;.equals(methodName)) &#123; return &quot;Welcome &quot; + args[0]; &#125; &#125;&#125; 通过 Spring 暴露泛化实现在 Spring 配置申明服务的实现： 12&lt;bean id=&quot;genericService&quot; class=&quot;com.foo.MyGenericService&quot; /&gt;&lt;dubbo:service interface=&quot;com.foo.BarService&quot; ref=&quot;genericService&quot; /&gt; 通过 API 方式暴露泛化实现1234567891011121314... // 用org.apache.dubbo.rpc.service.GenericService可以替代所有接口实现 GenericService xxxService = new XxxGenericService(); // 该实例很重量，里面封装了所有与注册中心及服务提供方连接，请缓存 ServiceConfig&lt;GenericService&gt; service = new ServiceConfig&lt;GenericService&gt;();// 弱类型接口名 service.setInterface(&quot;com.xxx.XxxService&quot;); service.setVersion(&quot;1.0.0&quot;); // 指向一个通用服务实现 service.setRef(xxxService); // 暴露及注册服务 service.export();","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"10使用泛化调用","slug":"dubbo/2.7/10使用泛化调用","date":"2021-11-14T03:00:11.000Z","updated":"2022-03-23T09:33:52.084Z","comments":true,"path":"blog/dubbo/2.7/10使用泛化调用/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/10%E4%BD%BF%E7%94%A8%E6%B3%9B%E5%8C%96%E8%B0%83%E7%94%A8/","excerpt":"","text":"实现一个通用的服务测试框架，可通过 GenericService 调用所有服务实现 泛化接口调用方式主要用于客户端没有 API 接口及模型类元的情况，参数及返回值中的所有 POJO 均用 Map 表示，通常用于框架集成，比如：实现一个通用的服务测试框架，可通过 GenericService 调用所有服务实现。 通过 Spring 使用泛化调用在 Spring 配置申明 generic=&quot;true&quot;： 1&lt;dubbo:reference id=&quot;barService&quot; interface=&quot;com.foo.BarService&quot; generic=&quot;true&quot; /&gt; 在 Java 代码获取 barService 并开始泛化调用： 12GenericService barService = (GenericService) applicationContext.getBean(&quot;barService&quot;);Object result = barService.$invoke(&quot;sayHello&quot;, new String[] &#123; &quot;java.lang.String&quot; &#125;, new Object[] &#123; &quot;World&quot; &#125;); 通过 API 方式使用泛化调用123456789101112131415161718192021222324252627import org.apache.dubbo.rpc.service.GenericService; ... // 引用远程服务 // 该实例很重量，里面封装了所有与注册中心及服务提供方连接，请缓存ReferenceConfig&lt;GenericService&gt; reference = new ReferenceConfig&lt;GenericService&gt;(); // 弱类型接口名reference.setInterface(&quot;com.xxx.XxxService&quot;); reference.setVersion(&quot;1.0.0&quot;);// 声明为泛化接口 reference.setGeneric(true); // 用org.apache.dubbo.rpc.service.GenericService可以替代所有接口引用 GenericService genericService = reference.get(); // 基本类型以及Date,List,Map等不需要转换，直接调用 Object result = genericService.$invoke(&quot;sayHello&quot;, new String[] &#123;&quot;java.lang.String&quot;&#125;, new Object[] &#123;&quot;world&quot;&#125;); // 用Map表示POJO参数，如l p果返回值为POJO也将自动转成Map Map&lt;String, Object&gt; person = new HashMap&lt;String, Object&gt;(); person.put(&quot;name&quot;, &quot;xxx&quot;); person.put(&quot;password&quot;, &quot;yyy&quot;); // 如果返回POJO将自动转成Map Object result = genericService.$invoke(&quot;findPerson&quot;, new String[]&#123;&quot;com.xxx.Person&quot;&#125;, new Object[]&#123;person&#125;); ... 有关泛化类型的进一步解释假设存在 POJO 如： 12345678910111213141516171819202122package com.xxx;public class PersonImpl implements Person &#123; private String name; private String password; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; 则 POJO 数据： 123Person person = new PersonImpl(); person.setName(&quot;xxx&quot;); person.setPassword(&quot;yyy&quot;); 可用下面 Map 表示： 12345Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); // 注意：如果参数类型是接口，或者List等丢失泛型，可通过class属性指定类型。map.put(&quot;class&quot;, &quot;com.xxx.PersonImpl&quot;); map.put(&quot;name&quot;, &quot;xxx&quot;); map.put(&quot;password&quot;, &quot;yyy&quot;);","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"09结果缓存","slug":"dubbo/2.7/09结果缓存","date":"2021-11-14T03:00:10.000Z","updated":"2022-03-23T09:33:52.083Z","comments":true,"path":"blog/dubbo/2.7/09结果缓存/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/09%E7%BB%93%E6%9E%9C%E7%BC%93%E5%AD%98/","excerpt":"","text":"通过缓存结果加速访问速度 结果缓存，用于加速热门数据的访问速度，Dubbo 提供声明式缓存，以减少用户加缓存的工作量。 缓存类型 lru 基于最近最少使用原则删除多余缓存，保持最热的数据被缓存。 threadlocal 当前线程缓存，比如一个页面渲染，用到很多 portal，每个 portal 都要去查用户信息，通过线程缓存，可以减少这种多余访问。 jcache 与 JSR107 集成，可以桥接各种缓存实现。 缓存类型可扩展，参见：缓存扩展 配置1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; cache=&quot;lru&quot; /&gt; 或： 123&lt;dubbo:reference interface=&quot;com.foo.BarService&quot;&gt; &lt;dubbo:method name=&quot;findBar&quot; cache=&quot;lru&quot; /&gt;&lt;/dubbo:reference&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"08参数验证","slug":"dubbo/2.7/08参数验证","date":"2021-11-14T03:00:09.000Z","updated":"2022-03-23T09:33:52.082Z","comments":true,"path":"blog/dubbo/2.7/08参数验证/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/08%E5%8F%82%E6%95%B0%E9%AA%8C%E8%AF%81/","excerpt":"","text":"参数验证功能是基于 JSR303 实现的，用户只需标识 JSR303 标准的验证 annotation，并通过声明 filter 来实现验证。 Maven 依赖12345678910&lt;dependency&gt; &lt;groupId&gt;javax.validation&lt;/groupId&gt; &lt;artifactId&gt;validation-api&lt;/artifactId&gt; &lt;version&gt;1.0.0.GA&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;4.2.0.Final&lt;/version&gt;&lt;/dependency&gt; 示例参数标注示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import java.io.Serializable;import java.util.Date; import javax.validation.constraints.Future;import javax.validation.constraints.Max;import javax.validation.constraints.Min;import javax.validation.constraints.NotNull;import javax.validation.constraints.Past;import javax.validation.constraints.Pattern;import javax.validation.constraints.Size; public class ValidationParameter implements Serializable &#123; private static final long serialVersionUID = 7158911668568000392L; @NotNull // 不允许为空 @Size(min = 1, max = 20) // 长度或大小范围 private String name; @NotNull(groups = ValidationService.Save.class) // 保存时不允许为空，更新时允许为空 ，表示不更新该字段 @Pattern(regexp = &quot;^\\\\s*\\\\w+(?:\\\\.&#123;0,1&#125;[\\\\w-]+)*@[a-zA-Z0-9]+(?:[-.][a-zA-Z0-9]+)*\\\\.[a-zA-Z]+\\\\s*$&quot;) private String email; @Min(18) // 最小值 @Max(100) // 最大值 private int age; @Past // 必须为一个过去的时间 private Date loginDate; @Future // 必须为一个未来的时间 private Date expiryDate; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public Date getLoginDate() &#123; return loginDate; &#125; public void setLoginDate(Date loginDate) &#123; this.loginDate = loginDate; &#125; public Date getExpiryDate() &#123; return expiryDate; &#125; public void setExpiryDate(Date expiryDate) &#123; this.expiryDate = expiryDate; &#125;&#125; 分组验证示例12345public interface ValidationService &#123; // 缺省可按服务接口区分验证场景，如：@NotNull(groups = ValidationService.class) @interface Save&#123;&#125; // 与方法同名接口，首字母大写，用于区分验证场景，如：@NotNull(groups = ValidationService.Save.class)，可选 void save(ValidationParameter parameter); void update(ValidationParameter parameter);&#125; 关联验证示例12345678910import javax.validation.GroupSequence; public interface ValidationService &#123; @GroupSequence(Update.class) // 同时验证Update组规则 @interface Save&#123;&#125; void save(ValidationParameter parameter); @interface Update&#123;&#125; void update(ValidationParameter parameter);&#125; 参数验证示例1234567import javax.validation.constraints.Min;import javax.validation.constraints.NotNull; public interface ValidationService &#123; void save(@NotNull ValidationParameter parameter); // 验证参数不为空 void delete(@Min(1) int id); // 直接对基本类型参数验证&#125; 配置在客户端验证参数1&lt;dubbo:reference id=&quot;validationService&quot; interface=&quot;org.apache.dubbo.examples.validation.api.ValidationService&quot; validation=&quot;true&quot; /&gt; 在服务器端验证参数1&lt;dubbo:service interface=&quot;org.apache.dubbo.examples.validation.api.ValidationService&quot; ref=&quot;validationService&quot; validation=&quot;true&quot; /&gt; 验证异常信息123456789101112131415161718192021222324252627import javax.validation.ConstraintViolationException;import javax.validation.ConstraintViolationException; import org.springframework.context.support.ClassPathXmlApplicationContext; import org.apache.dubbo.examples.validation.api.ValidationParameter;import org.apache.dubbo.examples.validation.api.ValidationService;import org.apache.dubbo.rpc.RpcException; public class ValidationConsumer &#123; public static void main(String[] args) throws Exception &#123; String config = ValidationConsumer.class.getPackage().getName().replace(&#x27;.&#x27;, &#x27;/&#x27;) + &quot;/validation-consumer.xml&quot;; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(config); context.start(); ValidationService validationService = (ValidationService)context.getBean(&quot;validationService&quot;); // Error try &#123; parameter = new ValidationParameter(); validationService.save(parameter); System.out.println(&quot;Validation ERROR&quot;); &#125; catch (RpcException e) &#123; // 抛出的是RpcException ConstraintViolationException ve = (ConstraintViolationException) e.getCause(); // 里面嵌了一个ConstraintViolationException Set&lt;ConstraintViolation&lt;?&gt;&gt; violations = ve.getConstraintViolations(); // 可以拿到一个验证错误详细信息的集合 System.out.println(violations); &#125; &#125; &#125;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"07多版本","slug":"dubbo/2.7/07多版本","date":"2021-11-14T03:00:08.000Z","updated":"2022-03-23T09:33:52.082Z","comments":true,"path":"blog/dubbo/2.7/07多版本/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/07%E5%A4%9A%E7%89%88%E6%9C%AC/","excerpt":"","text":"当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。 可以按照以下的步骤进行版本迁移： 在低压力时间段，先升级一半提供者为新版本 再将所有消费者升级为新版本 然后将剩下的一半提供者升级为新版本 老版本服务提供者配置： 1&lt;dubbo:service interface=&quot;com.foo.BarService&quot; version=&quot;1.0.0&quot; /&gt; 新版本服务提供者配置： 1&lt;dubbo:service interface=&quot;com.foo.BarService&quot; version=&quot;2.0.0&quot; /&gt; 老版本服务消费者配置： 1&lt;dubbo:reference id=&quot;barService&quot; interface=&quot;com.foo.BarService&quot; version=&quot;1.0.0&quot; /&gt; 新版本服务消费者配置： 1&lt;dubbo:reference id=&quot;barService&quot; interface=&quot;com.foo.BarService&quot; version=&quot;2.0.0&quot; /&gt; 如果不需要区分版本，可以按照以下的方式配置: 1&lt;dubbo:reference id=&quot;barService&quot; interface=&quot;com.foo.BarService&quot; version=&quot;*&quot; /&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"06服务分组和分组聚合","slug":"dubbo/2.7/06服务分组和分组聚合","date":"2021-11-14T03:00:07.000Z","updated":"2022-03-23T09:33:52.081Z","comments":true,"path":"blog/dubbo/2.7/06服务分组和分组聚合/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/06%E6%9C%8D%E5%8A%A1%E5%88%86%E7%BB%84%E5%92%8C%E5%88%86%E7%BB%84%E8%81%9A%E5%90%88/","excerpt":"","text":"服务分组当一个接口有多种实现时，可以用 group 区分。 服务12&lt;dubbo:service group=&quot;feedback&quot; interface=&quot;com.xxx.IndexService&quot; /&gt;&lt;dubbo:service group=&quot;member&quot; interface=&quot;com.xxx.IndexService&quot; /&gt; 引用12&lt;dubbo:reference id=&quot;feedbackIndexService&quot; group=&quot;feedback&quot; interface=&quot;com.xxx.IndexService&quot; /&gt;&lt;dubbo:reference id=&quot;memberIndexService&quot; group=&quot;member&quot; interface=&quot;com.xxx.IndexService&quot; /&gt; 任意组： 1&lt;dubbo:reference id=&quot;barService&quot; interface=&quot;com.foo.BarService&quot; group=&quot;*&quot; /&gt; 分组聚合按组合并返回结果，比如菜单服务，接口一样，但有多种实现，用group区分，现在消费方需从每种group中调用一次返回结果，合并结果返回，这样就可以实现聚合菜单项。 配置搜索所有分组 1&lt;dubbo:reference interface=&quot;com.xxx.MenuService&quot; group=&quot;*&quot; merger=&quot;true&quot; /&gt; 合并指定分组 1&lt;dubbo:reference interface=&quot;com.xxx.MenuService&quot; group=&quot;aaa,bbb&quot; merger=&quot;true&quot; /&gt; 指定方法合并结果，其它未指定的方法，将只调用一个 Group 123&lt;dubbo:reference interface=&quot;com.xxx.MenuService&quot; group=&quot;*&quot;&gt; &lt;dubbo:method name=&quot;getMenuItems&quot; merger=&quot;true&quot; /&gt;&lt;/dubbo:reference&gt; 某个方法不合并结果，其它都合并结果 123&lt;dubbo:reference interface=&quot;com.xxx.MenuService&quot; group=&quot;*&quot; merger=&quot;true&quot;&gt; &lt;dubbo:method name=&quot;getMenuItems&quot; merger=&quot;false&quot; /&gt;&lt;/dubbo:reference&gt; 指定合并策略，缺省根据返回值类型自动匹配，如果同一类型有两个合并器时，需指定合并器的名称 123&lt;dubbo:reference interface=&quot;com.xxx.MenuService&quot; group=&quot;*&quot;&gt; &lt;dubbo:method name=&quot;getMenuItems&quot; merger=&quot;mymerge&quot; /&gt;&lt;/dubbo:reference&gt; 指定合并方法，将调用返回结果的指定方法进行合并，合并方法的参数类型必须是返回结果类型本身 123&lt;dubbo:reference interface=&quot;com.xxx.MenuService&quot; group=&quot;*&quot;&gt; &lt;dubbo:method name=&quot;getMenuItems&quot; merger=&quot;.addAll&quot; /&gt;&lt;/dubbo:reference&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"05多注册中心","slug":"dubbo/2.7/05多注册中心","date":"2021-11-14T03:00:06.000Z","updated":"2022-03-23T09:33:52.080Z","comments":true,"path":"blog/dubbo/2.7/05多注册中心/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/05%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/","excerpt":"","text":"Dubbo 支持同一服务向多注册中心同时注册，或者不同服务分别注册到不同的注册中心上去，甚至可以同时引用注册在不同注册中心上的同名服务。另外，注册中心是支持自定义扩展的 多注册中心注册比如：中文站有些服务来不及在青岛部署，只在杭州部署，而青岛的其它应用需要引用此服务，就可以将服务同时注册到两个注册中心。 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://dubbo.apache.org/schema/dubbo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd&quot;&gt; &lt;dubbo:application name=&quot;world&quot; /&gt; &lt;!-- 多注册中心配置 --&gt; &lt;dubbo:registry id=&quot;hangzhouRegistry&quot; address=&quot;10.20.141.150:9090&quot; /&gt; &lt;dubbo:registry id=&quot;qingdaoRegistry&quot; address=&quot;10.20.141.151:9010&quot; default=&quot;false&quot; /&gt; &lt;!-- 向多个注册中心注册 --&gt; &lt;dubbo:service interface=&quot;com.alibaba.hello.api.HelloService&quot; version=&quot;1.0.0&quot; ref=&quot;helloService&quot; registry=&quot;hangzhouRegistry,qingdaoRegistry&quot; /&gt;&lt;/beans&gt; 不同服务使用不同注册中心比如：CRM 有些服务是专门为国际站设计的，有些服务是专门为中文站设计的。 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://dubbo.apache.org/schema/dubbo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd&quot;&gt; &lt;dubbo:application name=&quot;world&quot; /&gt; &lt;!-- 多注册中心配置 --&gt; &lt;dubbo:registry id=&quot;chinaRegistry&quot; address=&quot;10.20.141.150:9090&quot; /&gt; &lt;dubbo:registry id=&quot;intlRegistry&quot; address=&quot;10.20.154.177:9010&quot; default=&quot;false&quot; /&gt; &lt;!-- 向中文站注册中心注册 --&gt; &lt;dubbo:service interface=&quot;com.alibaba.hello.api.HelloService&quot; version=&quot;1.0.0&quot; ref=&quot;helloService&quot; registry=&quot;chinaRegistry&quot; /&gt; &lt;!-- 向国际站注册中心注册 --&gt; &lt;dubbo:service interface=&quot;com.alibaba.hello.api.DemoService&quot; version=&quot;1.0.0&quot; ref=&quot;demoService&quot; registry=&quot;intlRegistry&quot; /&gt;&lt;/beans&gt; 多注册中心引用比如：CRM 需同时调用中文站和国际站的 PC2 服务，PC2 在中文站和国际站均有部署，接口及版本号都一样，但连的数据库不一样。 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://dubbo.apache.org/schema/dubbo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd&quot;&gt; &lt;dubbo:application name=&quot;world&quot; /&gt; &lt;!-- 多注册中心配置 --&gt; &lt;dubbo:registry id=&quot;chinaRegistry&quot; address=&quot;10.20.141.150:9090&quot; /&gt; &lt;dubbo:registry id=&quot;intlRegistry&quot; address=&quot;10.20.154.177:9010&quot; default=&quot;false&quot; /&gt; &lt;!-- 引用中文站服务 --&gt; &lt;dubbo:reference id=&quot;chinaHelloService&quot; interface=&quot;com.alibaba.hello.api.HelloService&quot; version=&quot;1.0.0&quot; registry=&quot;chinaRegistry&quot; /&gt; &lt;!-- 引用国际站站服务 --&gt; &lt;dubbo:reference id=&quot;intlHelloService&quot; interface=&quot;com.alibaba.hello.api.HelloService&quot; version=&quot;1.0.0&quot; registry=&quot;intlRegistry&quot; /&gt;&lt;/beans&gt; 如果只是测试环境临时需要连接两个不同注册中心，使用竖号分隔多个不同注册中心地址： 1234567891011&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://dubbo.apache.org/schema/dubbo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd&quot;&gt; &lt;dubbo:application name=&quot;world&quot; /&gt; &lt;!-- 多注册中心配置，竖号分隔表示同时连接多个不同注册中心，同一注册中心的多个集群地址用逗号分隔 --&gt; &lt;dubbo:registry address=&quot;10.20.141.150:9090|10.20.154.177:9010&quot; /&gt; &lt;!-- 引用服务 --&gt; &lt;dubbo:reference id=&quot;helloService&quot; interface=&quot;com.alibaba.hello.api.HelloService&quot; version=&quot;1.0.0&quot; /&gt;&lt;/beans&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"04多协议","slug":"dubbo/2.7/04多协议","date":"2021-11-14T03:00:05.000Z","updated":"2022-03-23T09:33:52.080Z","comments":true,"path":"blog/dubbo/2.7/04多协议/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/04%E5%A4%9A%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"Dubbo 允许配置多协议，在不同服务上支持不同协议或者同一服务上同时支持多种协议。 不同服务不同协议不同服务在性能上适用不同协议进行传输，比如大数据用短连接协议，小数据大并发用长连接协议 123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://dubbo.apache.org/schema/dubbo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd&quot;&gt; &lt;dubbo:application name=&quot;world&quot; /&gt; &lt;dubbo:registry id=&quot;registry&quot; address=&quot;10.20.141.150:9090&quot; username=&quot;admin&quot; password=&quot;hello1234&quot; /&gt; &lt;!-- 多协议配置 --&gt; &lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot; /&gt; &lt;dubbo:protocol name=&quot;rmi&quot; port=&quot;1099&quot; /&gt; &lt;!-- 使用dubbo协议暴露服务 --&gt; &lt;dubbo:service interface=&quot;com.alibaba.hello.api.HelloService&quot; version=&quot;1.0.0&quot; ref=&quot;helloService&quot; protocol=&quot;dubbo&quot; /&gt; &lt;!-- 使用rmi协议暴露服务 --&gt; &lt;dubbo:service interface=&quot;com.alibaba.hello.api.DemoService&quot; version=&quot;1.0.0&quot; ref=&quot;demoService&quot; protocol=&quot;rmi&quot; /&gt; &lt;/beans&gt; 多协议暴露服务需要与 http 客户端互操作 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://dubbo.apache.org/schema/dubbo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd&quot;&gt; &lt;dubbo:application name=&quot;world&quot; /&gt; &lt;dubbo:registry id=&quot;registry&quot; address=&quot;10.20.141.150:9090&quot; username=&quot;admin&quot; password=&quot;hello1234&quot; /&gt; &lt;!-- 多协议配置 --&gt; &lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot; /&gt; &lt;dubbo:protocol name=&quot;hessian&quot; port=&quot;8080&quot; /&gt; &lt;!-- 使用多个协议暴露服务 --&gt; &lt;dubbo:service id=&quot;helloService&quot; interface=&quot;com.alibaba.hello.api.HelloService&quot; version=&quot;1.0.0&quot; protocol=&quot;dubbo,hessian&quot; /&gt;&lt;/beans&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"03直连提供者","slug":"dubbo/2.7/03直连提供者","date":"2021-11-14T03:00:04.000Z","updated":"2022-03-23T09:33:52.079Z","comments":true,"path":"blog/dubbo/2.7/03直连提供者/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/03%E7%9B%B4%E8%BF%9E%E6%8F%90%E4%BE%9B%E8%80%85/","excerpt":"","text":"如果是线上需求需要点对点，可在 &lt;dubbo:reference&gt; 中配置 url 指向提供者，将绕过注册中心，多个地址用分号隔开，配置如下： 1&lt;dubbo:reference id=&quot;xxxService&quot; interface=&quot;com.alibaba.xxx.XxxService&quot; url=&quot;dubbo://localhost:20890&quot; /&gt; 如果服务比较多，也可以用文件映射，用 -Ddubbo.resolve.file 指定映射文件路径，此配置优先级高于 &lt;dubbo:reference&gt; 中的配置 [^3]，如： 1java -Ddubbo.resolve.file=xxx.properties 然后在映射文件 xxx.properties 中加入配置，其中 key 为服务名，value 为服务提供者 URL： 1com.alibaba.xxx.XxxService=dubbo://localhost:20890 注意为了避免复杂化线上环境，不要在线上使用这个功能，只应在测试阶段使用。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"02负载均衡","slug":"dubbo/2.7/02负载均衡","date":"2021-11-14T03:00:03.000Z","updated":"2022-03-23T09:33:52.078Z","comments":true,"path":"blog/dubbo/2.7/02负载均衡/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/02%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/","excerpt":"","text":"在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 random 随机调用。 负载均衡策略Random LoadBalance 随机，按权重设置随机概率。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 RoundRobin LoadBalance 轮询，按公约后的权重设置轮询比率。 存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 LeastActive LoadBalance 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。 使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 ConsistentHash LoadBalance 一致性 Hash，相同参数的请求总是发到同一提供者。 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 算法参见：http://en.wikipedia.org/wiki/Consistent_hashing 缺省只对第一个参数 Hash，如果要修改，请配置 &lt;dubbo:parameter key=&quot;hash.arguments&quot; value=&quot;0,1&quot; /&gt; 缺省用 160 份虚拟节点，如果要修改，请配置 &lt;dubbo:parameter key=&quot;hash.nodes&quot; value=&quot;320&quot; /&gt; 配置服务端服务级别1&lt;dubbo:service interface=&quot;...&quot; loadbalance=&quot;roundrobin&quot; /&gt; 客户端服务级别1&lt;dubbo:reference interface=&quot;...&quot; loadbalance=&quot;roundrobin&quot; /&gt; 服务端方法级别123&lt;dubbo:service interface=&quot;...&quot;&gt; &lt;dubbo:method name=&quot;...&quot; loadbalance=&quot;roundrobin&quot;/&gt;&lt;/dubbo:service&gt; 客户端方法级别123&lt;dubbo:reference interface=&quot;...&quot;&gt; &lt;dubbo:method name=&quot;...&quot; loadbalance=&quot;roundrobin&quot;/&gt;&lt;/dubbo:reference&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"01集群容错","slug":"dubbo/2.7/01集群容错","date":"2021-11-14T03:00:02.000Z","updated":"2022-03-23T09:33:52.076Z","comments":true,"path":"blog/dubbo/2.7/01集群容错/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/01%E9%9B%86%E7%BE%A4%E5%AE%B9%E9%94%99/","excerpt":"","text":"在集群调用失败时，Dubbo 提供了多种容错方案，缺省为 failover 重试。 各节点关系： 这里的 Invoker 是 Provider 的一个可调用 Service 的抽象，Invoker 封装了 Provider 地址及 Service 接口信息 Directory 代表多个 Invoker，可以把它看成 List&lt;Invoker&gt; ，但与 List 不同的是，它的值可能是动态变化的，比如注册中心推送变更 Cluster 将 Directory 中的多个 Invoker 伪装成一个 Invoker，对上层透明，伪装过程包含了容错逻辑，调用失败后，重试另一个 Router 负责从多个 Invoker 中按路由规则选出子集，比如读写分离，应用隔离等 LoadBalance 负责从多个 Invoker 中选出具体的一个用于本次调用，选的过程包含了负载均衡算法，调用失败后，需要重选 集群模式配置按照以下示例在服务提供方和消费方配置集群模式 1&lt;dubbo:service cluster=&quot;failsafe&quot; /&gt; 或 1&lt;dubbo:reference cluster=&quot;failsafe&quot; /&gt; Failover Cluster失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries=&quot;2&quot; 来设置重试次数(不含第一次)。 1&lt;dubbo:service retries=&quot;2&quot; /&gt; 或 1&lt;dubbo:reference retries=&quot;2&quot; /&gt; 或 123&lt;dubbo:reference&gt; &lt;dubbo:method name=&quot;findFoo&quot; retries=&quot;2&quot; /&gt;&lt;/dubbo:reference&gt; Failfast Cluster快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。 Failsafe Cluster失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。 Failback Cluster失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 Forking Cluster并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=&quot;2&quot; 来设置最大并行数。 Broadcast Cluster广播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息。 现在广播调用中，可以通过 broadcast.fail.percent 配置节点调用失败的比例，当达到这个比例后，BroadcastClusterInvoker 将不再调用其他节点，直接抛出异常。 broadcast.fail.percent 取值在 0～100 范围内。默认情况下当全部调用失败后，才会抛出异常。 broadcast.fail.percent 只是控制的当失败后是否继续调用其他节点，并不改变结果(任意一台报错则报错)。broadcast.fail.percent 参数 在 dubbo2.7.10 及以上版本生效。 Broadcast Cluster 配置 broadcast.fail.percent。 broadcast.fail.percent&#x3D;20 代表了当 20% 的节点调用失败就抛出异常，不再调用其他节点。 1@reference(cluster = &quot;broadcast&quot;, parameters = &#123;&quot;broadcast.fail.percent&quot;, &quot;20&quot;&#125;)","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"00只注册","slug":"dubbo/2.7/00只注册","date":"2021-11-14T03:00:01.000Z","updated":"2022-03-23T09:33:51.084Z","comments":true,"path":"blog/dubbo/2.7/00只注册/","link":"","permalink":"http://sv.pointcut.cc/blog/dubbo/2.7/00%E5%8F%AA%E6%B3%A8%E5%86%8C/","excerpt":"","text":"如果有两个镜像环境，两个注册中心，有一个服务只在其中一个注册中心有部署，另一个注册中心还没来得及部署，而两个注册中心的其它应用都需要依赖此服务。这个时候，可以让服务提供者方只注册服务到另一注册中心，而不从另一注册中心订阅服务。 禁用订阅配置 12&lt;dubbo:registry id=&quot;hzRegistry&quot; address=&quot;10.20.153.10:9090&quot; /&gt;&lt;dubbo:registry id=&quot;qdRegistry&quot; address=&quot;10.20.141.150:9090&quot; subscribe=&quot;false&quot; /&gt; 或者 12&lt;dubbo:registry id=&quot;hzRegistry&quot; address=&quot;10.20.153.10:9090&quot; /&gt;&lt;dubbo:registry id=&quot;qdRegistry&quot; address=&quot;10.20.141.150:9090?subscribe=false&quot; /&gt;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"}]},{"title":"zxid详解","slug":"zookeeper/zxid详解","date":"2021-11-14T02:00:11.000Z","updated":"2022-03-23T09:33:51.083Z","comments":true,"path":"blog/zookeeper/zxid详解/","link":"","permalink":"http://sv.pointcut.cc/blog/zookeeper/zxid%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"在leader对每个事务操作生成Proposal同时都会生成一个ZXID。 在ZAB协议的事务编号ZXID设计中，ZXID是一个64位的数字，其中低32位可以看作是一个简单的单调递增的计数器，针对客户端的每一个事务请求，Leader服务器在产生一个新的事务Proposal的时候，都会对该计数器进行加1操作；而高32位则代表了Leader周期epoch的编号，每当选举产生一个新的Leader服务器，就会从这个Leader服务器上取出其本地日志中最大事务Proposal的ZXID，并从该ZXID中解析出对应的epoch值，然后再对其进行加1操作，之后就会以此编号作为新的epoch，并将低32位置0来开始生成新的ZXID。ZAB协议中的这一通过epoch编号来区分Leader周期变化的策略，能够有效地避免不同的Leader服务器错误地使用相同的ZXID编号提出不一样的事务Proposal的异常情况，这对于识别在Leader崩溃恢复前后生成的Proposal非常有帮助，大大简化和提升了数据恢复流程。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://sv.pointcut.cc/tags/zookeeper/"}]},{"title":"zookeeper详解","slug":"zookeeper/zookeeper详解","date":"2021-11-14T02:00:10.000Z","updated":"2022-03-23T09:33:51.082Z","comments":true,"path":"blog/zookeeper/zookeeper详解/","link":"","permalink":"http://sv.pointcut.cc/blog/zookeeper/zookeeper%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"分布式架构从集中式到分布式所谓的集中式系统就是指由一台或多台主计算机组成中心节点，数据集中存储于这个中心节点中，并且整个系统的所有业务单元都集中部署在这个中心节点上，系统的所有功能均由其集中处理。也就是说，在集中式系统中，每个终端或客户端机器仅仅负责数据的录人和输出，而数据的存储与控制处理完全交由主机来完成。 分布式的特点分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统。 分布式架构都会有如下几个特征： 分布性分布式系统中的多台计算机都会在空间上随意分布，同时，机器的分布情况也会随时变动。 对等性 分布式系统中的计算机没有主&#x2F;从之分，既没有控制整个系统的主机，也没有被控制的从机，组成分布式系统的所有计算机节点都是对等的。 副本( Replica)是分布式系统最常见的概念之一，指的是分布式系统对数据和服务提供的一种冗余方式。 在常见的分布式系统中，为了对外提供高可用的服务，我们往往会对数据和服务进行副本处理。 数据副本是指在不同的节点上持久化同一份数据，当某一个节点上存储的数据丢失时，可以从副本上读取到该数据，这是解决分布式系统数据丢失问题最为有效的手段。 另一类副本是服务副本，指多个节点提供同样的服务，毎个节点都有能力接收来自外部的请求并进行相应的处理。 并发性 在一个计算机网络中，程序运行过程中的并发性操作是非常常见的行为，例如同一个分布式系统中的多个节点，可能会并发地操作一些共享的资源，诸如数据库或分布式存储等，如何准确并高效地协调分布式并发操作也成为了分布式系统架构与设计中最大的挑战之 缺乏全局时钟 故障总是会发生 组成分布式系统的所有计算机，都有可能发生任何形式的故障。一个被大量工程实践所检验过的黄金定理是：任何在设计阶段考虑到的异常情况，一定会在系统实际运行中发生，并且，在系统实际运行过程中还会遇到很多在设计时未能考虑到的异常故障。所以，除非需求指标允许，在系统设计时不能放过任何异常情况。 分布式环境的各种问题 通信异常从集中式向分布式演变的过程中，必然引入了网络因素，而由于网络本身的不可靠性， 因此也引入了额外的问题。分布式系统需要在各个节点之间进行网络通信，因此每次网络通信都会伴随着网络不可用的风险。 网络分区当网络由于发生异常情况，导致分布式系统中部分节点之间的网络延时不断增大，最终导致组成分布式系统的所有节点中，只有部分节点之间能够进行正常通信，而另一些节点则不能一一我们将这个现象称为网络分区，就是俗称的“脑裂”。当网络分区出现时， 分布式系统会出现局部小集群，在极端情况下，这些局部小集群会独立完成原本需要整个分布式系统オ能完成的功能，包括对数据的事务处理，这就对分布式一致性提出了非常大的挑战。 三态从上面的介绍中，我们已经了解到了在分布式环境下，网络可能会出现各式各样的问题， 因此分布式系统的每一次请求与响应，存在特有的“三态”概念，即成功、失败与超时。 由于网络原因，该请求(消息)并没有被成功地发送到接收方，而是在发送过程就发生了消息丢失现象。 该请求(消息)成功的被接收方接收后，并进行了处理，但是在将响应反馈给发送方的过程中，发生了消息丢失现象。 当出现这样的超时现象时，网络通信的发起方是无法确定当前请求是否被成功处理的。 节点故障节点故障则是分布式环境下另一个比较常见的问题，指的是组成分布式系统的服务器节点出现的宕机或“僵死”现象。 从ACID 到 CAP&#x2F;BASEACID事务（Transaction）是由一系列对系统中数据进行访问与更新的操作所组成的一个程序执行逻辑单元（Unit），狭义上的事务特指数据库事务。一方面，当多个应用程序并发访问数据库时，事务可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。另一方面，事务为数据库操作序列提供了一个从失败中恢复到正常状态的方法，同时数据库即使在宕机了数据仍然存在。事务具有四个特征，分别是原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability），简称为事务的ACID特性。其中，原子性（Atomicity）、隔离性（Isolation）和持久性（Durability）是因，一致性（Consistency）是果。AID都是为了保证C而采用的方法。 原子性 事务的原子性是指事务必须是一个原子的操作序列单元。事务中包含的各项操作在一次执行过程中，只允许出现以下两种状态之一。· 全部成功执行。· 全部不执行。任何一项操作失败都将导致整个事务失败，同时其他已经被执行的操作都将被撤销并回滚，只有所有的操作全部成功，整个事务才算是成功完成。 一致性 事务的一致性是指事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行之前和执行之后，数据库都必须处于一致性状态。也就是说，事务执行的结果必须是使数据库从一个一致性状态转变到另一个一致性状态，因此当数据库只包含成功事务提交的结果时，就能说数据库处于一致性状态。而如果数据库系统在运行过程中发生故障，有些事务尚未完成就被迫中断，这些未完成的事务对数据库所做的修改有一部分已写入物理数据库，这时数据库就处于一种不正确的状态，或者说是不一致的状态。 隔离性 事务的隔离性是指在并发环境中，并发的事务是相互隔离的，一个事务的执行不能被其他事务干扰。也就是说，不同的事务并发操纵相同的数据时，每个事务都有各自完整的数据空间，即一个事务内部的操作及使用的数据对其他并发事务是隔离的，并发执行的各个事务之间不能互相干扰。 在标准 SQL 规范中，定义了 4 个事务隔离级别，不同的隔离级别对事务的处理不同，如包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 隔离级别 脏读 不可重复读 幻读 READ UNCOMMITTED Possible Possible Possible READ COMMITTED Not Possible Possible Possible REPEATABLE READ Not Possible Not Possible Possible SERIALIZABLE Not Possible Not Possible Not Possible 持久性 事务的持久性也被称为永久性，是指一个事务一旦提交，它对数据库中对应数据的状态变更就应该是永久性的。换句话说，一旦某个事务成功结束，那么它对数据库所做的更新就必须被永久保存下来——即使发生系统崩溃或机器宕机等故障，只要数据库能够重新启动，那么一定能够将其恢复到事务成功结束时的状态。 CAP定理CAP理论是分布式计算领域的公认定理。 CAP理论告诉我们，一个分布式系统不可能同时满足一致性(C： Consistency)、可用性(A： Availability)和分区容错性(P： Partition tolerance)这三个基本需求，最多只能同时满足其中的两项。 一致性(C： Consistency) 在分布式环境中，一致性是指数据在多个副本之间是否能够保持一致的特性。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。 对于一个将数据副本分布在不同分布式节点上的系统来说，如果对第一个节点的数据进行了更新操作并且更新成功后，却没有使得第二个节点上的数据得到相应的更新，于是在对第二个节点的数据进行读取操作时，获取的依然是老数据（或称为脏数据），这就是典型的分布式数据不一致情况。在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都可以读取到其最新的值，那么这样的系统就被认为具有强一致性（或严格的一致性）。 可用性(A： Availability) 可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。这里我们重点看下“有限的时间内”和“返回结果”。 “有限的时间内”是指，对于用户的一个操作请求，系统必须能够在指定的时间（即响应时间）内返回对应的处理结果，如果超过了这个时间范围，那么系统就被认为是不可用的。另外，“有限的时间内”是一个在系统设计之初就设定好的系统运行指标，通常不同的系统之间会有很大的不同。 “返回结果”是可用性的另一个非常重要的指标，它要求系统在完成对用户请求的处理后，返回一个正常的响应结果。正常的响应结果通常能够明确地反映出对请求的处理结果，即成功或失败 分区容错性(P： Partition tolerance) 分区容错性约束了一个分布式系统需要具有如下特性：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。 网络分区是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络等）中，由于一些特殊的原因导致这些子网络之间出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。需要注意的是，组成一个分布式系统的每个节点的加入与退出都可以看作是一个特殊的网络分区。 既然在上文中我们提到，一个分布式系统无法同时满足上述三个需求，而只能满足其中的两项，因此在进行对CAP定理的应用时，我们就需要抛弃其中的一项. 从CAP定理中我们可以看出，一个分布式系统不可能同时满足一致性、可用性和分区容错性这三个需求。另一方面，需要明确的一点是，对于一个分布式系统而言，分区容错性可以说是一个最基本的要求。为什么这样说，其实很简单，因为既然是一个分布式系统，那么分布式系统中的组件必然需要被部署到不同的节点，否则也就无所谓分布式系统了，因此必然出现子网络。而对于分布式系统而言，网络问题又是一个必定会出现的异常情况，因此分区容错性也就成为了一个分布式系统必然需要面对和解决的问题。因此系统架构设计师往往需要把精力花在如何根据业务特点在C(一致性)和A(可用性)之间寻求平衡 如果我们选择了 CA 而放弃了 P，那么当发生分区现象时，为了保证一致性，这个时候必须拒绝请求，但是 A 又不允许，所以分布式系统理论上 不可能选择 CA 架构，只能选择 CP 或者 AP 架构。 BASE理论BASE是 Basically Available(基本可用)、Soft state(软状态)和 () BASE是对CAP中一致性和可用性权衡的结果，其来源于大规模互联网分布式实践的总结。 其核心思想是即使无法做到强一致性( Strong consistency)，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性( Eventual consistency)。 基本可用（ Basically Available） 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用。以下两个就是“基本可用”的典型例子。 响应时间上的损失：正常情况下，一个在线搜索引擎需要在 0.5 秒之内返回给用户相应的查询结果，但由于出现故障（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1～2秒。 功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 软状态（Soft state） 软状态和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 最终一致性（Eventually consistent） 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 在现代关系型数据库中，大多都会采用同步和异步方式来实现主备数据复制技术。而对于异步模式，数据库还是能够保证最终数据达到一致——这就是系统提供最终一致性保证的经典案例。 总的来说，BASE 理论面向的是大型高可用可扩展的分布式系统，和传统事务的 ACID特性是相反的，它完全不同于ACID的强一致性模型，而是提出通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性与BASE理论往往又会结合在一起使用。 一致性协议2PC2PC，是Two-Phase Commit的缩写，即二阶段提交，是计算机网络尤其是在数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务处理过程中能够&#x3D;&#x3D;保持原子性和一致性而设计的一种算法&#x3D;&#x3D;。通常，二阶段提交协议也被认为是一种一致性协议，用来保证分布式系统数据的一致性。目前，绝大部分的关系型数据库都是采用二阶段提交协议来完成分布式事务处理的（XA协议），利用该协议能够非常方便地完成所有分布式事务参与者的协调，统一决定事务的提交或回滚，从而能够有效地保证分布式数据一致性，因此二阶段提交协议被广泛地应用在许多分布式系统中。 阶段一：提交事务请求 事务询问。 协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应。 执行事务。 各参与者节点执行事务操作，并将Undo和Redo信息记入事务日志中。 各参与者向协调者反馈事务询问的响应。 如果参与者成功执行了事务操作，那么就反馈给协调者 Yes 响应，表示事务可以执行；如果参与者没有成功执行事务，那么就反馈给协调者No响应，表示事务不可以执行。 由于上面讲述的内容在形式上近似是协调者组织各参与者对一次事务操作的投票表态过程，因此二阶段提交协议的阶段一也被称为“投票阶段”，即各参与者投票表明是否要继续执行接下去的事务提交操作。 阶段二：执行事务提交在阶段二中，协调者会根据各参与者的反馈情况来决定最终是否可以进行事务提交操作，正常情况下，包含以下两种可能。 执行事务提交 假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务提交。 发送提交请求 协调者向所有参与者节点发出Commit请求。 事务提交 参与者接收到 Commit 请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的事务资源。 反馈事务提交结果 参与者在完成事务提交之后，向协调者发送Ack消息。 完成事务 协调者接收到所有参与者反馈的Ack消息后，完成事务。 中断事务 假如任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务。 发送回滚请求。 协调者向所有参与者节点发出Rollback请求。 事务回滚。 参与者接收到 Rollback 请求后，会利用其在阶段一中记录的 Undo 信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源。 反馈事务回滚结果。 参与者在完成事务回滚之后，向协调者发送Ack消息。 中断事务。 协调者接收到所有参与者反馈的Ack消息后，完成事务中断。 以上就是二阶段提交过程中，前后两个阶段分别进行的处理逻辑。简单地讲，二阶段提交将一个事务的处理过程分为了投票和执行两个阶段，其核心是对每个事务都采用先尝试后提交的处理方式，因此也可以将二阶段提交看作一个强一致性的算法。 2PC优缺点二阶段提交协议的优点：原理简单，实现方便。二阶段提交协议的缺点：同步阻塞、单点问题、脑裂、太过保守。 同步阻塞 二阶段提交协议存在的最明显也是最大的一个问题就是同步阻塞，这会极大地限制分布式系统的性能。在二阶段提交的执行过程中，所有参与该事务操作的逻辑都处于阻塞状态，也就是说，各个参与者在等待其他参与者响应的过程中，将无法进行其他任何操作。 单点问题 协调者的角色在整个二阶段提交协议中起到了非常重要的作用。一旦协调者出现问题，那么整个二阶段提交流程将无法运转，更为严重的是，如果协调者是在阶段二中出现问题的话，那么其他参与者将会一直处于锁定事务资源的状态中，而无法继续完成事务操作。（具体的实现会添加参与者超时处理） 数据不一致 在二阶段提交协议的阶段二，即执行事务提交的时候，当协调者向所有的参与者发送Commit请求之后，发生了局部网络异常或者是协调者在尚未发送完Commit请求之前自身发生了崩溃，导致最终只有部分参与者收到了Commit请求。于是，这部分收到了Commit请求的参与者就会进行事务的提交，而其他没有收到Commit请求的参与者则无法进行事务提交，于是整个分布式系统便出现了数据不一致性现象。 太过保守 二阶段提交协议没有设计较为完善的容错机制，任意一个节点的失败都会导致整个事务的失败。 3PC3PC，是 Three-Phase Commit的缩写，即三阶段提交，是2PC的改进版，其将二阶段提交协议的“提交事务请求（阶段一）”过程一分为二，形成了由 CanCommit、Precommit和 doCommit 三个阶段组成的事务处理协议 与两阶段提交不同的是，三阶段提交有两个改动点： 1、引入超时机制。同时在协调者和参与者中都引入超时机制。 2、在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。 阶段一：CanCommit 事务询问。 协调者向所有的参与者发送一个包含事务内容的 canCommit请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应。 各参与者向协调者反馈事务询问的响应。 参与者在接收到来自协调者的 canCommit请求后，正常情况下，如果其自身认为可以顺利执行事务，那么会反馈Yes响应，并进入预备状态，否则反馈No响应。 阶段二：Precommit在阶段二中，协调者会根据各参与者的反馈情况来决定是否可以进行事务的PreCommit操作，正常情况下，包含两种可能。 执行事务预提交 假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务预提交。 发送预提交请求。 协调者向所有参与者节点发出preCommit的请求，并进入Prepared阶段。 事务预提交。 参与者接收到preCommit请求后，会执行事务操作，并将Undo和Redo信息记录到事务日志中。 各参与者向协调者反馈事务执行的响应。 如果参与者成功执行了事务操作，那么就会反馈给协调者Ack响应，同时等待最终的指令：提交（commit）或中止（abort）。 中断事务 假如任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务。 发送中断请求。 协调者向所有参与者节点发出abort请求。 中断事务。 无论是收到来自协调者的abort请求，或者是在等待协调者请求过程中出现超时，参与者都会中断事务。 阶段三：doCommit和2PC的阶段二一样。但是对2PC的第二阶段做了优化。在2PC中的第二阶段中，如果协调者出现宕机了，参与者会一直等待下去。而在3PC中，无论协调者出现问题，还是协调者和参与者之间的网络出现故障，最终都会导致参与者无法及时接收到来自协调者的doCommit或是abort请求，针对这样的异常情况，参与者都会在等待超时之后，继续进行事务提交。 优缺点三阶段提交协议的优点：相较于二阶段提交协议，三阶段提交协议最大的优点就是降低了参与者的阻塞范围，并且能够在出现单点故障后继续达成一致。（参与者超时就回滚） 三阶段提交协议的缺点：三阶段提交协议在去除阻塞的同时也引入了新的问题，那就是在参与者接收到preCommit消息后，如果网络出现分区，此时协调者所在的节点和参与者无法进行正常的网络通信，在这种情况下，该参与者依然会进行事务的提交，这必然出现数据的不一致性（部分节点由于无法及时接收到来自协调者的doCommit或是abort请求，而后会继续执行事务提交；部分节点接收到了调者的abort请求，而事务回滚）。 Paxos算法Paxos算法是一种基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一。 之前已经提到，在常见的分布式系统中，总会发生诸如机器宕机或网络异常等情况。Paxos算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。 在该一致性算法中，有三种参与角色，我们用 Proposer、Acceptor 和 Learner 来表示。而且提案是由编号和 Value 组成的组合体，因此我们以“[编号，Value]”来表示一个提案。 算法陈述 阶段一 Proposer选择一个提案编号Mn，然后向Acceptor的某个超过半数的子集成员发送编号为Mn的Prepare请求。 如果一个Acceptor收到一个编号为Mn的Prepare请求，且编号Mn大于该Acceptor已经响应的所有 Prepare 请求的编号，那么它就会将它已经批准过的最大编号的提案作为响应反馈给Proposer，同时该Acceptor会承诺不会再批准任何编号小于Mn的提案 举个例子来说，假定一个Acceptor已经响应过的所有Prepare请求对应的提案编号分别为 1、2、…、5和 7，那么该Acceptor在接收到一个编号为 8的 Prepare请求后，就会将编号为7的提案作为响应反馈给Proposer。 阶段二 如果Proposer收到来自半数以上的Acceptor对于其发出的编号为Mn的Prepare请求的响应，那么它就会发送一个针对[Mn，Vn]提案的Accept请求给Acceptor。注意，Vn的值就是收到的响应中编号最大的提案的值，如果响应中不包含任何提案，那么它就是任意值。 如果Acceptor收到这个针对[Mn，Vn]提案的Accept请求，只要该Acceptor尚未对编号大于Mn的Prepare请求做出响应，它就可以通过这个提案。最终，Acceptor会将通过的提案通知leader。 当然，在实际运行过程中，每一个 Proposer 都有可能会产生多个提案，但只要每个Proposer都遵循如上所述的算法运行，就一定能够保证算法执行的正确性。值得一提的是，每个Proposer都可以在任意时刻丢弃一个提案，哪怕针对该提案的请求和响应在提案被丢弃后会到达，但根据Paxos算法的一系列规约，依然可以保证其在提案选定上的正确性。事实上，如果某个Proposer已经在试图生成编号更大的提案，那么丢弃一些旧的提案未尝不是一个好的选择。因此，如果一个 Acceptor 因为已经收到过更大编号的Prepare请求而忽略某个编号更小的Prepare或者Accept请求，那么它也应当通知其对应的Proposer，以便该Proposer也能够将该提案进行丢弃。 提案的获取上边就是一个提案的选定，下面我们再来看看如何让 Learner 获取提案，大体可以有以下几种方案。 方案一 Learner 获取一个已经被选定的提案的前提是，该提案已经被半数以上的 Acceptor批准。因此，最简单的做法就是一旦Acceptor批准了一个提案，就将该提案发送给所有的Learner。 很显然，这种做法虽然可以让Learner尽快地获取被选定的提案，但是却需要让每个Acceptor与所有的Learner逐个进行一次通信，通信的次数至少为二者个数的乘积。 方案二 另一种可行的方案是，我们可以让所有的Acceptor将它们对提案的批准情况，统一发送给一个特定的 Learner（下文中我们将这样的 Learner 称为“主 Learner”），在不考拜占庭将军问题的前提下，我们假定Learner之间可以通过消息通信来互相感知提案的选定情况。基于这样的前提，当主Learner被通知一个提案已经被选定时，它会负责通知其他的Learner。 在这种方案中，Acceptor首先会将得到批准的提案发送给主 Learner，再由其同步给其他 Learner，因此较方案一而言，方案二虽然需要多一个步骤才能将提案通知到所有的 Learner，但其通信次数却大大减少了，通常只是 Acceptor 和Learner 的个数总和。但同时，该方案引入了一个新的不稳定因素：主 Learner随时可能出现故障。 方案三 在讲解方案二的时候，我们提到，方案二最大的问题在于主 Learner存在单点问题，即主 Learner随时可能出现故障。因此，对方案二进行改进，可以将主 Learner的范围扩大，即 Acceptor 可以将批准的提案发送给一个特定的 Learner 集合，该集合中的每个 Learner 都可以在一个提案被选定后通知所有其他的 Learner。这个 Learner集合中的 Learner个数越多，可靠性就越好，但同时网络通信的复杂度也就越高。 通过选取主Proposer保证算法的活性假设存在这样一种极端情况，有两个Proposer依次提出了一系列编号递增的议案，但是最终都无法被选定，具体流程如下： Proposer P1提出了一个编号为M1的提案，并完成了上述阶段一的流程。但与此同时，另外一个Proposer P2提出了一个编号为M2（M2＞M1）的提案，同样也完成了阶段一的流程，于是Acceptor已经承诺不再批准编号小于M2的提案了。因此，当P1进入阶段二的时候，其发出的Accept请求将被Acceptor忽略，于是 P1再次进入阶段一并提出了一个编号为 M3（M3＞M2）的提案，而这又导致P2在第二阶段的Accept请求被忽略，以此类推，提案的选定过程将陷入死循环，如图: 为了保证 Paxos 算法流程的可持续性，以避免陷入上述提到的“死循环”，就必须选择一个主Proposer，并规定只有主Proposer才能提出议案。这样一来，只要主Proposer和过半的Acceptor能够正常进行网络通信，那么但凡主Proposer提出一个编号更高的提案，该提案终将会被批准。当然，如果Proposer发现当前算法流程中已经有一个编号更大的提案被提出或正在接受批准，那么它会丢弃当前这个编号较小的提案，并最终能够选出一个编号足够大的提案。因此，如果系统中有足够多的组件（包括 Proposer、Acceptor和其他网络通信组件）能够正常工作，那么通过选择一个主Proposer，整套Paxos算法流程就能够保持活性。 总结我们主要从协议设计和原理实现角度详细讲解了二阶段提交协议、三阶段提交协议和Paxos这三种典型的一致性算法。可以说，这三种一致性协议都是非常优秀的分布式一致性协议，都从不同方面不同程度地解决了分布式数据一致性问题，使用范围都非常广泛。其中二阶段提交协议解决了分布式事务的原子性问题，保证了分布式事务的多个参与者要么都执行成功，要么都执行失败。但是，在二阶段解决部分分布式事务问题的同时，依然存在一些难以解决的诸如同步阻塞、无限期等待和“脑裂”等问题。三阶段提交协议则是在二阶段提交协议的基础上，添加了PreCommit过程和超时提交事务，从而避免了二阶段提交协议中的无限期等待问题。而Paxos算法引入了“过半”的理念，通俗地讲就是少数服从多数的原则。同时，Paxos 算法支持分布式节点角色之间的轮换，这极大地避免了分布式单点的出现，因此Paxos算法既解决了无限期等待问题，也解决了“脑裂”问题，是目前来说最优秀的分布式一致性协议之一。 zookeeperZooKeeper是一个开放源代码的分布式协调服务，由知名互联网公司雅虎创建，是Google Chubby的开源实现。ZooKeeper的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。 ZooKeeper 是一个典型的分布式数据一致性的解决方案，分布式应用程序可以基于它实现诸如数据发布&#x2F;订阅、负载均衡、命名服务、分布式协调&#x2F;通知、集群管理、Master 选举、分布式锁和分布式队列等功能。ZooKeeper可以保证如下分布式一致性特性。 顺序一致性 从同一个客户端发起的事务请求，最终将会严格地按照其发起顺序被应用到ZooKeeper中去。 原子性 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群所有机器都成功应用了某一个事务，要么都没有应用，一定不会出现集群中部分机器应用了该事务，而另外一部分没有应用的情况。 单一视图（Single System Image） 无论客户端连接的是哪个ZooKeeper服务器，其看到的服务端数据模型都是一致的。 可靠性 一旦服务端成功地应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会被一直保留下来，除非有另一个事务又对其进行了变更。 实时性 通常人们看到实时性的第一反应是，一旦一个事务被成功应用，那么客户端能够立即从服务端上读取到这个事务变更后的最新数据状态。这里需要注意的是，ZooKeeper仅仅保证在一定的时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。 Zookeeper的基本概念集群角色通常在分布式系统中，构成一个集群的每一台机器都有自己的角色，最典型的集群模式就是 Master&#x2F;Slave 模式（主备模式）。在这种模式中，我们把能够处理所有写操作的机器称为Master机器，把所有通过异步复制方式获取最新数据，并提供读服务的机器称为Slave机器。 在ZooKeeper中引入了 Leader、Follower和 Observer三种角色。ZooKeeper集群中的所有机器通过一个Leader选举过程来选定一台被称为“ Leader”的机器， Leader服务器为客户端提供读和写服务。除 Leader外，其他机器就是 Follower和 Observer其中一个角色。 Follower和 Observer只能够提供读服务，唯一的区别在于， Observer机器不参与 Leader选举过程，也不参与写操作的“过半写成功”策略，因此 Observer可以在不影响写性能的情况下提升集群的读性能。 会话( Session)Session是指客户端会话，在讲解会话之前，我们首先来了解一下容户端连接。在Zookeeper中，一个客户端连接是指客户端和服务器之间的一个TCP长连接。ZooKeeper对外的服务端口默认是2181，客户端启动的时候，首先会与服务器建立一个TCP连接， 从第一次连接建立开始，客户端会话的生命周期也开始了，通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向ZooKeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的 Watch事件通知。 Session的 sessionTimeout 值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在 sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。 数据节点( Znode)在ZooKeeper中，“节点”分为两类，第一类同样是指构成集群的机器，我们称之为机器节点；第二类则是指数据模型中的数据单元，我们称之为数据节点 ZNode。 Zookeeper 将所有数据存储在内存中，数据模型是一棵树( Znode Tree)，由斜杠(&#x2F;)进行分割的路径，就是一个 Znode，例如&#x2F;foo&#x2F;pahl。每个 Znode上都会保存自己的数据内容，同时还会保存一系列属性信息 在 Zookeeper中， Znode可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个 Znode被创建了，除非主动进行 Znode的移除操作，否则这个 Znode将一直保存在ZooKeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性： SEQUENTIAL。一旦节点被标记上这个属性，那么在这个节点被创建的时候， ZooKeeper会自动在其节点名后面追加上一个整型数字， 这个整型数字是一个由父节点维护的自增数字。 版本在前面我们已经提到ZooKeeper的每个 Znode上都会存储数据，对应于每个 Znode， ZooKeeper都会为其维护一个叫作Stat的数据结构，Stat中记录了这个 Znode的三个数据版本，分别是 version(当前 Znode的版本)、cversion(当前 Znode子节点的版本) 和 aversion(当前 Znode的ACL版本)。 WatcherWatcher(事件监听器)，是 Zookeeper中的一个很重要的特性。ZooKeeper允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是ZooKeeper实现分布式协调服务的重要特性。 ACLZooKeeper采用ACL（Access Control Lists）策略来进行权限控制，类似于UNIX文件系统的权限控制。ZooKeeper定义了如下5种权限。 CREATE：创建子节点的权限。 READ：获取节点数据和子节点列表的权限。 WRITE：更新节点数据的权限。 DELETE：删除子节点的权限。 ADMIN：设置节点ACL的权限。 ZooKeeper的ZAB协议ZooKeeper 并没有直接采用 Paxos 算法，而是采用了一种被称为 ZAB （ZooKeeper Atomic Broadcast）的一致性协议。ZAB协议是为分布式协调服务ZookKeeper专门设计的一种支持崩溃恢复的原子广播协议。 ZAB协议要求，所有事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被称为 Leader服务器，而余下的其他服务器则成为 Follower 服务器。Leader 服务器负责将一个客户端事务请求转换成一个事务 Proposal（提议），并将该 Proposal 分发给集群中所有的Follower 服务器。之后 Leader 服务器需要等待所有 Follower 服务器的反馈，一旦超过半数的 Follower 服务器进行了正确的反馈后，那么 Leader 就会再次向所有的 Follower服务器分发Commit消息，要求其将前一个Proposal进行提交。而且当Leader出现崩溃或宕机等异常时，需要在Follower中重新选择一个Leader。 协议介绍ZAB协议包括两种基本的模式，分別是崩溃恢复和消息广播。当整个服务框架在启动过程中，或是当 Leader服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB协议就会进入崩溃恢复模式并选举产生新的 Leader服务器。当选举产生了新的Leader服务器，同时集群中已经有过半的机器与该 Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和 Leader服务器的数据状态保持一致。 当集群中已经有过半的 Follower服务器完成了和 Leader服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。当一台同样遵守ZAB协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个 Leader服务器在负责进行消息广播，那么新加入的服务器就会自觉地进入数据恢复模式：找到 Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。 ZooKeeper设计成只允许唯一的一个 Leader服务器来进行事务请求的处理。 Leader服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议；而如果集群中的其他机器接收到客户端的事务请求，那么这些非 Leader服务器会首先将这个事务请求转发给Leader服务器。 当 Leader服务器出现崩溃退出或机器重启，亦或是集群中已经不存在过半的服务器与该Leader服务器保持正常通信时，那么在重新开始新一轮的原子广播事务操作之前，所有进程首先会使用崩溃恢复协议来使彼此达到一个一致的状态，于是整个ZAB流程就会从消息广播模式进入到崩溃恢复模式 一个机器要成为新的 Leader，必须获得过半进程的支持，同时由于每个进程都有可能会崩溃，因此，在ZAB协议运行过程中，前后会出现多个 Leader，并且每个进程也有可能会多次成为 Leader。进入崩溃恢复模式后，只要集群中存在过半的服务器能够彼此进行正常通信，那么就可以产生一个新的 Leader并再次进入消息广播模式。举个例子来说，一个由3台机器组成的ZAB服务，通常由1个Leader、2个Follower服务器组成。某一个时刻，假如其中一个 Follower服务器挂了，整个 ZAB 集群是不会中断服务的，这是因为Leader服务器依然能够获得过半机器（包括Leader自己）的支持。 消息广播ZAB协议的消息广播过程使用的是一个原子广播协议，类似于一个二阶段提交过程。针对客户端的事务请求， Leader服务器会为其生成对应的事务 Proposal，并将其发送给集群中其余所有的机器，然后再分别收集各自的选票，最后进行事务提交，如图4-2所示就是ZAB协议消息广播流程的示意图。 在ZAB协议的二阶段提交过程中，移除了中断逻辑，所有的Follower服务器要么正常反馈 Leader提出的事务 Proposal，要么就抛弃Leader服务器。同时，ZAB协议将二阶段提交中的中断逻辑移除意味着我们可以在过半的 Follower服务器已经反馈Ack之后就开始提交事务 Proposal了，而不需要等待集群中所有的 Follower服务器都反馈响应。当然，在这种简化了的二阶段提交模型下，是无法处理 Leader服务器崩溃退出而带来的数据不一致问题的，因此在ZAB协议中添加了另一个模式，即采用崩溃恢复模式来解决这个问题。另外，整个消息广播协议是基于具有FIFO特性的TCP协议来进行网络通信的，因此能够很容易地保证消息广播过程中消息接收与发送的顺序性。 在整个消息广播过程中， Leader服务器会为每个事务请求生成对应的 Proposal来进行广播，并且在广播事务 Proposal之前， Leader服务器会首先为这个事务 Proposal分配一个全局单调递增的唯一ID，我们称之为事务ID(即ZXID)。由于ZAB协议需要保证每个消息严格的因果关系，因此必须将每一个事务 Proposal按照其ZXID的先后顺序来进行排序与处理。 具体的，在消息广播过程中， Leader服务器会为每一个 Follower服务器都各自分配一个单独的队列，然后将需要广播的事务 Proposal依次放入这些队列中去，并且根据FIFO 策略进行消息发送。毎一个 Follower服务器在接收到这个事务 Proposal之后，都会首先将其以事务日志的形式写入到本地磁盘中去，并且在成功写入后反馈给 Leader服务器个Ack响应。当 Leader服务器接收到超过半数 FollowerI的Ack响应后，就会广播一个Commit消息给所有的 Follower服务器以通知其进行事务提交，同时 Leader自身也会完成对事务的提交，而每一个 Follower服务器在接收到 Commit消息后，也会完成对事务的提交。 崩溃恢复一且 Leader服务器出现崩溃，或者说由于网络原因导致 Leader服务器失去了与过半 Follower的联系，那么就会进入崩溃 ZAB协议需要确保那些已经在 Leader服务器上提交的事务最终被所有服务器都提交 假设一个事务在 Leader 服务器上被提交了，并且已经得到过半 Follower 服务器的Ack反馈，但是在它将Commit消息发送给所有Follower机器之前，Leader服务器挂了 例如：在集群正常运行过程中的某一个时刻，Server1 是 Leader 服务器，其先后广播了消息 P1、P2、C1、P3 和 C2，其中，当Leader服务器将消息C2（C2是Commit Of Proposal2的缩写，即提交事务Proposal2）发出后就立即崩溃退出了。针对这种情况，ZAB协议就需要确保事务Proposal2最终能够在所有的服务器上都被提交成功，否则将出现不一致。 P1 位Proposal1，C1是Commit Of Proposal1 ZAB协议需要确保丢弃那些只在 Leader服务器上被提出的事 相反，如果在崩溃恢复过程中出现一个需要被丢弃的提案，那么在崩溃恢复结束后需要跳过该事务Proposal。 例如： 假设初始的 Leader 服务器 Server1 在提出了一个事务Proposal3 之后就崩溃退出了，从而导致集群中的其他服务器都没有收到这个事务Proposal。于是，当 Server1 恢复过来再次加入到集群中的时候，ZAB 协议需要确保丢弃Proposal3这个事务。 根据上面的问题，Leader选举算法需要能确保提交已经被 Leader提交的事务Proposal，同时丢弃已经被跳过的事务 Proposal。 针对这个要求，如果让 Leader选举算法能够保证新选举出来的 Leader服务器拥有集群中所有机器最高編号(即ZXID最大)的事务 Proposal， 那么就可以保证这个新选举出来的 Leader一定具有所有已经提交的提案。更为重要的是， 如果让具有最高编号事务 Proposal的机器来成为 Leader，就可以省去 Leader服务器检査 Proposal的提交和丢弃工作的这一操作。 数据同步完成 Leader选举之后，在正式开始工作(即接收客户端的事务请求，然后提出新的提案) 之前， Leader 服务器会首先确认事务日志中的所有 Proposal是否都已经被集群中过半的机器提交了，即是否完成数据同步。 Leader服务器需要确保所有的 Follower服务器能够接收到每一条事务 Proposal，并且能够正确地将所有已经提交了的事务 Proposal应用到内存数据库中去。具体的，Leader服务器会为每一个 Follower服务器都准备一个队列，并将那些没有被各 Follower服务器同步的事务以 Proposal消息的形式逐个发送给 Follower服务器，并在每一个 Proposal消息后面紧接着再发送一个 Commit消息，以表示该事务已经被提交。等到 Follower服务器将所有其尚未同步的事务 Proposal都从 Leader服务器上同步过来并成功应用到本地数据库中后， Leader服务器就会将该 Follower服务器加入到真正的可用 Follower列表中， 并开始之后的其他流程。 上面讲到的是正常情况下的数据同步逻辑，下面来看ZAB协议是如何处理那些需要被丢弃的事务 Proposal的。在ZAB协议的事务编号ZXID设计中，ZXID是一个64位的数字，其中低32位可以看作是一个简单的单调递增的计数器，针对客户端的每一个事务请求，Leader服务器在产生一个新的事务 Proposal的时候，都会对该计数器进行加1 操作；而高32位则代表了 Leader周期 epoch的编号，每当选举产生一个新的 Leader服务器，就会从这个 Leader服务器上取出其本地日志中最大事务 Proposal的ZXID，并从该ZXID中解析出对应的 epoch值，然后再对其进行加1操作，之后就会以此编号作为新的 epoch，并将低32位置从0来开始生成新的ZXID。ZAB协议中的这一通过 epoch编号来区分 Leader周期变化的策略，能够有效地避免不同的 Leader服务器错误地使用相同的ZXID编号提出不一样的事务 Proposal的异常情况，这对于识别在 Leader崩溃恢复前后生成的 Proposal非常有帮助，大大简化和提升了数据恢复流程。 基于这样的策略，当一个包含了上一个 Leader周期中尚未提交过的事务 Proposal的服务器启动时，其肯定无法成为 Leader，原因很简单，因为当前集群中一定包含一个Quorum集合，该集合中的机器一定包含了更高 epoch的事务 Proposal，因此这台机器的事务 Proposal肯定不是最高，也就无法成为 Leader了。当这台机器加入到集群中，以Follower角色连接上 Leader服务器之后， Leader服务器会根据自己服务器上最后被提交的 Proposal来和 Follower 服务器的 Proposal进行比对，比对的结果当然是 Leader会要求 Follower进行一个回退操作一一回退到一个确实已经被集群中过半机器提交的最新的事务 Proposal。 ZAB协议的内部原理 下面就从发现、同步和广播这三个阶段展开来讲解ZAB协议的内部原理。 阶段一：发现阶段一主要就是Leader选举过程，用于在多个分布式进程中选举出主进程，准Leader L和Follower F的工作流程分别如下。 步骤F.1.1 Follower F将自己最后接受的事务Proposal的epoch值CEPOCH（F.p）发送给准Leader L。 步骤L.1.1 当接收到来自过半 Follower 的 CEPOCH（F.p）消息后，准 Leader L 会生成NEWEPOCH（e′）消息给这些过半的Follower。 关于这个epoch值e′，准Leader L会从所有接收到的CEPOCH（F.p）消息中选取出最大的epoch值，然后对其进行加1操作，即为e′。 步骤F.1.2 当Follower接收到来自准Leader L的NEWEPOCH（e′）消息后，如果其检测到当前的CEPOCH（F.p）值小于e′，那么就会将CEPOCH（F.p）赋值为e′，同时向这个准Leader L反馈Ack消息。在这个反馈消息（ACK-E（F.p，hf））中，包含了当前该Follower的epoch CEPOCH（F.p），以及该Follower的历史事务Proposal集合：hf。 当Leader L接收到来自过半Follower的确认消息Ack之后，Leader L就会从这过半服务器中选取出一个Follower F，并使用其作为初始化事务集合Ie’。 关于这个Follower F的选取，对于Quorum中其他任意一个Follower F′，F需要满足以下两个条件中的一个： 也就是epoch值最大的，或者zxid最大的 这一阶段就是选举阶段，从集群中选择一个Leader出来。 阶段二：同步在完成发现流程之后，就进入了同步阶段。在这一阶段中，Leader L和Follower F的工作流程分别如下。 在完成发现流程之后，就进入了同步阶段。在这一阶段中，Leader L和Follower F的工作流程分别如下。 步骤L.2.1 Leader L会将e’和Ie’以NEWLEADER（e’，Ie’）消息的形式发送给所有Quorum中的Follower。 步骤F.2.1 当 Follower 接收到来自 Leader L 的 NEWLEADER（e’，Ie’）消息后，如果Follower 发现 CEPOCH （F.p） ≠ e’，那么直接进入下一轮循环，因为此时Follower发现自己还在上一轮，或者更上轮，无法参与本轮的同步。（这时，该机器就需要执行阶段1，而由于此时集群中已经存在了一个Leader，那么该机器最后会发现 CEPOCH （F.p） &lt; e’，所以该机器会作为Follower，加入到集群中，接着才会进入到阶段二） 如果CEPOCH （F.p）&#x3D;e’，那么Follower就会执行事务应用操作。具体的，对于每一个事务 Proposal：＜v，z＞∈Ie′，Follower 都会接受＜e’，＜v，z＞＞。最后，Follower会反馈给Leader，表明自己已经接受并处理了所有Ie’中的事务Proposal。 步骤L.2.2 当Leader 接收到来自过半Follower 针对NEWLEADER（e’，Ie’）的反馈消息后，就会向所有的Follower发送Commit消息。至此Leader完成阶段二。 步骤F.2.2 当Follower收到来自Leader的Commit消息后，就会依次处理并提交所有在Ie′’中未处理的事务。至此Follower完成阶段二。 阶段三：广播完成同步阶段之后，ZAB协议就可以正式开始接收客户端新的事务请求，并进行消息广播流程。 步骤L.3.1 Leader L接收到客户端新的事务请求后，会生成对应的事务Proposal，并根据ZXID的顺序向所有Follower发送提案＜e’，＜v，z＞＞，其中epoch（z）&#x3D;e′。 步骤F.3.1 Follower根据消息接收的先后次序来处理这些来自Leader的事务Proposal，并将他们追加到hf中去，之后再反馈给Leader。 步骤L.3.1 当 Leader接收到来自过半 Follower针对事务 Proposal＜e’，＜v，z＞＞的 Ack消息后，就会发送Commit＜e’，＜v，z＞＞消息给所有的Follower，要求它们进行事务的提交。 步骤F.3.2 当Follower F接收到来自Leader的Commit＜e’，＜v，z＞＞消息后，就会开始提交事务Proposal＜e’，＜v，z＞＞。需要注意的是，此时该Follower F必定已经提交了事务Proposal＜v’，z’＞，其中＜v’，z’＞ 中的z’&lt; z。 总结 CEPOCH：Follower进程向准Leader发送自己处理过的最后一个事务Proposal的epoch值。NEWEPOCH：准Leader进程根据接收的各进程的epoch，来生成新一轮周期的epoch值。ACK-E：Follower进程反馈准Leader进程发来的NEWEPOCH消息。NEWLEADER：准Leader进程确立自己的领导地位，并发送NEWLEADER消息给各进程。ACK-LD：Follower进程反馈Leader进程发来的NEWLEADER消息。COMMIT-LD：要求Follower进程提交相应的历史事务Proposal。PROPOSE：Leader进程生成一个针对客户端事务请求的Proposal。ACK：Follower进程反馈Leader进程发来的PROPOSAL消息。COMMIT：Leader发送COMMIT消息，要求所有进程提交事务PROPOSE。 运行分析再ZAB协议中，每个进程都有可能处于下面三种状态之一: LOOKING：Leader选举阶段 FOLLOWING：Follower服务器和Leader保持同步状态 LEADING：Leader服务器作为主进程领导状态 组成ZAB协议的所有进程启动的时候，其初始化状态都是LOOKING状态，此时进程组中不存在Leader。所有处于这种状态的进程，都会试图去选举出一个新的Leader。随后，如果进程发现已经选举出新的Leader了，那么它就会马上切换到FOLLOWING状态，并开始和Leader保持同步。这里，我们将处于FOLLOWING状态的进程称为Follower，将处于LEADING状态的进程称为Leader。考虑到Leader进程随时会挂掉，当检测出Leader已经崩溃或者是放弃了领导地位时，其余的Follower进程就会转换到LOOKING状态，并开始进行新一轮的Leader选举。因此在ZAB协议运行过程中，毎个进程都会在LEADING、FOLLOWING和LOOKING状态之间不断地转换。 Leader的选举过程发生在前面两个阶段。 完成Leader选举以及数据同步之后，ZAB协议就进入了原子广播阶段。在这一阶段中，Leader会以队列的形式为每一个与自己保持同步的Follower创建一个操作队列。同一时刻，一个Follower只能和一个Leader保持同步，Leader进程与所有的Follower进程之间都通过心跳检测机制来感知彼此的情况。如果Leader能够在超时时间内正常收到心跳检测，那么Follower就会一直与该Leader保持连接。而如果在指定的超时时间内Leader无法从过半的 Follower 进程那里接收到心跳检测，或者是 TCP 连接本身断开了，那么Leader就会终止对当前周期的领导，并转换到LOOKING状态，所有的Follower也会选择放弃这个 Leader，同时转换到 LOOKING 状态。之后，所有进程就会开始新一轮的Leader选举，并在选举产生新的Leader之后开始新一轮的主进程周期。 ZAB与Paxos算法的联系与区别ZAB协议并不是Paxos算法的一个典型实现，在讲解ZAB和Paxos之间的区别之前，我们首先来看下两者的联系。 两者都存在一个类似于 Leader 进程的角色，由其负责协调多个 Follower 进程的运行。 Leader进程都会等待超过半数的Follower做出正确的反馈后，才会将一个提案进行提交。 在 ZAB 协议中，每个 Proposal 中都包含了一个 epoch 值，用来代表当前的 Leader周期，在Paxos算法中，同样存在这样的一个标识，只是名字变成了Ballot。 在Paxos算法中，一个新选举产生的主进程会进行两个阶段的工作。第一阶段被称为读阶段，在这个阶段中，这个新的主进程会通过和所有其他进程进行通信的方式来收集上一个主进程提出的提案，并将它们提交。第二阶段被称为写阶段，在这个阶段，当前主进程开始提出它自己的提案。在Paxos算法设计的基础上，ZAB协议额外添加了一个同步阶段。在同步阶段之前，ZAB协议也存在一个和Paxos算法中的读阶段非常类似的过程，称为发现（Discovery）阶段。在同步阶段中，新的Leader会确保存在过半的Follower已经提交了之前Leader周期中的所有事务Proposal。这一同步阶段的引入，能够有效地保证 Leader 在新的周期中提出事务 Proposal 之前，所有的进程都已经完成了对之前所有事务 Proposal 的提交。一旦完成同步阶段后，那么 ZAB 就会执行和 Paxos 算法类似的写阶段。 总的来讲，ZAB 协议和 Paxos 算法的本质区别在于，两者的设计目标不太一样。ZAB协议主要用于构建一个高可用的分布式数据主备系统，例如 ZooKeeper，而 Paxos 算法则是用于构建一个分布式的一致性状态机系统。 zookeeper的数据模型ZooKeeper的视图结构和标准的Unix文件系统类似，其中每个节点称为“数据节点”或ZNode，每个znode可以存储数据，还可以挂载子节点，因此可以称之为“树” 每一个znode都必须有值，如果没有值，节点是不能创建成功的。 Zookeeper节点类型 持久节点（PERSISTENT ）：是指一旦这个 Znode被创建了，除非主动进行 Znode的移除操作，否则这个 Znode将一直保存在ZooKeeper上。 临时节点（EPHEMERAL ）：它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。 另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性： SEQUENTIAL。一旦节点被标记上这个属性，那么在这个节点被创建的时候， ZooKeeper会自动在其节点名后面追加上一个整型数字， 这个整型数字是一个由父节点维护的自增数字。 所以结点类型有四种形式： PERSISTENT PERSISTENT_SEQUENTIAL（持久序列&#x2F;test0000000019 ） EPHEMERAL EPHEMERAL_SEQUENTIAL 在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序 zookeeper集群及配置集群启动启动前提：安装了对应的jdk版本，并且配置了JAVA_HOME环境变量在每个zookeeper配置文件中添加修改${zookeeper目录}&#x2F;conf&#x2F;zoo.cfg文件， 123server.0=192.168.212.154:2888:3888server.1=192.168.212.156:2888:3888server.2=192.168.212.157:2888:3888 启动${zookeeper目录}&#x2F;bin&#x2F;zkServer.sh start 配置说明 zookeeper服务器和客户端常用命令使用服务器启动ZK服务: sh bin&#x2F;zkServer.sh startZK服务状态: sh bin&#x2F;zkServer.sh status停止ZK服务: sh bin&#x2F;zkServer.sh stop重启ZK服务: sh bin&#x2F;zkServer.sh restart 客户端常用命令创建节点 createcreate [-s] [-e] [-c] [-t ttl] path [data] [acl] -s 创建有序节点 如果在创建znode时，我们使用排序标志的话，ZooKeeper会在我们指定的 znode 名字后面增加一个数字。我们继续加入相同名字的znode时，这个数字会不断增加。这个序号的计数器是由这些排序znode的父节点来维护的。 -e 创建临时节点 znode有两种类型：ephemeral 和 persistent。在创建znode时，我们指定znode的类型，并且在之后不会再被修改。当创建znode的客户端的session结束后，ephemeral类型的znode将被删除。persistent类型的znode在创建以后，就与客户端没什么联系了，除非主动去删除它，否则他会一直存在。Ephemeral znode没有任何子节点。 acl 在下面的《 ACL 操作》中详细介绍。 使用方法： 普通节点 123456789[zk: localhost:2181(CONNECTED) 3] create /mynode helloCreated /mynode[zk: localhost:2181(CONNECTED) 4] create /mynode/subnode worldCreated /mynode/subnode[zk: localhost:2181(CONNECTED) 9] get /mynodehello[zk: localhost:2181(CONNECTED) 10] get /mynode/subnodeworld 有序节点 1234[zk: localhost:2181(CONNECTED) 4] create -s /mynode helloCreated /mynode0000000004[zk: localhost:2181(CONNECTED) 6] create -s /mynode worldCreated /mynode0000000005 临时节点 12[zk: localhost:2181(CONNECTED) 7] create -e /temp helloCreated /temp 退出 zkCli，然后再重新打开它，&#x2F;temp 节点已经被删除了。 列出节点 lsls [-s] [-w] [-R] path -w 添加一个 watch（监视器），如果该节点发生变化，watch 可以使客户端得到通知。watch 只能被触发一次。如果要一直获得 znode 的创建和删除的通知，那么就需要不断的在znode上开启观察模式。如果在该 path 下节点发生变化，会产生 NodeChildrenChanged 事件，删除节点，会产生 NodeDeleted 事件。 使用方法： 123456789101112131415161718[zk: localhost:2181(CONNECTED) 12] ls /[mynode, mynode0000000003, mynode0000000004, test, zookeeper][zk: localhost:2181(CONNECTED) 13] ls -s /[mynode, mynode0000000003, mynode0000000004, test, zookeeper]cZxid = 0x0ctime = Thu Jan 01 08:00:00 CST 1970mZxid = 0x0mtime = Thu Jan 01 08:00:00 CST 1970pZxid = 0x300000053cversion = 7dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 5[zk: localhost:2181(CONNECTED) 2] ls /mynode[subnode] 使用 -w 查看 &#x2F;mynode 节点，然后在它下面添加（删除）子节点，就会触发该 watch。在其他节点下创建子节点，不会触发该 watch。 12345678910111213141516171819202122232425262728[zk: localhost:2181(CONNECTED) 20] ls -w /mynode[subnode][zk: localhost:2181(CONNECTED) 21] create /mynode/subnode2WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/mynodeCreated /mynode/subnode2# 监听父节点，删除子节点，产生 NodeChildrenChanged事件[zk: localhost:2181(CONNECTED) 22] ls -w /mynode[subnode, subnode2][zk: localhost:2181(CONNECTED) 23] delete /mynode/subnode2WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/mynode# 监听子节点，删除子节点，产生 NodeDeleted 事件[zk: localhost:2181(CONNECTED) 51] create /mynode/subnode2Created /mynode/subnode2[zk: localhost:2181(CONNECTED) 52] ls -w /mynode/subnode2[][zk: localhost:2181(CONNECTED) 53] delete /mynode/subnode2WATCHER::WatchedEvent state:SyncConnected type:NodeDeleted path:/mynode/subnode2 从上面的操作可以看到，在 &#x2F;mynode 下添加了 subnode2 节点之后，触发了 watch，WatchedEvent 的类型是 NodeChildrenChanged。之后再删除 subnode2 节点，也出发了 watch。 获取节点信息 getget [-s] [-w] path -w 添加一个 watch（监视器），如果节点内容发生改变，会产生 NodeDataChanged 事件；如果删除节点，会产生 NodeDeleted 事件。 使用方法 123456789101112131415161718192021222324252627282930313233[zk: localhost:2181(CONNECTED) 33] get /mynodehelloo[zk: localhost:2181(CONNECTED) 34] get -s /mynodehelloocZxid = 0x30000004cctime = Sun Apr 05 15:48:14 CST 2020mZxid = 0x30000005dmtime = Sun Apr 05 16:05:56 CST 2020pZxid = 0x30000005ccversion = 7dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 6numChildren = 1# 监听节点内容，发生变化后，产生 NodeDataChanged 事件[zk: localhost:2181(CONNECTED) 35] get -w /mynodehelloo[zk: localhost:2181(CONNECTED) 36] set /mynode helloWATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/mynode# 删除节点，产生 NodeDeleted 事件[zk: localhost:2181(CONNECTED) 43] get -w /mynode/subnodeworld[zk: localhost:2181(CONNECTED) 44] delete /mynode/subnodeWATCHER::WatchedEvent state:SyncConnected type:NodeDeleted path:/mynode/subnode 每一个对znode树的更新操作，都会被赋予一个全局唯一的ID，我们称之为zxid（ZooKeeper Transaction ID）。更新操作的ID按照发生的时间顺序升序排序。例如，z1大于z2，那么z1的操作就早于z2操作。 每个 znode 的状态信息包含以下内容： czxid，创建（create）该 znode 的 zxid mzxid，最后一次修改（modify）该 znode 的 zxid pzxid，最后一次修改该 znode 子节点的 zxid ctime，创建该 znode 的时间 mtime，最后一次修改该 znode 的时间 dataVersion，该节点内容的版本，每次修改内容，版本都会增加 cversion，该节点子节点的版本 aclVersion，该节点的 ACL 版本 ephemeralOwner，如果该节点是临时节点（ephemeral node），会列出该节点所在客户端的 session id；如果不是临时节点，该值为 0 dataLength，该节点存储的数据长度 numChildren，该节点子节点的个数 检查状态 statstat [-w] path -w 添加一个 watch（监视器），如果节点内容发生改变，会产生 NodeDataChanged 事件；如果删除节点，会产生 NodeDeleted 事件。 与 get 的区别是，不回列出 znode 的值。 使用方法 123456789101112[zk: localhost:2181(CONNECTED) 56] stat /mynodecZxid = 0x30000004cctime = Sun Apr 05 15:48:14 CST 2020mZxid = 0x30000005emtime = Sun Apr 05 16:09:32 CST 2020pZxid = 0x300000067cversion = 16dataVersion = 2aclVersion = 0ephemeralOwner = 0x0dataLength = 5numChildren = 0 修改节点 setset [-s] [-v version] path data 修改已经存在的节点的值 使用方法 123456789101112131415[zk: localhost:2181(CONNECTED) 57] set /mynode hello[zk: localhost:2181(CONNECTED) 58] ls /mynode[][zk: localhost:2181(CONNECTED) 59] stat /mynodecZxid = 0x30000004cctime = Sun Apr 05 15:48:14 CST 2020mZxid = 0x300000068mtime = Sun Apr 05 16:20:34 CST 2020pZxid = 0x300000067cversion = 16dataVersion = 3aclVersion = 0ephemeralOwner = 0x0dataLength = 5numChildren = 0 可以看到，在修改节点值之后，mZxid、mtime、dataVersion 都发生了变化。 删除节点 deletealldeleteall path [-b batch size] 使用方法 1[zk: localhost:2181(CONNECTED) 34] delete /mynode 删除 &#x2F;mynode，不会返回任何内容。如果有子节点的时候，都会删除。 删除节点 deletedelete [-v version] path 调用delete和set操作时，如果指定znode版本号，需要与当前的版本号匹配。如果版本号不匹配，操作将会失败。失败的原因可能是在我们提交之前，该znode已经被修改过了，版本号发生了增量变化。如果不指定版本号，就是直接操作最新版本的 znode。 使用方法 123[zk: localhost:2181(CONNECTED) 15] create /mynode helloCreated /mynode[zk: localhost:2181(CONNECTED) 16] delete /mynode 如果要删除的节点有子节点，不能删除 1234[zk: localhost:2181(CONNECTED) 33] create /mynode/sub subCreated /mynode/sub[zk: localhost:2181(CONNECTED) 34] delete /mynodeNode not empty: /mynode 其他指令历史记录 historyhistory 列出最近的10条历史记录 123456789[zk: localhost:2181(CONNECTED) 7] history0 - history1 - create /mynode hello2 - ls /3 - set /mynode worold4 - get /mynode5 - stat /mynode6 - rmr /mynode7 - history 重复之前的命令 redoredo cmdno 根据 cmdno 重复之前的命令，cmdno 就是方括号里面最后的数字，每次执行命令都会自增。 12345[zk: localhost:2181(CONNECTED) 5] create /mynode helloCreated /mynode[zk: localhost:2181(CONNECTED) 6] rmr /mynode[zk: localhost:2181(CONNECTED) 7] redo 5Created /mynode 是否输出 watch 事件（printwatches）语法 printwatches on|off 使用方法 12345678910[zk: localhost:2181(CONNECTED) 43] printwatchesprintwatches is on[zk: localhost:2181(CONNECTED) 44] ls /mynode 1[sub][zk: localhost:2181(CONNECTED) 45] create /mynode/child childWATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/mynodeCreated /mynode/child 如果设置 printwatches off ，就看不到上面的 WATCHER 事件了。 关闭连接 closeclose 1234[zk: localhost:2181(CONNECTED) 50] close[zk: localhost:2181(CLOSED) 51][zk: localhost:2181(CLOSED) 52] ls /Not connected 打开连接 connectconnect host:port 1234567[zk: localhost:2181(CLOSED) 52] connect[zk: localhost:2181(CONNECTING) 53]WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: localhost:2181(CONNECTED) 53] 指定 host:port 可以连接远程的 zk 服务。缺省的时候，会连接本地的 2181 端口。 退出连接 quitquit 直接退出当前的 zkCli 命令行。 强制同步 syncsync path sync方法会强制客户端所连接的服务器状态与leader的状态同步，这样在读取 path 的值就是最新的值了。 zookeeper的Java客户端使用zookeeper的Java客户端使用","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://sv.pointcut.cc/tags/zookeeper/"}]},{"title":"zookeeper的WatchManager实现","slug":"zookeeper/zookeeper的WatchManager实现","date":"2021-11-14T02:00:09.000Z","updated":"2022-03-23T09:33:32.682Z","comments":true,"path":"blog/zookeeper/zookeeper的WatchManager实现/","link":"","permalink":"http://sv.pointcut.cc/blog/zookeeper/zookeeper%E7%9A%84WatchManager%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"服务端在FinalRequestProcessor（这是处理请求的最后一个Processor）中每次都会对非事务请求判断是否需要watch。这些方法都是对应客户端中带boolean watch参数的方法 如果需要就把ServerCnxn传入WatchManager中 Watchmanager是ZooKeeper服务端Watcher的管理者，其内部管理的watchable和watch2Paths两个存储结构，分别从两个维度对Watcher进行存储 watchable是从数据节点路径的粒度来托管Watcher。 watch2Paths是从Watcher的粒度来控制事件触发需要触发的数据节点。 同时，Watchmanager还带有出发Watcher事件通知的方法triggerWatch。该方法同时会把已经注册在Watchmanager里的对应Watcher移除掉，这也是为什么客户端事件只能监听一次，想再次监听只能在非事务方法上watch参数设置为true的原因 当服务器接受到事务请求后在FinalRequestProcessor都会处理调用processRequest时再调用ZooKeeperServer.processTxn触发对应的watch事件 在最后都调用了Watchmanager.triggerWatch，在triggerWatch获取完对应的时间通知后会最后会调用NIOServerCnxn.process方法，而之所以是NIOServerCnxn因为： 在做了对应的处理后把buffer放入outgoingBuffers中，等待I&#x2F;O线程把请求发送给客户端。 客户端客户端在诸如getData、existe等非事务请求的方法内都有个boolean watch参数，在发起这些命令后客户端sendThread在处理完这些请求的响应最后在finishPacket中会判断并注册Watcher，放入到ZooKeeper.ZKWatchManager中。 服务端在接受了这样的一个请求后，也会注册一个watcher注意，在上述的方法中，每次执行完watcher.materialize后都会把ZooKeeper.ZKWatchManager中注册了的Watcher清除掉，这是为了和服务端保持的逻辑保持一致—-一次watch事件的监听在客户端只会触发一次，如果想再次监听变化，调用getData或者exists等非事务方法时都要把watch参数设置为true。 当客户端发起了一个事务请求后，服务器处理完事务事件后会触发上一次注册的对应的watcher，并把事务的结果和watch通知分开发送给客户端。客户端的在SendThread中接收到watch通知后，并把它放入到ClientCnxn.EventThread的阻塞队列中 回调是在ClientCnxn.EventThread中处理的","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://sv.pointcut.cc/tags/zookeeper/"}]},{"title":"zookeeper的Java客户端使用","slug":"zookeeper/zookeeper的Java客户端使用","date":"2021-11-14T02:00:08.000Z","updated":"2022-03-23T09:33:17.419Z","comments":true,"path":"blog/zookeeper/zookeeper的Java客户端使用/","link":"","permalink":"http://sv.pointcut.cc/blog/zookeeper/zookeeper%E7%9A%84Java%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BD%BF%E7%94%A8/","excerpt":"","text":"原生客户端引入12345&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.12&lt;/version&gt;&lt;/dependency&gt; 使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319public class ZooKeeperWatcher implements Watcher &#123; /** 定义原子变量 */ AtomicInteger seq = new AtomicInteger(); /** 定义session失效时间 */ private static final int SESSION_TIMEOUT = 10000; /** zookeeper服务器地址 */ private static final String CONNECTION_ADDR = &quot;127.0.0.1:2181&quot;; /** zk父路径设置 */ private static final String PARENT_PATH = &quot;/testWatch&quot;; /** zk子路径设置 */ private static final String CHILDREN_PATH = &quot;/testWatch/children&quot;; /** 进入标识 */ private static final String LOG_PREFIX_OF_MAIN = &quot;【Main】&quot;; /** zk变量 */ private ZooKeeper zk = null; /** 信号量设置，用于等待zookeeper连接建立之后 通知阻塞程序继续向下执行 */ private CountDownLatch connectedSemaphore = new CountDownLatch(1); /** * 创建ZK连接 * @param connectAddr ZK服务器地址列表 * @param sessionTimeout Session超时时间 */ public void createConnection(String connectAddr, int sessionTimeout) &#123; this.releaseConnection(); try &#123; zk = new ZooKeeper(connectAddr, sessionTimeout, this); System.out.println(LOG_PREFIX_OF_MAIN + &quot;开始连接ZK服务器&quot;); connectedSemaphore.await(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 关闭ZK连接 */ public void releaseConnection() &#123; if (this.zk != null) &#123; try &#123; this.zk.close(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 创建节点 * @param path 节点路径 * @param data 数据内容 * @return */ public boolean createPath(String path, String data) &#123; try &#123; //设置监控(由于zookeeper的监控都是一次性的所以 每次必须设置监控) this.zk.exists(path, true); System.out.println(LOG_PREFIX_OF_MAIN + &quot;节点创建成功, Path: &quot; + this.zk.create( /**路径*/ path, /**数据*/ data.getBytes(), /**所有可见*/ ZooDefs.Ids.OPEN_ACL_UNSAFE, /**永久存储*/ CreateMode.PERSISTENT ) + &quot;, content: &quot; + data); &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; return true; &#125; /** * 读取指定节点数据内容 * @param path 节点路径 * @return */ public String readData(String path, boolean needWatch) &#123; try &#123; return new String(this.zk.getData(path, needWatch, null)); &#125; catch (Exception e) &#123; e.printStackTrace(); return &quot;&quot;; &#125; &#125; /** * 更新指定节点数据内容 * @param path 节点路径 * @param data 数据内容 * @return */ public boolean writeData(String path, String data) &#123; try &#123; System.out.println(LOG_PREFIX_OF_MAIN + &quot;更新数据成功，path：&quot; + path + &quot;, stat: &quot; + this.zk.setData(path, data.getBytes(), -1)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * 删除指定节点 * * @param path * 节点path */ public void deleteNode(String path) &#123; try &#123; this.zk.delete(path, -1); System.out.println(LOG_PREFIX_OF_MAIN + &quot;删除节点成功，path：&quot; + path); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 判断指定节点是否存在 * @param path 节点路径 */ public Stat exists(String path, boolean needWatch) &#123; try &#123; return this.zk.exists(path, needWatch); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 获取子节点 * @param path 节点路径 */ private List&lt;String&gt; getChildren(String path, boolean needWatch) &#123; try &#123; return this.zk.getChildren(path, needWatch); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 删除所有节点 */ public void deleteAllTestPath() &#123; if(this.exists(CHILDREN_PATH, false) != null)&#123; this.deleteNode(CHILDREN_PATH); &#125; if(this.exists(PARENT_PATH, false) != null)&#123; this.deleteNode(PARENT_PATH); &#125; &#125; /** * 收到来自Server的Watcher通知后的处理。 */ @Override public void process(WatchedEvent event) &#123; System.out.println(&quot;进入 process 。。。。。event = &quot; + event); try &#123; Thread.sleep(200); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if (event == null) &#123; return; &#125; // 连接状态 Watcher.Event.KeeperState keeperState = event.getState(); // 事件类型 Watcher.Event.EventType eventType = event.getType(); // 受影响的path String path = event.getPath(); String logPrefix = &quot;【Watcher-&quot; + this.seq.incrementAndGet() + &quot;】&quot;; System.out.println(logPrefix + &quot;收到Watcher通知&quot;); System.out.println(logPrefix + &quot;连接状态:\\t&quot; + keeperState.toString()); System.out.println(logPrefix + &quot;事件类型:\\t&quot; + eventType.toString()); if (Event.KeeperState.SyncConnected == keeperState) &#123; // 成功连接上ZK服务器 if (Event.EventType.None == eventType) &#123; System.out.println(logPrefix + &quot;成功连接上ZK服务器&quot;); connectedSemaphore.countDown(); &#125; //创建节点 else if (Event.EventType.NodeCreated == eventType) &#123; System.out.println(logPrefix + &quot;节点创建&quot;); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; this.exists(path, true); &#125; //更新节点 else if (Event.EventType.NodeDataChanged == eventType) &#123; System.out.println(logPrefix + &quot;节点数据更新&quot;); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(logPrefix + &quot;数据内容: &quot; + this.readData(PARENT_PATH, true)); &#125; //更新子节点 else if (Event.EventType.NodeChildrenChanged == eventType) &#123; System.out.println(logPrefix + &quot;子节点变更&quot;); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(logPrefix + &quot;子节点列表：&quot; + this.getChildren(PARENT_PATH, true)); &#125; //删除节点 else if (Event.EventType.NodeDeleted == eventType) &#123; System.out.println(logPrefix + &quot;节点 &quot; + path + &quot; 被删除&quot;); &#125; &#125; else if (Watcher.Event.KeeperState.Disconnected == keeperState) &#123; System.out.println(logPrefix + &quot;与ZK服务器断开连接&quot;); &#125; else if (Watcher.Event.KeeperState.AuthFailed == keeperState) &#123; System.out.println(logPrefix + &quot;权限检查失败&quot;); &#125; else if (Watcher.Event.KeeperState.Expired == keeperState) &#123; System.out.println(logPrefix + &quot;会话失效&quot;); &#125; System.out.println(&quot;--------------------------------------------&quot;); &#125; public static void main(String[] args) throws Exception &#123; //建立watcher ZooKeeperWatcher zkWatch = new ZooKeeperWatcher(); //创建连接 zkWatch.createConnection(CONNECTION_ADDR, SESSION_TIMEOUT); //System.out.println(zkWatch.zk.toString()); Thread.sleep(1000); // 清理节点 zkWatch.deleteAllTestPath(); zkWatch.zk.exists(PARENT_PATH, true); zkWatch.zk.create(PARENT_PATH, &quot;1&quot;.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(new String(zkWatch.zk.getData(PARENT_PATH,true, null))); zkWatch.zk.setData(PARENT_PATH, &quot;fewfwe&quot;.getBytes(), -1); zkWatch.zk.exists(CHILDREN_PATH, true); zkWatch.zk.create(CHILDREN_PATH, &quot;v2&quot;.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT); zkWatch.zk.exists(CHILDREN_PATH, true); zkWatch.zk.delete(CHILDREN_PATH, -1); Transaction transaction = zkWatch.zk.transaction();// transaction.commit()// if (zkWatch.createPath(PARENT_PATH, System.currentTimeMillis() + &quot;&quot;)) &#123;//////// // 读取数据，在操作节点数据之前先调用zookeeper的getData()方法是为了可以watch到对节点的操作。watch是一次性的，// // 也就是说，如果第二次又重新调用了setData()方法，在此之前需要重新调用一次。// System.out.println(&quot;---------------------- read parent ----------------------------&quot;);//// // zkWatch.readData(PARENT_PATH, true);// // 更新数据// zkWatch.writeData(PARENT_PATH, System.currentTimeMillis() + &quot;&quot;);////// /** 读取子节点，设置对子节点变化的watch，如果不写该方法，则在创建子节点是只会输出NodeCreated，而不会输出NodeChildrenChanged，// 也就是说创建子节点时没有watch。// 如果是递归的创建子节点，如path=&quot;/p/c1/c2&quot;的话，getChildren(PARENT_PATH, ture)只会在创建c1时watch，输出c1的NodeChildrenChanged，// 而不会输出创建c2时的NodeChildrenChanged，如果watch到c2的NodeChildrenChanged，则需要再调用一次getChildren(String path, true)方法，// 其中path=&quot;/p/c1&quot;// */// System.out.println(&quot;---------------------- read children path ----------------------------&quot;);// zkWatch.getChildren(PARENT_PATH, true);////// Thread.sleep(1000);//// // 创建子节点，同理如果想要watch到NodeChildrenChanged状态，需要调用getChildren(CHILDREN_PATH, true)// zkWatch.createPath(CHILDREN_PATH, System.currentTimeMillis() + &quot;&quot;);//// Thread.sleep(1000);//// zkWatch.readData(CHILDREN_PATH, true);// zkWatch.writeData(CHILDREN_PATH, System.currentTimeMillis() + &quot;&quot;);// &#125; Thread.sleep(20000); // 清理节点 zkWatch.deleteAllTestPath(); Thread.sleep(1000); zkWatch.releaseConnection(); &#125;&#125; ZkClient引入12345&lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.11&lt;/version&gt;&lt;/dependency&gt; 使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546 //ZkClient zkc = new ZkClient(new ZkConnection(CONNECT_ADDR), SESSION_OUTTIME); ZkClient zkc = new ZkClient(CONNECT_ADDR, SESSION_OUTTIME);// zkc.multi(Lists.newArrayList(Op.Create).iterator()) //1. create and delete方法 // 创建临时节点 zkc.createEphemeral(&quot;/temp&quot;); //相当与rmr // 创建持久节点 zkc.createPersistent(&quot;/super/c1&quot;, true); Thread.sleep(10000); zkc.delete(&quot;/temp&quot;); zkc.deleteRecursive(&quot;/super&quot;); //2. 设置path和data 并且读取子节点和每个节点的内容 zkc.createPersistent(&quot;/super&quot;, &quot;1234&quot;); zkc.createPersistent(&quot;/super/c1&quot;, &quot;c1内容&quot;); zkc.createPersistent(&quot;/super/c2&quot;, &quot;c2内容&quot;); List&lt;String&gt; list = zkc.getChildren(&quot;/super&quot;); for(String p : list)&#123; System.out.println(p); String rp = &quot;/super/&quot; + p; String data = zkc.readData(rp); System.out.println(&quot;节点为：&quot; + rp + &quot;，内容为: &quot; + data); &#125; //3. 更新和判断节点是否存在 zkc.writeData(&quot;/super/c1&quot;, &quot;新内容&quot;); System.out.println(zkc.readData(&quot;/super/c1&quot;).toString()); System.out.println(zkc.exists(&quot;/super/c1&quot;));// 4.递归删除/super内容 zkc.deleteRecursive(&quot;/super&quot;); // 监听 zkc.subscribeChildChanges(&quot;/super&quot;, new IZkChildListener() &#123; @Override public void handleChildChange(String parentPath, List&lt;String&gt; currentChilds) throws Exception &#123; System.out.println(&quot;parentPath: &quot; + parentPath); System.out.println(&quot;currentChilds: &quot; + currentChilds); &#125; &#125;); Curator123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.11.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.11.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-test&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138public class CuratorUtil &#123; private static String connectStr = &quot;192.168.2.148:2181&quot;; private static TestingServer server = null; static &#123; try &#123; server = new TestingServer(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; private static class InnerClass &#123; private static CuratorFramework client = CuratorFrameworkFactory .builder() .connectString(connectStr) .sessionTimeoutMs(5000) .retryPolicy(new ExponentialBackoffRetry(1000,3)) .build(); &#125; public static CuratorFramework getInstance() &#123; return InnerClass.client; &#125;&#125;public class CuratorDemo &#123; public static String node = &quot;/curator/cn.enjoy.jack&quot;; public static void main(String[] args) &#123; CuratorFramework client = CuratorUtil.getInstance(); //启动连接 client.start();// create(client);// query(client);// update(client); createAsync(client);// transation(client); &#125; private static void transation(CuratorFramework client) &#123; try &#123; Collection&lt;CuratorTransactionResult&gt; results = client.inTransaction() .create() .withMode(CreateMode.EPHEMERAL) .forPath(&quot;/transation&quot;, &quot;ww&quot;.getBytes()) .and() .setData() .forPath(&quot;/transation&quot;, &quot;ww-modify&quot;.getBytes()) .and() .commit(); for (CuratorTransactionResult result : results) &#123; OperationType type = result.getType(); System.out.println(type.name()); System.out.println(result.getForPath() + &quot;==&quot; + result.getResultPath() + &quot;==&quot; + result.getResultStat()); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if(client != null) &#123; client.close(); &#125; &#125; &#125; private static void createAsync(CuratorFramework client) &#123; try &#123; client.create().creatingParentsIfNeeded() .withMode(CreateMode.EPHEMERAL) .inBackground(new BackgroundCallback() &#123; @Override public void processResult(CuratorFramework client, CuratorEvent event) throws Exception &#123; System.out.println(event.getName() + &quot;:&quot; + event.getPath()); &#125; &#125;).forPath(&quot;/createsync&quot;,&quot;sync&quot;.getBytes()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; private static void delete(CuratorFramework client) &#123; try &#123; Void aVoid = client.delete().deletingChildrenIfNeeded().withVersion(-1).forPath(node); System.out.println(&quot;delete--&gt;&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if(client != null) &#123; client.close(); &#125; &#125; &#125; private static void update(CuratorFramework client) &#123; try &#123; Stat stat = client.setData().withVersion(-1).forPath(node, &quot;rr&quot;.getBytes()); System.out.println(&quot;update--&gt;&quot; + stat); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if(client != null) &#123; client.close(); &#125; &#125; &#125; private static void query(CuratorFramework client) &#123; try &#123; byte[] bytes = client.getData().storingStatIn(new Stat()).forPath(node); System.out.println(&quot;query--&gt;&quot; + new String(bytes)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if(client != null) &#123; client.close(); &#125; &#125; &#125; private static void create(CuratorFramework client) &#123; try &#123; String path = client.create() .creatingParentsIfNeeded() .withMode(CreateMode.PERSISTENT) .forPath(node, &quot;123&quot;.getBytes()); System.out.println(path); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://sv.pointcut.cc/tags/zookeeper/"}]},{"title":"zookeeper技术内幕","slug":"zookeeper/zookeeper技术内幕","date":"2021-11-14T02:00:07.000Z","updated":"2022-03-23T09:33:17.418Z","comments":true,"path":"blog/zookeeper/zookeeper技术内幕/","link":"","permalink":"http://sv.pointcut.cc/blog/zookeeper/zookeeper%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95/","excerpt":"","text":"入口QuorumPeerMain.main 集群启动启动时，启动了两个线程ZooKeeperThread(Runable是NIOServerCnxnFactory)，QuorumPeer ZooKeeperThread(Runable是NIOServerCnxnFactory)在单机中已经说明了，线程用来处理写和读的网络请求，把请求成request，然后放入PrepRequestProcessor的队列中的，并把响应数据和watch通知发送给客户端。 QuorumPeer线程会接收到leader上发送过来的数据包（提案投票或事务提交等），然后根据角色作出对应的操作一开始的时候，机器处于LOOKING状态，要开始leader选举 leader选举zookeeper选举算法只有FastleaderElection这个实现类了。术语解析 SID：服务器IDSID是一个数字，用来唯一标识一台 ZooKeeper集群中的机器，每台机器不能重复，和myid的值一致。 ZXID：事务IDZXID是一个事务ID，用来唯一标识一次服务器状态的变更。在某一个时刻，集群中每台机器的ZXID值不一定全都一致，这和 ZooKeeper服务器对于客户端“更新请求”的处理逻辑有关。 Vote：投票Leader选举，顾名思义必须通过投票来实现。当集群中的机器发现自己无法检测到Leader机器的时候，就会开始尝试进行投票 Quorum：过半机器数这是整个 Leader选举算法中最重要的一个术语，我们可以把这个术语理解为是一个量词， 指的是zookKeeper集群中过半的机器数，如果集群中总的机器数是n的话，那么可以通过下面公式quorum=(n/2)+1来计算 quorum的值 算法分析进入leader选举当 ZooKeeper集群中的一台服务器出现以下两种情况之一时，就会开始进入 Leader选举。 服务器初始化启动。 服务器运行期间无法和 Leader保持连接 而当一台机器进入 Leader 选举流程时，当前集群也可能会处于以下两种状态。 集群中本来就已经存在一个 Leader 集群中确实不存在 Leader。 我们首先来看第一种已经存在 Leader I的情况。这种情况通常是集群中的某一台机器启动比较晚，在它启动之前，集群已经可以正常工作，即已经存在了一台 Leader服务器。针对这种情况，当该机器试图去选举 Leader的时候，会被告知当前服务器的 Leader信息， 对于该机器来说，仅仅需要和 Leader机器建立起连接，并进行状态同步即可。 开始第一次投票通常有两种情况会导致集群中不存在 Leader，一种情况是在整个服务器刚刚初始化启动时，此时尚未产生一台 Leader服务器；另一种情况就是在运行期间当前 Leader所在的服务器挂了。无论是哪种情况，此时集群中的所有机器都处于一种试图选举出一个Leader的状态，我们把这种状态称为“LOOKING”，意思是说正在寻找 Leader。当一台服务器处于 LOOKING状态的时候，那么它就会向集群中所有其他机器发送消息，我们称这个消息为“投票”。 在这个投票消息中包含了两个最基本的信息：所推举的服务器的SID和ZXID，分别表示了被推举服务器的唯一标识和事务ID。下文中我们将以“(SID，ZXID)”这样的形式来标识一次投票信息。 我们假设ZooKeeper由5台机器组成，SID分别为1、2、3、4和5，ZXID分别为9、9、9、8和8，并且此时SID为2的机器是 Leader服务器。某一时刻，1和2所在的机器出现故障，因此集群开始进行 Leader选举在第一次投票的时候，由于还无法检测到集群中其他机器的状态信息，因此每台机器都是将自己作为被推举的对象来进行投票。于是SID为3、4和5的机器，投票情况分别为：(3，9)、(4，8)和(5，8)。 变更投票集群中的毎台机器发出自己的投票后，也会接收到来自集群中其他机器的投票。每台机器都会根据一定的规则，来处理收到的其他机器的投票，并以此来决定是否需要变更自己的投票。这个规则也成为了整个 Leader选举算法的核心所在。为了便于描述，我们首先定义一些术语。 vote_sid：接收到的投票中所推举 Leader服务器的SID。vote_zxid：接收到的投票中所推举 Leader服务器的ZXID。self_sid：当前服务器自己的SID。self_zxid：当前服务器自己的ZXID。 每次对于收到的投票的处理，都是一个对(vote_sid，vote_zxid)和( self_sid， self_zxid) 对比的过程。 规则1：如果vote_zxid大于 self zxid，就认可当前收到的投票，并再次将该投票发送出去。 规则2：如果vote_zxid小于self_zxid，那么就坚持自己的投票，不做任何变更 规则3：如果vote_zxid等于self_zxid，那么就对比两者的SID。如果vote_sid大于self_sid，那么就认可当前接收到的投票，并再次将该投票发送出去。 规则4：如果vote_zxid等于self_zxid，并且vote_sid小于self_sid，那么同样坚持自己的投票，不做变更。 根据上面这个规则，我们结合图7-32来分析上面提到的5台机器组成的 ZooKeeper集群的投票变更过程 毎台机器都把投票发出后，同时也会接收到来自另外两台机器的投票。对于 Server3来说，它接收到了(4，8)和(5，8)两个投票，对比后，由于自己的ZXID要大于接收到的两个投票，因此不需要做任何变更。对于 Server4来说，它接收到了(3，9)和(5，8)两个投票，对比后，由于(3， 9)这个投票的ZXID大于自己，因此需要变更投票为(3，9)，然后继续将这个投票发送给另外两台机器。同样，对于 Server.5来说，它接收到了(3，9)和(4，8)两个投票，对比后，由于(3，9)这个投票的ZXID大于自己，因此需要变更投票为(3，9)，然后继续将这个投票发送给另外两台机器。 确定 Leader经过这第二次投票后，集群中的每台机器都会再次收到其他机器的投票，然后开始统计投票。如果一台机器收到了超过半数的相同的投票，那么这个投票对应的SID机器即为Leader 如图7-32所示的 Leader选举例子中，因为ZooKeeper集群的总机器数为5台，那么 1quorum=(5/2+1)=3 也就是说，只要收到3个或3个以上(含当前服务器自身在内)一致的投票即可。在这里， Server3、 Server4和 Server5都投票(3，9)，因此确定了 Server3为 Leader。 小结简单地说，通常哪台服务器上的数据越新，那么越有可能成为 Leader，原因很简单，数据越新，那么它的ZXID也就越大，也就越能够保证数据的恢复。当然，如果集群中有几个服务器具有相同的ZXID，那么SID较大的那台服务器成为 Leader Leader选举的实现细节leader选举涉及到了三个类 123QuorumPeerFastLeaderElectionQuorumCnxManager 服务器状态为了能够清楚地对 Zoo Keeper集群中每台机器的状态进行标识，在org.apache.zookeeper.server.quorum Quorumpeer.Serverstate类中列举了4种服务器状态，分别是： LOOKING、 FOLLOWING、 LEADING和 OBSERVING。 LOOKING：寻找 Leader状态。当服务器处于该状态时，它会认为当前集群中没有Leader， 因此需要进入 Leader选举流程。 FOLLOWING：跟随者状态，表明当前服务器角色是 Follower。 LEADING：领导者状态，表明当前服务器角色是 Leader OBSERVING：观察者状态，表明当前服务器角色是 Observer。 投票数据结构org.apache.zookeeper.server.quorum.Vote Quorumcnxmanager：网络I&#x2F;O在 Leader选举的过程中都会启动一个 Quorumcnxmanager，负责各台服务器之间的底层 Leader选举过程中的网络通信。 消息队列在 Quorumcnxmanager这个类内部维护了一系列的队列，用于保存接收到的、待发送的消息，以及消息的发送器。除接收队列以外，这里提到的所有队列都有一个共同点按SID分组形成队列集合，我们以发送队列为例来说明这个分组的概念。假设集群中除自身外还有4台机器，那么当前服务器就会为这4台服务器分别创建一个发送队列，互不干扰。 recvQueue：消息接收队列，用于存放那些从其他服务器接收到的消息。 queuesendMap：消息发送队列，用于保存那些待发送的消息。 queuesendMap 是一个Map，按照SID进行分组，分别为集群中的每台机器分配了一个单独队列， 从而保证各台机器之间的消息发送互不影响。 senderWorkerMap：发送器集合。每个 SendWorker消息发送器，都对应一台远程Zookeeper服务器，负责消息的发送。同样，在 senderworkerMap中，也按照SID进行了分组。 lastMessagesent：最近发送过的消息。在这个集合中，为每个SID保留最近发送过的一个消息。 建立连接为了能够进行互相投票，Zookeeper集群中的所有机器都需要两两建立起网络连接。Quorumcnxmanager在启动的时候，会创建一个Listent线程，在该线程中会创建ServerSocket来监听Leader选举的通信端口( Leader选举的通信端口默认是3888)。开启端口监听后， Zookeeper就能够不断地接收到来自其他服务器的“创建连接”请求，在接收到其他服务器的TCP连接请求时，会交由 receiveConnection函数来处理。为了避免两台机器之间重复地创建TCP连接， ZooKeeper设计了一种建立TCP连接的规则：只允许SID大的服务器主动和其他服务器建立连接，否则断开连接。在receiveConnection函数中，服务器通过对比自己和远程服务器的SID值，来判断是否接受连接请求。如果当前服务器发现自己的SID值更大，那么会断开当前连接，然后自己主动去和远程服务器建立连接。 旦建立起连接，就会根据远程服务器的SID来创建相应的消息发送器 SendWorker 和消息接收器 RecvWorker，并启动他们。 消息接收与发送消息的接收过程是由消息接收器 RecvWorker来负责的。在上面的讲解中，我们已经提到了ZooKeeper会为每个远程服务器分配一个单独的 RecvWorker，因此，每个 RecvWorker只需要不断地从这个TCP连接中读取消息，并将其保存到 recvqueue 队列中。 消息的发送过程也比较简单，由于ZooKeeper同样也已经为每个远程服务器单独分别分配了消息发送器 SendWorker，那么每个 SendWorker只需要不断地从对应的消息发送队列中获取出一个消息来发送即可，同时将这个消息放入 LastMessagesent中来作为最近发送过的消息。 Fastleaderelection：选举算法的核心部分 外部投票：特指其他服务器发来的投票。 内部投票：服务器自身当前的投票。 选举轮次：ZooKeeper服务器 Leader选举的轮次，即 logicalclock。 PK：指对内部投票和外部投票进行一个对比来确定是否需要变更内部投票。 选票管理图7-34所示是选票管理过程中相关组件之间的协作关系。 sendqueue：选票发送队列，用于保存待发送的选票。 recvqueue：选票接收队列，用于保存接收到的外部投票 WorkeRreceiver：选票接收器。该接收器会不断地从 QuorumcnxManager中获取出其他服务器发来的选举消息，并将其转换成一个选票，然后保存到recvqueue队列中去。在选票的接收过程中，如果发现该外部投票的选举轮次小于当前服务器，那么就直接忽略这个外部投票，同时立即发出自己的内部投票。当然，如果当前服务器并不是 LOOKING状态，即已经选举出了 Leader，那么也将忽略这个外部投票，同时将 Leader信息以投票的形式发送出去 另外，对于选票接收器，还有一个细节需要注意，如果接收到的消息来自 Observer 服务器，那么就忽略该消息，并将自己当前的投票发送出去。 WorkerSender：选票发送器，会不断地从 senqueue队列中获取待发送的选票， 并将其传递到底层QuorumcnxManager中去(toSend)，放入QuorumcnxManager.queuesendMap对应的队列中。 算法核心图7-35展示了 Leader选举算法实现的流程示意图。 图中展示了 Leader选举算法的基本流程，其实也就是lookForLeader方法的逻辑。当 ZooKeeper服务器检测到当前服务器状态变成 LOOKING时，就会触发 Leader 选举，即调用lookForLeader方法来进行 Leader选举。 自增选举轮次。在 Fastleaderelection实现中，有一个logicalclock属性，用于标识当前 Leader的选举轮次， Zookeeper规定了所有有效的投票都必须在同一轮次中。ZooKeeper在开始新一轮的投票时，会首先对 logicalclock进行自增操作。 初始化选票。在开始进行新一轮的投票之前，每个服务器都会首先初始化自己的选票。 发送初始化选票在完成选票的初始化后，服务器就会发起第一次投票。ZooKeeper会将刚刚初始化好的选票放入 senqueue队列中，由发送器 WorkerSender负责发送出去 接收外部投票每台服务器都会不断地从 recvqueue队列中获取外部投票。如果服务器发现无法获取到任何的外部投票，那么就会立即确认自己是否和集群中其他服务器保持着有效连接。如果发现没有建立连接，那么就会马上建立连接。如果已经建立了连接，那么就再次发送自己当前的内部投票。 判断选举轮次。当发送完初始化选票之后，接下来就要开始处理外部投票了。在处理外部投票的时侯，会根据选举轮次来进行不同的处理 外部投票的选举轮次大于内部投票。如果服务器发现自己的选举轮次已经落后于该外部投票对应服务器的选举轮次，那么就会立即更新自己的选举轮次( logicalclock)，并且清空所有已经收到的投票，然后使用初始化的投票来进行PK以确定是否变更内部投票(关于PK的逻辑会在步骤6中统一讲解)，最终再将内部投票发送出去。 外部投票的选举轮次小于内部投票。如果接收到的选票的选举轮次落后于服务器自身的，那么 ZooKeeper就会直接忽略该外部投票，不做任何处理，并返回步骤4 外部投票的选举轮次和内部投票一致这也是绝大多数投票的场景，如果外部投票的选举轮次和内部投票一致的话， 那么就开始进行选票PK。 总的来说，只有在同一个选举轮次的投票才是有效的投票。 选票PK。FastleaderElection.totalOrderPredicate方法的核心逻辑。选票PK的目的是为了确定当前服务器是否需要变更投票，主要从选举轮次、ZXID 和SID三个因素来考虑，具体条件如下：在选票PK的时候依次判断，符合任意 如果外部投票中被推举的 Leader服务器的选举轮次大于内部投票，那么就需要进行投票变更。 如果选举轮次一致的话，那么就对比两者的ZXID。如果外部投票的ZXID 大于内部投票，那么就需要进行投票变更。 如果两者的ZXID一致，那么就对比两者的SID。如果外部投票的SID大于内部投票，那么就需要进行投票变更。 变更投票通过选票PK后，如果确定了外部投票优于内部投票，那么就进行投票变更—使用外部投票的选票信息来覆盖内部投票。变更完成后，再次将这个变更后的内部投票发送出去 选票归档。无论是否进行了投票变更，都会将刚刚收到的那份外部投票放入“选票集合”recvset中进行归档。 recvset用于记录当前服务器在本轮次的 Leader选举中收到的所有外部投票一按照服务器对应的SID来区分，例如，{(1， votel)，(2， vote2)，…}。 统计投票。完成了选票归档之后，就可以开始统计投票了。统计投票的过程就是为了统计集群中是否已经有过半的服务器认可了当前的内部投票。如果确定已经有过半的服务器认可了该内部投票，则终止投票。否则返回步骤4。 更新服务器状态。统计投票后，如果已经确定可以终止投票，那么就开始更新服务器状态。服务器会首先判断当前被过半服务器认可的投票所对应的 Leader服务器是否是自己，如果是自己的话，那么就会将自己的服务器状态更新为 LEADING。如果自己不是被选举产生的 Leader的话，那么就会根据具体情况来确定自己是 FOLLOWING 或是 OBSERVING。 各服务器角色介绍LeaderLeader服务器是整个 ZooKeeper集群工作机制中的核心，其主要工作有以下两个。 事务请求的唯一调度和处理者，保证集群事务处理的顺序性。 集群内部各服务器的调度者。 LearnerHandler为了保持整个集群内部的实时通信，同时也是为了确保可以控制所有的 Follower&#x2F;Observes 服务器， Leader服务器会与毎一个 Follower&#x2F;Observer服务器都建立一个TCP长连接，同时也会为每个 Follower&#x2F;Observer服务器都创建一个名为 LearnerHandler的实体。 LearnerHandler，顾名思义，是ZooKeeper集群中 Learner服务器的管理器，主要负责 Follower&#x2F;Observer服务器和 Leader服务器之间的一系列网络通信，包括数据同步、请求转发和 Proposal提议的投票等。 Leader服务器中保存了所有 Follower&#x2F; Observer对应的 LearnerHandler 请求处理链 PrepRequestRrocessorPrepRequestRrocessor是 Leader 服务器的请求预处理器，也是 Leader服务器的第一个请求处理器。在 ZooKeeper中，我们将那些会改变服务器状态的请求称为”事务请求”一一通常指的就是那些创建节点、更新数据、删除节点以及创建会话等请求， PrepRequestRrocessor能够识别出当前客户端请求是否是事务请求。对于事务请求， PrepRequestRrocessor处理器会对其进行一系列预处理，诸如创建请求事务头、事务体、会话检査、ACL检査和版本检査等。 ProposalRequestPprocessorProposalRequestPprocessor处理器是 Leader服务器的事务投票处理器，也是 Leader 服务器事务处理流程的发起者。对于非事务请求， ProposalRequestPprocessor会直接将请求流转到 CommitProcessor处理器，不再做其他处理；而对于事务请求，除了将请求交给 CommitProcessor处理器外，还会根据请求类型创建对应的Proposal提议， 并发送给所有的 Follower服务器来发起一次集群内的事务投票。同时， ProposalRequestPprocessor还会将事务请求交付给 SyncRequestProcessor进行事务日志的记录。 SyncRequestProcessorSyncRequestProcessor是事务日志记录处理器，该处理器主要用来将事务请求记录到事务日志文件中去，同时还会触发Zookeeper进行数据快照。每进行一次事务日志记录之后， ZooKeeper都会检测当前是否需要进行数据快照理论上进行 snapCount次事务操作后就会开始数据快照，但是考虑到数据快照对于 ZooKeeper所在机器的整体性能的影响，需要尽量避免zooKeeper集群中的所有机器在同一时刻进行数据快照。因此zooKeeper在具体的实现中，并不是严格地按照这个策略执行的，而是采取“过半随机”策略，即符合如下条件就进行数据快照： AckRequestProcessorAckRequestProcessor处理器是 Leader 特有的处理器，其主要负责在SyncRequestProcessor处理器完成事务日志记录后，向Proposal的投票收集器发送ACK反馈，以通知投票收集器当前服务器已经完成了对该 Proposal的事务日志记录。（ACK收集在LearnerHandler线程上处理） CommitProcessorCommitProcessor是事务提交处理器。对于非事务请求，该处理器会直接将其交付给下一级处理器进行处理；而对于事务请求， CommitProcessor处理器会等待集群内针对 Proposal的投票直到该 Proposal可被提交。利用 CommitProcessor处理器，每个服务器都可以很好地控制对事务请求的顺序处理。 ToBeCommitProcessorToBeCommitProcessor处理器中有一个 toBeApplied队列， 专门用来存储那些已经被CommitProcessor处理过的可被提交的 ProposalToBeCommitProcessor处理器将这些请求逐个交付给 FinalRequestProcessor处理器进行处理一一等到 Finalrequestprocessor处理器处理完之后，再将其从toBeApplied队列中除。 FinalRequestProcessor是最后一个请求处理器。该处理器主要用来进行客户端请求返回之前的收尾工作，包括创建客户端请求的响应；针对事务请求，该处理器还会负责将事务应用到内存数据库中去。 Follower从角色名字上可以看出， Follower服务器是 Zookeeper集群状态的跟随者，其主要工作有以下三个。 处理客户端非事务请求，转发事务请求给 Leader服务器 参与事务请求 Proposal的投票。 参与 Leader选举投票。 请求处理链 FollowerRequestRrocessorFollowerRequestRrocessor是 Follower服务器的第一个请求处理器，其主要工作就是识别出当前请求是否是事务请求。如果是事务请求，那么 Follower就会将该事务请求转发给 Leader服务， Leader服务器在接收到这个事务请求后，就会将其提交到请求处理链，按照正常事务请求进行处理。 SendAckRequestProcessor Leader服务器上有一个叫AckRequestProcessor的请求处理器，其主要负责在 SyncRequestProcessor处理器完成事务日志记录后，向 Proposal的投票收集器进行反馈。而在 Follower服务器上，SendAckRequestProcessor处理器同样承担了事务日志记录反馈的角色，在完成事务日志记录后，会向 Leader服务器发送ACK消息以表明自身完成了事务日志的记录工作。和AckRequestProcessor唯一区别在于， AckRequestProcessor处理器和 Leader服务器在同一个服务器上，因此它的ACK反馈仅仅是一个本地操作；而 SendAckRequestProcessor 处理器由于在 Follower服务器上，因此需要通过以ACK消息的形式来向 Leader服务器进行反馈 Observer从字面意思看， 该服务器充当了一个观察者的角色一其观察 ZooKeeper集群的最新状态变化并将这些状态变更同步过来。 Observer服务器在工作原理上和 Follower基本是一致的，对于非事务请求，都可以进行独立的处理，而对于事务请求，则会转发给 Leader服务器进行处理。和 Follower唯一的区别在于， Observer不参与任何形式的投票，包括事务请求 Proposal 的投票和 Leader选举投票。简单地讲， Observer服务器只提供非事务服务，通常用于在不影响集群事务处理能力的前提下提升集群的非事务处理能力 请求处理链","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://sv.pointcut.cc/tags/zookeeper/"}]},{"title":"zookeeper客户端源码分析","slug":"zookeeper/zookeeper客户端源码分析","date":"2021-11-14T02:00:06.000Z","updated":"2022-03-23T09:32:54.127Z","comments":true,"path":"blog/zookeeper/zookeeper客户端源码分析/","link":"","permalink":"http://sv.pointcut.cc/blog/zookeeper/zookeeper%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"zookeeper有几个客户端，不过非官方的客户端都是在官方的客户端基础了做的二次开发，所以只看官方的客户端就好了。 客户端和服务端的网络部分都是使用java nio或netty，所以前置知识是要了解java nio 流程简介 执行完new ZooKeeper(connectAddr, sessionTimeout, this)后都会在创建两个线程SendThread和EventThread。接着调用某个方法后，会把请求包装成一个Pachet，放入outgoingQueue后执行了Pachet.wait方法，调用线程堵塞等待唤醒。 SendThread是IO线程，处理网络的IO，当outgoingQueue有数据后会注册OP_WRITE，接着写触发（写就绪相对有一点特殊，一般来说，你不应该注册写事件。写操作的就绪条件为底层缓冲区有空闲空间，而写缓冲区绝大部分时间都是有空闲空间的，所以当你注册写事件后，写操作一直是就绪的）。把请求发送到server后，Pachet放入到一个pendingQueue中。如果outgoingQueue没数据了，就取消注册OP_WRITE。当SendThread有可读数据时读取完数据后，如果是非watch通知，会获取pendingQueu的头Pachet，把响应数据放入该对象中，接着调用Pachet.notifyAll方法，唤醒对应的调用线程。 EventThread线程是处理watch通知的线程，详细信息看zookeeper的WatchManager实现。 总体流程 启动 方法调用 SendThread线程 EventThread线程 源码new ZooKeeper SendThread线程 可以看到，该线程会负责链接。 当链接可用时，处理I&#x2F;O操作 处理读 非watch事件通知 watch事件通知 处理写 EventThread线程 Watcher调用看zookeeper的WatchManager实现","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://sv.pointcut.cc/tags/zookeeper/"}]},{"title":"zookeeper单机源码分析","slug":"zookeeper/zookeeper单机源码分析","date":"2021-11-14T02:00:05.000Z","updated":"2022-03-23T09:32:36.681Z","comments":true,"path":"blog/zookeeper/zookeeper单机源码分析/","link":"","permalink":"http://sv.pointcut.cc/blog/zookeeper/zookeeper%E5%8D%95%E6%9C%BA%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"入口：QuorumPeerMain.main 也是用了队列的方式处理请求 总提流程 源码初始化时创建了NIOServerCnxnFactory，它通过通过configure，打开服务器套接字通道。 通过startup，打开服务器套接字通道和创建了3个线程：ZooKeeperThread（Runnable是NIOServerCnxnFactory），PrepRequestProcessor，SyncRequestProcessor。接着加载数据，开了PrepRequestProcessor，SyncRequestProcessor线程，并且使用了装饰器模式，组成了PrepRequestProcessor---&gt;SyncRequestProcessor---&gt;FinalRequestProcessor调用链，并把头PrepRequestProcessor赋值给了ZooKeeperServer.firstProcessor ZooKeeperThread线程ZooKeeperThread线程的Runnable是NIOServerCnxnFactory(如果使用java nio) 其中，ZooKeeperThread线程用来处理写和读的网络请求。 当有一个读请求时会将请求包装成对应的request放入PrepRequestProcessor.submittedRequests堵塞队列中。 处理写时 从outgoingBuffers中获取到数据然后write出去 PreprequestprocessorPrepRequestProcessor是实际处理请求的，他submittedRequests队列中拿request处理，做实际的处理并把变更记录ChangeRecord放入到outstandingChanges列表中（回滚时使用），最后把处理完后把request通过nextProcessor.processRequest(request)进入到SyncRequestProcessord的队列中 并识别出当前客户端请求是否是事务请求。对于事务请求，Preprequestprocessor处理器会生成一个zxid并对其进行一系列预处理，诸如创建请求事务头、事务体，会话检査、ACL检査和版本检査等，然后放入outstandingChanges列表中 预处理完毕后通过执行nextProcessor.processRequest(request)把请求放到了SyncRequestProcessor.queuedRequests的阻塞队列中 SyncRequestProcessorSyncrequestprocessor是事务日志记录处理器，该处理器主要用来将事务请求记录到事务日志文件中去，同时还会触发Zookeeper进行数据快照。每进行一次事务日志记录之后， ZooKeeper都会检测当前是否需要进行数据快照理论上进行 snapCount次事务操作后就会开始数据快照，但是考虑到数据快照对于 ZooKeeper所在机器的整体性能的影响，需要尽量避免zooKeeper集群中的所有机器在同一时刻进行数据快照。因此zooKeeper在具体的实现中，并不是严格地按照这个策略执行的，而是采取“过半随机”策略，即符合如下条件就进行数据快照: 符合条件后，zookeeper就开始数据快照了。 最后然后把请求交给FinalRequestProcessor处理。FinalRequestProcessor最终会把请求放入到outgoingBuffers中。 FinalRequestProcessorFinaLrequestprocessor是最后一个请求处理器。该处理器主要用来进行客户端请求返回之前的收尾工作，包括创建客户端请求的响应；针对事务请求，该处理器还会负责将事务应用到内存数据库中去；把响应对象放到outgoingBuffers队列中，让ZooKeeperThread把请求发送回客户端。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://sv.pointcut.cc/tags/zookeeper/"}]},{"title":"zookeeper分布式锁实现","slug":"zookeeper/zookeeper分布式锁实现","date":"2021-11-14T02:00:04.000Z","updated":"2022-03-23T09:32:18.896Z","comments":true,"path":"blog/zookeeper/zookeeper分布式锁实现/","link":"","permalink":"http://sv.pointcut.cc/blog/zookeeper/zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"排他锁排他锁( Exclusive Locks,简称X锁),又称为写锁或独占锁,是一种基本的锁类型。如果事务T对数据对象O加上了排他锁,那么在整个加锁期间,只允许事务T对O 进行读取和更新操作,其他任何事务都不能再对这个数据对象进行任何类型的操作一直到T释放了排他锁。 从上面讲解的排他锁的基本概念中,我们可以看到,排他锁的核心是如何保证当前有且仅有一个事务获得锁,并且锁被释放后,所有正在等待获取锁的事务都能够被通知到。 定义锁在通常的Java开发编程中,有两种常见的方式可以用来定义锁,分别是 synchronized机制和JDK5提供的 Reentrantlock。然而,在ZooKeeper中,没有类似于这样的API可以直接使用,而是通过 Zookeeper上的数据节点来表示一个锁,例如&#x2F;exclusive_lock&#x2F;lock 节点就可以被定义为一个锁,如图6-14所示。 获取锁在需要获取排他锁时,所有的客户端都会试图通过调用 create()接口,在&#x2F;exclusive_lock节点下创建临时子节点&#x2F;exclusive_lock&#x2F;lock。Zookeeper会保证在所有的客户端中,最终只有一个客户端能够创建成功,那么就可以认为该客户端获取了锁。同时,所有没有获取到锁的客户端就需要到&#x2F;exclusive_lock节点上注册一个子节点变更的 Watcher监听,以便实时监听到lock节点的变更情况。 释放锁在“定义锁”部分,我们已经提到,&#x2F;exclusive_lock&#x2F;lock是一个临时节点,因此在以下两种情况下,都有可能释放锁。 当前获取锁的客户端机器发生宕机,那么Zookeeper上的这个临时节点就会被移除。 正常执行完业务逻辑后,客户端就会主动将自己创建的临时节点删除。 无论在什么情况下移除了lock节点,ZooKeeper都会通知所有在exclusive_lock节点上注册了子节点变更 Watcher监听的客户端。这些客户端在接收到通知后,再次重新发起分布式锁获取,即重复“获取锁”过程。整个排他锁的获取和释放流程,可以用图6-15来表示。 代码1 共享锁（读写锁）定义锁和排他锁一样,同样是通过 ZooKeeper上的数据节点来表示一个锁,是一个类似于&#x2F;shared_lock&#x2F;[Hostname]-请求类型-序号”的临时顺序节点,例如&#x2F;shared_lock&#x2F;192.168.0.1-R-000000000l,那么,这个节点就代表了一个共享锁,如图6-16所示。 获取锁在需要获取共享锁时,所有客户端都会到shared_lock这个节点下面创建一个临时顺序节点,如果当前是读请求,那么就创建例如/shared_lock/192.168.0.1-R-000000000l的节点;如果是写请求,那么就创建例如/shared_lock/192.168.0.1-W-000000000l的节点。 判断读写顺序 代码实现Lock123456public interface Lock &#123; void getLock(); void unLock();&#125; AbstractLock123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public abstract class AbstractLock implements Lock, Closeable &#123; private static final String ROOT_PATH = &quot;/lock&quot;; private static final String EXCLUSIVE_LOCK_PATH = &quot;/exclusive_lock&quot;; private static final String SHARED_LOCK_PATH = &quot;/shared_lock&quot;; final ZkClient zkClient; final String path; final String prePath; private volatile boolean close = false; AbstractLock(ZkClient zkClient, boolean isSharedLock, String path) &#123; this.prePath = getLockPrefix(isSharedLock); this.path = prePath + &quot;/&quot; + path; this.zkClient = zkClient; try &#123; if (!zkClient.exists(ROOT_PATH)) &#123; zkClient.createPersistent(ROOT_PATH); &#125; if (!zkClient.exists(prePath)) &#123; zkClient.createPersistent(prePath); &#125; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; AbstractLock(boolean isSharedLock, String zookeeperUrl, String path, int timeOut) &#123; this(new ZkClient(new ZkConnection(zookeeperUrl), timeOut), isSharedLock, path); &#125; private static String getLockPrefix(boolean isSharedLock) &#123; if (isSharedLock) &#123; return ROOT_PATH + SHARED_LOCK_PATH; &#125; else &#123; return ROOT_PATH + EXCLUSIVE_LOCK_PATH; &#125; &#125; @Override public void getLock() &#123; while (true) &#123; if (close) &#123; throw new RuntimeException(&quot;客户端已经关闭&quot;); &#125; if (!tryLock()) &#123; waitLock(); &#125; else &#123; break; &#125; &#125; &#125; @Override public void close() throws IOException &#123; close = true; &#125; public String getPath() &#123; return path; &#125; abstract boolean tryLock(); abstract void waitLock();&#125; 可重入的独占锁123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113public class ZookeeperReentrantXLock extends AbstractLock &#123; private volatile Thread currentThread; private final AtomicInteger count; public ZookeeperReentrantXLock(String path) &#123; this(path, 10000); &#125; private final IZkStateListener stateListener; private final IZkDataListener dataListener; public ZookeeperReentrantXLock(String lockPatch, int timeOut) &#123; super(false, &quot;192.168.2.148:2181&quot;, lockPatch, timeOut); count = new AtomicInteger(0); ZookeeperReentrantXLock lock = this; stateListener = new StateListener(lock); dataListener = new IZkDataListener() &#123; @Override public void handleDataChange(String s, Object o) throws Exception &#123; &#125; @Override public void handleDataDeleted(String s) throws Exception &#123; synchronized (lock) &#123; lock.notifyAll(); &#125; &#125; &#125;; zkClient.subscribeStateChanges(stateListener); zkClient.subscribeDataChanges(getPath(), dataListener); &#125; @Override boolean tryLock() &#123; try &#123; if (count.compareAndSet(0, 1)) &#123; zkClient.createEphemeral(path); currentThread = Thread.currentThread(); &#125; else &#123; if (currentThread == Thread.currentThread()) &#123; count.incrementAndGet(); &#125; else &#123; return false; &#125; &#125; return true; &#125; catch (ZkNodeExistsException e) &#123; count.decrementAndGet(); return false; &#125; &#125; @Override void waitLock() &#123; synchronized (this) &#123; try &#123; if (zkClient.exists(path)) &#123; this.wait(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; @Override public void unLock() &#123; if (count.decrementAndGet() == 0) &#123; currentThread = null; zkClient.delete(path); &#125; &#125; @Override public void close() throws IOException &#123; zkClient.unsubscribeStateChanges(stateListener); zkClient.unsubscribeDataChanges(path, dataListener); super.close(); //如果考虑池化的化，就把这行代码注释掉 zkClient.close(); &#125; private static class StateListener implements IZkStateListener &#123; private final ZookeeperReentrantXLock lock; public StateListener(ZookeeperReentrantXLock lock) &#123; this.lock = lock; &#125; @Override public void handleStateChanged(Watcher.Event.KeeperState keeperState) throws Exception &#123; if (keeperState == Watcher.Event.KeeperState.SyncConnected) &#123; //由于zookeeper突然宕机，导致了线程都堵塞了，这是删除结点唤醒线程 lock.zkClient.delete(lock.path); &#125; else if (keeperState == Watcher.Event.KeeperState.Disconnected) &#123; //当zookeeper服务不可用时 synchronized (lock) &#123; lock.notifyAll(); &#125; &#125; &#125; @Override public void handleNewSession() throws Exception &#123; &#125; @Override public void handleSessionEstablishmentError(Throwable throwable) throws Exception &#123; &#125; &#125;&#125; ReadWriteLock12345678910111213141516public interface ReadWriteLock &#123; /** * Returns the lock used for reading. * * @return the lock used for reading */ Lock readLock(); /** * Returns the lock used for writing. * * @return the lock used for writing */ Lock writeLock();&#125; 共享锁ZookeeperSharedLock123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220/** * 这锁被设计成了和线程是一一对应的关系，所以new出来的对象不应该被别的线程感知到 */public class ZookeeperSharedLock implements ReadWriteLock, Closeable &#123; private ReadLock readLock; private WriteLock writeLock; //可以弄个池，在池里面分配链接对象。 private static ZkClient ZK_CLIENT = new ZkClient(new ZkConnection(&quot;192.168.2.148:2181&quot;)); private final ZkClient zkClient; private static final String WRITE_SIGN = &quot;W-&quot;; private static final String READ_SIGN = &quot;R-&quot;; private String path; public ZookeeperSharedLock(String lockPatch) &#123; this.zkClient = ZK_CLIENT; readLock = new ReadLock(this, zkClient, true, lockPatch); writeLock = new WriteLock(this, zkClient, true, lockPatch); path = readLock.path; if (!zkClient.exists(path)) &#123; try &#123; zkClient.createPersistent(path); &#125; catch (Exception e) &#123; &#125; &#125; &#125; public ZookeeperSharedLock(ZkClient zkClient, String lockPatch) &#123; this.zkClient = zkClient; readLock = new ReadLock(this, zkClient, true, lockPatch); writeLock = new WriteLock(this, zkClient, true, lockPatch); path = readLock.path; if (!zkClient.exists(path)) &#123; try &#123; zkClient.createPersistent(path); &#125; catch (Exception e) &#123; &#125; &#125; &#125; @Override public Lock readLock() &#123; return readLock; &#125; @Override public Lock writeLock() &#123; return writeLock; &#125; @Override public void close() throws IOException &#123; try &#123; zkClient.delete(path); &#125; catch (Exception e) &#123; &#125; readLock.close(); writeLock.close(); zkClient.close(); &#125; private static abstract class AbstractRWLock extends AbstractLock &#123; String beforePath;//当前请求的节点前一个节点 String currentPath;//当前请求的节点 final ZookeeperSharedLock lock; final String lock_path; final IZkDataListener dataListener; AbstractRWLock(ZookeeperSharedLock alock ,ZkClient zkClient, boolean isSharedLock, String serverLock, String operate) &#123; super(zkClient, isSharedLock, serverLock); this.lock = alock; this.lock_path = path + &quot;/&quot; + operate; dataListener = new IZkDataListener() &#123; @Override public void handleDataChange(String s, Object o) throws Exception &#123; &#125; @Override public void handleDataDeleted(String s) throws Exception &#123; synchronized (lock) &#123; lock.notifyAll(); &#125; &#125; &#125;; &#125; @Override void waitLock() &#123; zkClient.subscribeDataChanges(beforePath, dataListener); synchronized (lock) &#123; try &#123; if (zkClient.exists(beforePath)) &#123; lock.wait(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; zkClient.unsubscribeDataChanges(beforePath, dataListener); &#125; @Override public void unLock() &#123; zkClient.delete(currentPath); &#125; &#125; public static class ReadLock extends AbstractRWLock &#123; ReadLock(ZookeeperSharedLock lock, ZkClient zkClient, boolean isSharedLock, String path) &#123; super(lock, zkClient, isSharedLock, path, READ_SIGN); &#125; @Override boolean tryLock() &#123; if (currentPath == null) &#123; currentPath = zkClient.createEphemeralSequential(lock_path, &quot;1&quot;); &#125; List&lt;String&gt; children = zkClient.getChildren(path) .stream() .sorted(Comparator.comparing((s) -&gt; s.replaceAll(READ_SIGN + &quot;|&quot; + WRITE_SIGN, &quot;&quot;))) .collect(Collectors.toList()); String preWPath = null; String serialNum = currentPath.replace(path + &quot;/&quot;, &quot;&quot;); for (String s : children) &#123; if (s.equals(serialNum)) &#123; break; &#125; else &#123; if (s.contains(WRITE_SIGN)) &#123; preWPath = s; &#125; &#125; &#125; //前面没有写请求，允许获取读锁 if (preWPath == null) &#123; return true; &#125; if (beforePath == null) &#123; beforePath = path + &#x27;/&#x27; + preWPath; &#125; return false; &#125; &#125; public static class WriteLock extends AbstractRWLock &#123; WriteLock(ZookeeperSharedLock lock, ZkClient zkClient, boolean isSharedLock, String path) &#123; super(lock, zkClient, isSharedLock, path, WRITE_SIGN); &#125; @Override boolean tryLock() &#123; if (currentPath == null) &#123; currentPath = zkClient.createEphemeralSequential(lock_path, &quot;1&quot;); &#125; List&lt;String&gt; children = zkClient.getChildren(path) .stream() .sorted(Comparator.comparing((s) -&gt; s.replaceAll(READ_SIGN + &quot;|&quot; + WRITE_SIGN, &quot;&quot;))) .collect(Collectors.toList()); //前面没有读写请求，允许获取写锁 String serialNum = currentPath.substring(path.length() + 1); if (children.get(0).equals(serialNum)) &#123; return true; &#125; else &#123; if (beforePath == null) &#123; int i = indexOf(children, serialNum.replaceAll(READ_SIGN + &quot;|&quot; + WRITE_SIGN, &quot;&quot;), (s)-&gt;s.replaceAll(READ_SIGN + &quot;|&quot; + WRITE_SIGN, &quot;&quot;)); beforePath = path + &#x27;/&#x27; + children.get(i - 1); &#125; &#125; return false; &#125; private int indexOf(List&lt;String&gt; list, String target, Function&lt;String, String&gt; handleListStr) &#123; return indexOf(list, target, handleListStr, 0, list.size() - 1); &#125; private static int indexOf(List&lt;String&gt; list, String target, Function&lt;String, String&gt; handleListStr, int start, int end) &#123; int mid = (start + end) / 2; String s = handleListStr != null ? handleListStr.apply(list.get(mid)) : list.get(mid); int c = s.compareTo(target); if (c==0) &#123; return mid; &#125; else if (c &lt; 0) &#123; start = mid + 1; &#125; else &#123; end = mid - 1; &#125; if (start &gt;= list.size() || end &lt;0) &#123; return -1; &#125; return indexOf(list, target, handleListStr, start, end); &#125; public static void main(String[] args) throws InterruptedException &#123; CountDownLatch latch = new CountDownLatch(1); ZK_CLIENT.createPersistent(&quot;/lock/shared_lock/ffeff&quot;); ZK_CLIENT.subscribeDataChanges(&quot;/lock/shared_lock/ffeff&quot;, new IZkDataListener() &#123; @Override public void handleDataChange(String s, Object o) throws Exception &#123; &#125; @Override public void handleDataDeleted(String s) throws Exception &#123; System.out.println(&quot;被删除&quot;); System.out.println(s); &#125; &#125;); ZK_CLIENT.delete(&quot;/lock/shared_lock/ffeff&quot;); latch.await(); &#125; &#125;&#125; ZookeeperSharedLock2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232/** * 这锁被设计成了和线程是一一对应的关系，所以new出来的对象不应该被别的线程感知到 * * 感觉这样的实现性能会不如上一个版本，因为多了ConcurrentHashMap和AtomicInteger的操作 * 设计思路上没有改变，好处是在这种情况下 * X1 * S2 * S3 * S4 * X5 * 这种情况下，当X1写锁被删除后zookeeper只需要唤醒S2、S3、S4就好了。 * 而上一个版本中，由于全部都共享一把锁，所以也会把X5唤醒，但是这时后X5唤醒后它马上会堵塞的。 * */public class ZookeeperSharedLock2 implements ReadWriteLock, Closeable &#123; private ReadLock readLock; private WriteLock writeLock; //可以弄个池，在池里面分配链接对象。 private static ZkClient ZK_CLIENT = new ZkClient(new ZkConnection(&quot;192.168.2.148:2181&quot;)); private final ZkClient zkClient; private static final String WRITE_SIGN = &quot;W-&quot;; private static final String READ_SIGN = &quot;R-&quot;; private static ConcurrentHashMap&lt;String, AtomicInteger&gt; localLockMap = new ConcurrentHashMap&lt;&gt;(); private String path; public ZookeeperSharedLock2(String lockPatch) &#123; this.zkClient = ZK_CLIENT; readLock = new ReadLock(this, zkClient, true, lockPatch); writeLock = new WriteLock(this, zkClient, true, lockPatch); path = readLock.path; if (!zkClient.exists(path)) &#123; try &#123; zkClient.createPersistent(path); &#125; catch (Exception e) &#123; &#125; &#125; &#125; public ZookeeperSharedLock2(ZkClient zkClient, String lockPatch) &#123; this.zkClient = zkClient; readLock = new ReadLock(this, zkClient, true, lockPatch); writeLock = new WriteLock(this, zkClient, true, lockPatch); path = readLock.path; if (!zkClient.exists(path)) &#123; try &#123; zkClient.createPersistent(path); &#125; catch (Exception e) &#123; &#125; &#125; &#125; public Map&lt;String, AtomicInteger&gt; getMap() &#123; return localLockMap; &#125; @Override public Lock readLock() &#123; return readLock; &#125; @Override public Lock writeLock() &#123; return writeLock; &#125; @Override public void close() throws IOException &#123; try &#123; zkClient.delete(path); &#125; catch (Exception e) &#123; &#125; zkClient.close(); &#125; private static abstract class AbstractRWLock extends AbstractLock &#123; String beforePath;//当前请求的节点前一个节点 String currentPath;//当前请求的节点 final ZookeeperSharedLock2 lock; final String lock_path; final static IZkDataListener dataListener = new IZkDataListener() &#123; @Override public void handleDataChange(String s, Object o) throws Exception &#123; &#125; @Override public void handleDataDeleted(String s) throws Exception &#123; AtomicInteger lockKey = localLockMap.get(s); if (lockKey != null) &#123; synchronized (lockKey) &#123; lockKey.notifyAll(); &#125; &#125; &#125; &#125;; AbstractRWLock(ZookeeperSharedLock2 alock , ZkClient zkClient, boolean isSharedLock, String serverLock, String operate) &#123; super(zkClient, isSharedLock, serverLock); this.lock = alock; this.lock_path = path + &quot;/&quot; + operate; &#125; @Override void waitLock() &#123; AtomicInteger count = localLockMap.computeIfAbsent(beforePath, k -&gt; new AtomicInteger(0)); count.incrementAndGet(); zkClient.subscribeDataChanges(beforePath, dataListener); synchronized (count) &#123; try &#123; if (zkClient.exists(beforePath)) &#123; count.wait(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if (count.decrementAndGet() == 0) &#123; zkClient.unsubscribeDataChanges(beforePath, dataListener); &#125; &#125; @Override public void unLock() &#123; zkClient.delete(currentPath); if (beforePath != null) &#123; AtomicInteger count = localLockMap.get(beforePath); if (count != null &amp;&amp; count.get() == 0) &#123; localLockMap.remove(beforePath); &#125; &#125; &#125; &#125; public static class ReadLock extends AbstractRWLock &#123; ReadLock(ZookeeperSharedLock2 lock, ZkClient zkClient, boolean isSharedLock, String path) &#123; super(lock, zkClient, isSharedLock, path, READ_SIGN); &#125; @Override boolean tryLock() &#123; if (currentPath == null) &#123; currentPath = zkClient.createEphemeralSequential(lock_path, &quot;1&quot;); &#125; List&lt;String&gt; children = zkClient.getChildren(path) .stream() .sorted(Comparator.comparing((s) -&gt; s.replaceAll(READ_SIGN + &quot;|&quot; + WRITE_SIGN, &quot;&quot;))) .collect(Collectors.toList()); String preWPath = null; String serialNum = currentPath.replace(path + &quot;/&quot;, &quot;&quot;); for (String s : children) &#123; if (s.equals(serialNum)) &#123; break; &#125; else &#123; if (s.contains(WRITE_SIGN)) &#123; preWPath = s; &#125; &#125; &#125; //前面没有写请求，允许获取读锁 if (preWPath == null) &#123; return true; &#125; if (beforePath == null) &#123; beforePath = path + &#x27;/&#x27; + preWPath; &#125; return false; &#125; &#125; public static class WriteLock extends AbstractRWLock &#123; WriteLock(ZookeeperSharedLock2 lock, ZkClient zkClient, boolean isSharedLock, String path) &#123; super(lock, zkClient, isSharedLock, path, WRITE_SIGN); &#125; @Override boolean tryLock() &#123; if (currentPath == null) &#123; currentPath = zkClient.createEphemeralSequential(lock_path, &quot;1&quot;); &#125; List&lt;String&gt; children = zkClient.getChildren(path) .stream() .sorted(Comparator.comparing((s) -&gt; s.replaceAll(READ_SIGN + &quot;|&quot; + WRITE_SIGN, &quot;&quot;))) .collect(Collectors.toList()); //前面没有写请求，允许获取读锁 String serialNum = currentPath.substring(path.length() + 1); if (children.size() == 0 || children.get(0).equals(serialNum)) &#123; return true; &#125; else &#123; if (beforePath == null) &#123; int i = indexOf(children, serialNum.replaceAll(READ_SIGN + &quot;|&quot; + WRITE_SIGN, &quot;&quot;), (s)-&gt;s.replaceAll(READ_SIGN + &quot;|&quot; + WRITE_SIGN, &quot;&quot;)); beforePath = path + &#x27;/&#x27; + children.get(i - 1); &#125; &#125; return false; &#125; private int indexOf(List&lt;String&gt; list, String target, Function&lt;String, String&gt; handleListStr) &#123; return indexOf(list, target, handleListStr, 0, list.size() - 1); &#125; private static int indexOf(List&lt;String&gt; list, String target, Function&lt;String, String&gt; handleListStr, int start, int end) &#123; int mid = (start + end) / 2; String s = handleListStr != null ? handleListStr.apply(list.get(mid)) : list.get(mid); int c = s.compareTo(target); if (c==0) &#123; return mid; &#125; else if (c &lt; 0) &#123; start = mid + 1; &#125; else &#123; end = mid - 1; &#125; if (start &gt;= list.size() || end &lt;0) &#123; return -1; &#125; return indexOf(list, target, handleListStr, start, end); &#125; &#125; public static void main(String[] args) &#123; AtomicInteger c = new AtomicInteger(10); System.out.println(c.decrementAndGet()); &#125;&#125; 测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109public class Test &#123; @org.junit.Test public void testXlock() throws InterruptedException, IOException &#123; final int[] max_num = &#123;2000&#125;; ThreadPoolExecutor executor = new ThreadPoolExecutor(10, 10, 100, TimeUnit.MINUTES, new LinkedBlockingQueue&lt;Runnable&gt;()); CountDownLatch countDownLatch = new CountDownLatch(max_num[0]); int max = max_num[0]; long start = System.currentTimeMillis(); ZookeeperReentrantXLock lock = new ZookeeperReentrantXLock(&quot;test_l_03&quot;); for (int i=0; i&lt;max; i++) &#123; executor.execute(() -&gt; &#123; try &#123; lock.getLock(); lock.getLock(); max_num[0]--; if (max_num[0] % 10 == 0) &#123; System.out.println( max_num[0]); &#125; lock.unLock(); &#125; finally &#123; countDownLatch.countDown(); lock.unLock(); &#125; &#125;); &#125; countDownLatch.await(); lock.close(); System.out.println(&quot;结束，耗时：&quot; + (System.currentTimeMillis() - start)); System.out.println(max_num[0]); executor.shutdownNow(); &#125; @org.junit.Test public void testSharedLock() throws InterruptedException, IOException &#123; final int[] max_num = &#123;1000&#125;; ThreadPoolExecutor executor = new ThreadPoolExecutor(4, 4, 100, TimeUnit.MINUTES, new LinkedBlockingQueue&lt;Runnable&gt;()); CountDownLatch countDownLatch = new CountDownLatch(max_num[0]); int max = max_num[0]; for (int i=0; i&lt;max; i++) &#123; executor.execute(() -&gt; &#123; try &#123; ZookeeperSharedLock lock = new ZookeeperSharedLock(&quot;test_x_l&quot;); Lock readLock = lock.readLock(); Lock writeLock = lock.writeLock(); readLock.getLock(); if (max_num[0] % 10 == 0) &#123; System.out.println( max_num[0]); &#125; readLock.unLock(); writeLock.getLock(); max_num[0]--; writeLock.unLock(); &#125; finally &#123; countDownLatch.countDown(); &#125; &#125;); &#125; countDownLatch.await(); ZookeeperSharedLock lock = new ZookeeperSharedLock(&quot;test_x_l&quot;); lock.close(); System.out.println(max_num[0]); executor.shutdownNow(); &#125; @org.junit.Test public void testSharedLock2() throws InterruptedException, IOException &#123; final int[] max_num = &#123;1000&#125;; ThreadPoolExecutor executor = new ThreadPoolExecutor(4, 4, 100, TimeUnit.MINUTES, new LinkedBlockingQueue&lt;Runnable&gt;()); CountDownLatch countDownLatch = new CountDownLatch(max_num[0]); int max = max_num[0]; for (int i=0; i&lt;max; i++) &#123; executor.execute(() -&gt; &#123; ZookeeperSharedLock2 lock = new ZookeeperSharedLock2(&quot;test_x_l2&quot;); try &#123; Lock readLock = lock.readLock(); Lock writeLock = lock.writeLock(); readLock.getLock(); if (max_num[0] % 10 == 0) &#123; System.out.println( max_num[0]); &#125; readLock.unLock(); writeLock.getLock(); max_num[0]--; writeLock.unLock(); &#125; finally &#123; countDownLatch.countDown(); &#125; &#125;); &#125; countDownLatch.await(); ZookeeperSharedLock2 lock = new ZookeeperSharedLock2(&quot;test_x_l2&quot;); System.out.println(lock.getMap().size()); lock.close(); System.out.println(max_num[0]); executor.shutdownNow(); &#125; public static void main(String[] args) throws IOException, InterruptedException &#123; Test t = new Test(); t.testSharedLock2();// t.testXlock();// ZkClient zkClient = new ZkClient(new ZkConnection(&quot;192.168.2.148:2181&quot;));// zkClient.deleteRecursive(&quot;/lock/shared_lock/test_x_l2&quot;); &#125;&#125; Curator客户端中锁的使用获取Curator客户端12345678910111213141516171819202122232425262728public class CuratorUtil &#123; private static String connectStr = &quot;192.168.2.148:2181&quot;; private static TestingServer server = null; static &#123; try &#123; server = new TestingServer(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; private static class InnerClass &#123; private static CuratorFramework client = CuratorFrameworkFactory .builder() .connectString(connectStr) .sessionTimeoutMs(5000) .retryPolicy(new ExponentialBackoffRetry(1000,3)) .build(); &#125; public static CuratorFramework getInstance() &#123; return InnerClass.client; &#125;&#125; 互斥锁123456789101112public class Test &#123; public static void main(String[] args) throws Exception &#123; CuratorFramework client = CuratorUtil.getInstance(); client.start(); InterProcessMutex mutex = new InterProcessMutex(client,&quot;/curator/lock&quot;); mutex.acquire(); System.out.println(&quot;&quot;); mutex.release(); client.close(); &#125;&#125; 读写锁public class Test {1234567891011public class Test &#123; public static void main(String[] args) throws Exception &#123; CuratorFramework client = CuratorUtil.getInstance(); client.start(); InterProcessReadWriteLock mutex = new InterProcessReadWriteLock(client,&quot;/curator/lock&quot;); InterProcessMutex readLock = mutex.readLock(); InterProcessMutex writeLock = mutex.writeLock(); client.close(); &#125;&#125;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://sv.pointcut.cc/tags/zookeeper/"}]},{"title":"zookeeper分布式Id","slug":"zookeeper/zookeeper分布式Id","date":"2021-11-14T02:00:03.000Z","updated":"2022-03-23T09:32:14.482Z","comments":true,"path":"blog/zookeeper/zookeeper分布式Id/","link":"","permalink":"http://sv.pointcut.cc/blog/zookeeper/zookeeper%E5%88%86%E5%B8%83%E5%BC%8FId/","excerpt":"","text":"现在基本都使用redis实现了，或者用雪花算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class Id &#123; private static String connectStr = &quot;192.168.67.139:2184&quot;; private String root = &quot;/ID&quot;; private String nodeName = &quot;/order&quot;; private ZkClient zkClient = null; private ExecutorService delExector = null; public Id() &#123; init(); &#125; private void init() &#123; zkClient = new ZkClient(connectStr, 5000); delExector = Executors.newFixedThreadPool(10); if (!zkClient.exists(root)) &#123; zkClient.createPersistent(root); &#125; &#125; private String getId() &#123; String nodepath = root.concat(nodeName); String ephemeralSequential = zkClient.createEphemeralSequential(nodepath, &quot;&quot;); System.out.println(ephemeralSequential); delExector.execute(() -&gt; zkClient.delete(ephemeralSequential)); return getId(ephemeralSequential); &#125; private void stop() &#123; delExector.shutdown(); try &#123; delExector.awaitTermination(2, TimeUnit.SECONDS); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if(zkClient != null) &#123; zkClient.close(); &#125; &#125; private String getId(String str) &#123; int index = str.lastIndexOf(nodeName); return str.substring(index + nodeName.length()); &#125; public static void main(String[] args) &#123; Id id = new Id(); System.out.println(id.getId()); id.stop(); &#125;&#125;","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://sv.pointcut.cc/tags/zookeeper/"}]},{"title":"ZAB协议详情","slug":"zookeeper/ZAB协议详情","date":"2021-11-14T02:00:02.000Z","updated":"2022-03-23T09:32:14.471Z","comments":true,"path":"blog/zookeeper/ZAB协议详情/","link":"","permalink":"http://sv.pointcut.cc/blog/zookeeper/ZAB%E5%8D%8F%E8%AE%AE%E8%AF%A6%E6%83%85/","excerpt":"","text":"什么是Zab协议？Zab协议 的全称是 Zookeeper Atomic Broadcast （Zookeeper原子广播）。Zookeeper 是通过 Zab 协议来保证分布式事务的最终一致性。 Zab协议是为分布式协调服务Zookeeper专门设计的一种 支持崩溃恢复 的 原子广播协议 ，是Zookeeper保证数据一致性的核心算法。Zab借鉴了Paxos算法，但又不像Paxos那样，是一种通用的分布式一致性算法。它是特别为Zookeeper设计的支持崩溃恢复的原子广播协议。 在Zookeeper中主要依赖Zab协议来实现数据一致性，基于该协议，zk实现了一种主备模型（即Leader和Follower模型）的系统架构来保证集群中各个副本之间数据的一致性。这里的主备系统架构模型，就是指只有一台客户端（Leader）负责处理外部的写事务请求，然后Leader客户端将数据同步到其他Follower节点。 Zookeeper 客户端会随机的链接到 zookeeper 集群中的一个节点，如果是读请求，就直接从当前节点中读取数据；如果是写请求，那么节点就会向 Leader 提交事务，Leader 接收到事务提交，会广播该事务，只要超过半数节点写入成功，该事务就会被提交。 Zab 协议的特性： Zab 协议需要确保那些已经在 Leader 服务器上提交（Commit）的事务最终被所有的服务器提交。 Zab 协议需要确保丢弃那些只在 Leader 上被提出而没有被提交的事务。 Zab 协议实现的作用 使用一个单一的主进程（Leader）来接收并处理客户端的事务请求（也就是写请求），并采用了Zab的原子广播协议，将服务器数据的状态变更以 事务proposal （事务提议）的形式广播到所有的副本（Follower）进程上去。 保证一个全局的变更序列被顺序引用。Zookeeper是一个树形结构，很多操作都要先检查才能确定是否可以执行，比如P1的事务t1可能是创建节点”&#x2F;a”，t2可能是创建节点”&#x2F;a&#x2F;bb”，只有先创建了父节点”&#x2F;a”，才能创建子节点”&#x2F;a&#x2F;b”。 为了保证这一点，Zab要保证同一个Leader发起的事务要按顺序被apply，同时还要保证只有先前Leader的事务被apply之后，新选举出来的Leader才能再次发起事务。 当主进程出现异常的时候，整个zk集群依旧能正常工作。 Zab协议原理Zab协议要求每个 Leader 都要经历三个阶段：发现，同步，广播。 发现：要求zookeeper集群必须选举出一个 Leader 进程，同时 Leader 会维护一个 Follower 可用客户端列表。将来客户端可以和这些 Follower节点进行通信。 同步：Leader 要负责将本身的数据与 Follower 完成同步，做到多副本存储。这样也是提现了CAP中的高可用和分区容错。Follower将队列中未处理完的请求消费完成后，写入本地事务日志中。 广播：Leader 可以接受客户端新的事务Proposal请求，将新的Proposal请求广播给所有的 Follower。 Zab协议核心Zab协议的核心：定义了事务请求的处理方式 所有的事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被叫做 Leader服务器。其他剩余的服务器则是 Follower服务器。 Leader服务器 负责将一个客户端事务请求，转换成一个 事务Proposal，并将该 Proposal 分发给集群中所有的 Follower 服务器，也就是向所有 Follower 节点发送数据广播请求（或数据复制） 分发之后Leader服务器需要等待所有Follower服务器的反馈（Ack请求），在Zab协议中，只要超过半数的Follower服务器进行了正确的反馈后（也就是收到半数以上的Follower的Ack请求），那么 Leader 就会再次向所有的 Follower服务器发送 Commit 消息，要求其将上一个 事务proposal 进行提交。 ​ 事务请求处理 Zab协议内容Zab 协议包括两种基本的模式：崩溃恢复 和 消息广播 协议过程： 当整个集群启动过程中，或者当 Leader 服务器出现网络中弄断、崩溃退出或重启等异常时，Zab协议就会 进入崩溃恢复模式，选举产生新的Leader。 当选举产生了新的 Leader，同时集群中有过半的机器与该 Leader 服务器完成了状态同步（即数据同步）之后，Zab协议就会退出崩溃恢复模式，进入消息广播模式。 这时，如果有一台遵守Zab协议的服务器加入集群，因为此时集群中已经存在一个Leader服务器在广播消息，那么该新加入的服务器自动进入恢复模式：找到Leader服务器，并且完成数据同步。同步完成后，作为新的Follower一起参与到消息广播流程中。 协议状态切换： 当Leader出现崩溃退出或者机器重启，亦或是集群中不存在超过半数的服务器与Leader保存正常通信，Zab就会再一次进入崩溃恢复，发起新一轮Leader选举并实现数据同步。同步完成后又会进入消息广播模式，接收事务请求。 保证消息有序： 在整个消息广播中，Leader会将每一个事务请求转换成对应的 proposal 来进行广播，并且在广播 事务Proposal 之前，Leader服务器会首先为这个事务Proposal分配一个全局单递增的唯一ID，称之为事务ID（即zxid），由于Zab协议需要保证每一个消息的严格的顺序关系，因此必须将每一个proposal按照其zxid的先后顺序进行排序和处理。 消息广播 在zookeeper集群中，数据副本的传递策略就是采用消息广播模式。zookeeper中农数据副本的同步方式与二段提交相似，但是却又不同。二段提交要求协调者必须等到所有的参与者全部反馈ACK确认消息后，再发送commit消息。要求所有的参与者要么全部成功，要么全部失败。二段提交会产生严重的阻塞问题。 Zab协议中 Leader 等待 Follower 的ACK反馈消息是指“只要半数以上的Follower成功反馈即可，不需要收到全部Follower反馈” ​ 消息广播流程图 消息广播具体步骤: 客户端发起一个写操作请求。 Leader 服务器将客户端的请求转化为事务 Proposal 提案，同时为每个 Proposal 分配一个全局的ID，即zxid。 Leader 服务器为每个 Follower 服务器分配一个单独的队列，然后将需要广播的 Proposal 依次放到队列中取，并且根据 FIFO 策略进行消息发送。 Follower 接收到 Proposal 后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向 Leader 反馈一个 Ack 响应消息。 Leader 接收到超过半数以上 Follower 的 Ack 响应消息后，即认为消息发送成功，可以发送 commit 消息。 Leader 向所有 Follower 广播 commit 消息，同时自身也会完成事务提交。Follower 接收到 commit 消息后，会将上一条事务提交。 zookeeper 采用 Zab 协议的核心，就是只要有一台服务器提交了 Proposal，就要确保所有的服务器最终都能正确提交 Proposal。这也是 CAP&#x2F;BASE 实现最终一致性的一个体现。 Leader 服务器与每一个 Follower 服务器之间都维护了一个单独的 FIFO 消息队列进行收发消息，使用队列消息可以做到异步解耦。 Leader 和 Follower 之间只需要往队列中发消息即可。如果使用同步的方式会引起阻塞，性能要下降很多。 崩溃恢复一旦 Leader 服务器出现崩溃或者由于网络原因导致 Leader 服务器失去了与过半 Follower 的联系，那么就会进入崩溃恢复模式。 在 Zab 协议中，为了保证程序的正确运行，整个恢复过程结束后需要选举出一个新的 Leader 服务器。因此 Zab 协议需要一个高效且可靠的 Leader 选举算法，从而确保能够快速选举出新的 Leader 。 Leader 选举算法不仅仅需要让 Leader 自己知道自己已经被选举为 Leader ，同时还需要让集群中的所有其他机器也能够快速感知到选举产生的新 Leader 服务器。 崩溃恢复主要包括两部分：Leader选举 和 数据恢复 Zab 协议如何保证数据一致性 假设两种异常情况： 一个事务在 Leader 上提交了，并且过半的 Folower 都响应 Ack 了，但是 Leader 在 Commit 消息发出之前挂了。 假设一个事务在 Leader 提出之后，Leader 挂了。 要确保如果发生上述两种情况，数据还能保持一致性，那么 Zab 协议选举算法必须满足以下要求： Zab 协议崩溃恢复要求满足以下两个要求： 确保已经被 Leader 提交的 Proposal 必须最终被所有的 Follower 服务器提交。 确保丢弃已经被 Leader 提出的但是没有被提交的 Proposal。 根据上述要求 ，Zab协议需要保证选举出来的Leader需要满足以下条件： 新选举出来的 Leader 不能包含未提交的 Proposal 。 即新选举的 Leader 必须都是已经提交了 Proposal 的 Follower 服务器节点。 新选举的 Leader 节点中含有最大的 zxid 。这样做的好处是可以避免 Leader 服务器检查 Proposal 的提交和丢弃工作。 Zab 如何数据同步： 完成 Leader 选举后（新的 Leader 具有最高的zxid），在正式开始工作之前（接收事务请求，然后提出新的 Proposal），Leader 服务器会首先确认事务日志中的所有的 Proposal 是否已经被集群中过半的服务器 Commit。 Leader 服务器需要确保所有的 Follower 服务器能够接收到每一条事务的 Proposal ，并且能将所有已经提交的事务 Proposal 应用到内存数据中。等到 Follower 将所有尚未同步的事务 Proposal 都从 Leader 服务器上同步过啦并且应用到内存数据中以后，Leader 才会把该 Follower 加入到真正可用的 Follower 列表中。 Zab 数据同步过程中，如何处理需要丢弃的 Proposal： 在 Zab 的事务编号 zxid 设计中，zxid是一个64位的数字。 其中低32位可以看成一个简单的单增计数器，针对客户端每一个事务请求，Leader 在产生新的 Proposal 事务时，都会对该计数器加1。而高32位则代表了 Leader 周期的 epoch 编号。 epoch 编号可以理解为当前集群所处的年代，或者周期。每次Leader变更之后都会在 epoch 的基础上加1，这样旧的 Leader 崩溃恢复之后，其他Follower 也不会听它的了，因为 Follower 只服从epoch最高的 Leader 命令。 每当选举产生一个新的 Leader ，就会从这个 Leader 服务器上取出本地事务日志充最大编号 Proposal 的 zxid，并从 zxid 中解析得到对应的 epoch 编号，然后再对其加1，之后该编号就作为新的 epoch 值，并将低32位数字归零，由0开始重新生成zxid。 Zab 协议通过 epoch 编号来区分 Leader 变化周期，能够有效避免不同的 Leader 错误的使用了相同的 zxid 编号提出了不一样的 Proposal 的异常情况。 基于以上策略 ，当一个包含了上一个 Leader 周期中尚未提交过的事务 Proposal 的服务器启动时，当这台机器加入集群中，以 Follower 角色连上 Leader 服务器后，Leader 服务器会根据自己服务器上最后提交的 Proposal 来和 Follower 服务器的 Proposal 进行比对，比对的结果肯定是 Leader 要求 Follower 进行一个回退操作，回退到一个确实已经被集群中过半机器 Commit 的最新 Proposal。 实现原理Zab 节点有三种状态： Following：当前节点是跟随者，服从 Leader 节点的命令。 Leading：当前节点是 Leader，负责协调事务。 Election&#x2F;Looking：节点处于选举状态，正在寻找 Leader。 代码实现中，多了一种状态：Observing 状态 ，这是 Zookeeper 引入 Observer 之后加入的，Observer 不参与选举，是只读节点，跟 Zab 协议没有关系。 节点的持久状态： history：当前节点接收到事务 Proposal 的Log acceptedEpoch：Follower 已经接受的 Leader 更改 epoch 的 newEpoch 提议。 currentEpoch：当前所处的 Leader 年代 lastZxid：history 中最近接收到的Proposal 的 zxid（最大zxid） Zab 的四个阶段： 选举阶段（Leader Election）节点在一开始都处于选举节点，只要有一个节点得到超过半数节点的票数，它就可以当选准 Leader，只有到达第三个阶段（也就是同步阶段），这个准 Leader 才会成为真正的 Leader。 Zookeeper 规定所有有效的投票都必须在同一个 轮次 中，每个服务器在开始新一轮投票时，都会对自己维护的 logicalClock 进行自增操作。 每个服务器在广播自己的选票前，会将自己的投票箱（recvset）清空。该投票箱记录了所收到的选票。例如：Server_2 投票给 Server_3，Server_3 投票给 Server_1，则Server_1的投票箱为(2,3)、(3,1)、(1,1)。（每个服务器都会默认给自己投票） 前一个数字表示投票者，后一个数字表示被选举者。票箱中只会记录每一个投票者的最后一次投票记录，如果投票者更新自己的选票，则其他服务器收到该新选票后会在自己的票箱中更新该服务器的选票。 这一阶段的目的就是为了选出一个准 Leader ，然后进入下一个阶段。协议并没有规定详细的选举算法，后面会提到实现中使用的 Fast Leader Election。 ​ 选举流程 发现阶段（Descovery）在这个阶段，Followers 和上一轮选举出的准 Leader 进行通信，同步 Followers 最近接收的事务 Proposal。 这个阶段的主要目的是发现当前大多数节点接收的最新 Proposal，并且准 Leader 生成新的 epoch ，让****Followers 接收，更新它们的 acceptedEpoch。 ​ 发现流程 同步阶段（Synchronization）同步阶段主要是利用 Leader 前一阶段获得的最新 Proposal 历史，同步集群中所有的副本。只有当 quorum（超过半数的节点） 都同步完成，准 Leader 才会成为真正的 Leader。Follower 只会接收 zxid 比自己 lastZxid 大的 Proposal。 ​ 同步流程 广播阶段（Broadcast）到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 Leader 可以进行消息广播。同时，如果有新的节点加入，还需要对新节点进行同步。需要注意的是，Zab 提交事务并不像 2PC 一样需要全部 Follower 都 Ack，只需要得到 quorum（超过半数的节点）的Ack 就可以。 ​ 广播流程 协议实现协议的 Java 版本实现跟上面的定义略有不同，选举阶段使用的是 Fast Leader Election（FLE），它包含了步骤1的发现指责。因为FLE会选举拥有最新提议的历史节点作为 Leader，这样就省去了发现最新提议的步骤。 实际的实现将发现和同步阶段合并为 Recovery Phase（恢复阶段），所以，Zab 的实现实际上有三个阶段。 Zab协议三个阶段： 选举（Fast Leader Election） 恢复（Recovery Phase） 广播（Broadcast Phase） Fast Leader Election（快速选举）前面提到的 FLE 会选举拥有最新Proposal history （lastZxid最大）的节点作为 Leader，这样就省去了发现最新提议的步骤。这是基于拥有最新提议的节点也拥有最新的提交记录 成为 Leader 的条件： 选 epoch 最大的 若 epoch 相等，选 zxid 最大的 若 epoch 和 zxid 相等，选择 server_id 最大的（zoo.cfg中的myid） 节点在选举开始时，都默认投票给自己，当接收其他节点的选票时，会根据上面的 Leader条件 判断并且更改自己的选票，然后重新发送选票给其他节点。当有一个节点的得票超过半数，该节点会设置自己的状态为 Leading ，其他节点会设置自己的状态为 Following。 ​ 选举过程 Recovery Phase（恢复阶段）这一阶段 Follower 发送他们的 lastZxid 给 Leader，Leader 根据 lastZxid 决定如何同步数据。这里的实现跟前面的 Phase 2 有所不同：Follower 收到 TRUNC 指令会终止 L.lastCommitedZxid 之后的 Proposal ，收到 DIFF 指令会接收新的 Proposal。 history.lastCommitedZxid：最近被提交的 Proposal zxidhistory.oldThreshold：被认为已经太旧的已经提交的 Proposal zxid ​ 恢复阶段 什么情况下zab协议会进入崩溃恢复模式？ 1、当服务器启动时 2、当leader 服务器出现网络中断，崩溃或者重启的情况 3、当集群中已经不存在过半的服务器与Leader服务器保持正常通信。 zab协议进入崩溃恢复模式会做什么？1、当leader出现问题，zab协议进入崩溃恢复模式，并且选举出新的leader。当新的leader选举出来以后，如果集群中已经有过半机器完成了leader服务器的状态同（数据同步），退出崩溃恢复，进入消息广播模式。 2、当新的机器加入到集群中的时候，如果已经存在leader服务器，那么新加入的服务器就会自觉进入崩溃恢复模式，找到leader进行数据同步。 特殊情况下需要解决的两个问题：问题一：已经被处理的事务请求（proposal）不能丢（commit的） 当 leader 收到合法数量 follower 的 ACKs 后，就向各个 follower 广播 COMMIT 命令，同时也会在本地执行 COMMIT 并向连接的客户端返回「成功」。但是如果在各个 follower 在收到 COMMIT 命令前 leader 就挂了，导致剩下的服务器并没有执行都这条消息。 如何解决 已经被处理的事务请求（proposal）不能丢（commit的） 呢？ 1、选举拥有 proposal 最大值（即 zxid 最大） 的节点作为新的 leader。 由于所有提案被 COMMIT 之前必须有合法数量的 follower ACK，即必须有合法数量的服务器的事务日志上有该提案的 proposal，因此，zxid最大也就是数据最新的节点保存了所有被 COMMIT 消息的 proposal 状态。 2、新的 leader 将自己事务日志中 proposal 但未 COMMIT 的消息处理。 3、新的 leader 与 follower 建立先进先出的队列， 先将自身有而 follower 没有的 proposal 发送给 follower，再将这些 proposal 的 COMMIT 命令发送给 follower，以保证所有的 follower 都保存了所有的 proposal、所有的 follower 都处理了所有的消息。通过以上策略，能保证已经被处理的消息不会丢。 问题二：没被处理的事务请求（proposal）不能再次出现什么时候会出现事务请求被丢失呢？ 当 leader 接收到消息请求生成 proposal 后就挂了，其他 follower 并没有收到此 proposal，因此经过恢复模式重新选了 leader 后，这条消息是被跳过的。 此时，之前挂了的 leader 重新启动并注册成了 follower，他保留了被跳过消息的 proposal 状态，与整个系统的状态是不一致的，需要将其删除。 如果解决呢？ Zab 通过巧妙的设计 zxid 来实现这一目的。 一个 zxid 是64位，高 32 是纪元（epoch）编号，每经过一次 leader 选举产生一个新的 leader，新 leader 会将 epoch 号 +1。低 32 位是消息计数器，每接收到一条消息这个值 +1，新 leader 选举后这个值重置为 0。 这样设计的好处是旧的 leader 挂了后重启，它不会被选举为 leader，因为此时它的 zxid 肯定小于当前的新 leader。当旧的 leader 作为 follower 接入新的 leader 后，新的 leader 会让它将所有的拥有旧的 epoch 号的未被 COMMIT 的 proposal 清除。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://sv.pointcut.cc/tags/zookeeper/"}]},{"title":"01-zookeeper设计","slug":"zookeeper/01-zookeeper设计","date":"2021-11-14T02:00:01.000Z","updated":"2022-03-23T09:32:05.682Z","comments":true,"path":"blog/zookeeper/01-zookeeper设计/","link":"","permalink":"http://sv.pointcut.cc/blog/zookeeper/01-zookeeper%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"该文档不记录权限，这不好记，而且感觉也很少使用，所以只记录一些核心的东西 Zookeeper能做什么 数据发布订阅 负载均衡 命名服务（全局的id） Master选举 集群管理（集群的注册与发现） 配置管理 分布式队列 分布式锁 ZooKeeper可以保证如下分布式一致性特性顺序一致性从同一个客户端发起的事务请求，最终将会严格地按照其发起顺序被应用到Zookeeper中去。 原子性所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群所有机器都成功应用了某一个事务，要么都没有应用，一定不会出现集群中部分机器应用了该事务，而另外一部分没有应用的情况。 单一视图(SingleSystemImage)无论客户端连接的是哪个ZooKeeper服务器，其看到的服务端数据模型都是一致的。 可靠性一旦服务端成功地应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会被一直保留下来，除非有另一个事务又对其进行了变更。 实时性通常人们看到实时性的第一反应是，一且一个事务被成功应用，那么客户端能够立即从服务端上读取到这个事务变更后的最新数据状态。这里需要注意的是，ZooKeeper仅仅保证在一定的时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。 Zookeeper的设计目标目标一：简单的数据模型Zookeeper使得分布式程序能够通过一个共享的、树型结构的名字空间来进行相互协调。这里所说的材型结构的名字空间，是指 Zookeeper服务器内存中的一个数据模型，其由系列被称为 Znode的数据节点组成，总的来说，其数据模型类似于一个文件系统，而Znode之间的层级关系，就像文件系统的目录结构一样。不过和传统的磁盘文件系统不同的是，Zo0 Keeper将全量数据存储在内存中，以此来实现提高服务器吞吐、减少延退的目的。关于Zo0 Keeper I的数据模型，将会在7.1.1节中做详细阐述。 目标二：可以构建集群一个Zookeeper集群通常由一组机器组成，一般3~5台机器就可以组成一个可用的Zookeeper集群了，如图4-1所示。组成Zookeeper集群的每台机器都会在内存中维护当前的服务器状态，并且每台机器之间都互相保持着通信。值得一提的是，只要集群中存在超过一半的机器能够正常工作，那么整个集群就能够正常对外服务。 ZooKeeper的客户端程序会选择和集群中任意一台机器共同来创建一个TCP连接，而一旦客户端和某台ZooKeeper服务器之间的连接断开后，客户端会自动连接到集群中的其他机器。 目标三：顺序访问对于来自客户端的每个更新请求，ZooKeeper都会分配一个全局唯一的递增编号（ZXID），这个编号反映了所有事务操作的先后顺序，应用程序可以使用ZooKeeper的这个特性来实现更高层次的同步原语。 目标四：高性能由于ZooKeeper将全量数据存储在内存中，并直接服务于客户端的所有非事务请求，因此它尤其适用于以读操作为主的应用场景。以3台3.4.3版本的ZooKeeper服务器组成集群进行性能压测，100%读请求的场景下压测结果是12~13W的QPS。 ZooKeeper的基本概念集群角色在ZooKeeper中，引入了Leader、Follower和Observer三种角色。ZooKeeper集群中的所有机器通过一个Leader选举过程来选定一台被称为“Leader”的机器，Leader服务器为客户端提供读和写服务。除Leader外，其他机器包括Follower和Observer。Follower和Observer都能够提供读服务，唯一的区别在于，Observer机器不参与Leader选举过程，也不参与写操作的“过半写成功”策略，因此Observer可以在不影响写性能的情况下提升集群的读性能。当有事务请求发给了Follower或Observer，这个请求会重定向到leader机器。 会话( session)Session是指客户端会话，在讲解会话之前，我们首先来了解一下客户端连接。在ZooKeeper中，一个客户端连接是指客户端和服务器之间的一个TCP长连接。Zo0Ke°per对外的服务端口默认是2181，客户端启动的时候，首先会与服务器建立一个TCP连接，从第一次连接建立开始，客户端会话的生命周期也开始了，通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向Zookeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watch事件通知。Session的sessiontimeout值用来设置一个容户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。 数据节点(Znode，数据模型)Zookeeper将所有数据存储在内存中，数据模型是一棵树(ZnodeTree)，由斜杠(&#x2F;)进行分割的路径，就是一个Znode，例如&#x2F;oo&#x2F;pathi。每个Znode上都会保存自己的数据内容,同时还会保存一系列属性信息。 在ZooKeeper中，Znode可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个Znode被创建了，除非主动进行Znode的移除操作，否则这个Znode将一直保存在ZooKeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。另外，Zookeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL（序号）。一旦节点被标记上这个属性，那么在这个节点被创建的时候，ZooKeeper会自动在其节点名后面追加上一个整型数字这个整型数字是一个由父节点维护的自增数字。 Watcher（事件监听器）ZooKeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是ZooKeeper实现分布式协调服务的重要特性。 zookeeper集群及配置集群启动启动前提：安装了对应的jdk版本，并且配置了JAVA_HOME环境变量在每个zookeeper配置文件中添加修改${zookeeper目录}&#x2F;conf&#x2F;zoo.cfg文件， 123server.0=192.168.212.154:2888:3888server.1=192.168.212.156:2888:3888server.2=192.168.212.157:2888:3888 启动${zookeeper目录}&#x2F;bin&#x2F;zkServer.sh start 配置 zookeeper服务器和客户端常用命令使用服务器启动ZK服务: sh bin&#x2F;zkServer.sh startZK服务状态: sh bin&#x2F;zkServer.sh status停止ZK服务: sh bin&#x2F;zkServer.sh stop重启ZK服务: sh bin&#x2F;zkServer.sh restart 客户端查看目录&#x2F;文件：ls [-s] [-w] [-R] path查看目录&#x2F;文件详细信息：ls2 path创建文件，并设置初始内容：create [-s] [-e] [-c] [-t ttl] path [data] [acl]-s：是否带序号-e：临时节点data：valueacl：权限获取文件内容(value)： get path修改文件内容： set path data “对 zk 所关联的字符串进行设置。例如set &#x2F; “data。删除文件： delete &#x2F;zk 将刚才创建的 znode 删除，如果存在子节点删除失败 递归删除：rmr &#x2F;zk将刚才创建的 znode 删除，子节点同时删除退出客户端： quit 帮助命令： help java客户端就不说了 ZooKeeper的ZAB（ZooKeeper原子消息广播协议）协议（重点）ZAB的核心所有事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被称为Leader服务器，而余下的其他服务器则成为Follower服务器。Leader服务器负责将一个客户端事务请求转換成一个事务Proposal(提议)，并将该Proposal分发给集群中所有的Follower服务器。之后Leader服务器需要等待所有Follower服务器的反馈，一旦超过半数的Follower服务器进行了正确的反馈后，那么Leader就会再次向所有的Follower服务器分发Commit消息，要求其将前一个Proposal进行提交。 ZAB详细讲解ZAB协议包括两种基本的模式，分别是崩溃恢复和消息广播。当整个服务框架在启动过程中，或是当Leader服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB协议就会进入恢复模式并选举产生新的Leader服务器。当选举产生了新的Leader服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致。 当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。当一台同样遵守ZAB协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么新加入的服务器就会自觉地进入数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。正如上文介绍中所说的，ZooKeeper设计成只允许唯一的一个Leader服务器来进行事务请求的处理。Leader服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议；而如果集群中的其他机器接收到客户端的事务请求，那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。 当Leader服务器出现崩溃退出或机器重启，亦或是集群中已经不存在过半的服务器与该Leader服务器保持正常通信时，那么在重新开始新一轮的原子广播事务操作之前，所有进程首先会使用崩溃恢复协议来使彼此达到一个一致的状态，于是整个ZAB流程就会从消息广播模式进入到崩溃恢复模式。一个机器要成为新的Leader，必须获得过半进程的支持，同时由于每个进程都有可能会崩溃，因此，在ZAB协议运行过程中，前后会出现多个Leader，并且每个进程也有可能会多次成为Leader。进入崩溃恢复模式后，只要集群中存在过半的服务器能够彼此进行正常通信，那么就可以产生一个新的Leader并再次进入消息广播模式。举个例子来说，一个由3台机器组成的ZAB服务，通常由1个Leader、2个Follower服务器组成。某个时刻，假如其中一个Follower服务器挂了，整个ZAB集群是不会中断服务的，这是因为Leader服务器依然能够获得过半机器(包括Leader自己)的支持（公式：Math.cell(节点个数&#x2F;2) Math.cell：向上取整）。 消息广播ZAB协议的消息广播过程使用的是一个原子广播协议，类似于一个二阶段提交过程。针对客户端的事务请求，Leader服务器会为其生成对应的事务Proposal，并将其发送给集群中其余所有的机器，然后再分别收集各自的选票，最后进行事务提交，如图4-2所示就是ZAB协议消息广播流程的示意图。Zookeeper简化的二阶段提交模型下(不需要全部机器都响应)，是无法处理Leader服务器崩溃退出而带来的数据不一致问题的，因此在ZAB协议中添加了另一个模式，即采用崩溃恢复模式（选举leader）来解决这个问题。另外，整个消息广播协议是基于具有FIFO特性的TCP协议来进行网络通信的（在Leader中，所有事务请求放到一个队列中，由一个线程循环拿值），因此能够很容易地保证消息广播过程中消息接收与发送的顺序性。 在整个消息广播过程中，Leader服务器会为每个事务请求生成对应的Proposal来进行厂播并分配一个事务ID(即ZXID)。由于ZAB协议需要保证毎个消息严格的因果关系，因此必须将每一个事务Proposal按照其ZXID的先后顺序来进行排序与处理。 在消息广播过程中，Leader服务器会为每一个Follower服务器都各自分配一个单独的队列，然后将需要广播的事务Proposal依次放入这些队列中去，并且根据FIFO策略进行消息发送。毎一个Follower服务器在接收到这个事务Proposal之后（内存数据库中），都会首先将其以事务日志的形式写入到本地磁盘中去，并且在成功写入后反馈给Leader服务器一个Ack响应。当Leader服务器接收到超过半数Follower的Ack响应后，就会广播一个Commit消息给所有的Follower服务器以通知其进行事务提交，同时Leader自身也会完成对事务的提交（源码是leader先提交后发送commit消息），而每一个Follower服务器在接收到Commit消息后，也会完成对事务的提交。 崩溃恢复在开头已经说的很详细了，这里只说一点，每次选举其实都会选择zxid最大的机器，如果相同就会选择最大的服务器ID（启动时分配的ID）。这样做是为了丢失最少的数据（甚至不丢失数据） 数据同步用来保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致。 正常情况下Leader服务器会为每一个Follower服务器都准备一个队列，并将那些没有被各Follower服务器同步的事务（数据）以Proposal消息的形式逐个发送给Follower服务器，并在每一个Proposal消息后面紧接着再发送一个Commit消息，以表示该事务已经被提交。等到Follower服务器将所有其尚未同步的事务Proposal都从Leader服务器上同步过来并成功应用到本地数据库中后，Leader服务器就会将该Follower服务器加入到真正的可用Follower列表中，并开始之后的其他流程。 需要同步的数据的依据是ZXID，在zab就使用了zxid作为没一个事务的id（详情）。 基于zxid的生成策略，当一个包含了上一个Leader周期中尚未提交过的事务Proposal的服务器启动时，其肯定无法成为Leader，原因很简单，因为当前集群中一定包含一个Quorum集合，该集合中的机器一定包含了更高epoch的事务Proposal（zxid更大），因此这台机器的事务Proposal肯定不是最高，也就无法成为Leader了。当这台机器加入到集群中，以Follower角色连接上Leader服务器之后，Leader服务器会根据自己服务器上最后被提交的Proposal来和FollowerI服务器的Proposal进行比对，比对的结果当然是Leader会要求Follower进行一个回退操作一一回退到一个确实已经被集群中过半机器提交的最新的事务Proposal（这也就是说可能会丢失数据的原因）。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://sv.pointcut.cc/tags/zookeeper/"}]}],"categories":[{"name":"微服务","slug":"微服务","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"开源框架","slug":"微服务/开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"},{"name":"开源框架","slug":"开源框架","permalink":"http://sv.pointcut.cc/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"},{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/categories/java/"},{"name":"中间件","slug":"中间件","permalink":"http://sv.pointcut.cc/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://sv.pointcut.cc/tags/springcloud/"},{"name":"springboot","slug":"springboot","permalink":"http://sv.pointcut.cc/tags/springboot/"},{"name":"spring","slug":"spring","permalink":"http://sv.pointcut.cc/tags/spring/"},{"name":"netty","slug":"netty","permalink":"http://sv.pointcut.cc/tags/netty/"},{"name":"javaNIO","slug":"javaNIO","permalink":"http://sv.pointcut.cc/tags/javaNIO/"},{"name":"网络编程","slug":"网络编程","permalink":"http://sv.pointcut.cc/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"jvm","slug":"jvm","permalink":"http://sv.pointcut.cc/tags/jvm/"},{"name":"juc","slug":"juc","permalink":"http://sv.pointcut.cc/tags/juc/"},{"name":"mybatis","slug":"mybatis","permalink":"http://sv.pointcut.cc/tags/mybatis/"},{"name":"java","slug":"java","permalink":"http://sv.pointcut.cc/tags/java/"},{"name":"nginx","slug":"nginx","permalink":"http://sv.pointcut.cc/tags/nginx/"},{"name":"redis","slug":"redis","permalink":"http://sv.pointcut.cc/tags/redis/"},{"name":"dubbo","slug":"dubbo","permalink":"http://sv.pointcut.cc/tags/dubbo/"},{"name":"zookeeper","slug":"zookeeper","permalink":"http://sv.pointcut.cc/tags/zookeeper/"}]}